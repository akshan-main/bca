\section{Evaluation}\label{sec:evaluation}

% NOTE: This section contains two parts:
% 1. Infrastructure metrics (recall, budget adherence) — measured, real numbers
% 2. End-to-end pass@1 — requires running benchmark.py with an LLM API key
%
% Run: python -m paper.experiments.benchmark --tasks-file paper/experiments/eval_tasks.jsonl
% to generate pass@1 results and fill in Section 5.3.

We evaluate BCA along two axes: (1) infrastructure metrics (budget adherence,
symbol recall, assembly latency) that characterize the retrieval stage in
isolation, and (2) end-to-end pass@1 on mutation-based bug-fixing tasks that
measure whether the assembled context helps an LLM produce correct patches.

% --------------------------------------------------------------------------
\subsection{Setup}\label{sec:eval-setup}

\paragraph{Repository.}
We evaluate on the \textsc{CeGraph} codebase itself: 926 symbols across 58
files connected by 2507 edges, with 164 passing unit tests.

\paragraph{Infrastructure tasks (retrieval).}
We define 12 coding tasks with manually annotated ground-truth symbols (3--5
per task).  Tasks range from natural-language bug descriptions to specific
symbol references.

\paragraph{End-to-end tasks (pass@1).}
We define 11 mutation-based bug-fixing tasks.  Each task introduces a known
bug via a code mutation that causes a specific test to fail.  The benchmark
harness (\texttt{benchmark.py}) assembles context from the \emph{buggy}
codebase, sends it to an LLM, applies the LLM's patch to a sandboxed copy,
and runs the targeted test.  A task is ``passed'' if the test passes after
applying the patch.  Three tasks are \emph{cross-file}: the mutation is in
one file but the fix requires understanding a dependency in another file.

\paragraph{Methods.}
\begin{enumerate}
  \item \textbf{Full-file grep}: files containing task keywords, truncated.
  \item \textbf{BM25}: symbol-level BM25 scoring, greedy packing.
  \item \textbf{Vector (TF-IDF)}: TF-IDF cosine similarity over symbol text.
  \item \textbf{Embedding}: OpenAI \texttt{text-embedding-3-small} cosine similarity over symbol text.
  \item \textbf{Repo map}: structural summary plus relevant file content.
  \item \textbf{BCA (ours)}: full pipeline with dependency closure.
  \item \textbf{BCA $-$closure}: BCA without dependency closure constraint.
\end{enumerate}

\paragraph{Budgets.}
$B \in \{1000, 4000, 8000, 10000\}$ tokens.

% --------------------------------------------------------------------------
\subsection{Infrastructure Metrics}\label{sec:infra-results}

\input{experiments/tables}

\paragraph{Budget adherence.}
BCA never violates the token budget (0/12 tasks at all budget levels),
maintaining 84--88\% utilization.  Unweighted BFS (equivalently, BCA
$-$closure) violates the budget on 11--12 of 12 tasks, exceeding by 6--11\%.
Dependency closure is the mechanism that prevents this: it accounts for the
token cost of required base classes and parent types during selection.

\paragraph{Recall.}
BM25 achieves the highest keyword recall (0.60--0.74) because it directly
matches task keywords against symbol text.  BCA achieves lower recall
(0.16--0.23) due to graph-based expansion prioritizing structural proximity
over lexical similarity.  This gap motivates the end-to-end evaluation: recall
does not measure whether context is \emph{sufficient} for an LLM to produce a
correct fix.

\paragraph{Ablation: dependency closure is essential.}
Disabling dependency closure causes budget violations on nearly every task
(11--12 of 12).  No other component has this effect.  Submodular coverage,
centrality, and kind weights do not measurably affect recall, though they
influence context quality (ordering, diversity) in ways the recall metric does
not capture.

% --------------------------------------------------------------------------
\subsection{End-to-End Results}\label{sec:e2e-results}

% Generated by: python -m paper.experiments.benchmark
%   --tasks-file paper/experiments/eval_tasks.jsonl
%   --budgets 1000,4000,8000,10000
%   --methods grep,bm25,vector,embedding,repo_map,bca,bca_no_closure
%   --provider openai --model gpt-4o

Table~\ref{tab:pass1} shows pass@1 across 11 mutation tasks, 4 budget levels,
and 7 methods (308 LLM calls total).

% TODO: Re-run benchmark and fill in updated numbers after fixing:
%   - repo_map: now includes actual source code (not just signatures)
%   - grep: path resolution fallback for LLM dropping src/ prefix
%   - BCA: lean rendering (no metadata overhead), Phase 7b budget enforcement
%   - retry: 8s/20s/42s/50s delays for rate limiting
%   - edit parsing: flexible FILE: marker detection
\begin{table}[h]
\centering
\caption{Pass@1 by method and token budget.}\label{tab:pass1}
\begin{tabular}{lcccc|c}
\toprule
\textbf{Method} & $B{=}1000$ & $B{=}4000$ & $B{=}8000$ & $B{=}10000$ & \textbf{Avg} \\
\midrule
BCA (ours)       & --- & --- & --- & --- & --- \\
Embedding        & --- & --- & --- & --- & --- \\
Vector (TF-IDF)  & --- & --- & --- & --- & --- \\
BM25             & --- & --- & --- & --- & --- \\
BCA $-$closure   & --- & --- & --- & --- & --- \\
Repo map         & --- & --- & --- & --- & --- \\
Grep             & --- & --- & --- & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

% TODO: Update analysis paragraphs after re-running benchmark with all fixes.
% Previous run had grep=0.00 and repo_map=0.00 due to harness bugs (now fixed).
% Key fixes: lean rendering removes 22-45% metadata overhead from BCA context,
% repo_map now includes actual source code, grep has path fallback resolution,
% Phase 7b enforces budget post-source-loading, retry delays handle rate limits.

\paragraph{Analysis.}
Results will be updated after re-running the benchmark with all harness fixes
applied.  The 11 mutation tasks cover: partial match regression in symbol lookup,
edge type confusion in caller and callee analysis, state accumulation in graph
building, node ID format mismatch, enum value corruption, language detection
breakage, configuration default changes, iteration limit inflation,
cross-file enum rename, impact analysis return value corruption, and default
provider parameter changes.

% --------------------------------------------------------------------------
\subsection{Limitations}\label{sec:limitations}

\paragraph{Single-repository evaluation.}
Results are from a single medium-sized Python codebase.  Multi-language and
larger repositories may produce different results.

\paragraph{Mutation-based tasks.}
Mutation testing introduces synthetic bugs.  Real bugs from issue trackers
(e.g., SWE-bench) would provide stronger evidence.

\paragraph{Self-evaluation.}
Ground-truth annotations and mutations are authored by the system developers.
External annotation would reduce bias.
