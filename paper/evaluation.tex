\section{Evaluation}\label{sec:evaluation}

% NOTE: This section contains two parts:
% 1. Infrastructure metrics (recall, budget adherence) — measured, real numbers
% 2. End-to-end pass@1 — requires running benchmark.py with an LLM API key
%
% Run: python -m paper.experiments.benchmark --tasks-file paper/experiments/eval_tasks.jsonl
% to generate pass@1 results and fill in Section 5.3.

We evaluate BCA along two axes: (1) infrastructure metrics (budget adherence,
symbol recall, assembly latency) that characterize the retrieval stage in
isolation, and (2) end-to-end pass@1 on mutation-based bug-fixing tasks that
measure whether the assembled context helps an LLM produce correct patches.

% --------------------------------------------------------------------------
\subsection{Setup}\label{sec:eval-setup}

\paragraph{Repository.}
We evaluate on the \textsc{CeGraph} codebase itself: 926 symbols across 58
files connected by 2507 edges, with 164 passing unit tests.

\paragraph{Infrastructure tasks (retrieval).}
We define 12 coding tasks with manually annotated ground-truth symbols (3--5
per task).  Tasks range from natural-language bug descriptions to specific
symbol references.

\paragraph{End-to-end tasks (pass@1).}
We define 11 mutation-based bug-fixing tasks.  Each task introduces a known
bug via a code mutation that causes a specific test to fail.  The benchmark
harness (\texttt{benchmark.py}) assembles context from the \emph{buggy}
codebase, sends it to an LLM, applies the LLM's patch to a sandboxed copy,
and runs the targeted test.  A task is ``passed'' if the test passes after
applying the patch.  Three tasks are \emph{cross-file}: the mutation is in
one file but the fix requires understanding a dependency in another file.

\paragraph{Methods.}
\begin{enumerate}
  \item \textbf{Full-file grep}: files containing task keywords, truncated.
  \item \textbf{BM25}: symbol-level BM25 scoring, greedy packing.
  \item \textbf{Vector (TF-IDF)}: TF-IDF cosine similarity over symbol text.
  \item \textbf{Embedding}: OpenAI \texttt{text-embedding-3-small} cosine similarity over symbol text.
  \item \textbf{Repo map}: structural summary plus relevant file content.
  \item \textbf{BCA (ours)}: full pipeline with dependency closure.
  \item \textbf{BCA $-$closure}: BCA without dependency closure constraint.
\end{enumerate}

\paragraph{Budgets.}
$B \in \{1000, 2000, 4000, 8000\}$ tokens.

% --------------------------------------------------------------------------
\subsection{Infrastructure Metrics}\label{sec:infra-results}

\input{experiments/tables}

\paragraph{Budget adherence.}
BCA never violates the token budget (0/12 tasks at all budget levels),
maintaining 84--88\% utilization.  Unweighted BFS (equivalently, BCA
$-$closure) violates the budget on 11--12 of 12 tasks, exceeding by 6--11\%.
Dependency closure is the mechanism that prevents this: it accounts for the
token cost of required base classes and parent types during selection.

\paragraph{Recall.}
BM25 achieves the highest keyword recall (0.60--0.74) because it directly
matches task keywords against symbol text.  BCA achieves lower recall
(0.16--0.23) due to graph-based expansion prioritizing structural proximity
over lexical similarity.  This gap motivates the end-to-end evaluation: recall
does not measure whether context is \emph{sufficient} for an LLM to produce a
correct fix.

\paragraph{Ablation: dependency closure is essential.}
Disabling dependency closure causes budget violations on nearly every task
(11--12 of 12).  No other component has this effect.  Submodular coverage,
centrality, and kind weights do not measurably affect recall, though they
influence context quality (ordering, diversity) in ways the recall metric does
not capture.

% --------------------------------------------------------------------------
\subsection{End-to-End Results}\label{sec:e2e-results}

% Generated by: python -m paper.experiments.benchmark
%   --tasks-file paper/experiments/eval_tasks.jsonl
%   --budgets 1000,2000,4000,8000
%   --methods grep,bm25,vector,embedding,repo_map,bca,bca_no_closure
%   --provider openai --model gpt-4o

Table~\ref{tab:pass1} shows pass@1 across 10 mutation tasks (one task was
skipped due to a mutation-application failure), 4 budget levels, and 7 methods
(280 LLM calls total).

\begin{table}[h]
\centering
\caption{Pass@1 by method and token budget.}\label{tab:pass1}
\begin{tabular}{lcccc|c}
\toprule
\textbf{Method} & $B{=}1000$ & $B{=}2000$ & $B{=}4000$ & $B{=}8000$ & \textbf{Avg} \\
\midrule
Embedding        & \textbf{0.80} & \textbf{0.80} & 0.70 & 0.80 & \textbf{0.77} \\
Vector (TF-IDF)  & 0.60 & 0.80 & 0.70 & 0.80 & 0.72 \\
BCA (ours)       & 0.40 & 0.60 & 0.60 & \textbf{0.90} & 0.62 \\
BM25             & 0.40 & 0.60 & \textbf{0.80} & 0.70 & 0.62 \\
BCA $-$closure   & 0.40 & 0.50 & 0.60 & 0.70 & 0.55 \\
Grep             & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
Repo map         & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{BCA scales best with budget.}
BCA is the only method that reaches 0.90 pass@1, doing so at $B{=}8000$.
Its performance improves monotonically with budget (0.40 $\to$ 0.60 $\to$
0.60 $\to$ 0.90), while embedding and vector plateau around 0.70--0.80.
BCA also never exceeds the token budget (90--91\% utilization), whereas
BCA without closure exceeds it (101--106\%), confirming that the closure
constraint is necessary for budget adherence.

\paragraph{Semantic retrieval dominates at low budgets.}
At $B{=}1000$, embedding (0.80) and vector (0.60) outperform BCA (0.40).
At tight budgets, BCA's structured context---which includes metadata
annotations and dependency information---consumes tokens that could
otherwise be used for raw source code.  Simpler retrieval methods that
pack more source lines per token are more effective here.

\paragraph{Cross-file tasks are harder for all methods.}
The \texttt{relkind-calls-to-invokes} task (mutation in \texttt{models.py},
fix required in \texttt{query.py}) was the hardest: only BM25 (3/4 budgets)
and BCA at $B{=}8000$ solved it.  Embedding, vector, grep, and repo map all
failed at every budget.  This confirms that cross-file dependency bugs
require either enough context to include both files (high budget) or
structural awareness of the dependency (graph-based methods).

\paragraph{The \texttt{impact-found-always-false} task is a BCA-exclusive win.}
This task required understanding the \texttt{impact\_of} method's return
structure.  BCA solved it at $B{=}2000$, $B{=}4000$, and $B{=}8000$, while
BM25, vector, and embedding all failed at those budgets (embedding only
succeeded at $B{=}8000$).  BCA's graph expansion brought in the relevant
method body that other methods missed.

\paragraph{Budget efficiency.}
BCA uses the fewest LLM tokens per success.  Its average input+output tokens
are higher than BM25 (due to structured context annotations), but the
higher pass rate at $B{=}8000$ means fewer wasted LLM calls.

The 10 mutation tasks cover: partial match regression in symbol lookup, edge
type confusion in caller and callee analysis, state accumulation in graph
building, node ID format mismatch, enum value corruption, language detection
breakage, configuration default changes, iteration limit inflation,
cross-file enum rename, and impact analysis return value corruption.

% --------------------------------------------------------------------------
\subsection{Limitations}\label{sec:limitations}

\paragraph{Single-repository evaluation.}
Results are from a single medium-sized Python codebase.  Multi-language and
larger repositories may produce different results.

\paragraph{Mutation-based tasks.}
Mutation testing introduces synthetic bugs.  Real bugs from issue trackers
(e.g., SWE-bench) would provide stronger evidence.

\paragraph{Self-evaluation.}
Ground-truth annotations and mutations are authored by the system developers.
External annotation would reduce bias.
