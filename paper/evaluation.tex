\section{Evaluation}\label{sec:evaluation}

% NOTE: This section contains two parts:
% 1. Infrastructure metrics (recall, budget adherence) — measured, real numbers
% 2. End-to-end pass@1 — requires running benchmark.py with an LLM API key
%
% Run: python -m paper.experiments.benchmark --tasks-file paper/experiments/eval_tasks.jsonl
% to generate pass@1 results and fill in Section 5.3.

We evaluate BCA along two axes: (1) infrastructure metrics (budget adherence,
symbol recall, assembly latency) that characterize the retrieval stage in
isolation, and (2) end-to-end pass@1 on mutation-based bug-fixing tasks that
measure whether the assembled context helps an LLM produce correct patches.

% --------------------------------------------------------------------------
\subsection{Setup}\label{sec:eval-setup}

\paragraph{Repository.}
We evaluate on the \textsc{CeGraph} codebase itself: 926 symbols across 58
files connected by 2507 edges, with 164 passing unit tests.

\paragraph{Infrastructure tasks (retrieval).}
We define 12 coding tasks with manually annotated ground-truth symbols (3--5
per task).  Tasks range from natural-language bug descriptions to specific
symbol references.

\paragraph{End-to-end tasks (pass@1).}
We define 11 mutation-based bug-fixing tasks.  Each task introduces a known
bug via a code mutation that causes a specific test to fail.  The benchmark
harness (\texttt{benchmark.py}) assembles context from the \emph{buggy}
codebase, sends it to an LLM, applies the LLM's patch to a sandboxed copy,
and runs the targeted test.  A task is ``passed'' if the test passes after
applying the patch.  Three tasks are \emph{cross-file}: the mutation is in
one file but the fix requires understanding a dependency in another file.

\paragraph{Methods.}
\begin{enumerate}
  \item \textbf{Full-file grep}: files containing task keywords, truncated.
  \item \textbf{BM25}: symbol-level BM25 scoring, greedy packing.
  \item \textbf{Vector (TF-IDF)}: TF-IDF cosine similarity over symbol text.
  \item \textbf{Embedding}: OpenAI \texttt{text-embedding-3-small} cosine similarity over symbol text.
  \item \textbf{Repo map}: structural summary plus relevant file content.
  \item \textbf{BCA (ours)}: full pipeline with dependency closure.
  \item \textbf{BCA $-$closure}: BCA without dependency closure constraint.
\end{enumerate}

\paragraph{Budgets.}
$B \in \{1000, 2000, 4000, 8000\}$ tokens.

% --------------------------------------------------------------------------
\subsection{Infrastructure Metrics}\label{sec:infra-results}

\input{experiments/tables}

\paragraph{Budget adherence.}
BCA never violates the token budget (0/12 tasks at all budget levels),
maintaining 84--88\% utilization.  Unweighted BFS (equivalently, BCA
$-$closure) violates the budget on 11--12 of 12 tasks, exceeding by 6--11\%.
Dependency closure is the mechanism that prevents this: it accounts for the
token cost of required base classes and parent types during selection.

\paragraph{Recall.}
BM25 achieves the highest keyword recall (0.60--0.74) because it directly
matches task keywords against symbol text.  BCA achieves lower recall
(0.16--0.23) due to graph-based expansion prioritizing structural proximity
over lexical similarity.  This gap motivates the end-to-end evaluation: recall
does not measure whether context is \emph{sufficient} for an LLM to produce a
correct fix.

\paragraph{Ablation: dependency closure is essential.}
Disabling dependency closure causes budget violations on nearly every task
(11--12 of 12).  No other component has this effect.  Submodular coverage,
centrality, and kind weights do not measurably affect recall, though they
influence context quality (ordering, diversity) in ways the recall metric does
not capture.

% --------------------------------------------------------------------------
\subsection{End-to-End Results}\label{sec:e2e-results}

% TODO: Fill with results from:
%   python -m paper.experiments.benchmark \
%     --tasks-file paper/experiments/eval_tasks.jsonl \
%     --provider anthropic --model claude-sonnet-4-5-20250929 \
%     --output-dir paper/results/

The end-to-end benchmark harness is implemented and ready to run.  It requires
an LLM API key and produces:

\begin{itemize}
  \item \textbf{Pass@1 vs.\ budget}: fraction of mutation tasks where the LLM
    produces a correct patch, per method and budget level.
  \item \textbf{Tokens per success}: total LLM tokens (input + output) for
    each successful fix.
  \item \textbf{Per-task artifacts}: assembled context, LLM response, extracted
    patch, and test output for every run.
\end{itemize}

The 8 mutation tasks cover: partial match regression in symbol lookup, edge
type confusion in caller analysis, state accumulation in graph building,
node ID format mismatch, enum value corruption, language detection breakage,
configuration default changes, and iteration limit inflation.

% --------------------------------------------------------------------------
\subsection{Limitations}\label{sec:limitations}

\paragraph{Single-repository evaluation.}
Results are from a single medium-sized Python codebase.  Multi-language and
larger repositories may produce different results.

\paragraph{Mutation-based tasks.}
Mutation testing introduces synthetic bugs.  Real bugs from issue trackers
(e.g., SWE-bench) would provide stronger evidence.

\paragraph{Self-evaluation.}
Ground-truth annotations and mutations are authored by the system developers.
External annotation would reduce bias.
