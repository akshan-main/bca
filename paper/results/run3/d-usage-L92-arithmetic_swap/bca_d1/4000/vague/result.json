{
  "task_id": "d-usage-L92-arithmetic_swap",
  "method": "bca_d1",
  "budget": 4000,
  "query_type": "vague",
  "tokens_used": 3941,
  "symbols_selected": 18,
  "files_included": 12,
  "assembly_time_ms": 167.8,
  "llm_time_ms": 1257.3,
  "llm_input_tokens": 3722,
  "llm_output_tokens": 56,
  "tests_passed": false,
  "test_output": "yncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))",
  "patch": "FILE: pydantic_evals/pydantic_evals/dataset.py\nSEARCH:\n        incremented_value = current_value + amount\nREPLACE:\n        incremented_value = current_value + (amount * 1.0)  # Ensure float calculation\n",
  "error": "",
  "test_time_ms": 1189.2,
  "failure_mode": "test_fail",
  "target_file_hit": true,
  "target_symbol_hit": false,
  "context_patch_overlap": 0.083,
  "patch_files_changed": 1,
  "patch_lines_changed": 1,
  "edit_distance_lines": -1,
  "entity_count_extracted": 0,
  "entity_count_mapped": 0,
  "query_identifier_density": 0.0,
  "seed_symbol_keys": [],
  "mutation_symbol_key": "pydantic_ai_slim/pydantic_ai/usage.py::UsageBase.opentelemetry_attributes",
  "min_hops_seed_to_mutation": -1,
  "median_hops_seed_to_mutation": -1.0,
  "bca_closure_added_symbols": 1,
  "bca_closure_added_tokens": 9,
  "bca_frontier_visited": 34,
  "context_symbol_keys": [
    "examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py::agent",
    "pydantic_ai_slim/pydantic_ai/agent/__init__.py::Agent",
    "pydantic_ai_slim/pydantic_ai/agent/__init__.py::Agent.instructions",
    "pydantic_ai_slim/pydantic_ai/agent/__init__.py::Agent.output_validator",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::InstrumentationSettings",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::InstrumentationSettings.record_metrics",
    "pydantic_ai_slim/pydantic_ai/models/mistral.py::MistralModel._generate_user_output_format",
    "pydantic_ai_slim/pydantic_ai/usage.py::UsageBase.has_values",
    "pydantic_ai_slim/pydantic_ai/usage.py::RequestUsage",
    "pydantic_ai_slim/pydantic_ai/usage.py::RequestUsage.incr",
    "pydantic_ai_slim/pydantic_ai/usage.py::RunUsage.incr",
    "pydantic_ai_slim/pydantic_ai/usage.py::Usage",
    "pydantic_evals/pydantic_evals/dataset.py::_logfire",
    "pydantic_evals/pydantic_evals/dataset.py::Dataset.evaluate",
    "pydantic_evals/pydantic_evals/dataset.py::_TaskRun.record_metric",
    "pydantic_evals/pydantic_evals/dataset.py::_TaskRun.increment_metric",
    "pydantic_evals/pydantic_evals/dataset.py::_CURRENT_TASK_RUN",
    "pydantic_evals/pydantic_evals/dataset.py::increment_eval_metric",
    "pydantic_evals/pydantic_evals/evaluators/common.py::Equals.evaluate",
    "pydantic_evals/pydantic_evals/evaluators/common.py::EqualsExpected.evaluate",
    "pydantic_evals/pydantic_evals/evaluators/common.py::_truncated_repr",
    "pydantic_evals/pydantic_evals/evaluators/common.py::Contains",
    "pydantic_evals/pydantic_evals/evaluators/common.py::Contains.evaluate",
    "pydantic_evals/pydantic_evals/evaluators/common.py::IsInstance.evaluate",
    "pydantic_evals/pydantic_evals/evaluators/common.py::MaxDuration.evaluate",
    "pydantic_evals/pydantic_evals/evaluators/common.py::LLMJudge.evaluate",
    "pydantic_evals/pydantic_evals/evaluators/common.py::HasMatchingSpan.evaluate",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::_default_model",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::GradingOutput",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::_judge_output_agent",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::judge_output",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::_judge_input_output_agent",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::judge_input_output",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::set_default_judge_model",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::_make_section",
    "pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py::_build_prompt",
    "tests/evals/test_dataset.py::TaskInput",
    "tests/evals/test_dataset.py::TaskOutput",
    "tests/evals/test_dataset.py::TaskMetadata",
    "tests/evals/test_evaluators.py::TaskInput",
    "tests/evals/test_evaluators.py::TaskOutput",
    "tests/evals/test_evaluators.py::TaskMetadata",
    "tests/evals/test_evaluators.py::test_evaluator_with_null_values",
    "tests/test_agent.py::test_output_type_handoff_to_agent",
    "tests/test_agent.py::test_instructions_decorator_with_parenthesis",
    "tests/test_logfire.py::LogfireSummary",
    "tests/test_logfire.py::get_logfire_summary",
    "tests/test_logfire.py::test_logfire",
    "tests/test_logfire.py::test_logfire_metadata_values"
  ],
  "mutation_symbol_lines": 26,
  "mutation_symbol_kind": "method",
  "mutation_file_symbols": 65,
  "graph_node_count": 18010,
  "retrieval_top1_top2_gap": 0.052,
  "retrieval_softmax_entropy": 4.1435,
  "retrieval_softmax_tau": 0.809,
  "retrieval_effective_candidates": 17.67,
  "retrieval_top5_ratio": 0.9198,
  "retrieval_within95_count": 1,
  "retrieval_scored_symbols": 18,
  "retrieval_top1_score": 1.022,
  "retrieval_top5_mean_score": 0.94,
  "retrieval_budget_utilization": 0.9852,
  "retrieval_file_concentration": 0.0,
  "repo_name": "pydantic-ai",
  "category": "usage",
  "mutation_type": "arithmetic_swap",
  "source": "discovered"
}