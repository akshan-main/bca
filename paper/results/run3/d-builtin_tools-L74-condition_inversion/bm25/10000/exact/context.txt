# pydantic_ai_slim/pydantic_ai/retries.py:106-106
    retry_error_cls: type[RetryError]

# tests/test_tools.py:761-783
def test_dynamic_cls_tool():
    @dataclass
    class MyTool(Tool[int]):
        spam: int

        def __init__(self, spam: int = 0, **kwargs: Any):
            self.spam = spam
            kwargs.update(function=self.tool_function, takes_ctx=False)
            super().__init__(**kwargs)

        def tool_function(self, x: int, y: str) -> str:
            return f'{self.spam} {x} {y}'

        async def prepare_tool_def(self, ctx: RunContext[int]) -> ToolDefinition | None:
            if ctx.deps != 42:
                return await super().prepare_tool_def(ctx)

    agent = Agent('test', tools=[MyTool(spam=777)], deps_type=int)
    r = agent.run_sync('', deps=1)
    assert r.output == snapshot('{"tool_function":"777 0 a"}')

    r = agent.run_sync('', deps=42)
    assert r.output == snapshot('success (no tool calls)')

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:597-598
    def _streamed_response_cls(self):
        return OpenRouterStreamedResponse

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/models/openai.py:885-890
    def _streamed_response_cls(self) -> type[OpenAIStreamedResponse]:
        """Returns the `StreamedResponse` type that will be used for streamed responses.

        This method may be overridden by subclasses of `OpenAIChatModel` to provide their own `StreamedResponse` type.
        """
        return OpenAIStreamedResponse

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:47-47
    kind: str = 'unknown_builtin_tool'

# pydantic_graph/pydantic_graph/beta/decision.py:88-124
class DecisionBranch(Generic[SourceT]):
    """Represents a single branch within a decision node.

    Each branch defines the conditions under which it should be taken
    and the path to follow when those conditions are met.

    Note: with the current design, it is actually _critical_ that this class is invariant in SourceT for the sake
    of type-checking that inputs to a Decision are actually handled. See the `# type: ignore` comment in
    `tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch` for an example of how this works.
    """

    source: TypeOrTypeExpression[SourceT]
    """The expected type of data for this branch.

    This is necessary for exhaustiveness-checking when handling the inputs to a decision node."""

    matches: Callable[[Any], bool] | None
    """An optional predicate function used to determine whether input data matches this branch.

    If `None`, default logic is used which attempts to check the value for type-compatibility with the `source` type:
    * If `source` is `Any` or `object`, the branch will always match
    * If `source` is a `Literal` type, this branch will match if the value is one of the parametrizing literal values
    * If `source` is any other type, the value will be checked for matching using `isinstance`

    Inputs are tested against each branch of a decision node in order, and the path of the first matching branch is
    used to handle the input value.
    """

    path: Path
    """The execution path to follow when an input value matches this branch of a decision node.

    This can include transforming, mapping, and broadcasting the output before sending to the next node or nodes.

    The path can also include position-aware labels which are used when generating mermaid diagrams."""

    destinations: list[AnyDestinationNode]
    """The destination nodes that can be referenced by DestinationMarker in the path."""

# tests/test_dbos.py:976-980
def conditions(ctx: RunContext, city: str) -> str:
    if ctx.run_step % 2 == 0:
        return "It's sunny"  # pragma: lax no cover
    else:
        return "It's raining"

# tests/test_dbos.py:976-980
def conditions(ctx: RunContext, city: str) -> str:
    if ctx.run_step % 2 == 0:
        return "It's sunny"  # pragma: lax no cover
    else:
        return "It's raining"

# pydantic_graph/pydantic_graph/beta/decision.py:41-80
class Decision(Generic[StateT, DepsT, HandledT]):
    """Decision node for conditional branching in graph execution.

    A Decision node evaluates conditions and routes execution to different
    branches based on the input data type or custom matching logic.
    """

    id: NodeID
    """Unique identifier for this decision node."""

    branches: list[DecisionBranch[Any]]
    """List of branches that can be taken from this decision."""

    note: str | None
    """Optional documentation note for this decision."""

    def branch(self, branch: DecisionBranch[T]) -> Decision[StateT, DepsT, HandledT | T]:
        """Add a new branch to this decision.

        Args:
            branch: The branch to add to this decision.

        Returns:
            A new Decision with the additional branch.
        """
        return Decision(id=self.id, branches=self.branches + [branch], note=self.note)

    def _force_handled_contravariant(self, inputs: HandledT) -> Never:  # pragma: no cover
        """Forces this type to be contravariant in the HandledT type variable.

        This is an implementation detail of how we can type-check that all possible input types have
        been exhaustively covered.

        Args:
            inputs: Input data of handled types.

        Raises:
            RuntimeError: Always, as this method should never be executed.
        """
        raise RuntimeError('This method should never be called, it is just defined for typing purposes.')

# pydantic_ai_slim/pydantic_ai/models/fallback.py:152-158
def _default_fallback_condition_factory(exceptions: tuple[type[Exception], ...]) -> Callable[[Exception], bool]:
    """Create a default fallback condition for the given exceptions."""

    def fallback_condition(exception: Exception) -> bool:
        return isinstance(exception, exceptions)

    return fallback_condition

# tests/models/test_xai.py:1589-1599
async def test_xai_binary_content_unknown_media_type_raises(allow_model_requests: None):
    """Cover the unsupported BinaryContent media type branch."""
    response = create_response(content='ok', usage=create_usage(prompt_tokens=1, completion_tokens=1))
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    # Neither image/*, audio/*, nor a known document type => should fail during prompt mapping.
    bc = BinaryContent(b'123', media_type='video/mp4')
    with pytest.raises(RuntimeError, match='Unsupported binary content type: video/mp4'):
        await agent.run(['hello', bc])

# pydantic_graph/pydantic_graph/beta/decision.py:57-66
    def branch(self, branch: DecisionBranch[T]) -> Decision[StateT, DepsT, HandledT | T]:
        """Add a new branch to this decision.

        Args:
            branch: The branch to add to this decision.

        Returns:
            A new Decision with the additional branch.
        """
        return Decision(id=self.id, branches=self.branches + [branch], note=self.note)

# tests/models/test_google.py:3246-3279
async def test_google_builtin_tools_with_other_tools(allow_model_requests: None, google_provider: GoogleProvider):
    m = GoogleModel('gemini-2.5-flash', provider=google_provider)

    agent = Agent(m, builtin_tools=[WebFetchTool()])

    @agent.tool_plain
    async def get_user_country() -> str:
        return 'Mexico'  # pragma: no cover

    with pytest.raises(
        UserError,
        match=re.escape('Google does not support function tools and built-in tools at the same time.'),
    ):
        await agent.run('What is the largest city in the user country?')

    class CityLocation(BaseModel):
        city: str
        country: str

    agent = Agent(m, output_type=ToolOutput(CityLocation), builtin_tools=[WebFetchTool()])

    with pytest.raises(
        UserError,
        match=re.escape(
            'Google does not support output tools and built-in tools at the same time. Use `output_type=PromptedOutput(...)` instead.'
        ),
    ):
        await agent.run('What is the largest city in Mexico?')

    # Will default to prompted output
    agent = Agent(m, output_type=CityLocation, builtin_tools=[WebFetchTool()])

    result = await agent.run('What is the largest city in Mexico?')
    assert result.output == snapshot(CityLocation(city='Mexico City', country='Mexico'))

# pydantic_evals/pydantic_evals/otel/span_tree.py:244-366
    def _matches_query(self, query: SpanQuery) -> bool:  # noqa: C901
        """Check if the span matches the query conditions."""
        # Logical combinations
        if or_ := query.get('or_'):
            if len(query) > 1:
                raise ValueError("Cannot combine 'or_' conditions with other conditions at the same level")
            return any(self._matches_query(q) for q in or_)
        if not_ := query.get('not_'):
            if self._matches_query(not_):
                return False
        if and_ := query.get('and_'):
            results = [self._matches_query(q) for q in and_]
            if not all(results):
                return False
        # At this point, all existing ANDs and no existing ORs have passed, so it comes down to this condition

        # Name conditions
        if (name_equals := query.get('name_equals')) and self.name != name_equals:
            return False
        if (name_contains := query.get('name_contains')) and name_contains not in self.name:
            return False
        if (name_matches_regex := query.get('name_matches_regex')) and not re.match(name_matches_regex, self.name):
            return False

        # Attribute conditions
        if (has_attributes := query.get('has_attributes')) and not all(
            self.attributes.get(key) == value for key, value in has_attributes.items()
        ):
            return False
        if (has_attributes_keys := query.get('has_attribute_keys')) and not all(
            key in self.attributes for key in has_attributes_keys
        ):
            return False

        # Timing conditions
        if (min_duration := query.get('min_duration')) is not None:
            if not isinstance(min_duration, timedelta):
                min_duration = timedelta(seconds=min_duration)
            if self.duration < min_duration:
                return False
        if (max_duration := query.get('max_duration')) is not None:
            if not isinstance(max_duration, timedelta):
                max_duration = timedelta(seconds=max_duration)
            if self.duration > max_duration:
                return False

        # Children conditions
        if (min_child_count := query.get('min_child_count')) and len(self.children) < min_child_count:
            return False
        if (max_child_count := query.get('max_child_count')) and len(self.children) > max_child_count:
            return False
        if (some_child_has := query.get('some_child_has')) and not any(
            child._matches_query(some_child_has) for child in self.children
        ):
            return False
        if (all_children_have := query.get('all_children_have')) and not all(
            child._matches_query(all_children_have) for child in self.children
        ):
            return False
        if (no_child_has := query.get('no_child_has')) and any(
            child._matches_query(no_child_has) for child in self.children
        ):
            return False

        # Descendant conditions
        # The following local functions with cache decorators are used to avoid repeatedly evaluating these properties
        @cache
        def descendants():
            return self.descendants

        @cache
        def pruned_descendants():
            stop_recursing_when = query.get('stop_recursing_when')
            return (
                self._filter_descendants(lambda _: True, stop_recursing_when) if stop_recursing_when else descendants()
            )

        if (min_descendant_count := query.get('min_descendant_count')) and len(descendants()) < min_descendant_count:
            return False
        if (max_descendant_count := query.get('max_descendant_count')) and len(descendants()) > max_descendant_count:
            return False
        if (some_descendant_has := query.get('some_descendant_has')) and not any(
            descendant._matches_query(some_descendant_has) for descendant in pruned_descendants()
        ):
            return False
        if (all_descendants_have := query.get('all_descendants_have')) and not all(
            descendant._matches_query(all_descendants_have) for descendant in pruned_descendants()
        ):
            return False
        if (no_descendant_has := query.get('no_descendant_has')) and any(
            descendant._matches_query(no_descendant_has) for descendant in pruned_descendants()
        ):
            return False

        # Ancestor conditions
        # The following local functions with cache decorators are used to avoid repeatedly evaluating these properties
        @cache
        def ancestors():
            return self.ancestors

        @cache
        def pruned_ancestors():
            stop_recursing_when = query.get('stop_recursing_when')
            return self._filter_ancestors(lambda _: True, stop_recursing_when) if stop_recursing_when else ancestors()

        if (min_depth := query.get('min_depth')) and len(ancestors()) < min_depth:
            return False
        if (max_depth := query.get('max_depth')) and len(ancestors()) > max_depth:
            return False
        if (some_ancestor_has := query.get('some_ancestor_has')) and not any(
            ancestor._matches_query(some_ancestor_has) for ancestor in pruned_ancestors()
        ):
            return False
        if (all_ancestors_have := query.get('all_ancestors_have')) and not all(
            ancestor._matches_query(all_ancestors_have) for ancestor in pruned_ancestors()
        ):
            return False
        if (no_ancestor_has := query.get('no_ancestor_has')) and any(
            ancestor._matches_query(no_ancestor_has) for ancestor in pruned_ancestors()
        ):
            return False

        return True

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# pydantic_evals/pydantic_evals/dataset.py:51-51
from .reporting import EvaluationReport, ReportCase, ReportCaseAggregate, ReportCaseFailure

# pydantic_evals/pydantic_evals/otel/span_tree.py:237-242
    def matches(self, query: SpanQuery | SpanPredicate) -> bool:
        """Check if the span node matches the query conditions or predicate."""
        if callable(query):
            return query(self)

        return self._matches_query(query)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:22-22
from pydantic_ai._utils import dataclasses_no_defaults_repr, get_union_args, is_async_callable, now_utc, run_in_executor

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:22-22
from pydantic_ai._utils import dataclasses_no_defaults_repr, get_union_args, is_async_callable, now_utc, run_in_executor

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:22-22
from pydantic_ai._utils import dataclasses_no_defaults_repr, get_union_args, is_async_callable, now_utc, run_in_executor

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:22-22
from pydantic_ai._utils import dataclasses_no_defaults_repr, get_union_args, is_async_callable, now_utc, run_in_executor

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:66-68
    def __init_subclass__(cls, **kwargs: Any) -> None:
        super().__init_subclass__(**kwargs)
        BUILTIN_TOOL_TYPES[cls.kind] = cls

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:8-8
from pydantic_ai._utils import now_utc as _now_utc

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:8-8
from pydantic_ai._utils import now_utc as _now_utc

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:8-8
from pydantic_ai._utils import now_utc as _now_utc

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:8-8
from pydantic_ai._utils import now_utc as _now_utc

# tests/test_dbos.py:987-988
def now_func() -> datetime:
    return datetime.now()

# tests/test_dbos.py:987-988
def now_func() -> datetime:
    return datetime.now()

# tests/test_function_schema.py:151-158
def test_class_method_with_ctx():
    """Test class method that takes RunContext as first parameter (after cls)."""

    class TestClass:
        @classmethod
        def class_method_with_ctx(cls, ctx: RunContext[Any], x: int) -> str: ...  # pragma: no cover

    assert _takes_ctx(TestClass.class_method_with_ctx) is True

# tests/test_builtin_tools.py:20-24
async def test_builtin_tools_not_supported_web_search(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[WebSearchTool()])

    with pytest.raises(UserError):
        await agent.run('What day is tomorrow?')

# examples/pydantic_ai_examples/evals/agent.py:20-20
    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

# tests/test_builtin_tools.py:56-60
async def test_builtin_tools_not_supported_file_search(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[FileSearchTool(file_store_ids=['test-id'])])

    with pytest.raises(UserError):
        await agent.run('Search my files')

# examples/pydantic_ai_examples/evals/models.py:57-57
    now: AwareDatetime

# pydantic_evals/pydantic_evals/dataset.py:1120-1219
async def _run_task_and_evaluators(
    task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],
    case: Case[InputsT, OutputT, MetadataT],
    report_case_name: str,
    dataset_evaluators: list[Evaluator[InputsT, OutputT, MetadataT]],
    retry_task: RetryConfig | None,
    retry_evaluators: RetryConfig | None,
    *,
    source_case_name: str | None = None,
) -> ReportCase[InputsT, OutputT, MetadataT] | ReportCaseFailure[InputsT, OutputT, MetadataT]:
    """Run a task on a case and evaluate the results.

    Args:
        task: The task to run.
        case: The case to run the task on.
        report_case_name: The name to use for this case in the report.
        dataset_evaluators: Evaluators from the dataset to apply to this case.
        retry_task: The retry config to use for running the task.
        retry_evaluators: The retry config to use for running the evaluators.
        source_case_name: The original case name before run-indexing (for multi-run experiments).

    Returns:
        A ReportCase containing the evaluation results.
    """
    trace_id: str | None = None
    span_id: str | None = None
    try:
        with logfire_span(
            'case: {case_name}',
            task_name=get_unwrapped_function_name(task),
            case_name=report_case_name,
            inputs=case.inputs,
            metadata=case.metadata,
            expected_output=case.expected_output,
        ) as case_span:
            context = case_span.context
            if context is not None:  # pragma: no branch
                trace_id = f'{context.trace_id:032x}'
                span_id = f'{context.span_id:016x}'

            if source_case_name is not None:
                case_span.set_attribute('logfire.experiment.source_case_name', source_case_name)

            t0 = time.time()
            scoring_context = await _run_task(task, case, retry_task)

            case_span.set_attribute('output', scoring_context.output)
            case_span.set_attribute('task_duration', scoring_context.duration)
            case_span.set_attribute('metrics', scoring_context.metrics)
            case_span.set_attribute('attributes', scoring_context.attributes)

            evaluators = case.evaluators + dataset_evaluators
            evaluator_outputs: list[EvaluationResult] = []
            evaluator_failures: list[EvaluatorFailure] = []
            if evaluators:
                evaluator_outputs_by_task = await task_group_gather(
                    [lambda ev=ev: run_evaluator(ev, scoring_context, retry_evaluators) for ev in evaluators]
                )
                for outputs in evaluator_outputs_by_task:
                    if isinstance(outputs, EvaluatorFailure):
                        evaluator_failures.append(outputs)
                    else:
                        evaluator_outputs.extend(outputs)

            assertions, scores, labels = _group_evaluator_outputs_by_type(evaluator_outputs)
            case_span.set_attribute('assertions', _evaluation_results_adapter.dump_python(assertions))
            case_span.set_attribute('scores', _evaluation_results_adapter.dump_python(scores))
            case_span.set_attribute('labels', _evaluation_results_adapter.dump_python(labels))
        fallback_duration = time.time() - t0

        return ReportCase[InputsT, OutputT, MetadataT](
            name=report_case_name,
            inputs=case.inputs,
            metadata=case.metadata,
            expected_output=case.expected_output,
            output=scoring_context.output,
            metrics=scoring_context.metrics,
            attributes=scoring_context.attributes,
            scores=scores,
            labels=labels,
            assertions=assertions,
            task_duration=scoring_context.duration,
            total_duration=_get_span_duration(case_span, fallback_duration),
            source_case_name=source_case_name,
            trace_id=trace_id,
            span_id=span_id,
            evaluator_failures=evaluator_failures,
        )
    except Exception as exc:
        return ReportCaseFailure[InputsT, OutputT, MetadataT](
            name=report_case_name,
            inputs=case.inputs,
            metadata=case.metadata,
            expected_output=case.expected_output,
            error_message=f'{type(exc).__name__}: {exc}',
            error_stacktrace=traceback.format_exc(),
            source_case_name=source_case_name,
            trace_id=trace_id,
            span_id=span_id,
        )

# tests/models/test_fallback.py:571-577
async def test_fallback_condition_tuple() -> None:
    potato_model = FunctionModel(potato_exception_response)
    fallback_model = FallbackModel(potato_model, success_model, fallback_on=(PotatoException, ModelHTTPError))
    agent = Agent(model=fallback_model)

    response = await agent.run('hello')
    assert response.output == 'success'

# tests/test_builtin_tools.py:37-41
async def test_builtin_tools_not_supported_code_execution(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[CodeExecutionTool()])

    with pytest.raises(UserError):
        await agent.run('What day is tomorrow?')

# pydantic_graph/pydantic_graph/beta/graph_builder.py:1002-1016
def _build_placeholder_node_id_remapping(nodes: dict[NodeID, AnyNode]) -> dict[NodeID, NodeID]:
    """The determinism of the generated remapping here is dependent on the determinism of the ordering of the `nodes` dict.

    Note: If we want to generate more interesting names, we could try to make use of information about the edges
    into/out of the relevant nodes. I'm not sure if there's a good use case for that though so I didn't bother for now.
    """
    counter = Counter[str]()
    remapping: dict[NodeID, NodeID] = {}
    for node_id in nodes.keys():
        replaced_node_id = replace_placeholder_id(node_id)
        if replaced_node_id == node_id:
            continue
        counter[replaced_node_id] = count = counter[replaced_node_id] + 1
        remapping[node_id] = NodeID(f'{replaced_node_id}_{count}' if count > 1 else replaced_node_id)
    return remapping

# pydantic_evals/pydantic_evals/reporting/__init__.py:138-160
class ReportCaseGroup(Generic[InputsT, OutputT, MetadataT]):
    """Grouped results from running the same case multiple times.

    This is a computed view, not stored data. Obtain via
    `EvaluationReport.case_groups()`.
    """

    name: str
    """The original case name (shared across all runs)."""
    inputs: InputsT
    """The inputs (same for all runs)."""
    metadata: MetadataT | None
    """The metadata (same for all runs)."""
    expected_output: OutputT | None
    """The expected output (same for all runs)."""

    runs: Sequence[ReportCase[InputsT, OutputT, MetadataT]]
    """Individual run results."""
    failures: Sequence[ReportCaseFailure[InputsT, OutputT, MetadataT]]
    """Runs that failed with exceptions."""

    summary: ReportCaseAggregate
    """Aggregated statistics across runs."""

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# tests/models/test_fallback.py:554-561
async def test_fallback_condition_override() -> None:
    def should_fallback(exc: Exception) -> bool:
        return False

    fallback_model = FallbackModel(failure_model, success_model, fallback_on=should_fallback)
    agent = Agent(model=fallback_model)
    with pytest.raises(ModelHTTPError):
        await agent.run('hello')

# tests/test_builtin_tools.py:28-33
async def test_builtin_tools_not_supported_web_search_stream(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[WebSearchTool()])

    with pytest.raises(UserError):
        async with agent.run_stream('What day is tomorrow?'):
            ...  # pragma: no cover

# tests/evals/test_otel.py:628-671
async def test_span_query_timing_conditions():
    """Test timing-related conditions in SpanQuery."""
    from datetime import timedelta

    with context_subtree() as tree:
        with logfire.span('fast_operation'):
            pass

        with logfire.span('medium_operation'):
            logfire.info('add a wait')

        with logfire.span('slow_operation'):
            logfire.info('add a wait')
            logfire.info('add a wait')
    assert isinstance(tree, SpanTree)

    durations = sorted([node.duration for node in tree if node.duration > timedelta(seconds=0)])
    fast_threshold = (durations[0] + durations[1]) / 2
    medium_threshold = (durations[1] + durations[2]) / 2

    # Test min_duration
    min_duration_query: SpanQuery = {'min_duration': fast_threshold}
    matched_nodes = list(tree.find(min_duration_query))
    assert len(matched_nodes) == 2
    assert 'fast_operation' not in [node.name for node in matched_nodes]

    # Test max_duration
    max_duration_queries: list[SpanQuery] = [
        {'min_duration': 0.001, 'max_duration': medium_threshold},
        {'min_duration': 0.001, 'max_duration': medium_threshold.seconds},
    ]
    for max_duration_query in max_duration_queries:
        matched_nodes = list(tree.find(max_duration_query))
        assert len(matched_nodes) == 2
        assert 'slow_operation' not in [node.name for node in matched_nodes]

    # Test min and max duration together using timedelta
    duration_range_query: SpanQuery = {
        'min_duration': fast_threshold,
        'max_duration': medium_threshold,
    }
    matched_node = tree.first(duration_range_query)
    assert matched_node is not None
    assert matched_node.name == 'medium_operation'

# tests/test_tools.py:402-404
def unknown_docstring(**kwargs: int) -> str:  # pragma: no cover
    """Unknown style docstring."""
    return str(kwargs)

# tests/test_builtin_tools.py:64-69
async def test_builtin_tools_not_supported_file_search_stream(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[FileSearchTool(file_store_ids=['test-id'])])

    with pytest.raises(UserError):
        async with agent.run_stream('Search my files'):
            ...  # pragma: no cover

# tests/test_agent.py:2986-3029
def test_unknown_tool():
    def empty(_: list[ModelMessage], _info: AgentInfo) -> ModelResponse:
        return ModelResponse(parts=[ToolCallPart('foobar', '{}')])

    agent = Agent(FunctionModel(empty))

    with capture_run_messages() as messages:
        with pytest.raises(UnexpectedModelBehavior, match=r'Exceeded maximum retries \(1\) for output validation'):
            agent.run_sync('Hello')
    assert messages == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='Hello', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='foobar', args='{}', tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=51, output_tokens=2),
                model_name='function:empty:',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    RetryPromptPart(
                        tool_name='foobar',
                        content="Unknown tool name: 'foobar'. No tools available.",
                        tool_call_id=IsStr(),
                        timestamp=IsNow(tz=timezone.utc),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='foobar', args='{}', tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=65, output_tokens=4),
                model_name='function:empty:',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

# pydantic_evals/pydantic_evals/dataset.py:457-487
    def add_case(
        self,
        *,
        name: str | None = None,
        inputs: InputsT,
        metadata: MetadataT | None = None,
        expected_output: OutputT | None = None,
        evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (),
    ) -> None:
        """Adds a case to the dataset.

        This is a convenience method for creating a [`Case`][pydantic_evals.Case] and adding it to the dataset.

        Args:
            name: Optional name for the case. If not provided, a generic name will be assigned.
            inputs: The inputs to the task being evaluated.
            metadata: Optional metadata for the case, which can be used by evaluators.
            expected_output: The expected output of the task, used for comparison in evaluators.
            evaluators: Tuple of evaluators specific to this case, in addition to dataset-level evaluators.
        """
        if name in {case.name for case in self.cases}:
            raise ValueError(f'Duplicate case name: {name!r}')

        case = Case[InputsT, OutputT, MetadataT](
            name=name,
            inputs=inputs,
            metadata=metadata,
            expected_output=expected_output,
            evaluators=evaluators,
        )
        self.cases.append(case)

# pydantic_graph/pydantic_graph/beta/decision.py:260-276
    def label(self, label: str) -> DecisionBranchBuilder[StateT, DepsT, OutputT, SourceT, HandledT]:
        """Apply a label to the branch at the current point in the path being built.

        These labels are only used in generated mermaid diagrams.

        Args:
            label: The label to apply.

        Returns:
            A new DecisionBranchBuilder where the label has been applied at the end of the current path being built.
        """
        return DecisionBranchBuilder(
            decision=self._decision,
            source=self._source,
            matches=self._matches,
            path_builder=self._path_builder.label(label),
        )

# pydantic_ai_slim/pydantic_ai/_function_schema.py:300-302
def _is_call_ctx(annotation: Any) -> bool:
    """Return whether the annotation is the `RunContext` class, parameterized or not."""
    return annotation is RunContext or get_origin(annotation) is RunContext

# tests/test_builtin_tools.py:45-50
async def test_builtin_tools_not_supported_code_execution_stream(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[CodeExecutionTool()])

    with pytest.raises(UserError):
        async with agent.run_stream('What day is tomorrow?'):
            ...  # pragma: no cover

# tests/evals/test_otel.py:674-710
async def test_span_query_descendant_conditions():
    """Test descendant-related conditions in SpanQuery."""

    with context_subtree() as tree:
        with logfire.span('parent1'):
            with logfire.span('child1', type='important'):
                pass
            with logfire.span('child2', type='normal'):
                pass

        with logfire.span('parent2'):
            with logfire.span('child3', type='normal'):
                pass
            with logfire.span('child4', type='normal'):
                pass
    assert isinstance(tree, SpanTree)

    # Test some_child_has condition
    some_child_query: SpanQuery = {'some_child_has': {'has_attributes': {'type': 'important'}}}
    matched_node = tree.first(some_child_query)
    assert matched_node is not None
    assert matched_node.name == 'parent1'

    # Test all_children_have condition
    all_children_query: SpanQuery = {'all_children_have': {'has_attributes': {'type': 'normal'}}, 'min_child_count': 1}
    matched_node = tree.first(all_children_query)
    assert matched_node is not None
    assert matched_node.name == 'parent2'
    # A couple more tests for coverage reasons:
    assert tree.first({'all_children_have': {'has_attributes': {'type': 'unusual'}}, 'min_child_count': 1}) is None
    assert not matched_node.matches({'no_child_has': {'has_attributes': {'type': 'normal'}}})

    # Test no_child_has condition
    no_child_query: SpanQuery = {'no_child_has': {'has_attributes': {'type': 'important'}}, 'min_child_count': 1}
    matched_node = tree.first(no_child_query)
    assert matched_node is not None
    assert matched_node.name == 'parent2'

# tests/graph/test_mermaid.py:387-395
def test_save_pdf_known(tmp_path: Path, httpx_with_handler: HttpxWithHandler):
    def get_pdf(request: httpx.Request) -> httpx.Response:
        assert dict(request.url.params) == snapshot({})
        assert request.url.path.startswith('/pdf/')
        return httpx.Response(200, content=b'fake pdf')

    path2 = tmp_path / 'graph'
    graph1.mermaid_save(str(path2), start_node=Foo(), image_type='pdf', httpx_client=httpx_with_handler(get_pdf))
    assert path2.read_bytes() == b'fake pdf'

# tests/test_tools.py:747-758
def test_return_unknown():
    agent = Agent('test')

    class Foobar:
        pass

    @agent.tool_plain
    def return_pydantic_model() -> Foobar:
        return Foobar()

    with pytest.raises(PydanticSerializationError, match='Unable to serialize unknown type:'):
        agent.run_sync('')

# pydantic_graph/pydantic_graph/beta/decision.py:51-51
    branches: list[DecisionBranch[Any]]

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:51-56
    def unique_id(self) -> str:
        """A unique identifier for the builtin tool.

        If multiple instances of the same builtin tool can be passed to the model, subclasses should override this property to allow them to be distinguished.
        """
        return self.kind