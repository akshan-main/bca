# pydantic_ai_slim/pydantic_ai/retries.py:106-106
    retry_error_cls: type[RetryError]

# tests/test_tools.py:761-783
def test_dynamic_cls_tool():
    @dataclass
    class MyTool(Tool[int]):
        spam: int

        def __init__(self, spam: int = 0, **kwargs: Any):
            self.spam = spam
            kwargs.update(function=self.tool_function, takes_ctx=False)
            super().__init__(**kwargs)

        def tool_function(self, x: int, y: str) -> str:
            return f'{self.spam} {x} {y}'

        async def prepare_tool_def(self, ctx: RunContext[int]) -> ToolDefinition | None:
            if ctx.deps != 42:
                return await super().prepare_tool_def(ctx)

    agent = Agent('test', tools=[MyTool(spam=777)], deps_type=int)
    r = agent.run_sync('', deps=1)
    assert r.output == snapshot('{"tool_function":"777 0 a"}')

    r = agent.run_sync('', deps=42)
    assert r.output == snapshot('success (no tool calls)')

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:597-598
    def _streamed_response_cls(self):
        return OpenRouterStreamedResponse

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/models/openai.py:885-890
    def _streamed_response_cls(self) -> type[OpenAIStreamedResponse]:
        """Returns the `StreamedResponse` type that will be used for streamed responses.

        This method may be overridden by subclasses of `OpenAIChatModel` to provide their own `StreamedResponse` type.
        """
        return OpenAIStreamedResponse

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:23-23
from pydantic_ai.builtin_tools import AbstractBuiltinTool

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:47-47
    kind: str = 'unknown_builtin_tool'

# pydantic_graph/pydantic_graph/beta/decision.py:88-124
class DecisionBranch(Generic[SourceT]):
    """Represents a single branch within a decision node.

    Each branch defines the conditions under which it should be taken
    and the path to follow when those conditions are met.

    Note: with the current design, it is actually _critical_ that this class is invariant in SourceT for the sake
    of type-checking that inputs to a Decision are actually handled. See the `# type: ignore` comment in
    `tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch` for an example of how this works.
    """

    source: TypeOrTypeExpression[SourceT]
    """The expected type of data for this branch.

    This is necessary for exhaustiveness-checking when handling the inputs to a decision node."""

    matches: Callable[[Any], bool] | None
    """An optional predicate function used to determine whether input data matches this branch.

    If `None`, default logic is used which attempts to check the value for type-compatibility with the `source` type:
    * If `source` is `Any` or `object`, the branch will always match
    * If `source` is a `Literal` type, this branch will match if the value is one of the parametrizing literal values
    * If `source` is any other type, the value will be checked for matching using `isinstance`

    Inputs are tested against each branch of a decision node in order, and the path of the first matching branch is
    used to handle the input value.
    """

    path: Path
    """The execution path to follow when an input value matches this branch of a decision node.

    This can include transforming, mapping, and broadcasting the output before sending to the next node or nodes.

    The path can also include position-aware labels which are used when generating mermaid diagrams."""

    destinations: list[AnyDestinationNode]
    """The destination nodes that can be referenced by DestinationMarker in the path."""

# tests/test_dbos.py:976-980
def conditions(ctx: RunContext, city: str) -> str:
    if ctx.run_step % 2 == 0:
        return "It's sunny"  # pragma: lax no cover
    else:
        return "It's raining"

# tests/test_dbos.py:976-980
def conditions(ctx: RunContext, city: str) -> str:
    if ctx.run_step % 2 == 0:
        return "It's sunny"  # pragma: lax no cover
    else:
        return "It's raining"

# pydantic_graph/pydantic_graph/beta/decision.py:41-80
class Decision(Generic[StateT, DepsT, HandledT]):
    """Decision node for conditional branching in graph execution.

    A Decision node evaluates conditions and routes execution to different
    branches based on the input data type or custom matching logic.
    """

    id: NodeID
    """Unique identifier for this decision node."""

    branches: list[DecisionBranch[Any]]
    """List of branches that can be taken from this decision."""

    note: str | None
    """Optional documentation note for this decision."""

    def branch(self, branch: DecisionBranch[T]) -> Decision[StateT, DepsT, HandledT | T]:
        """Add a new branch to this decision.

        Args:
            branch: The branch to add to this decision.

        Returns:
            A new Decision with the additional branch.
        """
        return Decision(id=self.id, branches=self.branches + [branch], note=self.note)

    def _force_handled_contravariant(self, inputs: HandledT) -> Never:  # pragma: no cover
        """Forces this type to be contravariant in the HandledT type variable.

        This is an implementation detail of how we can type-check that all possible input types have
        been exhaustively covered.

        Args:
            inputs: Input data of handled types.

        Raises:
            RuntimeError: Always, as this method should never be executed.
        """
        raise RuntimeError('This method should never be called, it is just defined for typing purposes.')

# pydantic_ai_slim/pydantic_ai/models/fallback.py:152-158
def _default_fallback_condition_factory(exceptions: tuple[type[Exception], ...]) -> Callable[[Exception], bool]:
    """Create a default fallback condition for the given exceptions."""

    def fallback_condition(exception: Exception) -> bool:
        return isinstance(exception, exceptions)

    return fallback_condition

# tests/models/test_xai.py:1589-1599
async def test_xai_binary_content_unknown_media_type_raises(allow_model_requests: None):
    """Cover the unsupported BinaryContent media type branch."""
    response = create_response(content='ok', usage=create_usage(prompt_tokens=1, completion_tokens=1))
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    # Neither image/*, audio/*, nor a known document type => should fail during prompt mapping.
    bc = BinaryContent(b'123', media_type='video/mp4')
    with pytest.raises(RuntimeError, match='Unsupported binary content type: video/mp4'):
        await agent.run(['hello', bc])

# pydantic_graph/pydantic_graph/beta/decision.py:57-66
    def branch(self, branch: DecisionBranch[T]) -> Decision[StateT, DepsT, HandledT | T]:
        """Add a new branch to this decision.

        Args:
            branch: The branch to add to this decision.

        Returns:
            A new Decision with the additional branch.
        """
        return Decision(id=self.id, branches=self.branches + [branch], note=self.note)

# tests/models/test_google.py:3246-3279
async def test_google_builtin_tools_with_other_tools(allow_model_requests: None, google_provider: GoogleProvider):
    m = GoogleModel('gemini-2.5-flash', provider=google_provider)

    agent = Agent(m, builtin_tools=[WebFetchTool()])

    @agent.tool_plain
    async def get_user_country() -> str:
        return 'Mexico'  # pragma: no cover

    with pytest.raises(
        UserError,
        match=re.escape('Google does not support function tools and built-in tools at the same time.'),
    ):
        await agent.run('What is the largest city in the user country?')

    class CityLocation(BaseModel):
        city: str
        country: str

    agent = Agent(m, output_type=ToolOutput(CityLocation), builtin_tools=[WebFetchTool()])

    with pytest.raises(
        UserError,
        match=re.escape(
            'Google does not support output tools and built-in tools at the same time. Use `output_type=PromptedOutput(...)` instead.'
        ),
    ):
        await agent.run('What is the largest city in Mexico?')

    # Will default to prompted output
    agent = Agent(m, output_type=CityLocation, builtin_tools=[WebFetchTool()])

    result = await agent.run('What is the largest city in Mexico?')
    assert result.output == snapshot(CityLocation(city='Mexico City', country='Mexico'))

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# pydantic_evals/pydantic_evals/otel/span_tree.py:237-242
    def matches(self, query: SpanQuery | SpanPredicate) -> bool:
        """Check if the span node matches the query conditions or predicate."""
        if callable(query):
            return query(self)

        return self._matches_query(query)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:22-22
from pydantic_ai._utils import dataclasses_no_defaults_repr, get_union_args, is_async_callable, now_utc, run_in_executor

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:22-22
from pydantic_ai._utils import dataclasses_no_defaults_repr, get_union_args, is_async_callable, now_utc, run_in_executor

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:22-22
from pydantic_ai._utils import dataclasses_no_defaults_repr, get_union_args, is_async_callable, now_utc, run_in_executor

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:22-22
from pydantic_ai._utils import dataclasses_no_defaults_repr, get_union_args, is_async_callable, now_utc, run_in_executor

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:66-68
    def __init_subclass__(cls, **kwargs: Any) -> None:
        super().__init_subclass__(**kwargs)
        BUILTIN_TOOL_TYPES[cls.kind] = cls

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:8-8
from pydantic_ai._utils import now_utc as _now_utc

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:8-8
from pydantic_ai._utils import now_utc as _now_utc

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:8-8
from pydantic_ai._utils import now_utc as _now_utc

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:8-8
from pydantic_ai._utils import now_utc as _now_utc

# tests/test_dbos.py:987-988
def now_func() -> datetime:
    return datetime.now()

# tests/test_dbos.py:987-988
def now_func() -> datetime:
    return datetime.now()

# tests/test_function_schema.py:151-158
def test_class_method_with_ctx():
    """Test class method that takes RunContext as first parameter (after cls)."""

    class TestClass:
        @classmethod
        def class_method_with_ctx(cls, ctx: RunContext[Any], x: int) -> str: ...  # pragma: no cover

    assert _takes_ctx(TestClass.class_method_with_ctx) is True

# tests/test_builtin_tools.py:20-24
async def test_builtin_tools_not_supported_web_search(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[WebSearchTool()])

    with pytest.raises(UserError):
        await agent.run('What day is tomorrow?')

# examples/pydantic_ai_examples/evals/agent.py:20-20
    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

# tests/test_builtin_tools.py:56-60
async def test_builtin_tools_not_supported_file_search(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[FileSearchTool(file_store_ids=['test-id'])])

    with pytest.raises(UserError):
        await agent.run('Search my files')

# examples/pydantic_ai_examples/evals/models.py:57-57
    now: AwareDatetime

# tests/models/test_fallback.py:571-577
async def test_fallback_condition_tuple() -> None:
    potato_model = FunctionModel(potato_exception_response)
    fallback_model = FallbackModel(potato_model, success_model, fallback_on=(PotatoException, ModelHTTPError))
    agent = Agent(model=fallback_model)

    response = await agent.run('hello')
    assert response.output == 'success'

# tests/test_builtin_tools.py:37-41
async def test_builtin_tools_not_supported_code_execution(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[CodeExecutionTool()])

    with pytest.raises(UserError):
        await agent.run('What day is tomorrow?')

# pydantic_graph/pydantic_graph/beta/graph_builder.py:1002-1016
def _build_placeholder_node_id_remapping(nodes: dict[NodeID, AnyNode]) -> dict[NodeID, NodeID]:
    """The determinism of the generated remapping here is dependent on the determinism of the ordering of the `nodes` dict.

    Note: If we want to generate more interesting names, we could try to make use of information about the edges
    into/out of the relevant nodes. I'm not sure if there's a good use case for that though so I didn't bother for now.
    """
    counter = Counter[str]()
    remapping: dict[NodeID, NodeID] = {}
    for node_id in nodes.keys():
        replaced_node_id = replace_placeholder_id(node_id)
        if replaced_node_id == node_id:
            continue
        counter[replaced_node_id] = count = counter[replaced_node_id] + 1
        remapping[node_id] = NodeID(f'{replaced_node_id}_{count}' if count > 1 else replaced_node_id)
    return remapping

# pydantic_evals/pydantic_evals/reporting/__init__.py:138-160
class ReportCaseGroup(Generic[InputsT, OutputT, MetadataT]):
    """Grouped results from running the same case multiple times.

    This is a computed view, not stored data. Obtain via
    `EvaluationReport.case_groups()`.
    """

    name: str
    """The original case name (shared across all runs)."""
    inputs: InputsT
    """The inputs (same for all runs)."""
    metadata: MetadataT | None
    """The metadata (same for all runs)."""
    expected_output: OutputT | None
    """The expected output (same for all runs)."""

    runs: Sequence[ReportCase[InputsT, OutputT, MetadataT]]
    """Individual run results."""
    failures: Sequence[ReportCaseFailure[InputsT, OutputT, MetadataT]]
    """Runs that failed with exceptions."""

    summary: ReportCaseAggregate
    """Aggregated statistics across runs."""

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName