# pydantic_ai_slim/pydantic_ai/providers/bedrock.py:99-100
    def name(self) -> str:
        return 'bedrock'

# pydantic_ai_slim/pydantic_ai/providers/google.py:29-30
    def name(self) -> str:
        return 'google-vertex' if self._client._api_client.vertexai else 'google-gla'  # type: ignore[reportPrivateUsage]

# pydantic_ai_slim/pydantic_ai/providers/outlines.py:13-15
    def name(self) -> str:
        """The provider name."""
        return 'outlines'

# pydantic_ai_slim/pydantic_ai/providers/__init__.py:30-32
    def name(self) -> str:
        """The provider name."""
        raise NotImplementedError()

# pydantic_ai_slim/pydantic_ai/providers/sentence_transformers.py:12-14
    def name(self) -> str:
        """The provider name."""
        return 'sentence-transformers'  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/providers/xai.py:24-25
    def name(self) -> str:
        return 'xai'

# pydantic_ai_slim/pydantic_ai/tools.py:273-273
    name: str

# pydantic_ai_slim/pydantic_ai/providers/sambanova.py:36-38
    def name(self) -> str:
        """Return the provider name."""
        return 'sambanova'

# pydantic_ai_slim/pydantic_ai/providers/google_gla.py:20-21
    def name(self):
        return 'google-gla'

# pydantic_ai_slim/pydantic_ai/ext/langchain.py:21-21
    def name(self) -> str: ...

# pydantic_ai_slim/pydantic_ai/providers/cohere.py:26-27
    def name(self) -> str:
        return 'cohere'

# pydantic_ai_slim/pydantic_ai/providers/grok.py:54-55
    def name(self) -> str:
        return 'grok'

# pydantic_ai_slim/pydantic_ai/providers/openai.py:26-27
    def name(self) -> str:
        return 'openai'

# pydantic_ai_slim/pydantic_ai/providers/deepseek.py:32-33
    def name(self) -> str:
        return 'deepseek'

# pydantic_ai_slim/pydantic_ai/providers/openrouter.py:98-99
    def name(self) -> str:
        return 'openrouter'

# pydantic_ai_slim/pydantic_ai/providers/alibaba.py:29-30
    def name(self) -> str:
        return 'alibaba'

# pydantic_ai_slim/pydantic_ai/providers/anthropic.py:33-34
    def name(self) -> str:
        return 'anthropic'

# pydantic_ai_slim/pydantic_ai/providers/azure.py:36-37
    def name(self) -> str:
        return 'azure'

# pydantic_ai_slim/pydantic_ai/providers/cerebras.py:31-32
    def name(self) -> str:
        return 'cerebras'

# pydantic_ai_slim/pydantic_ai/providers/fireworks.py:33-34
    def name(self) -> str:
        return 'fireworks'

# pydantic_ai_slim/pydantic_ai/providers/github.py:36-37
    def name(self) -> str:
        return 'github'

# pydantic_ai_slim/pydantic_ai/providers/google_vertex.py:39-40
    def name(self) -> str:
        return 'google-vertex'

# pydantic_ai_slim/pydantic_ai/providers/groq.py:51-52
    def name(self) -> str:
        return 'groq'

# pydantic_ai_slim/pydantic_ai/providers/heroku.py:28-29
    def name(self) -> str:
        return 'heroku'

# pydantic_ai_slim/pydantic_ai/providers/huggingface.py:32-33
    def name(self) -> str:
        return 'huggingface'

# pydantic_ai_slim/pydantic_ai/providers/litellm.py:37-38
    def name(self) -> str:
        return 'litellm'

# pydantic_ai_slim/pydantic_ai/providers/mistral.py:27-28
    def name(self) -> str:
        return 'mistral'

# pydantic_ai_slim/pydantic_ai/providers/moonshotai.py:36-37
    def name(self) -> str:
        return 'moonshotai'

# pydantic_ai_slim/pydantic_ai/providers/nebius.py:34-35
    def name(self) -> str:
        return 'nebius'

# pydantic_ai_slim/pydantic_ai/providers/ollama.py:34-35
    def name(self) -> str:
        return 'ollama'

# pydantic_ai_slim/pydantic_ai/providers/ovhcloud.py:32-33
    def name(self) -> str:
        return 'ovhcloud'

# pydantic_ai_slim/pydantic_ai/providers/together.py:33-34
    def name(self) -> str:
        return 'together'

# pydantic_ai_slim/pydantic_ai/providers/vercel.py:34-35
    def name(self) -> str:
        return 'vercel'

# pydantic_ai_slim/pydantic_ai/providers/voyageai.py:22-23
    def name(self) -> str:
        return 'voyageai'

# tests/models/test_bedrock.py:102-103
    def name(self) -> str:
        return 'bedrock-stub'

# pydantic_graph/pydantic_graph/beta/graph.py:123-123
    name: str | None

# pydantic_graph/pydantic_graph/graph.py:68-68
    name: str | None

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:22-22
    name: str

# pydantic_ai_slim/pydantic_ai/tools.py:482-482
    name: str

# pydantic_evals/pydantic_evals/dataset.py:228-228
    name: str | None = None

# pydantic_evals/pydantic_evals/dataset.py:132-132
    name: str | None

# pydantic_ai_slim/pydantic_ai/concurrency.py:141-143
    def name(self) -> str | None:
        """Name of the limiter for observability."""
        return self._name

# pydantic_evals/pydantic_evals/otel/span_tree.py:85-85
    name: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:72-72
    name: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:302-302
    name: str

# pydantic_graph/pydantic_graph/beta/graph_builder.py:79-79
    name: str | None

# pydantic_evals/pydantic_evals/evaluators/context.py:55-55
    name: str | None

# pydantic_ai_slim/pydantic_ai/agent/wrapper.py:47-48
    def name(self, value: str | None) -> None:
        self.wrapped.name = value

# pydantic_ai_slim/pydantic_ai/output.py:162-162
    name: str | None

# pydantic_ai_slim/pydantic_ai/output.py:111-111
    name: str | None

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:72-72
    name: str

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:101-103
    def name(self, value: str | None) -> None:
        """Set the name of the agent, used for logging."""
        raise NotImplementedError

# pydantic_ai_slim/pydantic_ai/output.py:264-264
    name: str | None = None

# pydantic_evals/pydantic_evals/evaluators/spec.py:35-35
    name: str

# tests/test_native_output_schema.py:10-10
    name: str

# pydantic_ai_slim/pydantic_ai/output.py:235-235
    name: str | None

# pydantic_evals/pydantic_evals/reporting/__init__.py:166-166
    name: str

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:448-450
    def name(self, value: str | None) -> None:
        """Set the name of the agent, used for logging."""
        self._name = value

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py:210-213
    def name(self, value: str | None) -> None:  # pragma: no cover
        raise UserError(
            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'
        )

# pydantic_evals/pydantic_evals/reporting/__init__.py:109-109
    name: str

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py:127-130
    def name(self, value: str | None) -> None:  # pragma: no cover
        raise UserError(
            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'
        )

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py:235-238
    def name(self, value: str | None) -> None:  # pragma: no cover
        raise UserError(
            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'
        )

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:101-101
    name: str

# pydantic_evals/pydantic_evals/evaluators/report_evaluator.py:31-31
    name: str

# pydantic_ai_slim/pydantic_ai/models/function.py:244-244
    name: str | None = None

# pydantic_evals/pydantic_evals/reporting/__init__.py:145-145
    name: str

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:31-31
    name: str

# pydantic_ai_slim/pydantic_ai/mcp.py:131-131
    name: str

# pydantic_evals/pydantic_evals/dataset.py:104-104
    name: str | None = None

# tests/ext/test_langchain.py:14-14
    name: str

# examples/pydantic_ai_examples/stream_whales.py:28-28
    name: str

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:30-30
    name: str

# pydantic_evals/pydantic_evals/dataset.py:92-92
    name: str | None = None

# pydantic_evals/pydantic_evals/reporting/analyses.py:41-41
    name: str

# tests/test_format_as_xml.py:41-41
    name: str = Field(description="The person's name")

# pydantic_evals/pydantic_evals/_utils.py:49-75
def get_unwrapped_function_name(func: Callable[..., Any]) -> str:
    """Get the name of a function, unwrapping partials and decorators.

    Args:
        func: The function to get the name of.

    Returns:
        The name of the function.

    Raises:
        AttributeError: If the function doesn't have a __name__ attribute and isn't a method.
    """

    def _unwrap(f: Callable[..., Any]) -> Callable[..., Any]:
        """Unwraps f, also unwrapping partials, for the sake of getting f's name."""
        if isinstance(f, partial):
            return _unwrap(f.func)
        return inspect.unwrap(f)

    try:
        return _unwrap(func).__name__
    except AttributeError as e:
        # Handle instances of types with `__call__` as a method
        if inspect.ismethod(getattr(func, '__call__', None)):
            return f'{type(func).__qualname__}.__call__'
        else:
            raise e

# pydantic_ai_slim/pydantic_ai/models/gemini.py:822-822
    name: str

# tests/test_logfire.py:38-38
    name: str

# examples/pydantic_ai_examples/ag_ui/api/shared_state.py:53-53
    name: str

# pydantic_ai_slim/pydantic_ai/models/gemini.py:763-763
    name: str

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:30-30
    name: str

# tests/test_a2a.py:53-53
    name: str

# pydantic_ai_slim/pydantic_ai/models/gemini.py:778-778
    name: str

# pydantic_ai_slim/pydantic_ai/models/groq.py:659-659
    name: str

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:38-38
    name: str

# tests/test_agent.py:104-104
    name: str

# tests/test_format_as_xml.py:21-21
    name: str

# tests/test_format_as_xml.py:26-26
    name: str

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:142-144
    def name(cls) -> str:
        """`name` has been renamed, use `get_serialization_name` instead."""
        return cls.get_serialization_name()

# pydantic_ai_slim/pydantic_ai/tools.py:389-431
    def from_schema(
        cls,
        function: Callable[..., Any],
        name: str,
        description: str | None,
        json_schema: JsonSchemaValue,
        takes_ctx: bool = False,
        sequential: bool = False,
    ) -> Self:
        """Creates a Pydantic tool from a function and a JSON schema.

        Args:
            function: The function to call.
                This will be called with keywords only, and no validation of
                the arguments will be performed.
            name: The unique name of the tool that clearly communicates its purpose
            description: Used to tell the model how/when/why to use the tool.
                You can provide few-shot examples as a part of the description.
            json_schema: The schema for the function arguments
            takes_ctx: An optional boolean parameter indicating whether the function
                accepts the context object as an argument.
            sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.

        Returns:
            A Pydantic tool that calls the function
        """
        function_schema = _function_schema.FunctionSchema(
            function=function,
            description=description,
            validator=SchemaValidator(schema=core_schema.any_schema()),
            json_schema=json_schema,
            takes_ctx=takes_ctx,
            is_async=_utils.is_async_callable(function),
        )

        return cls(
            function,
            takes_ctx=takes_ctx,
            name=name,
            description=description,
            function_schema=function_schema,
            sequential=sequential,
        )

# pydantic_ai_slim/pydantic_ai/tools.py:270-270
    function: ToolFuncEither[ToolAgentDepsT]

# tests/test_usage_limits.py:3-3
import operator

# pydantic_ai_slim/pydantic_ai/models/function.py:49-49
    function: FunctionDef | None

# pydantic_ai_slim/pydantic_ai/_function_schema.py:38-38
    function: Callable[..., Any]

# pydantic_ai_slim/pydantic_ai/_output.py:163-163
    function: OutputValidatorFunc[AgentDepsT, OutputDataT_inv]

# pydantic_ai_slim/pydantic_ai/_system_prompt.py:15-15
    function: SystemPromptFunc[AgentDepsT]

# pydantic_ai_slim/pydantic_ai/models/function.py:86-122
    def __init__(
        self,
        function: FunctionDef | None = None,
        *,
        stream_function: StreamFunctionDef | None = None,
        model_name: str | None = None,
        profile: ModelProfileSpec | None = None,
        settings: ModelSettings | None = None,
    ):
        """Initialize a `FunctionModel`.

        Either `function` or `stream_function` must be provided, providing both is allowed.

        Args:
            function: The function to call for non-streamed requests.
            stream_function: The function to call for streamed requests.
            model_name: The name of the model. If not provided, a name is generated from the function names.
            profile: The model profile to use.
            settings: Model-specific settings that will be used as defaults for this model.
        """
        if function is None and stream_function is None:
            raise TypeError('Either `function` or `stream_function` must be provided')

        self.function = function
        self.stream_function = stream_function

        function_name = self.function.__name__ if self.function is not None else ''
        stream_function_name = self.stream_function.__name__ if self.stream_function is not None else ''
        self._model_name = model_name or f'function:{function_name}:{stream_function_name}'

        # Use a default profile that supports JSON schema and object output if none provided
        if profile is None:
            profile = ModelProfile(
                supports_json_schema_output=True,
                supports_json_object_output=True,
            )
        super().__init__(settings=settings, profile=profile)

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:374-374
    function: _OpenRouterFunction  # type: ignore[reportIncompatibleVariableOverride]

# pydantic_evals/pydantic_evals/evaluators/_base.py:48-54
    def get_serialization_name(cls) -> str:
        """Return the 'name' of this evaluator to use during serialization.

        Returns:
            The name of the evaluator, which is typically the class name.
        """
        return cls.__name__

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:239-321
    def add_function(
        self,
        func: ToolFuncEither[AgentDepsT, ToolParams],
        takes_ctx: bool | None = None,
        name: str | None = None,
        description: str | None = None,
        retries: int | None = None,
        prepare: ToolPrepareFunc[AgentDepsT] | None = None,
        docstring_format: DocstringFormat | None = None,
        require_parameter_descriptions: bool | None = None,
        schema_generator: type[GenerateJsonSchema] | None = None,
        strict: bool | None = None,
        sequential: bool | None = None,
        requires_approval: bool | None = None,
        metadata: dict[str, Any] | None = None,
        timeout: float | None = None,
    ) -> None:
        """Add a function as a tool to the toolset.

        Can take a sync or async function.

        The docstring is inspected to extract both the tool description and description of each parameter,
        [learn more](../tools.md#function-tools-and-schema).

        Args:
            func: The tool function to register.
            takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] as its first argument. If `None`, this is inferred from the function signature.
            name: The name of the tool, defaults to the function name.
            description: The description of the tool, defaults to the function docstring.
            retries: The number of retries to allow for this tool, defaults to the agent's default retries,
                which defaults to 1.
            prepare: custom method to prepare the tool definition for each step, return `None` to omit this
                tool from a given step. This is useful if you want to customise a tool at call time,
                or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].
            docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].
                If `None`, the default value is determined by the toolset.
            require_parameter_descriptions: If True, raise an error if a parameter description is missing.
                If `None`, the default value is determined by the toolset.
            schema_generator: The JSON schema generator class to use for this tool.
                If `None`, the default value is determined by the toolset.
            strict: Whether to enforce JSON schema compliance (only affects OpenAI).
                See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.
                If `None`, the default value is determined by the toolset.
            sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.
                If `None`, the default value is determined by the toolset.
            requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.
                See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.
                If `None`, the default value is determined by the toolset.
            metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.
                If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata.
            timeout: Timeout in seconds for tool execution. If the tool takes longer, a retry prompt is returned to the model.
                Defaults to None (no timeout).
        """
        if docstring_format is None:
            docstring_format = self.docstring_format
        if require_parameter_descriptions is None:
            require_parameter_descriptions = self.require_parameter_descriptions
        if schema_generator is None:
            schema_generator = self.schema_generator
        if strict is None:
            strict = self.strict
        if sequential is None:
            sequential = self.sequential
        if requires_approval is None:
            requires_approval = self.requires_approval

        tool = Tool[AgentDepsT](
            func,
            takes_ctx=takes_ctx,
            name=name,
            description=description,
            max_retries=retries,
            prepare=prepare,
            docstring_format=docstring_format,
            require_parameter_descriptions=require_parameter_descriptions,
            schema_generator=schema_generator,
            strict=strict,
            sequential=sequential,
            requires_approval=requires_approval,
            metadata=metadata,
            timeout=timeout,
        )
        self.add_tool(tool)

# pydantic_graph/pydantic_graph/_utils.py:169-204
def infer_obj_name(obj: Any, *, depth: int) -> str | None:
    """Infer the variable name of an object from the calling frame's scope.

    This function examines the call stack to find what variable name was used
    for the given object in the calling scope. This is useful for automatic
    naming of objects based on their variable names.

    Args:
        obj: The object whose variable name to infer.
        depth: Number of stack frames to traverse upward from the current frame.

    Returns:
        The inferred variable name if found, None otherwise.

    Example:
        Usage should generally look like `infer_name(self, depth=2)` or similar.
    """
    target_frame = inspect.currentframe()
    if target_frame is None:
        return None  # pragma: no cover
    for _ in range(depth):
        target_frame = target_frame.f_back
        if target_frame is None:
            return None

    for name, item in target_frame.f_locals.items():
        if item is obj:
            return name

    if target_frame.f_locals != target_frame.f_globals:  # pragma: no branch
        # if we couldn't find the agent in locals and globals are a different dict, try globals
        for name, item in target_frame.f_globals.items():
            if item is obj:
                return name

    return None

# pydantic_evals/pydantic_evals/dataset.py:407-455
    def evaluate_sync(
        self,
        task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],
        name: str | None = None,
        max_concurrency: int | None = None,
        progress: bool = True,
        retry_task: RetryConfig | None = None,
        retry_evaluators: RetryConfig | None = None,
        *,
        task_name: str | None = None,
        metadata: dict[str, Any] | None = None,
        repeat: int = 1,
    ) -> EvaluationReport[InputsT, OutputT, MetadataT]:
        """Evaluates the test cases in the dataset using the given task.

        This is a synchronous wrapper around [`evaluate`][pydantic_evals.dataset.Dataset.evaluate] provided for convenience.

        Args:
            task: The task to evaluate. This should be a callable that takes the inputs of the case
                and returns the output.
            name: The name of the experiment being run, this is used to identify the experiment in the report.
                If omitted, the task_name will be used; if that is not specified, the name of the task function is used.
            max_concurrency: The maximum number of concurrent evaluations of the task to allow.
                If None, all cases will be evaluated concurrently.
            progress: Whether to show a progress bar for the evaluation. Defaults to `True`.
            retry_task: Optional retry configuration for the task execution.
            retry_evaluators: Optional retry configuration for evaluator execution.
            task_name: Optional override to the name of the task being executed, otherwise the name of the task
                function will be used.
            metadata: Optional dict of experiment metadata.
            repeat: Number of times to run each case. When > 1, each case is run multiple times and
                results are grouped by the original case name for aggregation. Defaults to 1.

        Returns:
            A report containing the results of the evaluation.
        """
        return get_event_loop().run_until_complete(
            self.evaluate(
                task,
                name=name,
                max_concurrency=max_concurrency,
                progress=progress,
                retry_task=retry_task,
                retry_evaluators=retry_evaluators,
                task_name=task_name,
                metadata=metadata,
                repeat=repeat,
            )
        )

# pydantic_ai_slim/pydantic_ai/models/__init__.py:1098-1202
def infer_model(  # noqa: C901
    model: Model | KnownModelName | str, provider_factory: Callable[[str], Provider[Any]] = infer_provider
) -> Model:
    """Infer the model from the name.

    Args:
        model:
            Model name to instantiate, in the format of `provider:model`. Use the string "test" to instantiate TestModel.
        provider_factory:
            Function that instantiates a provider object. The provider name is passed into the function parameter. Defaults to `provider.infer_provider`.
    """
    if isinstance(model, Model):
        return model
    elif model == 'test':
        from .test import TestModel

        return TestModel()

    try:
        provider_name, model_name = model.split(':', maxsplit=1)
    except ValueError:
        provider_name = None
        model_name = model
        if model_name.startswith(('gpt', 'o1', 'o3')):
            provider_name = 'openai'
        elif model_name.startswith('claude'):
            provider_name = 'anthropic'
        elif model_name.startswith('gemini'):
            provider_name = 'google-gla'

        if provider_name is not None:
            warnings.warn(
                f"Specifying a model name without a provider prefix is deprecated. Instead of {model_name!r}, use '{provider_name}:{model_name}'.",
                DeprecationWarning,
            )
        else:
            raise UserError(f'Unknown model: {model}')

    if provider_name == 'vertexai':  # pragma: no cover
        warnings.warn(
            "The 'vertexai' provider name is deprecated. Use 'google-vertex' instead.",
            DeprecationWarning,
        )
        provider_name = 'google-vertex'

    provider = provider_factory(provider_name)

    model_kind = provider_name
    if model_kind.startswith('gateway/'):
        from ..providers.gateway import normalize_gateway_provider

        model_kind = normalize_gateway_provider(model_kind)

    # OpenRouter and Cerebras need to be checked before OpenAI,
    # as they are in `OpenAIChatCompatibleProvider` but have their own model classes.
    if model_kind == 'openrouter':
        from .openrouter import OpenRouterModel

        return OpenRouterModel(model_name, provider=provider)
    elif model_kind == 'cerebras':
        from .cerebras import CerebrasModel

        return CerebrasModel(model_name, provider=provider)
    elif model_kind in ('openai-chat', 'openai', *get_args(OpenAIChatCompatibleProvider.__value__)):
        from .openai import OpenAIChatModel

        return OpenAIChatModel(model_name, provider=provider)
    elif model_kind == 'openai-responses':
        from .openai import OpenAIResponsesModel

        return OpenAIResponsesModel(model_name, provider=provider)
    elif model_kind in ('google', 'google-gla', 'google-vertex'):
        from .google import GoogleModel

        return GoogleModel(model_name, provider=provider)
    elif model_kind == 'groq':
        from .groq import GroqModel

        return GroqModel(model_name, provider=provider)
    elif model_kind == 'cohere':
        from .cohere import CohereModel

        return CohereModel(model_name, provider=provider)
    elif model_kind == 'mistral':
        from .mistral import MistralModel

        return MistralModel(model_name, provider=provider)
    elif model_kind == 'anthropic':
        from .anthropic import AnthropicModel

        return AnthropicModel(model_name, provider=provider)
    elif model_kind == 'bedrock':
        from .bedrock import BedrockConverseModel

        return BedrockConverseModel(model_name, provider=provider)
    elif model_kind == 'huggingface':
        from .huggingface import HuggingFaceModel

        return HuggingFaceModel(model_name, provider=provider)
    elif model_kind == 'xai':
        from .xai import XaiModel

        return XaiModel(model_name, provider=provider)
    else:
        raise UserError(f'Unknown model: {model}')  # pragma: no cover

# pydantic_evals/pydantic_evals/dataset.py:281-405
    async def evaluate(
        self,
        task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],
        name: str | None = None,
        max_concurrency: int | None = None,
        progress: bool = True,
        retry_task: RetryConfig | None = None,
        retry_evaluators: RetryConfig | None = None,
        *,
        task_name: str | None = None,
        metadata: dict[str, Any] | None = None,
        repeat: int = 1,
    ) -> EvaluationReport[InputsT, OutputT, MetadataT]:
        """Evaluates the test cases in the dataset using the given task.

        This method runs the task on each case in the dataset, applies evaluators,
        and collects results into a report. Cases are run concurrently, limited by `max_concurrency` if specified.

        Args:
            task: The task to evaluate. This should be a callable that takes the inputs of the case
                and returns the output.
            name: The name of the experiment being run, this is used to identify the experiment in the report.
                If omitted, the task_name will be used; if that is not specified, the name of the task function is used.
            max_concurrency: The maximum number of concurrent evaluations of the task to allow.
                If None, all cases will be evaluated concurrently.
            progress: Whether to show a progress bar for the evaluation. Defaults to `True`.
            retry_task: Optional retry configuration for the task execution.
            retry_evaluators: Optional retry configuration for evaluator execution.
            task_name: Optional override to the name of the task being executed, otherwise the name of the task
                function will be used.
            metadata: Optional dict of experiment metadata.
            repeat: Number of times to run each case. When > 1, each case is run multiple times and
                results are grouped by the original case name for aggregation. Defaults to 1.

        Returns:
            A report containing the results of the evaluation.
        """
        if repeat < 1:
            raise ValueError(f'repeat must be >= 1, got {repeat}')

        task_name = task_name or get_unwrapped_function_name(task)
        name = name or task_name

        tasks_to_run = self._build_tasks_to_run(repeat)
        total_tasks = len(tasks_to_run)
        progress_bar = Progress() if progress else None

        limiter = anyio.Semaphore(max_concurrency) if max_concurrency is not None else AsyncExitStack()

        extra_attributes: dict[str, Any] = {'gen_ai.operation.name': 'experiment'}
        if metadata is not None:
            extra_attributes['metadata'] = metadata
        if repeat > 1:
            extra_attributes['logfire.experiment.repeat'] = repeat
        with (
            logfire_span(
                'evaluate {name}',
                name=name,
                task_name=task_name,
                dataset_name=self.name,
                n_cases=len(self.cases),
                **extra_attributes,
            ) as eval_span,
            progress_bar or nullcontext(),
        ):
            task_id = progress_bar.add_task(f'Evaluating {task_name}', total=total_tasks) if progress_bar else None

            async def _handle_case(
                case: Case[InputsT, OutputT, MetadataT],
                report_case_name: str,
                source_case_name: str | None,
            ):
                async with limiter:
                    result = await _run_task_and_evaluators(
                        task,
                        case,
                        report_case_name,
                        self.evaluators,
                        retry_task,
                        retry_evaluators,
                        source_case_name=source_case_name,
                    )
                    if progress_bar and task_id is not None:  # pragma: no branch
                        progress_bar.update(task_id, advance=1)
                    return result

            if (context := eval_span.context) is None:  # pragma: no cover
                trace_id = None
                span_id = None
            else:
                trace_id = f'{context.trace_id:032x}'
                span_id = f'{context.span_id:016x}'
            cases_and_failures = await task_group_gather(
                [
                    lambda case=case, rn=report_name, scn=source_name: _handle_case(case, rn, scn)
                    for case, report_name, source_name in tasks_to_run
                ]
            )
            cases: list[ReportCase] = []
            failures: list[ReportCaseFailure] = []
            for item in cases_and_failures:
                if isinstance(item, ReportCase):
                    cases.append(item)
                else:
                    failures.append(item)
            report = EvaluationReport(
                name=name,
                cases=cases,
                failures=failures,
                experiment_metadata=metadata,
                span_id=span_id,
                trace_id=trace_id,
            )

            # Run report evaluators
            if self.report_evaluators:
                report_ctx = ReportEvaluatorContext(
                    name=name,
                    report=report,
                    experiment_metadata=metadata,
                )
                await _run_report_evaluators(self.report_evaluators, report_ctx)

            _set_experiment_span_attributes(eval_span, report, metadata, len(self.cases), repeat)
        return report

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:146-162
    def get_default_evaluation_name(self) -> str:
        """Return the default name to use in reports for the output of this evaluator.

        By default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used.
        Otherwise, the serialization name of the evaluator (which is usually the class name) will be used.

        This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.

        Note that evaluators that return a mapping of results will always use the keys of that mapping as the names
        of the associated evaluation results.
        """
        evaluation_name = getattr(self, 'evaluation_name', None)
        if isinstance(evaluation_name, str):
            # If the evaluator has an attribute `name` of type string, use that
            return evaluation_name

        return self.get_serialization_name()

# pydantic_graph/pydantic_graph/beta/util.py:81-90
def get_callable_name(callable_: Any) -> str:
    """Extract a human-readable name from a callable object.

    Args:
        callable_: Any callable object (function, method, class, etc.).

    Returns:
        The callable's __name__ attribute if available, otherwise its string representation.
    """
    return getattr(callable_, '__name__', str(callable_))

# pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py:89-94
    def model_name(self) -> str:
        """The model name.

        Since the model name isn't known until the request is made, this property always returns `'mcp-sampling'`.
        """
        return 'mcp-sampling'

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:118-129
    async def call_tool(
        self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]
    ) -> Any:
        """Call a tool with the given arguments.

        Args:
            name: The name of the tool to call.
            tool_args: The arguments to pass to the tool.
            ctx: The run context.
            tool: The tool definition returned by [`get_tools`][pydantic_ai.toolsets.AbstractToolset.get_tools] that was called.
        """
        raise NotImplementedError()

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:1287-1377
    def tool_plain(
        self,
        func: ToolFuncPlain[ToolParams] | None = None,
        /,
        *,
        name: str | None = None,
        description: str | None = None,
        retries: int | None = None,
        prepare: ToolPrepareFunc[AgentDepsT] | None = None,
        docstring_format: DocstringFormat = 'auto',
        require_parameter_descriptions: bool = False,
        schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,
        strict: bool | None = None,
        sequential: bool = False,
        requires_approval: bool = False,
        metadata: dict[str, Any] | None = None,
        timeout: float | None = None,
    ) -> Any:
        """Decorator to register a tool function which DOES NOT take `RunContext` as an argument.

        Can decorate a sync or async functions.

        The docstring is inspected to extract both the tool description and description of each parameter,
        [learn more](../tools.md#function-tools-and-schema).

        We can't add overloads for every possible signature of tool, since the return type is a recursive union
        so the signature of functions decorated with `@agent.tool` is obscured.

        Example:
        ```python
        from pydantic_ai import Agent, RunContext

        agent = Agent('test')

        @agent.tool
        def foobar(ctx: RunContext[int]) -> int:
            return 123

        @agent.tool(retries=2)
        async def spam(ctx: RunContext[str]) -> float:
            return 3.14

        result = agent.run_sync('foobar', deps=1)
        print(result.output)
        #> {"foobar":123,"spam":3.14}
        ```

        Args:
            func: The tool function to register.
            name: The name of the tool, defaults to the function name.
            description: The description of the tool, defaults to the function docstring.
            retries: The number of retries to allow for this tool, defaults to the agent's default retries,
                which defaults to 1.
            prepare: custom method to prepare the tool definition for each step, return `None` to omit this
                tool from a given step. This is useful if you want to customise a tool at call time,
                or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].
            docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].
                Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.
            require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.
            schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.
            strict: Whether to enforce JSON schema compliance (only affects OpenAI).
                See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.
            sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.
            requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.
                See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.
            metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.
            timeout: Timeout in seconds for tool execution. If the tool takes longer, a retry prompt is returned to the model.
                Overrides the agent-level `tool_timeout` if set. Defaults to None (no timeout).
        """

        def tool_decorator(func_: ToolFuncPlain[ToolParams]) -> ToolFuncPlain[ToolParams]:
            # noinspection PyTypeChecker
            self._function_toolset.add_function(
                func_,
                takes_ctx=False,
                name=name,
                description=description,
                retries=retries,
                prepare=prepare,
                docstring_format=docstring_format,
                require_parameter_descriptions=require_parameter_descriptions,
                schema_generator=schema_generator,
                strict=strict,
                sequential=sequential,
                requires_approval=requires_approval,
                metadata=metadata,
                timeout=timeout,
            )
            return func_

        return tool_decorator if func is None else tool_decorator(func)

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:16-16
from typing_extensions import Self, TypeVar, deprecated