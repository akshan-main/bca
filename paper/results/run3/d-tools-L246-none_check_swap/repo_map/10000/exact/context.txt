# Repository structure
.github/set_docs_main_preview_url.py
.github/set_docs_pr_preview_url.py
clai/clai/__init__.py
clai/clai/__main__.py
clai/update_readme.py
docs/.hooks/algolia.py
docs/.hooks/main.py
docs/.hooks/snippets.py
docs/.hooks/test_snippets.py
examples/pydantic_ai_examples/__main__.py
examples/pydantic_ai_examples/ag_ui/__init__.py
examples/pydantic_ai_examples/ag_ui/api/__init__.py
examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py
examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py
examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py
examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py
examples/pydantic_ai_examples/ag_ui/api/shared_state.py
examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py
examples/pydantic_ai_examples/bank_support.py
examples/pydantic_ai_examples/chat_app.py
examples/pydantic_ai_examples/data_analyst.py
examples/pydantic_ai_examples/evals/__init__.py
examples/pydantic_ai_examples/evals/agent.py
examples/pydantic_ai_examples/evals/custom_evaluators.py
examples/pydantic_ai_examples/evals/example_01_generate_dataset.py
examples/pydantic_ai_examples/evals/example_02_add_custom_evaluators.py
examples/pydantic_ai_examples/evals/example_03_unit_testing.py
examples/pydantic_ai_examples/evals/example_04_compare_models.py
examples/pydantic_ai_examples/evals/models.py
examples/pydantic_ai_examples/flight_booking.py
examples/pydantic_ai_examples/pydantic_model.py
examples/pydantic_ai_examples/question_graph.py
examples/pydantic_ai_examples/rag.py
examples/pydantic_ai_examples/roulette_wheel.py
examples/pydantic_ai_examples/slack_lead_qualifier/agent.py
examples/pydantic_ai_examples/slack_lead_qualifier/app.py
examples/pydantic_ai_examples/slack_lead_qualifier/functions.py
examples/pydantic_ai_examples/slack_lead_qualifier/modal.py
examples/pydantic_ai_examples/slack_lead_qualifier/models.py
examples/pydantic_ai_examples/slack_lead_qualifier/slack.py
examples/pydantic_ai_examples/slack_lead_qualifier/store.py
examples/pydantic_ai_examples/sql_gen.py
examples/pydantic_ai_examples/stream_markdown.py
examples/pydantic_ai_examples/stream_whales.py
examples/pydantic_ai_examples/weather_agent.py
examples/pydantic_ai_examples/weather_agent_gradio.py
pydantic_ai_slim/pydantic_ai/__init__.py
pydantic_ai_slim/pydantic_ai/__main__.py
pydantic_ai_slim/pydantic_ai/_a2a.py
pydantic_ai_slim/pydantic_ai/_agent_graph.py
pydantic_ai_slim/pydantic_ai/_cli/__init__.py
pydantic_ai_slim/pydantic_ai/_cli/web.py
pydantic_ai_slim/pydantic_ai/_function_schema.py
pydantic_ai_slim/pydantic_ai/_griffe.py
pydantic_ai_slim/pydantic_ai/_instrumentation.py
pydantic_ai_slim/pydantic_ai/_json_schema.py
pydantic_ai_slim/pydantic_ai/_mcp.py
pydantic_ai_slim/pydantic_ai/_otel_messages.py
pydantic_ai_slim/pydantic_ai/_output.py
pydantic_ai_slim/pydantic_ai/_parts_manager.py
pydantic_ai_slim/pydantic_ai/_run_context.py
pydantic_ai_slim/pydantic_ai/_ssrf.py
pydantic_ai_slim/pydantic_ai/_system_prompt.py
pydantic_ai_slim/pydantic_ai/_thinking_part.py
pydantic_ai_slim/pydantic_ai/_tool_manager.py
pydantic_ai_slim/pydantic_ai/_utils.py
pydantic_ai_slim/pydantic_ai/ag_ui.py
pydantic_ai_slim/pydantic_ai/agent/__init__.py
pydantic_ai_slim/pydantic_ai/agent/abstract.py
pydantic_ai_slim/pydantic_ai/agent/wrapper.py
pydantic_ai_slim/pydantic_ai/builtin_tools.py
pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py
pydantic_ai_slim/pydantic_ai/common_tools/exa.py
pydantic_ai_slim/pydantic_ai/common_tools/tavily.py
pydantic_ai_slim/pydantic_ai/concurrency.py
pydantic_ai_slim/pydantic_ai/direct.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/__init__.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_fastmcp_toolset.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp_server.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_utils.py
pydantic_ai_slim/pydantic_ai/durable_exec/prefect/__init__.py
pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py
pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py
pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py
pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py
pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py
pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_toolset.py
pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_types.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_dynamic_toolset.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_fastmcp_toolset.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_function_toolset.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_logfire.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_mcp.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_mcp_server.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py
pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_workflow.py
pydantic_ai_slim/pydantic_ai/embeddings/__init__.py
pydantic_ai_slim/pydantic_ai/embeddings/base.py
pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py
pydantic_ai_slim/pydantic_ai/embeddings/cohere.py
pydantic_ai_slim/pydantic_ai/embeddings/google.py
pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py
pydantic_ai_slim/pydantic_ai/embeddings/openai.py
pydantic_ai_slim/pydantic_ai/embeddings/result.py
pydantic_ai_slim/pydantic_ai/embeddings/sentence_transformers.py
pydantic_ai_slim/pydantic_ai/embeddings/settings.py
pydantic_ai_slim/pydantic_ai/embeddings/test.py
pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py
pydantic_ai_slim/pydantic_ai/embeddings/wrapper.py
pydantic_ai_slim/pydantic_ai/exceptions.py
pydantic_ai_slim/pydantic_ai/ext/aci.py
pydantic_ai_slim/pydantic_ai/ext/langchain.py
pydantic_ai_slim/pydantic_ai/format_prompt.py
pydantic_ai_slim/pydantic_ai/mcp.py
pydantic_ai_slim/pydantic_ai/messages.py
pydantic_ai_slim/pydantic_ai/models/__init__.py
pydantic_ai_slim/pydantic_ai/models/anthropic.py
pydantic_ai_slim/pydantic_ai/models/bedrock.py
pydantic_ai_slim/pydantic_ai/models/cerebras.py
pydantic_ai_slim/pydantic_ai/models/cohere.py
pydantic_ai_slim/pydantic_ai/models/concurrency.py
pydantic_ai_slim/pydantic_ai/models/fallback.py
pydantic_ai_slim/pydantic_ai/models/function.py
pydantic_ai_slim/pydantic_ai/models/gemini.py
pydantic_ai_slim/pydantic_ai/models/google.py
pydantic_ai_slim/pydantic_ai/models/groq.py
pydantic_ai_slim/pydantic_ai/models/huggingface.py
pydantic_ai_slim/pydantic_ai/models/instrumented.py
pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py
pydantic_ai_slim/pydantic_ai/models/mistral.py
pydantic_ai_slim/pydantic_ai/models/openai.py
pydantic_ai_slim/pydantic_ai/models/openrouter.py
pydantic_ai_slim/pydantic_ai/models/outlines.py
pydantic_ai_slim/pydantic_ai/models/test.py
pydantic_ai_slim/pydantic_ai/models/wrapper.py
pydantic_ai_slim/pydantic_ai/models/xai.py
pydantic_ai_slim/pydantic_ai/output.py
pydantic_ai_slim/pydantic_ai/profiles/__init__.py
pydantic_ai_slim/pydantic_ai/profiles/amazon.py
pydantic_ai_slim/pydantic_ai/profiles/anthropic.py
pydantic_ai_slim/pydantic_ai/profiles/cohere.py
pydantic_ai_slim/pydantic_ai/profiles/deepseek.py
pydantic_ai_slim/pydantic_ai/profiles/google.py
pydantic_ai_slim/pydantic_ai/profiles/grok.py
pydantic_ai_slim/pydantic_ai/profiles/groq.py
pydantic_ai_slim/pydantic_ai/profiles/harmony.py
pydantic_ai_slim/pydantic_ai/profiles/meta.py
pydantic_ai_slim/pydantic_ai/profiles/mistral.py
pydantic_ai_slim/pydantic_ai/profiles/moonshotai.py
pydantic_ai_slim/pydantic_ai/profiles/openai.py
pydantic_ai_slim/pydantic_ai/profiles/qwen.py
pydantic_ai_slim/pydantic_ai/profiles/zai.py
pydantic_ai_slim/pydantic_ai/providers/__init__.py
pydantic_ai_slim/pydantic_ai/providers/alibaba.py
pydantic_ai_slim/pydantic_ai/providers/anthropic.py
pydantic_ai_slim/pydantic_ai/providers/azure.py
pydantic_ai_slim/pydantic_ai/providers/bedrock.py
pydantic_ai_slim/pydantic_ai/providers/cerebras.py
pydantic_ai_slim/pydantic_ai/providers/cohere.py
pydantic_ai_slim/pydantic_ai/providers/deepseek.py
pydantic_ai_slim/pydantic_ai/providers/fireworks.py
pydantic_ai_slim/pydantic_ai/providers/gateway.py
pydantic_ai_slim/pydantic_ai/providers/github.py
pydantic_ai_slim/pydantic_ai/providers/google.py
pydantic_ai_slim/pydantic_ai/providers/google_gla.py
pydantic_ai_slim/pydantic_ai/providers/google_vertex.py
pydantic_ai_slim/pydantic_ai/providers/grok.py
pydantic_ai_slim/pydantic_ai/providers/groq.py
pydantic_ai_slim/pydantic_ai/providers/heroku.py
pydantic_ai_slim/pydantic_ai/providers/huggingface.py
pydantic_ai_slim/pydantic_ai/providers/litellm.py
pydantic_ai_slim/pydantic_ai/providers/mistral.py
pydantic_ai_slim/pydantic_ai/providers/moonshotai.py
pydantic_ai_slim/pydantic_ai/providers/nebius.py
pydantic_ai_slim/pydantic_ai/providers/ollama.py
pydantic_ai_slim/pydantic_ai/providers/openai.py
pydantic_ai_slim/pydantic_ai/providers/openrouter.py
pydantic_ai_slim/pydantic_ai/providers/outlines.py
pydantic_ai_slim/pydantic_ai/providers/ovhcloud.py
pydantic_ai_slim/pydantic_ai/providers/sambanova.py
pydantic_ai_slim/pydantic_ai/providers/sentence_transformers.py
pydantic_ai_slim/pydantic_ai/providers/together.py
pydantic_ai_slim/pydantic_ai/providers/vercel.py
pydantic_ai_slim/pydantic_ai/providers/voyageai.py
pydantic_ai_slim/pydantic_ai/providers/xai.py
pydantic_ai_slim/pydantic_ai/result.py
pydantic_ai_slim/pydantic_ai/retries.py
pydantic_ai_slim/pydantic_ai/run.py
pydantic_ai_slim/pydantic_ai/settings.py
pydantic_ai_slim/pydantic_ai/tools.py
pydantic_ai_slim/pydantic_ai/toolsets/__init__.py
pydantic_ai_slim/pydantic_ai/toolsets/_dynamic.py
pydantic_ai_slim/pydantic_ai/toolsets/abstract


# Relevant source code


# pydantic_ai_slim/pydantic_ai/agent/__init__.py:236-418
    def __init__(
        self,
        model: models.Model | models.KnownModelName | str | None = None,
        *,
        output_type: OutputSpec[OutputDataT] = str,
        instructions: Instructions[AgentDepsT] = None,
        system_prompt: str | Sequence[str] = (),
        deps_type: type[AgentDepsT] = NoneType,
        name: str | None = None,
        model_settings: ModelSettings | None = None,
        retries: int = 1,
        validation_context: Any | Callable[[RunContext[AgentDepsT]], Any] = None,
        output_retries: int | None = None,
        tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = (),
        builtin_tools: Sequence[AbstractBuiltinTool | BuiltinToolFunc[AgentDepsT]] = (),
        prepare_tools: ToolsPrepareFunc[AgentDepsT] | None = None,
        prepare_output_tools: ToolsPrepareFunc[AgentDepsT] | None = None,
        toolsets: Sequence[AbstractToolset[AgentDepsT] | ToolsetFunc[AgentDepsT]] | None = None,
        defer_model_check: bool = False,
        end_strategy: EndStrategy = 'early',
        instrument: InstrumentationSettings | bool | None = None,
        metadata: AgentMetadata[AgentDepsT] | None = None,
        history_processors: Sequence[HistoryProcessor[AgentDepsT]] | None = None,
        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,
        tool_timeout: float | None = None,
        max_concurrency: _concurrency.AnyConcurrencyLimit = None,
        **_deprecated_kwargs: Any,
    ):
        """Create an agent.

        Args:
            model: The default model to use for this agent, if not provided,
                you must provide the model when calling it. We allow `str` here since the actual list of allowed models changes frequently.
            output_type: The type of the output data, used to validate the data returned by the model,
                defaults to `str`.
            instructions: Instructions to use for this agent, you can also register instructions via a function with
                [`instructions`][pydantic_ai.agent.Agent.instructions] or pass additional, temporary, instructions when executing a run.
            system_prompt: Static system prompts to use for this agent, you can also register system
                prompts via a function with [`system_prompt`][pydantic_ai.agent.Agent.system_prompt].
            deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully
                parameterize the agent, and therefore get the best out of static type checking.
                If you're not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright
                or add a type hint `: Agent[None, <return type>]`.
            name: The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame
                when the agent is first run.
            model_settings: Optional model request settings to use for this agent's runs, by default.
            retries: The default number of retries to allow for tool calls and output validation, before raising an error.
                For model request retries, see the [HTTP Request Retries](../retries.md) documentation.
            validation_context: Pydantic [validation context](https://docs.pydantic.dev/latest/concepts/validators/#validation-context) used to validate tool arguments and outputs.
            output_retries: The maximum number of retries to allow for output validation, defaults to `retries`.
            tools: Tools to register with the agent, you can also register tools via the decorators
                [`@agent.tool`][pydantic_ai.agent.Agent.tool] and [`@agent.tool_plain`][pydantic_ai.agent.Agent.tool_plain].
            builtin_tools: The builtin tools that the agent will use. This depends on the model, as some models may not
                support certain tools. If the model doesn't support the builtin tools, an error will be raised.
            prepare_tools: Custom function to prepare the tool definition of all tools for each step, except output tools.
                This is useful if you want to customize the definition of multiple tools or you want to register
                a subset of tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]
            prepare_output_tools: Custom function to prepare the tool definition of all output tools for each step.
                This is useful if you want to customize the definition of multiple output tools or you want to register
                a subset of output tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]
            toolsets: Toolsets to register with the agent, including MCP servers and functions which take a run context
                and return a toolset. See [`ToolsetFunc`][pydantic_ai.toolsets.ToolsetFunc] for more information.
            defer_model_check: by default, if you provide a [named][pydantic_ai.models.KnownModelName] model,
                it's evaluated to create a [`Model`][pydantic_ai.models.Model] instance immediately,
                which checks for the necessary environment variables. Set this to `false`
                to defer the evaluation until the first run. Useful if you want to
                [override the model][pydantic_ai.agent.Agent.override] for testing.
            end_strategy: Strategy for handling tool calls that are requested alongside a final result.
                See [`EndStrategy`][pydantic_ai.agent.EndStrategy] for more information.
            instrument: Set to True to automatically instrument with OpenTelemetry,
                which will use Logfire if it's configured.
                Set to an instance of [`InstrumentationSettings`][pydantic_ai.agent.InstrumentationSettings] to customize.
                If this isn't set, then the last value set by
                [`Agent.instrument_all()`][pydantic_ai.agent.Agent.instrument_all]
                will be used, which defaults to False.
                See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.
            metadata: Optional metadata to store with each run.
                Provide a dictionary of primitives, or a callable returning one
                computed from the [`RunContext`][pydantic_ai.tools.RunContext] on each run.
                Metadata is resolved when a run starts and recomputed after a successful run finishes so it
                can reflect the final state.
                Resolved metadata can be read after the run completes via
                [`AgentRun.metadata`][pydantic_ai.agent.AgentRun],
                [`AgentRunResult.metadata`][pydantic_ai.agent.AgentRunResult], and
                [`StreamedRunResult.metadata`][pydantic_ai.result.StreamedRunResult],
                and is attached to the agent run span when instrumentation is enabled.
            history_processors: Optional list of callables to process the message history before sending it to the model.
                Each processor takes a list of messages and returns a modified list of messages.
                Processors can be sync or async and are applied in sequence.
            event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools.
            tool_timeout: Default timeout in seconds for tool execution. If a tool takes longer than this,
                the tool is considered to have failed and a retry prompt is returned to the model (counting towards the retry limit).
                Individual tools can override this with their own timeout. Defaults to None (no timeout).
            max_concurrency: Optional limit on concurrent agent runs. Can be an integer for simple limiting,
                a [`ConcurrencyLimit`][pydantic_ai.ConcurrencyLimit] for advanced configuration with backpressure,
                a [`ConcurrencyLimiter`][pydantic_ai.ConcurrencyLimiter] for sharing limits across
                multiple agents, or None (default) for no limiting. When the limit is reached, additional calls
                to `run()` or `iter()` will wait until a slot becomes available.
        """
        if model is None or defer_model_check:
            self._model = model
        else:
            self._model = models.infer_model(model)

        self._name = name
        self.end_strategy = end_strategy
        self.model_settings = model_settings

        self._output_type = output_type
        self.instrument = instrument
        self._metadata = metadata
        self._deps_type = deps_type

        if mcp_servers := _deprecated_kwargs.pop('mcp_servers', None):
            if toolsets is not None:  # pragma: no cover
                raise TypeError('`mcp_servers` and `toolsets` cannot be set at the same time.')
            warnings.warn('`mcp_servers` is deprecated, use `toolsets` instead', DeprecationWarning)
            toolsets = mcp_servers

        _utils.validate_empty_kwargs(_deprecated_kwargs)

        self._output_schema = _output.OutputSchema[OutputDataT].build(output_type)
        self._output_validators = []

        self._instructions = self._normalize_instructions(instructions)

        self._system_prompts = (system_prompt,) if isinstance(system_prompt, str) else tuple(system_prompt)
        self._system_prompt_functions = []
        self._system_prompt_dynamic_functions = {}

        self._max_result_retries = output_retries if output_retries is not None else retries
        self._max_tool_retries = retries
        self._tool_timeout = tool_timeout

        self._validation_context = validation_context

        self._builtin_tools = builtin_tools

        self._prepare_tools = prepare_tools
        self._prepare_output_tools = prepare_output_tools

        self._output_toolset = self._output_schema.toolset
        if self._output_toolset:
            self._output_toolset.max_retries = self._max_result_retries

        self._function_toolset = _AgentFunctionToolset(
            tools,
            max_retries=self._max_tool_retries,
            timeout=self._tool_timeout,
            output_schema=self._output_schema,
        )
        self._dynamic_toolsets = [
            DynamicToolset[AgentDepsT](toolset_func=toolset)
            for toolset in toolsets or []
            if not isinstance(toolset, AbstractToolset)
        ]
        self._user_toolsets = [toolset for toolset in toolsets or [] if isinstance(toolset, AbstractToolset)]

        self.history_processors = history_processors or []

        self._event_stream_handler = event_stream_handler

        self._concurrency_limiter = _concurrency.normalize_to_limiter(max_concurrency)

        self._override_name: ContextVar[_utils.Option[str]] = ContextVar('_override_name', default=None)
        self._override_deps: ContextVar[_utils.Option[AgentDepsT]] = ContextVar('_override_deps', default=None)
        self._override_model: ContextVar[_utils.Option[models.Model]] = ContextVar('_override_model', default=None)
        self._override_toolsets: ContextVar[_utils.Option[Sequence[AbstractToolset[AgentDepsT]]]] = ContextVar(
            '_override_toolsets', default=None
        )
        self._override_tools: ContextVar[
            _utils.Option[Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]]]
        ] = ContextVar('_override_tools', default=None)
        self._override_instructions: ContextVar[
            _utils.Option[list[str | _system_prompt.SystemPromptFunc[AgentDepsT]]]
        ] = ContextVar('_override_instructions', default=None)
        self._override_metadata: ContextVar[_utils.Option[AgentMetadata[AgentDepsT]]] = ContextVar(
            '_override_metadata', default=None
        )

        self._enter_lock = Lock()
        self._entered_count = 0
        self._exit_stack = None

# pydantic_ai_slim/pydantic_ai/ag_ui.py:112-172
def run_ag_ui(
    agent: AbstractAgent[AgentDepsT, Any],
    run_input: RunAgentInput,
    accept: str = SSE_CONTENT_TYPE,
    *,
    output_type: OutputSpec[Any] | None = None,
    message_history: Sequence[ModelMessage] | None = None,
    deferred_tool_results: DeferredToolResults | None = None,
    model: Model | KnownModelName | str | None = None,
    deps: AgentDepsT = None,
    model_settings: ModelSettings | None = None,
    usage_limits: UsageLimits | None = None,
    usage: RunUsage | None = None,
    metadata: AgentMetadata[AgentDepsT] | None = None,
    infer_name: bool = True,
    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,
    on_complete: OnCompleteFunc[BaseEvent] | None = None,
) -> AsyncIterator[str]:
    """Run the agent with the AG-UI run input and stream AG-UI protocol events.

    Args:
        agent: The agent to run.
        run_input: The AG-UI run input containing thread_id, run_id, messages, etc.
        accept: The accept header value for the run.

        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
            output validators since output validators would expect an argument that matches the agent's output type.
        message_history: History of the conversation so far.
        deferred_tool_results: Optional results for deferred tool calls in the message history.
        model: Optional model to use for this run, required if `model` was not set when creating the agent.
        deps: Optional dependencies to use for this run.
        model_settings: Optional settings to use for this model's request.
        usage_limits: Optional limits on model request count or token usage.
        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
        metadata: Optional metadata to attach to this run. Accepts a dictionary or a callable taking
            [`RunContext`][pydantic_ai.tools.RunContext]; merged with the agent's configured metadata.
        infer_name: Whether to try to infer the agent name from the call frame if it's not set.
        toolsets: Optional additional toolsets for this run.
        on_complete: Optional callback function called when the agent run completes successfully.
            The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.

    Yields:
        Streaming event chunks encoded as strings according to the accept header value.
    """
    adapter = AGUIAdapter(agent=agent, run_input=run_input, accept=accept)
    return adapter.encode_stream(
        adapter.run_stream(
            output_type=output_type,
            message_history=message_history,
            deferred_tool_results=deferred_tool_results,
            model=model,
            deps=deps,
            model_settings=model_settings,
            usage_limits=usage_limits,
            usage=usage,
            metadata=metadata,
            infer_name=infer_name,
            toolsets=toolsets,
            on_complete=on_complete,
        ),
    )

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:340-424
    async def dispatch_request(
        cls,
        request: Request,
        *,
        agent: AbstractAgent[DispatchDepsT, DispatchOutputDataT],
        message_history: Sequence[ModelMessage] | None = None,
        deferred_tool_results: DeferredToolResults | None = None,
        model: Model | KnownModelName | str | None = None,
        instructions: Instructions[DispatchDepsT] = None,
        deps: DispatchDepsT = None,
        output_type: OutputSpec[Any] | None = None,
        model_settings: ModelSettings | None = None,
        usage_limits: UsageLimits | None = None,
        usage: RunUsage | None = None,
        metadata: AgentMetadata[DispatchDepsT] | None = None,
        infer_name: bool = True,
        toolsets: Sequence[AbstractToolset[DispatchDepsT]] | None = None,
        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,
        on_complete: OnCompleteFunc[EventT] | None = None,
    ) -> Response:
        """Handle a protocol-specific HTTP request by running the agent and returning a streaming response of protocol-specific events.

        Args:
            request: The incoming Starlette/FastAPI request.
            agent: The agent to run.
            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
                output validators since output validators would expect an argument that matches the agent's output type.
            message_history: History of the conversation so far.
            deferred_tool_results: Optional results for deferred tool calls in the message history.
            model: Optional model to use for this run, required if `model` was not set when creating the agent.
            instructions: Optional additional instructions to use for this run.
            deps: Optional dependencies to use for this run.
            model_settings: Optional settings to use for this model's request.
            usage_limits: Optional limits on model request count or token usage.
            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
            metadata: Optional metadata to attach to this run. Accepts a dictionary or a callable taking
                [`RunContext`][pydantic_ai.tools.RunContext]; merged with the agent's configured metadata.
            infer_name: Whether to try to infer the agent name from the call frame if it's not set.
            toolsets: Optional additional toolsets for this run.
            builtin_tools: Optional additional builtin tools to use for this run.
            on_complete: Optional callback function called when the agent run completes successfully.
                The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.

        Returns:
            A streaming Starlette response with protocol-specific events encoded per the request's `Accept` header value.
        """
        try:
            from starlette.responses import Response
        except ImportError as e:  # pragma: no cover
            raise ImportError(
                'Please install the `starlette` package to use `dispatch_request()` method, '
                'you can use the `ui` optional group â€” `pip install "pydantic-ai-slim[ui]"`'
            ) from e

        try:
            # The DepsT and OutputDataT come from `agent`, not from `cls`; the cast is necessary to explain this to pyright
            adapter = cast(
                UIAdapter[RunInputT, MessageT, EventT, DispatchDepsT, DispatchOutputDataT],
                await cls.from_request(request, agent=cast(AbstractAgent[AgentDepsT, OutputDataT], agent)),
            )
        except ValidationError as e:  # pragma: no cover
            return Response(
                content=e.json(),
                media_type='application/json',
                status_code=HTTPStatus.UNPROCESSABLE_ENTITY,
            )

        return adapter.streaming_response(
            adapter.run_stream(
                message_history=message_history,
                deferred_tool_results=deferred_tool_results,
                deps=deps,
                output_type=output_type,
                model=model,
                instructions=instructions,
                model_settings=model_settings,
                usage_limits=usage_limits,
                usage=usage,
                metadata=metadata,
                infer_name=infer_name,
                toolsets=toolsets,
                builtin_tools=builtin_tools,
                on_complete=on_complete,
            ),
        )

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:258-350
    def handle_tool_call_delta(
        self,
        *,
        vendor_part_id: Hashable | None,
        tool_name: str | None = None,
        args: str | dict[str, Any] | None = None,
        tool_call_id: str | None = None,
        provider_name: str | None = None,
        provider_details: dict[str, Any] | None = None,
    ) -> ModelResponseStreamEvent | None:
        """Handle or update a tool call, creating or updating a `ToolCallPart`, `BuiltinToolCallPart`, or `ToolCallPartDelta`.

        Managed items remain as `ToolCallPartDelta`s until they have at least a tool_name, at which
        point they are upgraded to `ToolCallPart`s.

        If `vendor_part_id` is None, updates the latest matching ToolCallPart (or ToolCallPartDelta)
        if any. Otherwise, a new part (or delta) may be created.

        Args:
            vendor_part_id: The ID the vendor uses for this tool call.
                If None, the latest matching tool call may be updated.
            tool_name: The name of the tool. If None, the manager does not enforce
                a name match when `vendor_part_id` is None.
            args: Arguments for the tool call, either as a string, a dictionary of key-value pairs, or None.
            tool_call_id: An optional string representing an identifier for this tool call.
            provider_name: An optional provider name for the tool call part.
            provider_details: An optional dictionary of provider-specific details for the tool call part.

        Returns:
            - A `PartStartEvent` if a new ToolCallPart or BuiltinToolCallPart is created.
            - A `PartDeltaEvent` if an existing part is updated.
            - `None` if no new event is emitted (e.g., the part is still incomplete).

        Raises:
            UnexpectedModelBehavior: If attempting to apply a tool call delta to a part that is not
                a ToolCallPart, BuiltinToolCallPart, or ToolCallPartDelta.
        """
        existing_matching_part_and_index: tuple[ToolCallPartDelta | ToolCallPart | BuiltinToolCallPart, int] | None = (
            None
        )

        if vendor_part_id is None:
            # vendor_part_id is None, so check if the latest part is a matching tool call or delta to update
            # When the vendor_part_id is None, if the tool_name is _not_ None, assume this should be a new part rather
            # than a delta on an existing one. We can change this behavior in the future if necessary for some model.
            if tool_name is None:
                existing_matching_part_and_index = self._latest_part_if_of_type(
                    ToolCallPart, BuiltinToolCallPart, ToolCallPartDelta
                )
        else:
            # vendor_part_id is provided, so look up the corresponding part or delta
            part_index = self._vendor_id_to_part_index.get(vendor_part_id)
            if part_index is not None:
                existing_part = self._parts[part_index]
                if not isinstance(existing_part, ToolCallPartDelta | ToolCallPart | BuiltinToolCallPart):
                    raise UnexpectedModelBehavior(f'Cannot apply a tool call delta to {existing_part=}')
                existing_matching_part_and_index = existing_part, part_index

        if existing_matching_part_and_index is None:
            # No matching part/delta was found, so create a new ToolCallPartDelta (or ToolCallPart if fully formed)
            delta = ToolCallPartDelta(
                tool_name_delta=tool_name,
                args_delta=args,
                tool_call_id=tool_call_id,
                provider_name=provider_name,
                provider_details=provider_details,
            )
            part = delta.as_part() or delta
            new_part_index = self._append_part(part, vendor_part_id)
            # Only emit a PartStartEvent if we have enough information to produce a full ToolCallPart
            if isinstance(part, ToolCallPart | BuiltinToolCallPart):
                return PartStartEvent(index=new_part_index, part=part)
        else:
            # Update the existing part or delta with the new information
            existing_part, part_index = existing_matching_part_and_index
            delta = ToolCallPartDelta(
                tool_name_delta=tool_name,
                args_delta=args,
                tool_call_id=tool_call_id,
                provider_name=self._resolve_provider_name(existing_part, provider_name),
                provider_details=provider_details,
            )
            updated_part = delta.apply(existing_part)
            self._parts[part_index] = updated_part
            if isinstance(updated_part, ToolCallPart | BuiltinToolCallPart):
                if isinstance(existing_part, ToolCallPartDelta):
                    # We just upgraded a delta to a full part, so emit a PartStartEvent
                    return PartStartEvent(index=part_index, part=updated_part)
                else:
                    # We updated an existing part, so emit a PartDeltaEvent
                    if updated_part.tool_call_id and not delta.tool_call_id:
                        delta = replace(delta, tool_call_id=updated_part.tool_call_id)
                    return PartDeltaEvent(index=part_index, delta=delta)

# pydantic_ai_slim/pydantic_ai/ag_ui.py:52-109
async def handle_ag_ui_request(
    agent: AbstractAgent[AgentDepsT, Any],
    request: Request,
    *,
    output_type: OutputSpec[Any] | None = None,
    message_history: Sequence[ModelMessage] | None = None,
    deferred_tool_results: DeferredToolResults | None = None,
    model: Model | KnownModelName | str | None = None,
    deps: AgentDepsT = None,
    model_settings: ModelSettings | None = None,
    usage_limits: UsageLimits | None = None,
    usage: RunUsage | None = None,
    metadata: AgentMetadata[AgentDepsT] | None = None,
    infer_name: bool = True,
    toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,
    on_complete: OnCompleteFunc[BaseEvent] | None = None,
) -> Response:
    """Handle an AG-UI request by running the agent and returning a streaming response.

    Args:
        agent: The agent to run.
        request: The Starlette request (e.g. from FastAPI) containing the AG-UI run input.

        output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
            output validators since output validators would expect an argument that matches the agent's output type.
        message_history: History of the conversation so far.
        deferred_tool_results: Optional results for deferred tool calls in the message history.
        model: Optional model to use for this run, required if `model` was not set when creating the agent.
        deps: Optional dependencies to use for this run.
        model_settings: Optional settings to use for this model's request.
        usage_limits: Optional limits on model request count or token usage.
        usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
        metadata: Optional metadata to attach to this run. Accepts a dictionary or a callable taking
            [`RunContext`][pydantic_ai.tools.RunContext]; merged with the agent's configured metadata.
        infer_name: Whether to try to infer the agent name from the call frame if it's not set.
        toolsets: Optional additional toolsets for this run.
        on_complete: Optional callback function called when the agent run completes successfully.
            The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.

    Returns:
        A streaming Starlette response with AG-UI protocol events.
    """
    return await AGUIAdapter[AgentDepsT].dispatch_request(
        request,
        agent=agent,
        deps=deps,
        output_type=output_type,
        message_history=message_history,
        deferred_tool_results=deferred_tool_results,
        model=model,
        model_settings=model_settings,
        usage_limits=usage_limits,
        usage=usage,
        metadata=metadata,
        infer_name=infer_name,
        toolsets=toolsets,
        on_complete=on_complete,
    )

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:511-770
    async def iter(
        self,
        user_prompt: str | Sequence[_messages.UserContent] | None = None,
        *,
        output_type: OutputSpec[Any] | None = None,
        message_history: Sequence[_messages.ModelMessage] | None = None,
        deferred_tool_results: DeferredToolResults | None = None,
        model: models.Model | models.KnownModelName | str | None = None,
        instructions: Instructions[AgentDepsT] = None,
        deps: AgentDepsT = None,
        model_settings: ModelSettings | None = None,
        usage_limits: _usage.UsageLimits | None = None,
        usage: _usage.RunUsage | None = None,
        metadata: AgentMetadata[AgentDepsT] | None = None,
        infer_name: bool = True,
        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,
        builtin_tools: Sequence[AbstractBuiltinTool | BuiltinToolFunc[AgentDepsT]] | None = None,
    ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:
        """A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.

        This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an
        `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are
        executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the
        stream of events coming from the execution of tools.

        The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,
        and the final result of the run once it has completed.

        For more details, see the documentation of `AgentRun`.

        Example:
        ```python
        from pydantic_ai import Agent

        agent = Agent('openai:gpt-5.2')

        async def main():
            nodes = []
            async with agent.iter('What is the capital of France?') as agent_run:
                async for node in agent_run:
                    nodes.append(node)
            print(nodes)
            '''
            [
                UserPromptNode(
                    user_prompt='What is the capital of France?',
                    instructions_functions=[],
                    system_prompts=(),
                    system_prompt_functions=[],
                    system_prompt_dynamic_functions={},
                ),
                ModelRequestNode(
                    request=ModelRequest(
                        parts=[
                            UserPromptPart(
                                content='What is the capital of France?',
                                timestamp=datetime.datetime(...),
                            )
                        ],
                        timestamp=datetime.datetime(...),
                        run_id='...',
                    )
                ),
                CallToolsNode(
                    model_response=ModelResponse(
                        parts=[TextPart(content='The capital of France is Paris.')],
                        usage=RequestUsage(input_tokens=56, output_tokens=7),
                        model_name='gpt-5.2',
                        timestamp=datetime.datetime(...),
                        run_id='...',
                    )
                ),
                End(data=FinalResult(output='The capital of France is Paris.')),
            ]
            '''
            print(agent_run.result.output)
            #> The capital of France is Paris.
        ```

        Args:
            user_prompt: User input to start/continue the conversation.
            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
                output validators since output validators would expect an argument that matches the agent's output type.
            message_history: History of the conversation so far.
            deferred_tool_results: Optional results for deferred tool calls in the message history.
            model: Optional model to use for this run, required if `model` was not set when creating the agent.
            instructions: Optional additional instructions to use for this run.
            deps: Optional dependencies to use for this run.
            model_settings: Optional settings to use for this model's request.
            usage_limits: Optional limits on model request count or token usage.
            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
            metadata: Optional metadata to attach to this run. Accepts a dictionary or a callable taking
                [`RunContext`][pydantic_ai.tools.RunContext]; merged with the agent's configured metadata.
            infer_name: Whether to try to infer the agent name from the call frame if it's not set.
            toolsets: Optional additional toolsets for this run.
            builtin_tools: Optional additional builtin tools for this run.

        Returns:
            The result of the run.
        """
        if infer_name and self.name is None:
            self._infer_name(inspect.currentframe())

        model_used = self._get_model(model)
        del model

        deps = self._get_deps(deps)
        output_schema = self._prepare_output_schema(output_type)

        output_type_ = output_type or self.output_type

        # We consider it a user error if a user tries to restrict the result type while having an output validator that
        # may change the result type from the restricted type to something else. Therefore, we consider the following
        # typecast reasonable, even though it is possible to violate it with otherwise-type-checked code.
        output_validators = self._output_validators

        output_toolset = self._output_toolset
        if output_schema != self._output_schema or output_validators:
            output_toolset = output_schema.toolset
            if output_toolset:
                output_toolset.max_retries = self._max_result_retries
                output_toolset.output_validators = output_validators
        toolset = self._get_toolset(output_toolset=output_toolset, additional_toolsets=toolsets)
        tool_manager = ToolManager[AgentDepsT](toolset, default_max_retries=self._max_tool_retries)

        # Build the graph
        graph = _agent_graph.build_agent_graph(self.name, self._deps_type, output_type_)

        # Build the initial state
        usage = usage or _usage.RunUsage()
        state = _agent_graph.GraphAgentState(
            message_history=list(message_history) if message_history else [],
            usage=usage,
            retries=0,
            run_step=0,
        )

        # Merge model settings in order of precedence: run > agent > model
        merged_settings = merge_model_settings(model_used.settings, self.model_settings)
        model_settings = merge_model_settings(merged_settings, model_settings)
        usage_limits = usage_limits or _usage.UsageLimits()

        instructions_literal, instructions_functions = self._get_instructions(additional_instructions=instructions)

        async def get_instructions(run_context: RunContext[AgentDepsT]) -> str | None:
            parts = [
                instructions_literal,
                *[await func.run(run_context) for func in instructions_functions],
            ]

            parts = [p for p in parts if p]
            if not parts:
                return None
            return '\n\n'.join(parts).strip()

        if isinstance(model_used, InstrumentedModel):
            instrumentation_settings = model_used.instrumentation_settings
            tracer = model_used.instrumentation_settings.tracer
        else:
            instrumentation_settings = None
            tracer = NoOpTracer()

        graph_deps = _agent_graph.GraphAgentDeps[AgentDepsT, OutputDataT](
            user_deps=deps,
            prompt=user_prompt,
            new_message_index=len(message_history) if message_history else 0,
            model=model_used,
            model_settings=model_settings,
            usage_limits=usage_limits,
            max_result_retries=self._max_result_retries,
            end_strategy=self.end_strategy,
            output_schema=output_schema,
            output_validators=output_validators,
            validation_context=self._validation_context,
            history_processors=self.history_processors,
            builtin_tools=[*self._builtin_tools, *(builtin_tools or [])],
            tool_manager=tool_manager,
            tracer=tracer,
            get_instructions=get_instructions,
            instrumentation_settings=instrumentation_settings,
        )

        user_prompt_node = _agent_graph.UserPromptNode[AgentDepsT](
            user_prompt=user_prompt,
            deferred_tool_results=deferred_tool_results,
            instructions=instructions_literal,
            instructions_functions=instructions_functions,
            system_prompts=self._system_prompts,
            system_prompt_functions=self._system_prompt_functions,
            system_prompt_dynamic_functions=self._system_prompt_dynamic_functions,
        )

        agent_name = self.name or 'agent'
        instrumentation_names = InstrumentationNames.for_version(
            instrumentation_settings.version if instrumentation_settings else DEFAULT_INSTRUMENTATION_VERSION
        )

        run_span = tracer.start_span(
            instrumentation_names.get_agent_run_span_name(agent_name),
            attributes={
                'model_name': model_used.model_name if model_used else 'no-model',
                'agent_name': agent_name,
                'gen_ai.agent.name': agent_name,
                'logfire.msg': f'{agent_name} run',
            },
        )

        run_metadata: dict[str, Any] | None = None
        try:
            async with (
                _concurrency.get_concurrency_context(self._concurrency_limiter, f'agent:{agent_name}'),
                graph.iter(
                    inputs=user_prompt_node,
                    state=state,
                    deps=graph_deps,
                    span=use_span(run_span) if run_span.is_recording() else None,
                    infer_name=False,
                ) as graph_run,
            ):
                async with toolset:
                    agent_run = AgentRun(graph_run)
                    run_metadata = self._resolve_and_store_metadata(agent_run.ctx, metadata)

                    try:
                        yield agent_run
                    finally:
                        if agent_run.result is not None:
                            run_metadata = self._resolve_and_store_metadata(agent_run.ctx, metadata)
                        else:
                            run_metadata = graph_run.state.metadata

                    final_result = agent_run.result
                    if (
                        instrumentation_settings
                        and instrumentation_settings.include_content
                        and run_span.is_recording()
                        and final_result is not None
                    ):
                        run_span.set_attribute(
                            'final_result',
                            (
                                final_result.output
                                if isinstance(final_result.output, str)
                                else json.dumps(InstrumentedModel.serialize_any(final_result.output))
                            ),
                        )
        finally:
            try:
                if instrumentation_settings and run_span.is_recording():
                    run_span.set_attributes(
                        self._run_span_end_attributes(
                            instrumentation_settings,
                            usage,
                            state.message_history,
                            graph_deps.new_message_index,
                            run_metadata,
                        )
                    )
            finally:
                run_span.end()

# pydantic_graph/pydantic_graph/beta/util.py:58-67
class Some(Generic[T]):
    """Container for explicitly present values in Maybe type pattern.

    This class represents a value that is definitely present, as opposed to None.
    It's part of the Maybe pattern, similar to Option/Maybe in functional programming,
    allowing distinction between "no value" (None) and "value is None" (Some(None)).
    """

    value: T
    """The wrapped value."""