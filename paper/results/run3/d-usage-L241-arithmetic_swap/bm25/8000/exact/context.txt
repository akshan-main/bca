# pydantic_ai_slim/pydantic_ai/usage.py:200-200
    details: dict[str, int] = dataclasses.field(default_factory=dict[str, int])

# pydantic_ai_slim/pydantic_ai/usage.py:46-50
    details: Annotated[
        dict[str, int],
        # `details` can not be `None` any longer, but we still want to support deserializing model responses stored in a DB before this was changed
        BeforeValidator(lambda d: d or {}),
    ] = dataclasses.field(default_factory=dict[str, int])

# tests/models/test_mistral.py:82-83
    def get_server_details(self) -> tuple[str, ...]:
        return ('https://api.mistral.ai',)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:450-450
    api_key: str

# tests/providers/test_together.py:39-48
def test_together_provider_need_api_key(env: TestEnv) -> None:
    env.remove('TOGETHER_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `TOGETHER_API_KEY` environment variable or pass it via `TogetherProvider(api_key=...)`'
            'to use the Together AI provider.'
        ),
    ):
        TogetherProvider()

# examples/pydantic_ai_examples/slack_lead_qualifier/slack.py:8-8
API_KEY = os.getenv('SLACK_API_KEY')

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:443-443
    cost_details: _OpenRouterCostDetails | None = None

# tests/conftest.py:435-436
def co_api_key() -> str:
    return os.getenv('CO_API_KEY', 'mock-api-key')

# tests/conftest.py:470-471
def xai_api_key() -> str:
    return os.getenv('XAI_API_KEY', 'mock-api-key')

# tests/models/mock_xai.py:65-65
    api_key: str = 'test-api-key'

# tests/test_agent.py:15-15
from pydantic_core import ErrorDetails, to_json

# tests/test_agent.py:15-15
from pydantic_core import ErrorDetails, to_json

# tests/test_agent.py:15-15
from pydantic_core import ErrorDetails, to_json

# tests/conftest.py:425-426
def groq_api_key() -> str:
    return os.getenv('GROQ_API_KEY', 'mock-api-key')

# pydantic_evals/pydantic_evals/otel/span_tree.py:113-114
    def node_key(self) -> str:
        return f'{self.trace_id:032x}:{self.span_id:016x}'

# tests/conftest.py:415-416
def openai_api_key() -> str:
    return os.getenv('OPENAI_API_KEY', 'mock-api-key')

# tests/conftest.py:420-421
def gemini_api_key() -> str:
    return os.getenv('GEMINI_API_KEY', os.getenv('GOOGLE_API_KEY', 'mock-api-key'))

# tests/conftest.py:440-441
def voyage_api_key() -> str:
    return os.getenv('VOYAGE_API_KEY', 'mock-api-key')

# pydantic_ai_slim/pydantic_ai/models/gemini.py:447-454
class ApiKeyAuth:
    """Authentication using an API key for the `X-Goog-Api-Key` header."""

    api_key: str

    async def headers(self) -> dict[str, str]:
        # https://cloud.google.com/docs/authentication/api-keys-use#using-with-rest
        return {'X-Goog-Api-Key': self.api_key}  # pragma: no cover

# tests/conftest.py:445-446
def mistral_api_key() -> str:
    return os.getenv('MISTRAL_API_KEY', 'mock-api-key')

# tests/providers/test_gateway.py:72-73
def gateway_api_key():
    return os.getenv('PYDANTIC_AI_GATEWAY_API_KEY', 'test-api-key')

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:447-447
    prompt_tokens_details: _OpenRouterPromptTokenDetails | None = None  # type: ignore[reportIncompatibleVariableOverride]

# pydantic_ai_slim/pydantic_ai/messages.py:927-927
error_details_ta = pydantic.TypeAdapter(list[pydantic_core.ErrorDetails], config=pydantic.ConfigDict(defer_build=True))

# examples/pydantic_ai_examples/flight_booking.py:28-35
class FlightDetails(BaseModel):
    """Details of the most suitable flight."""

    flight_number: str
    price: int
    origin: str = Field(description='Three-letter airport code')
    destination: str = Field(description='Three-letter airport code')
    date: datetime.date

# tests/conftest.py:410-411
def deepseek_api_key() -> str:
    return os.getenv('DEEPSEEK_API_KEY', 'mock-api-key')

# tests/conftest.py:465-466
def cerebras_api_key() -> str:
    return os.getenv('CEREBRAS_API_KEY', 'mock-api-key')

# tests/providers/test_google_gla.py:16-20
def test_api_key_arg(env: TestEnv):
    env.set('GEMINI_API_KEY', 'via-env-var')
    provider = GoogleGLAProvider(api_key='via-arg')
    assert provider.client.headers['x-goog-api-key'] == 'via-arg'
    assert provider.client.base_url == 'https://generativelanguage.googleapis.com/v1beta/models/'

# tests/test_format_as_xml.py:606-608
def test_invalid_key():
    with pytest.raises(TypeError, match='Unsupported key type for XML formatting'):
        format_as_xml({(1, 2): 42})

# pydantic_ai_slim/pydantic_ai/models/gemini.py:902-904
    cache_tokens_details: NotRequired[
        Annotated[list[_GeminiModalityTokenCount], pydantic.Field(alias='cacheTokensDetails')]
    ]

# tests/conftest.py:430-431
def anthropic_api_key() -> str:
    return os.getenv('ANTHROPIC_API_KEY', 'mock-api-key')

# pydantic_ai_slim/pydantic_ai/models/gemini.py:899-901
    prompt_tokens_details: NotRequired[
        Annotated[list[_GeminiModalityTokenCount], pydantic.Field(alias='promptTokensDetails')]
    ]

# tests/conftest.py:450-451
def openrouter_api_key() -> str:
    return os.getenv('OPENROUTER_API_KEY', 'mock-api-key')

# tests/providers/test_google_gla.py:40-48
def test_api_key_empty(env: TestEnv):
    env.set('GEMINI_API_KEY', '')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `GEMINI_API_KEY` environment variable or pass it via `GoogleGLAProvider(api_key=...)`'
        ),
    ):
        GoogleGLAProvider()

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:449-449
    completion_tokens_details: _OpenRouterCompletionTokenDetails | None = None  # type: ignore[reportIncompatibleVariableOverride]

# tests/conftest.py:455-456
def huggingface_api_key() -> str:
    return os.getenv('HF_TOKEN', 'hf_token')

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:21-34
from pydantic_ai.messages import (
    BuiltinToolCallPart,
    ModelResponsePart,
    ModelResponseStreamEvent,
    PartDeltaEvent,
    PartStartEvent,
    ProviderDetailsDelta,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
)

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:21-34
from pydantic_ai.messages import (
    BuiltinToolCallPart,
    ModelResponsePart,
    ModelResponseStreamEvent,
    PartDeltaEvent,
    PartStartEvent,
    ProviderDetailsDelta,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
)

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:21-34
from pydantic_ai.messages import (
    BuiltinToolCallPart,
    ModelResponsePart,
    ModelResponseStreamEvent,
    PartDeltaEvent,
    PartStartEvent,
    ProviderDetailsDelta,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
)

# pydantic_evals/pydantic_evals/otel/span_tree.py:117-118
    def parent_node_key(self) -> str | None:
        return None if self.parent_span_id is None else f'{self.trace_id:032x}:{self.parent_span_id:016x}'

# pydantic_ai_slim/pydantic_ai/messages.py:1081-1081
    provider_details: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:1171-1171
    provider_details: dict[str, Any] | None = None

# tests/conftest.py:460-461
def heroku_inference_key() -> str:
    return os.getenv('HEROKU_INFERENCE_KEY', 'mock-api-key')

# tests/providers/test_google_gla.py:23-26
def test_api_key_env_var(env: TestEnv):
    env.set('GEMINI_API_KEY', 'via-env-var')
    provider = GoogleGLAProvider()
    assert 'x-goog-api-key' in dict(provider.client.headers)

# tests/providers/test_google_gla.py:29-37
def test_api_key_not_set(env: TestEnv):
    env.remove('GEMINI_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `GEMINI_API_KEY` environment variable or pass it via `GoogleGLAProvider(api_key=...)`'
        ),
    ):
        GoogleGLAProvider()

# pydantic_ai_slim/pydantic_ai/messages.py:1533-1534
    def vendor_details(self) -> dict[str, Any] | None:
        return self.provider_details

# pydantic_ai_slim/pydantic_ai/models/openai.py:2825-2836
def _map_provider_details(
    choice: chat_completion_chunk.Choice | chat_completion.Choice,
) -> dict[str, Any] | None:
    provider_details: dict[str, Any] = {}

    # Add logprobs to vendor_details if available
    if choice.logprobs is not None and choice.logprobs.content:
        provider_details['logprobs'] = _map_logprobs(choice.logprobs.content)
    if raw_finish_reason := choice.finish_reason:
        provider_details['finish_reason'] = raw_finish_reason

    return provider_details or None

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:417-420
class _OpenRouterCostDetails:
    """OpenRouter specific cost details."""

    upstream_inference_cost: float | None = None

# pydantic_ai_slim/pydantic_ai/models/gemini.py:905-907
    candidates_tokens_details: NotRequired[
        Annotated[list[_GeminiModalityTokenCount], pydantic.Field(alias='candidatesTokensDetails')]
    ]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_utils.py:10-10
PROVIDER_METADATA_KEY = 'pydantic_ai'

# pydantic_evals/pydantic_evals/evaluators/report_common.py:91-91
    score_key: str

# pydantic_ai_slim/pydantic_ai/messages.py:1133-1133
    provider_details: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:1327-1331
    provider_details: Annotated[
        dict[str, Any] | None,
        # `vendor_details` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        pydantic.Field(validation_alias=pydantic.AliasChoices('provider_details', 'vendor_details')),
    ] = None

# pydantic_ai_slim/pydantic_ai/messages.py:1573-1573
    provider_details: dict[str, Any] | None = None

# tests/models/test_openrouter.py:347-378
async def test_openrouter_with_provider_details_but_no_parent_details(openrouter_api_key: str) -> None:
    from typing import Any

    class TestOpenRouterModel(OpenRouterModel):
        def _process_provider_details(self, response: ChatCompletion) -> dict[str, Any] | None:
            from pydantic_ai.models.openrouter import (
                _map_openrouter_provider_details,  # pyright: ignore[reportPrivateUsage]
                _OpenRouterChatCompletion,  # pyright: ignore[reportPrivateUsage]
            )

            assert isinstance(response, _OpenRouterChatCompletion)
            openrouter_details = _map_openrouter_provider_details(response)
            return openrouter_details or None

    provider = OpenRouterProvider(api_key=openrouter_api_key)
    model = TestOpenRouterModel('google/gemini-2.0-flash-exp:free', provider=provider)

    choice = Choice.model_construct(
        index=0, message={'role': 'assistant', 'content': 'test'}, finish_reason='stop', native_finish_reason='stop'
    )
    response = ChatCompletion.model_construct(
        id='test', choices=[choice], created=1704067200, object='chat.completion', model='test', provider='TestProvider'
    )
    result = model._process_response(response)  # type: ignore[reportPrivateUsage]

    assert result.provider_details == snapshot(
        {
            'downstream_provider': 'TestProvider',
            'finish_reason': 'stop',
            'timestamp': datetime.datetime(2024, 1, 1, 0, 0, tzinfo=datetime.timezone.utc),
        }
    )

# pydantic_evals/pydantic_evals/otel/span_tree.py:44-44
    has_attribute_keys: list[str]

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:79-79
    provider_details: dict[str, Any] | None = None

# pydantic_evals/pydantic_evals/evaluators/spec.py:124-142
    def enforce_one_key(cls, value: str | dict[str, Any]) -> Any:
        """Enforce that the root value has exactly one key (the evaluator name) when it is a dict.

        Args:
            value: The value to validate.

        Returns:
            The validated value.

        Raises:
            ValueError: If the value is a dict with multiple keys.
        """
        if isinstance(value, str):
            return value
        if len(value) != 1:
            raise ValueError(
                f'Expected a single key containing the Evaluator class name, found keys {list(value.keys())}'
            )
        return value

# pydantic_ai_slim/pydantic_ai/messages.py:1223-1223
    provider_details: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/models/__init__.py:928-928
    provider_details: dict[str, Any] | None = field(default=None, init=False)

# pydantic_ai_slim/pydantic_ai/messages.py:1628-1628
    provider_details: ProviderDetailsDelta = None

# pydantic_ai_slim/pydantic_ai/messages.py:1743-1743
    provider_details: dict[str, Any] | None = None

# pydantic_evals/pydantic_evals/evaluators/report_common.py:32-32
    expected_key: str | None = None

# pydantic_evals/pydantic_evals/evaluators/report_common.py:93-93
    positive_key: str | None = None

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:179-192
_STRICT_INCOMPATIBLE_KEYS = [
    'minLength',
    'maxLength',
    'patternProperties',
    'unevaluatedProperties',
    'propertyNames',
    'minProperties',
    'maxProperties',
    'unevaluatedItems',
    'contains',
    'minContains',
    'maxContains',
    'uniqueItems',
]

# tests/providers/test_litellm.py:25-28
def test_init_without_api_key():
    provider = LiteLLMProvider()
    assert provider.name == 'litellm'
    assert provider.client.api_key == 'litellm-placeholder'

# pydantic_ai_slim/pydantic_ai/models/gemini.py:908-910
    tool_use_prompt_tokens_details: NotRequired[
        Annotated[list[_GeminiModalityTokenCount], pydantic.Field(alias='toolUsePromptTokensDetails')]
    ]

# pydantic_evals/pydantic_evals/evaluators/report_common.py:29-29
    predicted_key: str | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:917-917
    provider_details: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/tools.py:491-491
    outer_typed_dict_key: str | None = None

# tests/test_usage_limits.py:258-284
def test_add_usages_with_none_detail_value():
    """Test that None values in details are skipped when incrementing usage."""
    usage = RunUsage(
        requests=1,
        input_tokens=10,
        output_tokens=20,
        details={'reasoning_tokens': 5},
    )

    # Create a usage with None in details (simulating model response with missing detail)
    incr_usage = RunUsage(
        requests=1,
        input_tokens=5,
        output_tokens=10,
    )
    # Manually set a None value in details to simulate edge case from model responses
    incr_usage.details = {'reasoning_tokens': None, 'other_tokens': 10}  # type: ignore[dict-item]

    result = usage + incr_usage
    assert result == snapshot(
        RunUsage(
            requests=2,
            input_tokens=15,
            output_tokens=30,
            details={'reasoning_tokens': 5, 'other_tokens': 10},
        )
    )

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:613-613
    reasoning_details: list[_OpenRouterReasoningDetail] | None = None

# tests/providers/test_alibaba_provider.py:26-29
def test_alibaba_provider_env_key(env: TestEnv):
    env.set('ALIBABA_API_KEY', 'env-key')
    provider = AlibabaProvider()
    assert provider.client.api_key == 'env-key'

# examples/pydantic_ai_examples/flight_booking.py:70-75
async def extract_flights(ctx: RunContext[Deps]) -> list[FlightDetails]:
    """Get details of all flights."""
    # we pass the usage to the search agent so requests within this agent are counted
    result = await extraction_agent.run(ctx.deps.web_page_text, usage=ctx.usage)
    logfire.info('found {flight_count} flights', flight_count=len(result.output))
    return result.output

# tests/providers/test_xai.py:24-33
def test_xai_provider_need_api_key(env: TestEnv) -> None:
    env.remove('XAI_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `XAI_API_KEY` environment variable or pass it via `XaiProvider(api_key=...)`'
            'to use the xAI provider.'
        ),
    ):
        XaiProvider()

# tests/graph/beta/test_joins_and_reducers.py:235-261
async def test_reduce_dict_update_with_overlapping_keys():
    """Test that reduce_dict_update properly handles overlapping keys (later values win)."""
    g = GraphBuilder(state_type=SimpleState, output_type=dict[str, int])

    @g.step
    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:
        return [1, 2, 3]

    @g.step
    async def create_dict(ctx: StepContext[SimpleState, None, int]) -> dict[str, int]:
        # All create the same key
        return {'key': ctx.inputs}

    dict_join = g.join(reduce_dict_update, initial_factory=dict[str, int])

    g.add(
        g.edge_from(g.start_node).to(generate),
        g.edge_from(generate).map().to(create_dict),
        g.edge_from(create_dict).to(dict_join),
        g.edge_from(dict_join).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=SimpleState())
    # One of the values should win (1, 2, or 3)
    assert 'key' in result
    assert result['key'] in [1, 2, 3]

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:390-390
    reasoning_details: list[_OpenRouterReasoningDetail] | None = None

# pydantic_ai_slim/pydantic_ai/_output.py:533-533
    outer_typed_dict_key: str | None = None

# tests/providers/test_grok.py:35-44
def test_grok_provider_need_api_key(env: TestEnv) -> None:
    env.remove('GROK_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `GROK_API_KEY` environment variable or pass it via `GrokProvider(api_key=...)`'
            'to use the Grok provider.'
        ),
    ):
        GrokProvider()

# tests/providers/test_groq.py:39-48
def test_groq_provider_need_api_key(env: TestEnv) -> None:
    env.remove('GROQ_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `GROQ_API_KEY` environment variable or pass it via `GroqProvider(api_key=...)`'
            'to use the Groq provider.'
        ),
    ):
        GroqProvider()

# tests/providers/test_sambanova_provider.py:26-29
def test_sambanova_provider_env_key(env: TestEnv):
    env.set('SAMBANOVA_API_KEY', 'env-key')
    provider = SambaNovaProvider()
    assert provider.client.api_key == 'env-key'

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:468-487
def _map_openrouter_provider_details(
    response: _OpenRouterChatCompletion | _OpenRouterChatCompletionChunk,
) -> dict[str, Any]:
    provider_details: dict[str, Any] = {}

    provider_details['downstream_provider'] = response.provider
    if native_finish_reason := response.choices[0].native_finish_reason:
        provider_details['finish_reason'] = native_finish_reason

    if usage := response.usage:
        if cost := usage.cost:
            provider_details['cost'] = cost

        if cost_details := usage.cost_details:
            provider_details['upstream_inference_cost'] = cost_details.upstream_inference_cost

        if (is_byok := usage.is_byok) is not None:
            provider_details['is_byok'] = is_byok

    return provider_details

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:569-574
    def _process_provider_details(self, response: chat.ChatCompletion) -> dict[str, Any] | None:
        assert isinstance(response, _OpenRouterChatCompletion)

        provider_details = super()._process_provider_details(response) or {}
        provider_details.update(_map_openrouter_provider_details(response))
        return provider_details or None

# tests/providers/test_alibaba_provider.py:47-51
def test_alibaba_provider_missing_key(env: TestEnv):
    env.remove('ALIBABA_API_KEY')
    env.remove('DASHSCOPE_API_KEY')
    with pytest.raises(UserError, match='Set the `ALIBABA_API_KEY`'):
        AlibabaProvider()

# tests/providers/test_cohere.py:28-31
def test_cohere_provider_need_api_key(env: TestEnv) -> None:
    env.remove('CO_API_KEY')
    with pytest.raises(UserError, match='CO_API_KEY'):
        CohereProvider()

# tests/providers/test_github.py:34-43
def test_github_provider_need_api_key(env: TestEnv) -> None:
    env.remove('GITHUB_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `GITHUB_API_KEY` environment variable or pass it via `GitHubProvider(api_key=...)`'
            ' to use the GitHub Models provider.'
        ),
    ):
        GitHubProvider()

# tests/providers/test_heroku.py:34-43
def test_heroku_provider_need_api_key(env: TestEnv) -> None:
    env.remove('HEROKU_INFERENCE_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `HEROKU_INFERENCE_KEY` environment variable or pass it via `HerokuProvider(api_key=...)`'
            'to use the Heroku provider.'
        ),
    ):
        HerokuProvider()

# tests/providers/test_nebius.py:41-50
def test_nebius_provider_need_api_key(env: TestEnv) -> None:
    env.remove('NEBIUS_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `NEBIUS_API_KEY` environment variable or pass it via '
            '`NebiusProvider(api_key=...)` to use the Nebius AI Studio provider.'
        ),
    ):
        NebiusProvider()

# tests/providers/test_vercel.py:41-51
def test_vercel_provider_need_api_key(env: TestEnv) -> None:
    env.remove('VERCEL_AI_GATEWAY_API_KEY')
    env.remove('VERCEL_OIDC_TOKEN')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `VERCEL_AI_GATEWAY_API_KEY` or `VERCEL_OIDC_TOKEN` environment variable '
            'or pass the API key via `VercelProvider(api_key=...)` to use the Vercel provider.'
        ),
    ):
        VercelProvider()

# tests/models/test_openrouter.py:487-507
async def test_openrouter_no_openrouter_details(openrouter_api_key: str) -> None:
    """Test _process_provider_details when _map_openrouter_provider_details returns empty dict."""
    from unittest.mock import patch

    provider = OpenRouterProvider(api_key=openrouter_api_key)
    model = OpenRouterModel('google/gemini-2.0-flash-exp:free', provider=provider)

    choice = Choice.model_construct(
        index=0, message={'role': 'assistant', 'content': 'test'}, finish_reason='stop', native_finish_reason='stop'
    )
    response = ChatCompletion.model_construct(
        id='test', choices=[choice], created=1704067200, object='chat.completion', model='test', provider='TestProvider'
    )

    with patch('pydantic_ai.models.openrouter._map_openrouter_provider_details', return_value={}):
        result = model._process_response(response)  # type: ignore[reportPrivateUsage]

    # With empty openrouter_details, we should still get the parent's provider_details (timestamp + finish_reason)
    assert result.provider_details == snapshot(
        {'finish_reason': 'stop', 'timestamp': datetime.datetime(2024, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)}
    )

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:687-692
    def _map_provider_details(self, chunk: chat.ChatCompletionChunk) -> dict[str, Any] | None:
        assert isinstance(chunk, _OpenRouterChatCompletionChunk)

        provider_details = super()._map_provider_details(chunk) or {}
        provider_details.update(_map_openrouter_provider_details(chunk))
        return provider_details or None

# examples/pydantic_ai_examples/flight_booking.py:35-35
    date: datetime.date

# tests/providers/test_mistral.py:29-38
def test_mistral_provider_need_api_key(env: TestEnv) -> None:
    env.remove('MISTRAL_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `MISTRAL_API_KEY` environment variable or pass it via `MistralProvider(api_key=...)`'
            'to use the Mistral provider.'
        ),
    ):
        MistralProvider()

# tests/models/test_xai.py:128-134
def test_xai_init_with_fixture_api_key(xai_api_key: str):
    """Test that xai_api_key fixture is properly used."""
    provider = XaiProvider(api_key=xai_api_key)
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=provider)

    assert m.model_name == XAI_NON_REASONING_MODEL
    assert m.system == 'xai'

# pydantic_ai_slim/pydantic_ai/models/openai.py:348-348
    openai_prompt_cache_key: str

# tests/providers/test_cerebras.py:37-46
def test_cerebras_provider_need_api_key(env: TestEnv) -> None:
    env.remove('CEREBRAS_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `CEREBRAS_API_KEY` environment variable or pass it via `CerebrasProvider(api_key=...)` '
            'to use the Cerebras provider.'
        ),
    ):
        CerebrasProvider()

# tests/providers/test_ovhcloud.py:39-48
def test_ovhcloud_provider_need_api_key(env: TestEnv) -> None:
    env.remove('OVHCLOUD_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `OVHCLOUD_API_KEY` environment variable or pass it via '
            '`OVHcloudProvider(api_key=...)` to use OVHcloud AI Endpoints provider.'
        ),
    ):
        OVHcloudProvider()

# tests/providers/test_sambanova_provider.py:32-35
def test_sambanova_provider_missing_key(env: TestEnv):
    env.remove('SAMBANOVA_API_KEY')
    with pytest.raises(UserError, match='Set the `SAMBANOVA_API_KEY`'):
        SambaNovaProvider()

# tests/providers/test_voyageai.py:25-28
def test_voyageai_provider_need_api_key(env: TestEnv) -> None:
    env.remove('VOYAGE_API_KEY')
    with pytest.raises(UserError, match='VOYAGE_API_KEY'):
        VoyageAIProvider()

# examples/pydantic_ai_examples/flight_booking.py:32-32
    price: int

# tests/providers/test_deepseek.py:28-37
def test_deep_seek_provider_need_api_key(env: TestEnv) -> None:
    env.remove('DEEPSEEK_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `DEEPSEEK_API_KEY` environment variable or pass it via `DeepSeekProvider(api_key=...)`'
            'to use the DeepSeek provider.'
        ),
    ):
        DeepSeekProvider()

# tests/providers/test_fireworks.py:39-48
def test_fireworks_provider_need_api_key(env: TestEnv) -> None:
    env.remove('FIREWORKS_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `FIREWORKS_API_KEY` environment variable or pass it via `FireworksProvider(api_key=...)`'
            'to use the Fireworks AI provider.'
        ),
    ):
        FireworksProvider()

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:85-100
    def compute_key(
        self,
        task_ctx: TaskRunContext,
        inputs: dict[str, Any],
        flow_parameters: dict[str, Any],
        **kwargs: Any,
    ) -> str | None:
        """Compute cache key from inputs with timestamps removed and RunContext serialized."""
        if not inputs:
            return None

        inputs_without_toolsets = _replace_toolsets(inputs)
        inputs_with_hashable_context = _replace_run_context(inputs_without_toolsets)
        filtered_inputs = _strip_timestamps(inputs_with_hashable_context)

        return INPUTS.compute_key(task_ctx, filtered_inputs, flow_parameters, **kwargs)

# pydantic_evals/pydantic_evals/reporting/__init__.py:736-742
    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:578-578
        reasoning_details: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])

# tests/providers/test_openrouter.py:60-69
def test_openrouter_provider_need_api_key(env: TestEnv) -> None:
    env.remove('OPENROUTER_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `OPENROUTER_API_KEY` environment variable or pass it via `OpenRouterProvider(api_key=...)`'
            'to use the OpenRouter provider.'
        ),
    ):
        OpenRouterProvider()

# tests/providers/test_moonshotai.py:29-39
def test_moonshotai_provider_need_api_key(env: TestEnv) -> None:
    """Test that MoonshotAI provider requires an API key."""
    env.remove('MOONSHOTAI_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `MOONSHOTAI_API_KEY` environment variable or pass it via `MoonshotAIProvider(api_key=...)`'
            ' to use the MoonshotAI provider.'
        ),
    ):
        MoonshotAIProvider()

# pydantic_ai_slim/pydantic_ai/models/gemini.py:452-454
    async def headers(self) -> dict[str, str]:
        # https://cloud.google.com/docs/authentication/api-keys-use#using-with-rest
        return {'X-Goog-Api-Key': self.api_key}  # pragma: no cover

# examples/pydantic_ai_examples/flight_booking.py:33-33
    origin: str = Field(description='Three-letter airport code')

# tests/models/test_xai.py:3246-3261
async def test_xai_usage_without_details(allow_model_requests: None):
    """Test that xAI model handles usage without reasoning_tokens or cached tokens."""
    mock_usage = create_usage(prompt_tokens=20, completion_tokens=10)
    response = create_response(
        content='Simple answer',
        usage=mock_usage,
    )
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    result = await agent.run('Simple question')
    assert result.output == 'Simple answer'

    # Verify usage without details (empty dict when no additional usage info)
    assert result.usage() == snapshot(RunUsage(input_tokens=20, output_tokens=10, requests=1))

# tests/providers/test_huggingface.py:37-46
def test_huggingface_provider_need_api_key(env: TestEnv) -> None:
    env.remove('HF_TOKEN')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `HF_TOKEN` environment variable or pass it via `HuggingFaceProvider(api_key=...)`'
            'to use the HuggingFace provider.'
        ),
    ):
        HuggingFaceProvider()

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:426-429
class _OpenRouterPromptTokenDetails(completion_usage.PromptTokensDetails):
    """Wraps OpenAI completion token details with OpenRouter specific attributes."""

    video_tokens: int | None = None

# tests/providers/test_alibaba_provider.py:32-36
def test_alibaba_provider_dashscope_env_key(env: TestEnv):
    env.remove('ALIBABA_API_KEY')
    env.set('DASHSCOPE_API_KEY', 'dashscope-key')
    provider = AlibabaProvider()
    assert provider.client.api_key == 'dashscope-key'

# tests/providers/test_alibaba_provider.py:39-44
def test_alibaba_provider_env_key_precedence(env: TestEnv):
    # ALIBABA_API_KEY takes precedence over DASHSCOPE_API_KEY
    env.set('ALIBABA_API_KEY', 'alibaba-key')
    env.set('DASHSCOPE_API_KEY', 'dashscope-key')
    provider = AlibabaProvider()
    assert provider.client.api_key == 'alibaba-key'

# tests/evals/test_evaluator_spec.py:113-127
def test_evaluator_spec_with_non_string_keys():
    """Test EvaluatorSpec with non-string keys in arguments."""
    # Test with non-string keys in dict
    spec = _SerializedEvaluatorSpec.model_validate({'TestEvaluator': {1: 'value', 2: 'value2'}})
    assert spec.to_evaluator_spec().name == 'TestEvaluator'
    assert spec.to_evaluator_spec().arguments == (
        {1: 'value', 2: 'value2'},
    )  # Should be treated as a single positional argument

    # Test with mixed keys
    spec = _SerializedEvaluatorSpec.model_validate({'TestEvaluator': {'key': 'value', 1: 'value2'}})
    assert spec.to_evaluator_spec().name == 'TestEvaluator'
    assert spec.to_evaluator_spec().arguments == (
        {'key': 'value', 1: 'value2'},
    )  # Should be treated as a single positional argument

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:14-14
from typing_inspection.introspection import get_literal_values

# tests/evals/test_report_evaluators.py:236-243
def test_confusion_matrix_labels_requires_key():
    evaluator = ConfusionMatrixEvaluator(predicted_from='labels', predicted_key=None)
    cases = [_make_report_case('c1', expected_output='a', labels={})]
    report = _make_report(cases)
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)

    with pytest.raises(ValueError, match="'key' is required"):
        evaluator.evaluate(ctx)

# tests/evals/test_report_evaluators.py:326-338
def test_precision_recall_labels_requires_key():
    evaluator = PrecisionRecallEvaluator(
        score_from='scores',
        score_key='s',
        positive_from='labels',
        positive_key=None,
    )
    cases = [_make_report_case('c1', scores={'s': 0.5})]
    report = _make_report(cases)
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)

    with pytest.raises(ValueError, match="'positive_key' is required"):
        evaluator.evaluate(ctx)

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:432-435
class _OpenRouterCompletionTokenDetails(completion_usage.CompletionTokensDetails):
    """Wraps OpenAI completion token details with OpenRouter specific attributes."""

    image_tokens: int | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:224-241
def _incr_usage_tokens(slf: RunUsage | RequestUsage, incr_usage: RunUsage | RequestUsage) -> None:
    """Increment the usage in place.

    Args:
        slf: The usage to increment.
        incr_usage: The usage to increment by.
    """
    slf.input_tokens += incr_usage.input_tokens
    slf.cache_write_tokens += incr_usage.cache_write_tokens
    slf.cache_read_tokens += incr_usage.cache_read_tokens
    slf.input_audio_tokens += incr_usage.input_audio_tokens
    slf.cache_audio_read_tokens += incr_usage.cache_audio_read_tokens
    slf.output_tokens += incr_usage.output_tokens

    for key, value in incr_usage.details.items():
        # Note: value can be None at runtime from model responses despite the type annotation
        if isinstance(value, (int, float)):
            slf.details[key] = slf.details.get(key, 0) - value

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter.py:41-41
from ._utils import dump_provider_metadata, load_provider_metadata

# tests/providers/test_huggingface.py:80-83
def test_huggingface_provider_init_api_key_error(monkeypatch: pytest.MonkeyPatch):
    monkeypatch.delenv('HF_TOKEN', raising=False)
    with pytest.raises(UserError, match='Set the `HF_TOKEN` environment variable'):
        HuggingFaceProvider()

# tests/test_exceptions.py:77-87
def test_tool_retry_error_str_with_error_details():
    """Test that ToolRetryError formats ErrorDetails automatically."""
    validation_error = ValidationError.from_exception_data(
        'Test', [{'type': 'string_type', 'loc': ('name',), 'input': 123}]
    )
    part = RetryPromptPart(content=validation_error.errors(include_url=False), tool_name='my_tool')
    error = ToolRetryError(part)

    assert str(error) == (
        "1 validation error for 'my_tool'\nname\n  Input should be a valid string [type=string_type, input_value=123]"
    )

# tests/evals/test_report_evaluators.py:311-323
def test_precision_recall_assertions_requires_key():
    evaluator = PrecisionRecallEvaluator(
        score_from='scores',
        score_key='s',
        positive_from='assertions',
        positive_key=None,
    )
    cases = [_make_report_case('c1', scores={'s': 0.5})]
    report = _make_report(cases)
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)

    with pytest.raises(ValueError, match="'positive_key' is required"):
        evaluator.evaluate(ctx)

# pydantic_evals/pydantic_evals/reporting/__init__.py:884-890
    def _get_value_str(self, value: float | int | None) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# pydantic_ai_slim/pydantic_ai/models/openai.py:2314-2319
    def _map_provider_details(self, chunk: ChatCompletionChunk) -> dict[str, Any] | None:
        """Hook that generates the provider details from chunk content.

        This method may be overridden by subclasses of `OpenAIStreamResponse` to customize the provider details.
        """
        return _map_provider_details(chunk.choices[0])

# tests/providers/test_gateway.py:54-62
def test_init_gateway_without_api_key_raises_error(env: TestEnv):
    env.remove('PYDANTIC_AI_GATEWAY_API_KEY')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `PYDANTIC_AI_GATEWAY_API_KEY` environment variable or pass it via `gateway_provider(..., api_key=...)` to use the Pydantic AI Gateway provider.'
        ),
    ):
        gateway_provider('openai')

# tests/providers/test_huggingface.py:139-142
def test_huggingface_provider_init_api_key_is_none(monkeypatch: pytest.MonkeyPatch):
    monkeypatch.delenv('HF_TOKEN', raising=False)
    with pytest.raises(UserError):
        HuggingFaceProvider(api_key=None)

# tests/providers/test_huggingface.py:87-92
def test_huggingface_provider_init_api_key_from_env(
    MockAsyncInferenceClient: MagicMock, monkeypatch: pytest.MonkeyPatch
):
    monkeypatch.setenv('HF_TOKEN', 'env-key')
    HuggingFaceProvider()
    MockAsyncInferenceClient.assert_called_with(api_key='env-key', provider=None, base_url=None)

# tests/providers/test_huggingface.py:96-101
def test_huggingface_provider_init_api_key_from_arg(
    MockAsyncInferenceClient: MagicMock, monkeypatch: pytest.MonkeyPatch
):
    monkeypatch.setenv('HF_TOKEN', 'env-key')
    HuggingFaceProvider(api_key='arg-key')
    MockAsyncInferenceClient.assert_called_with(api_key='arg-key', provider=None, base_url=None)

# tests/test_parts_manager.py:631-646
def test_handle_thinking_delta_provider_details_callback():
    """Test that provider_details can be a callback function."""
    manager = ModelResponsePartsManager()

    # Create initial part with provider_details
    list(manager.handle_thinking_delta(vendor_part_id='t', content='initial', provider_details={'count': 1}))

    # Update using callback to modify provider_details
    def update_details(existing: dict[str, Any] | None) -> dict[str, Any]:
        details = dict(existing or {})
        details['count'] = details.get('count', 0) + 1
        return details

    list(manager.handle_thinking_delta(vendor_part_id='t', content=' more', provider_details=update_details))

    assert manager.get_parts() == snapshot([ThinkingPart(content='initial more', provider_details={'count': 2})])

# examples/pydantic_ai_examples/flight_booking.py:34-34
    destination: str = Field(description='Three-letter airport code')

# tests/providers/test_openai.py:33-39
def test_init_of_openai_without_api_key_raises_error(env: TestEnv):
    env.remove('OPENAI_API_KEY')
    with pytest.raises(
        OpenAIError,
        match='^The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable$',
    ):
        OpenAIProvider()

# tests/providers/test_openai.py:23-25
def test_init_with_no_api_key_will_still_setup_client():
    provider = OpenAIProvider(base_url='http://localhost:19434/v1')
    assert provider.base_url == 'http://localhost:19434/v1/'

# tests/test_vercel_ai.py:4436-4450
    async def test_dump_provider_metadata_wrapper_key(self):
        """Test that dump_provider_metadata includes the wrapper key."""

        result = dump_provider_metadata(
            wrapper_key='test', id='test_id', provider_name='test_provider', provider_details={'test_detail': 1}
        )
        assert result == {
            'test': {'id': 'test_id', 'provider_name': 'test_provider', 'provider_details': {'test_detail': 1}}
        }

        # Test with None wrapper key
        result = dump_provider_metadata(
            None, id='test_id', provider_name='test_provider', provider_details={'test_detail': 1}
        )
        assert result == {'id': 'test_id', 'provider_name': 'test_provider', 'provider_details': {'test_detail': 1}}