# pydantic_evals/pydantic_evals/reporting/__init__.py:1261-1275
    def _render_dict(
        self,
        case_dict: Mapping[str, EvaluationResult[T] | T],
        renderers: Mapping[str, _AbstractRenderer[T]],
        *,
        include_names: bool = True,
    ) -> str:
        diff_lines: list[str] = []
        for key, val in case_dict.items():
            value = cast(EvaluationResult[T], val).value if isinstance(val, EvaluationResult) else val
            rendered = renderers[key].render_value(key if include_names else None, value)
            if self.include_reasons and isinstance(val, EvaluationResult) and (reason := val.reason):
                rendered += f'\n  Reason: {reason}\n'
            diff_lines.append(rendered)
        return '\n'.join(diff_lines) if diff_lines else EMPTY_CELL_STR

# pydantic_evals/pydantic_evals/reporting/__init__.py:1243-1259
    def _render_dicts_diff(
        baseline_dict: dict[str, T],
        new_dict: dict[str, T],
        renderers: Mapping[str, _AbstractRenderer[T]],
        *,
        include_names: bool = True,
    ) -> str:
        keys: set[str] = set()
        keys.update(baseline_dict.keys())
        keys.update(new_dict.keys())
        diff_lines: list[str] = []
        for key in sorted(keys):
            old_val = baseline_dict.get(key)
            new_val = new_dict.get(key)
            rendered = renderers[key].render_diff(key if include_names else None, old_val, new_val)
            diff_lines.append(rendered)
        return '\n'.join(diff_lines) if diff_lines else EMPTY_CELL_STR

# tests/models/test_openai.py:4153-4168
def test_azure_400_non_dict_body(allow_model_requests: None) -> None:
    """Test a 400 error from Azure where the body is not a dictionary."""
    mock_client = MockOpenAI.create_mock(
        APIStatusError(
            'Bad Request',
            response=httpx.Response(status_code=400, request=httpx.Request('POST', 'https://example.com/v1')),
            body='Raw string body',
        )
    )
    m = OpenAIChatModel('gpt-5-mini', provider=AzureProvider(openai_client=cast(AsyncAzureOpenAI, mock_client)))
    agent = Agent(m)

    with pytest.raises(ModelHTTPError) as exc_info:
        agent.run_sync('hello')

    assert exc_info.value.status_code == 400

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:11-12
def _is_dict(obj: Any) -> TypeGuard[dict[str, Any]]:
    return isinstance(obj, dict)

# tests/mcp_server.py:146-147
async def get_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_utils.py:31-35
from typing_extensions import (
    ParamSpec,
    TypeIs,
    is_typeddict,
)

# tests/test_fastmcp.py:596-603
    async def test_from_mcp_config_dict(self):
        """Test creating toolset from MCP config dictionary."""

        config_dict = {'mcpServers': {'test_server': {'command': 'python', 'args': ['-c', 'print("test")']}}}

        toolset = FastMCPToolset(config_dict)
        client = toolset.client
        assert isinstance(client.transport, MCPConfigTransport)

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# tests/typed_agent.py:176-182
structured_dict = StructuredDict(
    {
        'type': 'object',
        'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}},
        'required': ['name', 'age'],
    }
)

# tests/models/test_openai.py:1700-1701
class MyNormalTypedDict(TypedDict):
    foo: str

# examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py:27-45
async def document_predict_state() -> list[CustomEvent]:
    """Enable document state prediction.

    Returns:
        CustomEvent containing the event to enable state prediction.
    """
    return [
        CustomEvent(
            type=EventType.CUSTOM,
            name='PredictState',
            value=[
                {
                    'state_key': 'document',
                    'tool': 'write_document',
                    'tool_argument': 'document',
                },
            ],
        ),
    ]

# tests/graph/beta/test_joins_and_reducers.py:84-107
async def test_reduce_dict_update():
    """Test reduce_dict_update that merges dictionaries."""
    g = GraphBuilder(state_type=SimpleState, output_type=dict[str, int])

    @g.step
    async def generate_keys(ctx: StepContext[SimpleState, None, None]) -> list[str]:
        return ['a', 'b', 'c']

    @g.step
    async def create_dict(ctx: StepContext[SimpleState, None, str]) -> dict[str, int]:
        return {ctx.inputs: len(ctx.inputs)}

    dict_join = g.join(reduce_dict_update, initial_factory=dict[str, int])

    g.add(
        g.edge_from(g.start_node).to(generate_keys),
        g.edge_from(generate_keys).map().to(create_dict),
        g.edge_from(create_dict).to(dict_join),
        g.edge_from(dict_join).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=SimpleState())
    assert result == {'a': 1, 'b': 1, 'c': 1}

# pydantic_graph/pydantic_graph/beta/join.py:118-121
def reduce_dict_update(current: dict[K, V], inputs: Mapping[K, V]) -> dict[K, V]:
    """A reducer that updates a dict."""
    current.update(inputs)
    return current

# pydantic_ai_slim/pydantic_ai/messages.py:1230-1241
    def args_as_dict(self) -> dict[str, Any]:
        """Return the arguments as a Python dictionary.

        This is just for convenience with models that require dicts as input.
        """
        if not self.args:
            return {}
        if isinstance(self.args, dict):
            return self.args
        args = pydantic_core.from_json(self.args)
        assert isinstance(args, dict), 'args should be a dict'
        return cast(dict[str, Any], args)

# pydantic_graph/pydantic_graph/beta/join.py:118-121
def reduce_dict_update(current: dict[K, V], inputs: Mapping[K, V]) -> dict[K, V]:
    """A reducer that updates a dict."""
    current.update(inputs)
    return current

# tests/models/test_openai.py:1709-1710
class MyPartialTypedDict(TypedDict, total=False):
    foo: str

# pydantic_evals/pydantic_evals/evaluators/spec.py:56-64
    def kwargs(self) -> dict[str, Any]:
        """Get the keyword arguments for the evaluator.

        Returns:
            A dictionary of keyword arguments if arguments is a dict, otherwise an empty dict.
        """
        if isinstance(self.arguments, dict):
            return self.arguments
        return {}

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:9-9
from weakref import WeakValueDictionary

# tests/models/test_openai.py:1704-1706
class MyOptionalTypedDict(TypedDict):
    foo: NotRequired[str]
    bar: str

# tests/models/test_openai.py:1753-1754
def tool_with_typed_dict(x: MyNormalTypedDict) -> str:
    return f'{x}'  # pragma: no cover

# tests/evals/test_report_evaluators.py:632-651
def test_confusion_matrix_evaluator_metadata_key_with_non_dict():
    """ConfusionMatrixEvaluator with metadata key but non-dict metadata skips the case."""
    cases = [
        _make_report_case('c1', expected_output='A', metadata='some_string'),
        _make_report_case('c2', expected_output='B', metadata={'pred': 'B'}),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='metadata',
        predicted_key='pred',
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    # c1 should be skipped (non-dict metadata with key), only c2 used
    assert result.class_labels == ['B']
    assert result.matrix == [[1]]

# tests/mcp_server.py:151-152
async def get_unstructured_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

# tests/typed_agent.py:183-183
structured_dict_agent = Agent(output_type=structured_dict)

# tests/evals/test_evaluator_common.py:93-117
async def test_contains_dict():
    """Test Contains evaluator with dictionaries."""
    evaluator = Contains(value={'key': 'value'})

    # Test dictionary containment
    assert evaluator.evaluate(MockContext(output={'key': 'value', 'extra': 'data'})) == snapshot(
        EvaluationReason(value=True)
    )

    # Test dictionary key missing
    assert evaluator.evaluate(MockContext(output={'different': 'value'})) == snapshot(
        EvaluationReason(value=False, reason="Output dictionary does not contain expected key 'key'")
    )

    # Test dictionary value mismatch
    assert evaluator.evaluate(MockContext(output={'key': 'different'})) == snapshot(
        EvaluationReason(
            value=False,
            reason="Output dictionary has different value for key 'key': 'different' != 'value'",
        )
    )

    # Test non-dict value in dict
    evaluator_single = Contains(value='key')
    assert evaluator_single.evaluate(MockContext(output={'key': 'value'})) == snapshot(EvaluationReason(value=True))

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:549-556
    def event_to_dict(event: LogRecord) -> dict[str, Any]:
        if not event.body:
            body = {}  # pragma: no cover
        elif isinstance(event.body, Mapping):
            body = event.body
        else:
            body = {'body': event.body}
        return {**body, **(event.attributes or {})}

# pydantic_evals/pydantic_evals/evaluators/report_common.py:29-29
    predicted_key: str | None = None

# pydantic_evals/pydantic_evals/evaluators/report_common.py:28-28
    predicted_from: Literal['expected_output', 'output', 'metadata', 'labels'] = 'output'

# pydantic_ai_slim/pydantic_ai/tools.py:491-491
    outer_typed_dict_key: str | None = None

# examples/pydantic_ai_examples/ag_ui/__init__.py:18-25
from .api import (
    agentic_chat_app,
    agentic_generative_ui_app,
    human_in_the_loop_app,
    predictive_state_updates_app,
    shared_state_app,
    tool_based_generative_ui_app,
)

# examples/pydantic_ai_examples/ag_ui/__init__.py:18-25
from .api import (
    agentic_chat_app,
    agentic_generative_ui_app,
    human_in_the_loop_app,
    predictive_state_updates_app,
    shared_state_app,
    tool_based_generative_ui_app,
)

# tests/models/test_openai.py:1761-1762
def tool_with_partial_typed_dict(x: MyPartialTypedDict) -> str:
    return f'{x}'  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/tools.py:239-252
    def typed_dict_schema(self, schema: core_schema.TypedDictSchema) -> JsonSchemaValue:
        json_schema = super().typed_dict_schema(schema)
        # Workaround for https://github.com/pydantic/pydantic/issues/12123
        if 'additionalProperties' not in json_schema:  # pragma: no branch
            extra = schema.get('extra_behavior') or schema.get('config', {}).get('extra_fields_behavior')
            if extra == 'allow':
                extras_schema = schema.get('extras_schema', None)
                if extras_schema is not None:
                    json_schema['additionalProperties'] = self.generate_inner(extras_schema) or True
                else:
                    json_schema['additionalProperties'] = True  # pragma: no cover
            elif extra == 'forbid':
                json_schema['additionalProperties'] = False
        return json_schema

# pydantic_ai_slim/pydantic_ai/models/openai.py:342-342
    openai_prediction: ChatCompletionPredictionContentParam

# tests/models/test_openai.py:1757-1758
def tool_with_optional_typed_dict(x: MyOptionalTypedDict) -> str:
    return f'{x}'  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/_output.py:533-533
    outer_typed_dict_key: str | None = None

# tests/test_tools.py:708-711
def test_tool_call_part_args_as_dict(args: str | dict[str, Any], expected: dict[str, Any]):
    part = ToolCallPart(tool_name='foo', args=args)
    result = part.args_as_dict()
    assert result == expected

# pydantic_evals/pydantic_evals/evaluators/common.py:138-147
    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> EvaluationReason:
        output = ctx.output
        for cls in type(output).__mro__:
            if cls.__name__ == self.type_name or cls.__qualname__ == self.type_name:
                return EvaluationReason(value=True)

        reason = f'output is of type {type(output).__name__}'
        if type(output).__qualname__ != type(output).__name__:
            reason += f' (qualname: {type(output).__qualname__})'
        return EvaluationReason(value=False, reason=reason)

# tests/test_agent.py:2950-2955
async def test_agent_run_metadata_kwarg_dict() -> None:
    agent = Agent(TestModel(custom_output_text='kwarg dict output'))

    result = await agent.run('kwarg dict prompt', metadata={'env': 'run'})

    assert result.metadata == {'env': 'run'}

# pydantic_evals/pydantic_evals/evaluators/common.py:135-135
    type_name: str

# tests/models/test_groq.py:503-503
    first: str

# tests/models/test_groq.py:503-503
    first: str

# tests/models/test_groq.py:503-503
    first: str

# tests/models/test_groq.py:503-503
    first: str

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:1207-1214
    def is_call_tools_node(
        node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],
    ) -> TypeIs[_agent_graph.CallToolsNode[T, S]]:
        """Check if the node is a `CallToolsNode`, narrowing the type if it is.

        This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
        """
        return isinstance(node, _agent_graph.CallToolsNode)

# tests/models/test_groq.py:504-504
    second: str

# tests/models/test_groq.py:504-504
    second: str

# tests/models/test_groq.py:504-504
    second: str

# tests/models/test_openai.py:1701-1701
    foo: str

# tests/models/test_groq.py:504-504
    second: str

# tests/models/test_openai.py:1710-1710
    foo: str