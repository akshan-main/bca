## docs/.hooks/algolia.py

MAX_CONTENT_LENGTH = 90_000

## examples/pydantic_ai_examples/data_analyst.py

class AnalystAgentDeps:
    output: dict[str, pd.DataFrame] = field(default_factory=dict[str, pd.DataFrame])

    def store(self, value: pd.DataFrame) -> str:
        """Store the output in deps and return the reference such as Out[1] to be used by the LLM."""
        ref = f'Out[{len(self.output) + 1}]'
        self.output[ref] = value
        return ref

    def get(self, ref: str) -> pd.DataFrame:
        if ref not in self.output:
            raise ModelRetry(
                f'Error: {ref} is not a valid variable reference. Check the previous messages and try again.'
            )
        return self.output[ref]

    def get(self, ref: str) -> pd.DataFrame:
        if ref not in self.output:
            raise ModelRetry(
                f'Error: {ref} is not a valid variable reference. Check the previous messages and try again.'
            )
        return self.output[ref]

## examples/pydantic_ai_examples/question_graph.py

from pydantic_graph import (
    BaseNode,
    End,
    Graph,
    GraphRunContext,
)

class Ask(BaseNode[QuestionState]):
    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:
        result = await ask_agent.run(
            'Ask a simple question with a single correct answer.',
            message_history=ctx.state.ask_agent_messages,
        )
        ctx.state.ask_agent_messages += result.all_messages()
        ctx.state.question = result.output
        return Answer(result.output)

    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:
        result = await ask_agent.run(
            'Ask a simple question with a single correct answer.',
            message_history=ctx.state.ask_agent_messages,
        )
        ctx.state.ask_agent_messages += result.all_messages()
        ctx.state.question = result.output
        return Answer(result.output)

## examples/pydantic_ai_examples/stream_whales.py

    length: Annotated[
        float, Field(description='Average length of an adult whale in meters.')
    ]

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    builtin_tools: list[AbstractBuiltinTool | BuiltinToolFunc[DepsT]] = dataclasses.field(repr=False)

## pydantic_ai_slim/pydantic_ai/_cli/__init__.py

PROMPT_HISTORY_FILENAME = 'prompt-history.txt'

## pydantic_ai_slim/pydantic_ai/builtin_tools.py

DEPRECATED_BUILTIN_TOOLS: frozenset[type[AbstractBuiltinTool]] = frozenset({UrlContextTool})  # pyright: ignore[reportDeprecated]

SUPPORTED_BUILTIN_TOOLS = frozenset(cls for cls in BUILTIN_TOOL_TYPES.values() if cls not in DEPRECATED_BUILTIN_TOOLS)

BUILTIN_TOOLS_REQUIRING_CONFIG: frozenset[type[AbstractBuiltinTool]] = frozenset({MCPServerTool, MemoryTool})

## pydantic_ai_slim/pydantic_ai/mcp.py

    tools_list_changed: bool = False

## pydantic_ai_slim/pydantic_ai/models/__init__.py

    builtin_tools: list[AbstractBuiltinTool] = field(default_factory=list[AbstractBuiltinTool])

## pydantic_ai_slim/pydantic_ai/models/openai.py

    violence: _AzureContentFilterResultDetail | None = None

    openai_builtin_tools: Sequence[FileSearchToolParam | WebSearchToolParam | ComputerToolParam]

## pydantic_ai_slim/pydantic_ai/profiles/__init__.py

    supported_builtin_tools: frozenset[type[AbstractBuiltinTool]] = field(
        default_factory=lambda: SUPPORTED_BUILTIN_TOOLS
    )

## pydantic_ai_slim/pydantic_ai/profiles/google.py

    google_supports_native_output_with_builtin_tools: bool = False

## pydantic_ai_slim/pydantic_ai/profiles/grok.py

    grok_supports_builtin_tools: bool = False

## pydantic_ai_slim/pydantic_ai/providers/bedrock.py

def bedrock_amazon_model_profile(model_name: str) -> ModelProfile | None:
    """Get the model profile for an Amazon model used via Bedrock."""
    profile = _without_builtin_tools(amazon_model_profile(model_name))
    if 'nova' in model_name:
        profile = BedrockModelProfile(
            bedrock_supports_tool_choice=True,
            bedrock_supports_prompt_caching=True,
        ).update(profile)

    if 'nova-2' in model_name:
        profile.supported_builtin_tools = frozenset({CodeExecutionTool})

    return profile

def _without_builtin_tools(profile: ModelProfile | None) -> ModelProfile:
    return replace(profile or BedrockModelProfile(), supported_builtin_tools=frozenset())

    def model_profile(self, model_name: str) -> ModelProfile | None:
        provider_to_profile: dict[str, Callable[[str], ModelProfile | None]] = {
            'anthropic': lambda model_name: BedrockModelProfile(
                bedrock_supports_tool_choice=True,
                bedrock_send_back_thinking_parts=True,
                bedrock_supports_prompt_caching=True,
                bedrock_supports_tool_caching=True,
            ).update(_without_builtin_tools(anthropic_model_profile(model_name))),
            'mistral': lambda model_name: BedrockModelProfile(bedrock_tool_result_format='json').update(
                _without_builtin_tools(mistral_model_profile(model_name))
            ),
            'cohere': lambda model_name: _without_builtin_tools(cohere_model_profile(model_name)),
            'amazon': bedrock_amazon_model_profile,
            'meta': lambda model_name: _without_builtin_tools(meta_model_profile(model_name)),
            'deepseek': lambda model_name: _without_builtin_tools(bedrock_deepseek_model_profile(model_name)),
        }

        # Split the model name into parts
        parts = model_name.split('.', 2)

        # Handle regional prefixes
        if len(parts) > 2 and parts[0] in BEDROCK_GEO_PREFIXES:
            parts = parts[1:]

        # required format is provider.model-name-with-version
        if len(parts) < 2:
            return None

        provider = parts[0]
        model_name_with_version = parts[1]

        # Remove version suffix if it matches the format (e.g. "-v1:0" or "-v14")
        version_match = re.match(r'(.+)-v\d+(?::\d+)?$', model_name_with_version)
        if version_match:
            model_name = version_match.group(1)
        else:
            model_name = model_name_with_version

        if provider in provider_to_profile:
            return provider_to_profile[provider](model_name)

        return None

## pydantic_ai_slim/pydantic_ai/ui/_web/api.py

    builtin_tools: list[str]

    builtin_tools: list[BuiltinToolInfo]

    builtin_tools: list[str] = []

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py

    filename: str | None = None

    filename: str | None = None

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py

    filename: str | None = None

## pydantic_graph/pydantic_graph/beta/node_types.py

MiddleNode = TypeAliasType(
    'MiddleNode',
    Step[StateT, DepsT, InputT, OutputT] | Join[StateT, DepsT, InputT, OutputT] | Fork[InputT, OutputT],
    type_params=(StateT, DepsT, InputT, OutputT),
)

## tests/conftest.py

def sanitize_filename(name: str, max_len: int) -> str:
    """Sanitize a string for safe use as a filename across platforms."""
    # Windows does not allow these characters in paths. Linux bans slashes only.
    return re.sub('[' + re.escape('<>:"/\\|?*') + ']', '-', name)[:max_len]

## tests/graph/test_file_persistence.py

class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

## tests/graph/test_graph.py

class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

    input_data: str

## tests/models/test_cohere.py

async def test_cohere_model_builtin_tools(allow_model_requests: None, co_api_key: str):
    m = CohereModel('command-r7b-12-2024', provider=CohereProvider(api_key=co_api_key))
    agent = Agent(m, builtin_tools=[WebSearchTool()])
    with pytest.raises(UserError, match=r"Builtin tool\(s\) \['WebSearchTool'\] not supported by this model"):
        await agent.run('Hello')

## tests/test_agent.py

def test_empty_response_with_finish_reason_length():
    def return_empty_response(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        resp = ModelResponse(parts=[])
        resp.finish_reason = 'length'
        return resp

    agent = Agent(FunctionModel(return_empty_response), output_type=str)

    with pytest.raises(
        UnexpectedModelBehavior,
        match=r'Model token limit \(10\) exceeded before any response was generated.',
    ):
        agent.run_sync('Hello', model_settings=ModelSettings(max_tokens=10))

    with pytest.raises(
        UnexpectedModelBehavior,
        match=r'Model token limit \(provider default\) exceeded before any response was generated.',
    ):
        agent.run_sync('Hello')

def test_agent_builtin_tools_runtime_vs_agent_level():
    """Test that runtime builtin_tools parameter is merged with agent-level builtin_tools."""
    model = TestModel()

    agent = Agent(
        model=model,
        builtin_tools=[
            WebSearchTool(),
            CodeExecutionTool(),
            MCPServerTool(id='deepwiki', url='https://mcp.deepwiki.com/mcp'),
            MCPServerTool(id='github', url='https://api.githubcopilot.com/mcp'),
        ],
    )

    # Runtime tool with same unique ID should override agent-level tool
    with pytest.raises(Exception, match='TestModel does not support built-in tools'):
        agent.run_sync(
            'Hello',
            builtin_tools=[
                WebSearchTool(search_context_size='high'),
                MCPServerTool(id='example', url='https://mcp.example.com/mcp'),
                MCPServerTool(id='github', url='https://mcp.githubcopilot.com/mcp', authorization_token='token'),
            ],
        )

    assert model.last_model_request_parameters is not None
    assert model.last_model_request_parameters.builtin_tools == snapshot(
        [
            WebSearchTool(search_context_size='high'),
            CodeExecutionTool(),
            MCPServerTool(id='deepwiki', url='https://mcp.deepwiki.com/mcp'),
            MCPServerTool(id='github', url='https://mcp.githubcopilot.com/mcp', authorization_token='token'),
            MCPServerTool(id='example', url='https://mcp.example.com/mcp'),
        ]
    )

class UserContext:
    location: str | None

async def test_mixed_static_and_dynamic_builtin_tools():
    model = TestModel()

    static_tool = CodeExecutionTool()
    agent = Agent(model, builtin_tools=[static_tool, prepared_web_search], deps_type=UserContext)

    # Case 1: Dynamic tool returns None
    with pytest.raises(UserError, match='TestModel does not support built-in tools'):
        await agent.run('Hello', deps=UserContext(location=None))

    assert model.last_model_request_parameters is not None
    tools = model.last_model_request_parameters.builtin_tools
    assert len(tools) == 1
    assert tools[0] == static_tool

    # Case 2: Dynamic tool returns a tool
    with pytest.raises(UserError, match='TestModel does not support built-in tools'):
        await agent.run('Hello', deps=UserContext(location='Paris'))

    assert model.last_model_request_parameters is not None
    tools = model.last_model_request_parameters.builtin_tools
    assert len(tools) == 2
    assert tools[0] == static_tool
    dynamic_tool = tools[1]
    assert isinstance(dynamic_tool, WebSearchTool)
    assert dynamic_tool.user_location is not None
    assert dynamic_tool.user_location.get('city') == 'Paris'

## tests/test_fastmcp.py

class TestFastMCPToolsetToolDiscovery:
    """Test FastMCP Toolset tool discovery functionality."""

    async def test_get_tools(
        self,
        fastmcp_client: Client[FastMCPTransport],
        run_context: RunContext[None],
    ):
        """Test getting tools from the FastMCP client."""
        toolset = FastMCPToolset(fastmcp_client)

        async with toolset:
            tools = await toolset.get_tools(run_context)

            # Should have all the tools we defined in the server
            expected_tools = {
                'test_tool',
                'another_tool',
                'audio_tool',
                'error_tool',
                'binary_tool',
                'text_tool',
                'text_list_tool',
                'text_tool_wo_return_annotation',
                'json_tool',
                'resource_link_tool',
                'resource_tool',
                'resource_tool_blob',
            }
            assert set(tools.keys()) == expected_tools

            # Check tool definitions
            test_tool = tools['test_tool']
            assert test_tool.tool_def.name == 'test_tool'
            assert test_tool.tool_def.description is not None
            assert 'test tool that returns a formatted string' in test_tool.tool_def.description
            assert test_tool.max_retries == 1
            assert test_tool.toolset is toolset

            # Check that the tool has proper schema
            schema = test_tool.tool_def.parameters_json_schema
            assert schema['type'] == 'object'
            assert 'param1' in schema['properties']
            assert 'param2' in schema['properties']

    async def test_get_tools_with_empty_server(self, run_context: RunContext[None]):
        """Test getting tools from an empty FastMCP server."""
        empty_server = FastMCP('empty_server')
        empty_client = Client(transport=empty_server)
        toolset = FastMCPToolset(empty_client)

        async with toolset:
            tools = await toolset.get_tools(run_context)
            assert len(tools) == 0

    async def test_get_tools(
        self,
        fastmcp_client: Client[FastMCPTransport],
        run_context: RunContext[None],
    ):
        """Test getting tools from the FastMCP client."""
        toolset = FastMCPToolset(fastmcp_client)

        async with toolset:
            tools = await toolset.get_tools(run_context)

            # Should have all the tools we defined in the server
            expected_tools = {
                'test_tool',
                'another_tool',
                'audio_tool',
                'error_tool',
                'binary_tool',
                'text_tool',
                'text_list_tool',
                'text_tool_wo_return_annotation',
                'json_tool',
                'resource_link_tool',
                'resource_tool',
                'resource_tool_blob',
            }
            assert set(tools.keys()) == expected_tools

            # Check tool definitions
            test_tool = tools['test_tool']
            assert test_tool.tool_def.name == 'test_tool'
            assert test_tool.tool_def.description is not None
            assert 'test tool that returns a formatted string' in test_tool.tool_def.description
            assert test_tool.max_retries == 1
            assert test_tool.toolset is toolset

            # Check that the tool has proper schema
            schema = test_tool.tool_def.parameters_json_schema
            assert schema['type'] == 'object'
            assert 'param1' in schema['properties']
            assert 'param2' in schema['properties']

    async def test_get_tools_with_empty_server(self, run_context: RunContext[None]):
        """Test getting tools from an empty FastMCP server."""
        empty_server = FastMCP('empty_server')
        empty_client = Client(transport=empty_server)
        toolset = FastMCPToolset(empty_client)

        async with toolset:
            tools = await toolset.get_tools(run_context)
            assert len(tools) == 0

## tests/test_ui_web.py

def test_supported_builtin_tools(profile_name: str):
    """Test profile.supported_builtin_tools returns proper tool types."""
    if profile_name == 'base':
        profile: ModelProfile = ModelProfile()
    elif profile_name == 'openai':
        profile = OpenAIModelProfile()
    elif profile_name == 'google':
        profile = GoogleModelProfile()
    else:
        profile = GroqModelProfile()

    result = profile.supported_builtin_tools
    assert isinstance(result, frozenset)
    assert all(issubclass(t, AbstractBuiltinTool) for t in result)

## tests/typed_graph.py

class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

    input_data: str
