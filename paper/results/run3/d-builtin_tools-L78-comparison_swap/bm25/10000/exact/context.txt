# tests/test_fastmcp.py:230-283
class TestFastMCPToolsetToolDiscovery:
    """Test FastMCP Toolset tool discovery functionality."""

    async def test_get_tools(
        self,
        fastmcp_client: Client[FastMCPTransport],
        run_context: RunContext[None],
    ):
        """Test getting tools from the FastMCP client."""
        toolset = FastMCPToolset(fastmcp_client)

        async with toolset:
            tools = await toolset.get_tools(run_context)

            # Should have all the tools we defined in the server
            expected_tools = {
                'test_tool',
                'another_tool',
                'audio_tool',
                'error_tool',
                'binary_tool',
                'text_tool',
                'text_list_tool',
                'text_tool_wo_return_annotation',
                'json_tool',
                'resource_link_tool',
                'resource_tool',
                'resource_tool_blob',
            }
            assert set(tools.keys()) == expected_tools

            # Check tool definitions
            test_tool = tools['test_tool']
            assert test_tool.tool_def.name == 'test_tool'
            assert test_tool.tool_def.description is not None
            assert 'test tool that returns a formatted string' in test_tool.tool_def.description
            assert test_tool.max_retries == 1
            assert test_tool.toolset is toolset

            # Check that the tool has proper schema
            schema = test_tool.tool_def.parameters_json_schema
            assert schema['type'] == 'object'
            assert 'param1' in schema['properties']
            assert 'param2' in schema['properties']

    async def test_get_tools_with_empty_server(self, run_context: RunContext[None]):
        """Test getting tools from an empty FastMCP server."""
        empty_server = FastMCP('empty_server')
        empty_client = Client(transport=empty_server)
        toolset = FastMCPToolset(empty_client)

        async with toolset:
            tools = await toolset.get_tools(run_context)
            assert len(tools) == 0

# examples/pydantic_ai_examples/sql_gen.py:23-23
from annotated_types import MinLen

# tests/models/test_model_test.py:12-12
from annotated_types import Ge, Gt, Le, Lt, MaxLen, MinLen

# examples/pydantic_ai_examples/sql_gen.py:23-23
from annotated_types import MinLen

# tests/test_fastmcp.py:233-273
    async def test_get_tools(
        self,
        fastmcp_client: Client[FastMCPTransport],
        run_context: RunContext[None],
    ):
        """Test getting tools from the FastMCP client."""
        toolset = FastMCPToolset(fastmcp_client)

        async with toolset:
            tools = await toolset.get_tools(run_context)

            # Should have all the tools we defined in the server
            expected_tools = {
                'test_tool',
                'another_tool',
                'audio_tool',
                'error_tool',
                'binary_tool',
                'text_tool',
                'text_list_tool',
                'text_tool_wo_return_annotation',
                'json_tool',
                'resource_link_tool',
                'resource_tool',
                'resource_tool_blob',
            }
            assert set(tools.keys()) == expected_tools

            # Check tool definitions
            test_tool = tools['test_tool']
            assert test_tool.tool_def.name == 'test_tool'
            assert test_tool.tool_def.description is not None
            assert 'test tool that returns a formatted string' in test_tool.tool_def.description
            assert test_tool.max_retries == 1
            assert test_tool.toolset is toolset

            # Check that the tool has proper schema
            schema = test_tool.tool_def.parameters_json_schema
            assert schema['type'] == 'object'
            assert 'param1' in schema['properties']
            assert 'param2' in schema['properties']

# examples/pydantic_ai_examples/stream_whales.py:29-31
    length: Annotated[
        float, Field(description='Average length of an adult whale in meters.')
    ]

# pydantic_graph/pydantic_graph/beta/node_types.py:24-28
MiddleNode = TypeAliasType(
    'MiddleNode',
    Step[StateT, DepsT, InputT, OutputT] | Join[StateT, DepsT, InputT, OutputT] | Fork[InputT, OutputT],
    type_params=(StateT, DepsT, InputT, OutputT),
)

# tests/graph/test_file_persistence.py:35-39
class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# tests/graph/test_file_persistence.py:35-39
class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# tests/graph/test_file_persistence.py:35-39
class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:86-86
    filename: str | None = None

# tests/models/test_google.py:3246-3279
async def test_google_builtin_tools_with_other_tools(allow_model_requests: None, google_provider: GoogleProvider):
    m = GoogleModel('gemini-2.5-flash', provider=google_provider)

    agent = Agent(m, builtin_tools=[WebFetchTool()])

    @agent.tool_plain
    async def get_user_country() -> str:
        return 'Mexico'  # pragma: no cover

    with pytest.raises(
        UserError,
        match=re.escape('Google does not support function tools and built-in tools at the same time.'),
    ):
        await agent.run('What is the largest city in the user country?')

    class CityLocation(BaseModel):
        city: str
        country: str

    agent = Agent(m, output_type=ToolOutput(CityLocation), builtin_tools=[WebFetchTool()])

    with pytest.raises(
        UserError,
        match=re.escape(
            'Google does not support output tools and built-in tools at the same time. Use `output_type=PromptedOutput(...)` instead.'
        ),
    ):
        await agent.run('What is the largest city in Mexico?')

    # Will default to prompted output
    agent = Agent(m, output_type=CityLocation, builtin_tools=[WebFetchTool()])

    result = await agent.run('What is the largest city in Mexico?')
    assert result.output == snapshot(CityLocation(city='Mexico City', country='Mexico'))

# tests/models/test_bedrock.py:4-4
from types import SimpleNamespace

# docs/.hooks/algolia.py:35-35
MAX_CONTENT_LENGTH = 90_000

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:188-188
    filename: str | None = None

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:71-71
    filename: str | None = None

# pydantic_ai_slim/pydantic_ai/models/openai.py:186-186
    violence: _AzureContentFilterResultDetail | None = None

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:57-57
PROMPT_HISTORY_FILENAME = 'prompt-history.txt'

# tests/conftest.py:121-124
def sanitize_filename(name: str, max_len: int) -> str:
    """Sanitize a string for safe use as a filename across platforms."""
    # Windows does not allow these characters in paths. Linux bans slashes only.
    return re.sub('[' + re.escape('<>:"/\\|?*') + ']', '-', name)[:max_len]

# pydantic_ai_slim/pydantic_ai/mcp.py:241-241
    tools_list_changed: bool = False

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:31-31
    builtin_tools: list[str]

# tests/graph/test_file_persistence.py:38-39
    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# tests/graph/test_file_persistence.py:38-39
    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# tests/graph/test_file_persistence.py:38-39
    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:113-115
    async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:
        """The tools that are available in this toolset."""
        raise NotImplementedError()

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:149-149
    builtin_tools: list[AbstractBuiltinTool | BuiltinToolFunc[DepsT]] = dataclasses.field(repr=False)

# tests/test_fastmcp.py:275-283
    async def test_get_tools_with_empty_server(self, run_context: RunContext[None]):
        """Test getting tools from an empty FastMCP server."""
        empty_server = FastMCP('empty_server')
        empty_client = Client(transport=empty_server)
        toolset = FastMCPToolset(empty_client)

        async with toolset:
            tools = await toolset.get_tools(run_context)
            assert len(tools) == 0

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:297-299
    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """The set of builtin tool types this model can handle."""
        return frozenset({WebSearchTool, CodeExecutionTool, WebFetchTool, MemoryTool, MCPServerTool})

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:53-53
    builtin_tools: list[str] = []

# pydantic_ai_slim/pydantic_ai/models/xai.py:202-204
    def supported_builtin_tools(cls) -> frozenset[type]:
        """Return the set of builtin tool types this model can handle."""
        return frozenset({WebSearchTool, CodeExecutionTool, MCPServerTool})

# pydantic_ai_slim/pydantic_ai/providers/bedrock.py:91-92
def _without_builtin_tools(profile: ModelProfile | None) -> ModelProfile:
    return replace(profile or BedrockModelProfile(), supported_builtin_tools=frozenset())

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:45-45
    builtin_tools: list[BuiltinToolInfo]

# pydantic_ai_slim/pydantic_ai/models/groq.py:176-178
    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """Return the set of builtin tool types this model can handle."""
        return frozenset({WebSearchTool})

# tests/test_dbos.py:971-971
weather_toolset = FunctionToolset(tools=[temperature_celsius, temperature_fahrenheit])

# tests/test_dbos.py:971-971
weather_toolset = FunctionToolset(tools=[temperature_celsius, temperature_fahrenheit])

# pydantic_ai_slim/pydantic_ai/models/google.py:254-256
    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """Return the set of builtin tool types this model can handle."""
        return frozenset({WebSearchTool, CodeExecutionTool, FileSearchTool, WebFetchTool, ImageGenerationTool})

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:19-19
from ..builtin_tools import BUILTIN_TOOLS_REQUIRING_CONFIG, SUPPORTED_BUILTIN_TOOLS

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:19-19
from ..builtin_tools import BUILTIN_TOOLS_REQUIRING_CONFIG, SUPPORTED_BUILTIN_TOOLS

# pydantic_ai_slim/pydantic_ai/models/groq.py:379-391
    def _get_builtin_tools(
        self, model_request_parameters: ModelRequestParameters
    ) -> list[chat.ChatCompletionToolParam]:
        tools: list[chat.ChatCompletionToolParam] = []
        for tool in model_request_parameters.builtin_tools:
            if isinstance(tool, WebSearchTool):
                if not GroqModelProfile.from_profile(self.profile).groq_always_has_web_search_builtin_tool:
                    raise UserError('`WebSearchTool` is not supported by Groq')  # pragma: no cover
            else:  # pragma: no cover
                raise UserError(
                    f'`{tool.__class__.__name__}` is not supported by `GroqModel`. If it should be, please file an issue.'
                )
        return tools

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:19-19
from ..builtin_tools import BUILTIN_TOOLS_REQUIRING_CONFIG, SUPPORTED_BUILTIN_TOOLS

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:19-19
from ..builtin_tools import BUILTIN_TOOLS_REQUIRING_CONFIG, SUPPORTED_BUILTIN_TOOLS

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:19-19
from ..builtin_tools import BUILTIN_TOOLS_REQUIRING_CONFIG, SUPPORTED_BUILTIN_TOOLS

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:354-356
    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """The set of builtin tool types this model can handle."""
        return frozenset({CodeExecutionTool})

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:475-475
DEPRECATED_BUILTIN_TOOLS: frozenset[type[AbstractBuiltinTool]] = frozenset({UrlContextTool})  # pyright: ignore[reportDeprecated]

# pydantic_ai_slim/pydantic_ai/models/__init__.py:603-603
    builtin_tools: list[AbstractBuiltinTool] = field(default_factory=list[AbstractBuiltinTool])

# pydantic_ai_slim/pydantic_ai/models/openai.py:570-572
    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """Return the set of builtin tool types this model can handle."""
        return frozenset({WebSearchTool})

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:636-697
    def _add_builtin_tools(
        self, tools: list[BetaToolUnionParam], model_request_parameters: ModelRequestParameters
    ) -> tuple[list[BetaToolUnionParam], list[BetaRequestMCPServerURLDefinitionParam], set[str]]:
        beta_features: set[str] = set()
        mcp_servers: list[BetaRequestMCPServerURLDefinitionParam] = []
        for tool in model_request_parameters.builtin_tools:
            if isinstance(tool, WebSearchTool):
                user_location = UserLocation(type='approximate', **tool.user_location) if tool.user_location else None
                tools.append(
                    BetaWebSearchTool20250305Param(
                        name='web_search',
                        type='web_search_20250305',
                        max_uses=tool.max_uses,
                        allowed_domains=tool.allowed_domains,
                        blocked_domains=tool.blocked_domains,
                        user_location=user_location,
                    )
                )
            elif isinstance(tool, CodeExecutionTool):  # pragma: no branch
                tools.append(BetaCodeExecutionTool20250522Param(name='code_execution', type='code_execution_20250522'))
                beta_features.add('code-execution-2025-05-22')
            elif isinstance(tool, WebFetchTool):  # pragma: no branch
                citations = BetaCitationsConfigParam(enabled=tool.enable_citations) if tool.enable_citations else None
                tools.append(
                    BetaWebFetchTool20250910Param(
                        name='web_fetch',
                        type='web_fetch_20250910',
                        max_uses=tool.max_uses,
                        allowed_domains=tool.allowed_domains,
                        blocked_domains=tool.blocked_domains,
                        citations=citations,
                        max_content_tokens=tool.max_content_tokens,
                    )
                )
                beta_features.add('web-fetch-2025-09-10')
            elif isinstance(tool, MemoryTool):  # pragma: no branch
                if 'memory' not in model_request_parameters.tool_defs:
                    raise UserError("Built-in `MemoryTool` requires a 'memory' tool to be defined.")
                # Replace the memory tool definition with the built-in memory tool
                tools = [tool for tool in tools if tool.get('name') != 'memory']
                tools.append(BetaMemoryTool20250818Param(name='memory', type='memory_20250818'))
                beta_features.add('context-management-2025-06-27')
            elif isinstance(tool, MCPServerTool) and tool.url:
                mcp_server_url_definition_param = BetaRequestMCPServerURLDefinitionParam(
                    type='url',
                    name=tool.id,
                    url=tool.url,
                )
                if tool.allowed_tools is not None:  # pragma: no branch
                    mcp_server_url_definition_param['tool_configuration'] = BetaRequestMCPServerToolConfigurationParam(
                        enabled=bool(tool.allowed_tools),
                        allowed_tools=tool.allowed_tools,
                    )
                if tool.authorization_token:  # pragma: no cover
                    mcp_server_url_definition_param['authorization_token'] = tool.authorization_token
                mcp_servers.append(mcp_server_url_definition_param)
                beta_features.add('mcp-client-2025-04-04')
            else:
                raise UserError(  # pragma: no cover
                    f'`{tool.__class__.__name__}` is not supported by `AnthropicModel`. If it should be, please file an issue.'
                )
        return tools, mcp_servers, beta_features

# pydantic_ai_slim/pydantic_ai/models/openai.py:1346-1348
    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """Return the set of builtin tool types this model can handle."""
        return frozenset({WebSearchTool, CodeExecutionTool, FileSearchTool, MCPServerTool, ImageGenerationTool})

# pydantic_ai_slim/pydantic_ai/models/openai.py:1711-1787
    def _get_builtin_tools(self, model_request_parameters: ModelRequestParameters) -> list[responses.ToolParam]:
        tools: list[responses.ToolParam] = []
        has_image_generating_tool = False
        for tool in model_request_parameters.builtin_tools:
            if isinstance(tool, WebSearchTool):
                web_search_tool = responses.WebSearchToolParam(
                    type='web_search', search_context_size=tool.search_context_size
                )
                if tool.user_location:
                    web_search_tool['user_location'] = responses.web_search_tool_param.UserLocation(
                        type='approximate', **tool.user_location
                    )
                if tool.allowed_domains:
                    web_search_tool['filters'] = responses.web_search_tool_param.Filters(
                        allowed_domains=tool.allowed_domains
                    )
                tools.append(web_search_tool)
            elif isinstance(tool, FileSearchTool):
                file_search_tool = cast(
                    responses.FileSearchToolParam,
                    {'type': 'file_search', 'vector_store_ids': list(tool.file_store_ids)},
                )
                tools.append(file_search_tool)
            elif isinstance(tool, CodeExecutionTool):
                has_image_generating_tool = True
                tools.append({'type': 'code_interpreter', 'container': {'type': 'auto'}})
            elif isinstance(tool, MCPServerTool):
                mcp_tool = responses.tool_param.Mcp(
                    type='mcp',
                    server_label=tool.id,
                    require_approval='never',
                )

                if tool.authorization_token:  # pragma: no branch
                    mcp_tool['authorization'] = tool.authorization_token

                if tool.allowed_tools is not None:  # pragma: no branch
                    mcp_tool['allowed_tools'] = tool.allowed_tools

                if tool.description:  # pragma: no branch
                    mcp_tool['server_description'] = tool.description

                if tool.headers:  # pragma: no branch
                    mcp_tool['headers'] = tool.headers

                if tool.url.startswith(MCP_SERVER_TOOL_CONNECTOR_URI_SCHEME + ':'):
                    _, connector_id = tool.url.split(':', maxsplit=1)
                    mcp_tool['connector_id'] = connector_id  # pyright: ignore[reportGeneralTypeIssues]
                else:
                    mcp_tool['server_url'] = tool.url

                tools.append(mcp_tool)
            elif isinstance(tool, ImageGenerationTool):  # pragma: no branch
                has_image_generating_tool = True
                size = _resolve_openai_image_generation_size(tool)
                output_compression = tool.output_compression if tool.output_compression is not None else 100
                tools.append(
                    responses.tool_param.ImageGeneration(
                        type='image_generation',
                        background=tool.background,
                        input_fidelity=tool.input_fidelity,
                        moderation=tool.moderation,
                        output_compression=output_compression,
                        output_format=tool.output_format or 'png',
                        partial_images=tool.partial_images,
                        quality=tool.quality,
                        size=size,
                    )
                )
            else:
                raise UserError(  # pragma: no cover
                    f'`{tool.__class__.__name__}` is not supported by `OpenAIResponsesModel`. If it should be, please file an issue.'
                )

        if model_request_parameters.allow_image_output and not has_image_generating_tool:
            tools.append({'type': 'image_generation'})
        return tools

# tests/test_agent.py:2812-2854
def test_empty_response_without_recovery():
    def llm(messages: list[ModelMessage], _info: AgentInfo) -> ModelResponse:
        return ModelResponse(parts=[])

    agent = Agent(FunctionModel(llm), output_type=tuple[str, int])

    with capture_run_messages() as messages:
        with pytest.raises(UnexpectedModelBehavior, match=r'Exceeded maximum retries \(1\) for output validation'):
            agent.run_sync('Hello')

    assert messages == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content='Hello',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[],
                usage=RequestUsage(input_tokens=51),
                model_name='function:llm:',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[],
                usage=RequestUsage(input_tokens=51),
                model_name='function:llm:',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
        ]
    )

# pydantic_ai_slim/pydantic_ai/models/xai.py:935-975
def _get_builtin_tools(model_request_parameters: ModelRequestParameters) -> list[chat_types.chat_pb2.Tool]:
    """Convert pydantic_ai built-in tools to xAI SDK server-side tools."""
    tools: list[chat_types.chat_pb2.Tool] = []
    for builtin_tool in model_request_parameters.builtin_tools:
        if isinstance(builtin_tool, WebSearchTool):
            # xAI web_search supports:
            # - excluded_domains (from blocked_domains)
            # - allowed_domains
            # Note: user_location and search_context_size are not supported by xAI SDK
            tools.append(
                web_search(
                    excluded_domains=builtin_tool.blocked_domains,
                    allowed_domains=builtin_tool.allowed_domains,
                    enable_image_understanding=False,  # Not supported by PydanticAI
                )
            )
        elif isinstance(builtin_tool, CodeExecutionTool):
            # xAI code_execution takes no parameters
            tools.append(code_execution())
        elif isinstance(builtin_tool, MCPServerTool):
            # xAI mcp supports:
            # - server_url, server_label, server_description
            # - allowed_tool_names, authorization, extra_headers
            tools.append(
                mcp(
                    server_url=builtin_tool.url,
                    server_label=builtin_tool.id,
                    server_description=builtin_tool.description,
                    allowed_tool_names=builtin_tool.allowed_tools,
                    authorization=builtin_tool.authorization_token,
                    extra_headers=builtin_tool.headers,
                )
            )
        else:  # pragma: no cover
            # Defensive fallback - validation in models/__init__.py catches unsupported tools earlier
            raise UserError(
                f'`{builtin_tool.__class__.__name__}` is not supported by `XaiModel`. '
                f'Supported built-in tools: WebSearchTool, CodeExecutionTool, MCPServerTool. '
                f'If XSearchTool should be supported, please file an issue.'
            )
    return tools

# tests/test_agent.py:3186-3204
def test_empty_response_with_finish_reason_length():
    def return_empty_response(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        resp = ModelResponse(parts=[])
        resp.finish_reason = 'length'
        return resp

    agent = Agent(FunctionModel(return_empty_response), output_type=str)

    with pytest.raises(
        UnexpectedModelBehavior,
        match=r'Model token limit \(10\) exceeded before any response was generated.',
    ):
        agent.run_sync('Hello', model_settings=ModelSettings(max_tokens=10))

    with pytest.raises(
        UnexpectedModelBehavior,
        match=r'Model token limit \(provider default\) exceeded before any response was generated.',
    ):
        agent.run_sync('Hello')

# pydantic_ai_slim/pydantic_ai/profiles/__init__.py:71-73
    supported_builtin_tools: frozenset[type[AbstractBuiltinTool]] = field(
        default_factory=lambda: SUPPORTED_BUILTIN_TOOLS
    )

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:19-19
from ..builtin_tools import BUILTIN_TOOLS_REQUIRING_CONFIG, SUPPORTED_BUILTIN_TOOLS

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:19-19
from ..builtin_tools import BUILTIN_TOOLS_REQUIRING_CONFIG, SUPPORTED_BUILTIN_TOOLS

# tests/models/test_cohere.py:575-579
async def test_cohere_model_builtin_tools(allow_model_requests: None, co_api_key: str):
    m = CohereModel('command-r7b-12-2024', provider=CohereProvider(api_key=co_api_key))
    agent = Agent(m, builtin_tools=[WebSearchTool()])
    with pytest.raises(UserError, match=r"Builtin tool\(s\) \['WebSearchTool'\] not supported by this model"):
        await agent.run('Hello')

# tests/graph/test_file_persistence.py:36-36
    input_data: str

# tests/graph/test_file_persistence.py:36-36
    input_data: str

# tests/graph/test_file_persistence.py:36-36
    input_data: str

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter.py:88-90
    def label(self) -> str:
        """Return the label for this toolset."""
        return 'the AG-UI frontend tools'  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/models/openai.py:382-382
    openai_builtin_tools: Sequence[FileSearchToolParam | WebSearchToolParam | ComputerToolParam]

# pydantic_ai_slim/pydantic_ai/models/__init__.py:825-847
    def profile(self) -> ModelProfile:
        """The model profile.

        We use this to compute the intersection of the profile's supported_builtin_tools
        and the model's implemented tools, ensuring model.profile.supported_builtin_tools
        is the single source of truth for what builtin tools are actually usable.
        """
        _profile = self._profile
        if callable(_profile):
            _profile = _profile(self.model_name)

        if _profile is None:
            _profile = DEFAULT_PROFILE

        # Compute intersection: profile's allowed tools & model's implemented tools
        model_supported = self.__class__.supported_builtin_tools()
        profile_supported = _profile.supported_builtin_tools
        effective_tools = profile_supported & model_supported

        if effective_tools != profile_supported:
            _profile = replace(_profile, supported_builtin_tools=effective_tools)

        return _profile

# tests/test_ui_web.py:318-331
def test_supported_builtin_tools(profile_name: str):
    """Test profile.supported_builtin_tools returns proper tool types."""
    if profile_name == 'base':
        profile: ModelProfile = ModelProfile()
    elif profile_name == 'openai':
        profile = OpenAIModelProfile()
    elif profile_name == 'google':
        profile = GoogleModelProfile()
    else:
        profile = GroqModelProfile()

    result = profile.supported_builtin_tools
    assert isinstance(result, frozenset)
    assert all(issubclass(t, AbstractBuiltinTool) for t in result)

# pydantic_ai_slim/pydantic_ai/profiles/grok.py:16-16
    grok_supports_builtin_tools: bool = False

# tests/test_agent.py:7477-7511
def test_agent_builtin_tools_runtime_vs_agent_level():
    """Test that runtime builtin_tools parameter is merged with agent-level builtin_tools."""
    model = TestModel()

    agent = Agent(
        model=model,
        builtin_tools=[
            WebSearchTool(),
            CodeExecutionTool(),
            MCPServerTool(id='deepwiki', url='https://mcp.deepwiki.com/mcp'),
            MCPServerTool(id='github', url='https://api.githubcopilot.com/mcp'),
        ],
    )

    # Runtime tool with same unique ID should override agent-level tool
    with pytest.raises(Exception, match='TestModel does not support built-in tools'):
        agent.run_sync(
            'Hello',
            builtin_tools=[
                WebSearchTool(search_context_size='high'),
                MCPServerTool(id='example', url='https://mcp.example.com/mcp'),
                MCPServerTool(id='github', url='https://mcp.githubcopilot.com/mcp', authorization_token='token'),
            ],
        )

    assert model.last_model_request_parameters is not None
    assert model.last_model_request_parameters.builtin_tools == snapshot(
        [
            WebSearchTool(search_context_size='high'),
            CodeExecutionTool(),
            MCPServerTool(id='deepwiki', url='https://mcp.deepwiki.com/mcp'),
            MCPServerTool(id='github', url='https://mcp.githubcopilot.com/mcp', authorization_token='token'),
            MCPServerTool(id='example', url='https://mcp.example.com/mcp'),
        ]
    )

# tests/models/test_xai.py:2287-2420
async def test_xai_builtin_tools_with_custom_tools(allow_model_requests: None, xai_provider: XaiProvider):
    """Test mixing xAI's built-in tools with custom (client-side) tools (recorded via proto cassette).

    This test verifies that both builtin tools (`web_search`) and custom tools
    (`get_local_temperature`) can be used in the same conversation.
    """
    m = XaiModel(XAI_REASONING_MODEL, provider=xai_provider)
    agent = Agent(
        m,
        instructions=(
            'Use tools to get the users city and then use the web search tool to find a famous landmark in that city.'
        ),
        builtin_tools=[WebSearchTool()],
        model_settings=XaiModelSettings(
            xai_include_encrypted_content=True,  # Encrypted tool calls
            xai_include_web_search_output=True,
            parallel_tool_calls=False,
        ),
    )

    # Track if custom tool was called
    tool_was_called = False

    @agent.tool_plain
    def guess_city() -> str:
        """The city to guess"""
        nonlocal tool_was_called
        tool_was_called = True
        return 'Chicago'

    result = await agent.run('I am thinking of a city, can you tell me about a famours landmark in this city?')
    assert result.output == snapshot(
        "One of the most famous landmarks in Chicago is **Cloud Gate**, often nicknamed \"The Bean.\" It's a massive, reflective stainless steel sculpture in Millennium Park, designed by artist Anish Kapoor and unveiled in 2006. The 110-ton, bean-shaped installation mirrors the city's skyline, Lake Michigan, and visitors, creating surreal and interactive photo opportunities. It's become an iconic symbol of Chicago, drawing millions of visitors annually for its playful design and free public access. If that's not the city you had in mind, feel free to give me a hint!"
    )

    # Verify custom tool was actually called
    assert tool_was_called, 'Custom tool guess_city should have been called'

    # Verify full message history with both custom and builtin tool calls
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content='I am thinking of a city, can you tell me about a famours landmark in this city?',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                instructions='Use tools to get the users city and then use the web search tool to find a famous landmark in that city.',
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ThinkingPart(
                        content='',
                        signature=IsStr(),
                        provider_name='xai',
                    ),
                    ToolCallPart(tool_name='guess_city', args='{}', tool_call_id=IsStr()),
                ],
                usage=RequestUsage(
                    input_tokens=743, output_tokens=15, details={'reasoning_tokens': 483, 'cache_read_tokens': 170}
                ),
                model_name='grok-4-fast-reasoning',
                timestamp=IsDatetime(),
                provider_name='xai',
                provider_url='https://api.x.ai/v1',
                provider_response_id=IsStr(),
                finish_reason='tool_call',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='guess_city',
                        content='Chicago',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                instructions='Use tools to get the users city and then use the web search tool to find a famous landmark in that city.',
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ThinkingPart(
                        content='',
                        signature=IsStr(),
                        provider_name='xai',
                    ),
                    BuiltinToolCallPart(
                        tool_name='web_search',
                        args={'query': 'famous landmarks in Chicago', 'num_results': 5},
                        tool_call_id=IsStr(),
                        provider_name='xai',
                    ),
                    ThinkingPart(
                        content='',
                        signature=IsStr(),
                        provider_name='xai',
                    ),
                    BuiltinToolReturnPart(
                        tool_name='web_search',
                        content=None,
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                        provider_name='xai',
                    ),
                    ThinkingPart(
                        content='',
                        signature=IsStr(),
                        provider_name='xai',
                    ),
                    TextPart(
                        content="One of the most famous landmarks in Chicago is **Cloud Gate**, often nicknamed \"The Bean.\" It's a massive, reflective stainless steel sculpture in Millennium Park, designed by artist Anish Kapoor and unveiled in 2006. The 110-ton, bean-shaped installation mirrors the city's skyline, Lake Michigan, and visitors, creating surreal and interactive photo opportunities. It's become an iconic symbol of Chicago, drawing millions of visitors annually for its playful design and free public access. If that's not the city you had in mind, feel free to give me a hint!"
                    ),
                ],
                usage=RequestUsage(
                    input_tokens=2973,
                    output_tokens=150,
                    details={'reasoning_tokens': 168, 'cache_read_tokens': 1506, 'server_side_tools_web_search': 1},
                ),
                model_name='grok-4-fast-reasoning',
                timestamp=IsDatetime(),
                provider_name='xai',
                provider_url='https://api.x.ai/v1',
                provider_response_id=IsStr(),
                finish_reason='stop',
                run_id=IsStr(),
            ),
        ]
    )

# tests/test_agent.py:7636-7662
async def test_mixed_static_and_dynamic_builtin_tools():
    model = TestModel()

    static_tool = CodeExecutionTool()
    agent = Agent(model, builtin_tools=[static_tool, prepared_web_search], deps_type=UserContext)

    # Case 1: Dynamic tool returns None
    with pytest.raises(UserError, match='TestModel does not support built-in tools'):
        await agent.run('Hello', deps=UserContext(location=None))

    assert model.last_model_request_parameters is not None
    tools = model.last_model_request_parameters.builtin_tools
    assert len(tools) == 1
    assert tools[0] == static_tool

    # Case 2: Dynamic tool returns a tool
    with pytest.raises(UserError, match='TestModel does not support built-in tools'):
        await agent.run('Hello', deps=UserContext(location='Paris'))

    assert model.last_model_request_parameters is not None
    tools = model.last_model_request_parameters.builtin_tools
    assert len(tools) == 2
    assert tools[0] == static_tool
    dynamic_tool = tools[1]
    assert isinstance(dynamic_tool, WebSearchTool)
    assert dynamic_tool.user_location is not None
    assert dynamic_tool.user_location.get('city') == 'Paris'

# tests/test_builtin_tools.py:20-24
async def test_builtin_tools_not_supported_web_search(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[WebSearchTool()])

    with pytest.raises(UserError):
        await agent.run('What day is tomorrow?')

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter.py:111-115
    def toolset(self) -> AbstractToolset[AgentDepsT] | None:
        """Toolset representing frontend tools from the AG-UI run input."""
        if self.run_input.tools:
            return _AGUIFrontendToolset[AgentDepsT](self.run_input.tools)
        return None

# pydantic_ai_slim/pydantic_ai/models/test.py:162-164
    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """TestModel supports all builtin tools for testing flexibility."""
        return SUPPORTED_BUILTIN_TOOLS

# pydantic_ai_slim/pydantic_ai/__init__.py:115-128
from .toolsets import (
    AbstractToolset,
    ApprovalRequiredToolset,
    CombinedToolset,
    ExternalToolset,
    FilteredToolset,
    FunctionToolset,
    PrefixedToolset,
    PreparedToolset,
    RenamedToolset,
    ToolsetFunc,
    ToolsetTool,
    WrapperToolset,
)

# tests/test_builtin_tools.py:56-60
async def test_builtin_tools_not_supported_file_search(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[FileSearchTool(file_store_ids=['test-id'])])

    with pytest.raises(UserError):
        await agent.run('Search my files')

# pydantic_ai_slim/pydantic_ai/__init__.py:115-128
from .toolsets import (
    AbstractToolset,
    ApprovalRequiredToolset,
    CombinedToolset,
    ExternalToolset,
    FilteredToolset,
    FunctionToolset,
    PrefixedToolset,
    PreparedToolset,
    RenamedToolset,
    ToolsetFunc,
    ToolsetTool,
    WrapperToolset,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:115-128
from .toolsets import (
    AbstractToolset,
    ApprovalRequiredToolset,
    CombinedToolset,
    ExternalToolset,
    FilteredToolset,
    FunctionToolset,
    PrefixedToolset,
    PreparedToolset,
    RenamedToolset,
    ToolsetFunc,
    ToolsetTool,
    WrapperToolset,
)

# pydantic_ai_slim/pydantic_ai/models/function.py:205-209
    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """FunctionModel supports all builtin tools for testing flexibility."""
        from ..builtin_tools import SUPPORTED_BUILTIN_TOOLS

        return SUPPORTED_BUILTIN_TOOLS

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:141-150
    def filtered(
        self, filter_func: Callable[[RunContext[AgentDepsT], ToolDefinition], bool]
    ) -> FilteredToolset[AgentDepsT]:
        """Returns a new toolset that filters this toolset's tools using a filter function that takes the agent context and the tool definition.

        See [toolset docs](../toolsets.md#filtering-tools) for more information.
        """
        from .filtered import FilteredToolset

        return FilteredToolset(self, filter_func)

# tests/models/test_google.py:3888-3905
async def test_google_image_generation_silently_ignored_by_gemini_api(google_provider: GoogleProvider) -> None:
    """Test that output_format and compression are silently ignored by Gemini API (google-gla)."""
    model = GoogleModel('gemini-2.5-flash-image', provider=google_provider)

    # Test output_format ignored
    params = ModelRequestParameters(builtin_tools=[ImageGenerationTool(output_format='png')])
    _, image_config = model._get_tools(params)  # pyright: ignore[reportPrivateUsage]
    assert image_config == {}

    # Test output_compression ignored
    params = ModelRequestParameters(builtin_tools=[ImageGenerationTool(output_compression=90)])
    _, image_config = model._get_tools(params)  # pyright: ignore[reportPrivateUsage]
    assert image_config == {}

    # Test both ignored when None
    params = ModelRequestParameters(builtin_tools=[ImageGenerationTool()])
    _, image_config = model._get_tools(params)  # pyright: ignore[reportPrivateUsage]
    assert image_config == {}

# tests/test_builtin_tools.py:37-41
async def test_builtin_tools_not_supported_code_execution(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[CodeExecutionTool()])

    with pytest.raises(UserError):
        await agent.run('What day is tomorrow?')

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:168-170
    def toolset(self) -> AbstractToolset[AgentDepsT] | None:
        """Toolset representing frontend tools from the protocol-specific run input."""
        return None

# pydantic_ai_slim/pydantic_ai/models/xai.py:409-420
    async def _upload_file_to_xai(self, data: bytes, filename: str) -> str:
        """Upload a file to xAI files API and return the file ID.

        Args:
            data: The file content as bytes
            filename: The filename to use for the upload

        Returns:
            The file ID from xAI
        """
        uploaded_file = await self._provider.client.files.upload(data, filename=filename)
        return uploaded_file.id

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:86-91
    def label(self) -> str:
        """The name of the toolset for use in error messages."""
        label = self.__class__.__name__
        if self.id:  # pragma: no branch
            label += f' {self.id!r}'
        return label

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:152-159
    def prefixed(self, prefix: str) -> PrefixedToolset[AgentDepsT]:
        """Returns a new toolset that prefixes the names of this toolset's tools.

        See [toolset docs](../toolsets.md#prefixing-tool-names) for more information.
        """
        from .prefixed import PrefixedToolset

        return PrefixedToolset(self, prefix)

# tests/test_builtin_tools.py:28-33
async def test_builtin_tools_not_supported_web_search_stream(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[WebSearchTool()])

    with pytest.raises(UserError):
        async with agent.run_stream('What day is tomorrow?'):
            ...  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:125-130
    def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:
        """All toolsets registered on the agent.

        Output tools are not included.
        """
        raise NotImplementedError

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:49-49
    toolset: AbstractToolset[AgentDepsT]

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:43-43
    tools: dict[str, Tool[Any]]

# pydantic_ai_slim/pydantic_ai/profiles/google.py:16-16
    google_supports_native_output_with_builtin_tools: bool = False

# pydantic_ai_slim/pydantic_ai/toolsets/combined.py:32-32
    toolsets: Sequence[AbstractToolset[AgentDepsT]]