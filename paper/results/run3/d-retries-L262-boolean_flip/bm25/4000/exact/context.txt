# pydantic_ai_slim/pydantic_ai/models/wrapper.py:24-24
    wrapped: Model

# pydantic_ai_slim/pydantic_ai/toolsets/wrapper.py:20-20
    wrapped: AbstractToolset[AgentDepsT]

# pydantic_ai_slim/pydantic_ai/embeddings/wrapper.py:27-27
    wrapped: EmbeddingModel

# pydantic_evals/pydantic_evals/_utils.py:49-75
def get_unwrapped_function_name(func: Callable[..., Any]) -> str:
    """Get the name of a function, unwrapping partials and decorators.

    Args:
        func: The function to get the name of.

    Returns:
        The name of the function.

    Raises:
        AttributeError: If the function doesn't have a __name__ attribute and isn't a method.
    """

    def _unwrap(f: Callable[..., Any]) -> Callable[..., Any]:
        """Unwraps f, also unwrapping partials, for the sake of getting f's name."""
        if isinstance(f, partial):
            return _unwrap(f.func)
        return inspect.unwrap(f)

    try:
        return _unwrap(func).__name__
    except AttributeError as e:
        # Handle instances of types with `__call__` as a method
        if inspect.ismethod(getattr(func, '__call__', None)):
            return f'{type(func).__qualname__}.__call__'
        else:
            raise e

# tests/evals/test_utils.py:94-101
def test_get_unwrapped_function_name_method():
    """Test get_unwrapped_function_name with method."""

    class TestClass:
        def test_method(self):
            pass

    assert get_unwrapped_function_name(TestClass().test_method) == 'test_method'

# tests/evals/test_utils.py:104-113
def test_get_unwrapped_function_name_error():
    """Test get_unwrapped_function_name with invalid input."""

    class InvalidCallable:
        pass

    with pytest.raises(AttributeError) as exc_info:
        get_unwrapped_function_name(InvalidCallable())  # type: ignore

    assert str(exc_info.value) == "'InvalidCallable' object has no attribute '__name__'"

# tests/evals/test_utils.py:45-51
def test_get_unwrapped_function_name_basic():
    """Test get_unwrapped_function_name with basic function."""

    def test_func():
        pass

    assert get_unwrapped_function_name(test_func) == 'test_func'

# tests/evals/test_utils.py:54-61
def test_get_unwrapped_function_name_partial():
    """Test get_unwrapped_function_name with partial function."""

    def test_func(x: int, y: int):
        raise NotImplementedError

    partial_func = partial(test_func, y=42)
    assert get_unwrapped_function_name(partial_func) == 'test_func'

# pydantic_ai_slim/pydantic_ai/models/test.py:45-45
    value: str | None

# pydantic_ai_slim/pydantic_ai/models/test.py:52-52
    value: dict[str, Any] | None

# tests/evals/test_utils.py:64-78
def test_get_unwrapped_function_name_decorated():
    """Test get_unwrapped_function_name with decorated function."""

    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            raise NotImplementedError

        return wrapper

    @decorator
    def test_func():
        pass

    assert get_unwrapped_function_name(test_func) == 'test_func'

# tests/evals/test_utils.py:81-91
def test_get_unwrapped_function_name_callable_class():
    """Test get_unwrapped_function_name with callable class."""

    class CallableClass:
        def __call__(self):
            pass

    assert (
        get_unwrapped_function_name(CallableClass())
        == 'test_get_unwrapped_function_name_callable_class.<locals>.CallableClass.__call__'
    )

# pydantic_ai_slim/pydantic_ai/models/test.py:54-55
    def __init__(self, value: Any | None):
        self.value = pydantic_core.to_jsonable_python(value)

# pydantic_ai_slim/pydantic_ai/models/test.py:49-55
class _WrappedToolOutput:
    """A wrapper class to tag an output that came from the custom_output_args field."""

    value: dict[str, Any] | None

    def __init__(self, value: Any | None):
        self.value = pydantic_core.to_jsonable_python(value)

# pydantic_ai_slim/pydantic_ai/retries.py:18-26
from httpx import (
    AsyncBaseTransport,
    AsyncHTTPTransport,
    BaseTransport,
    HTTPStatusError,
    HTTPTransport,
    Request,
    Response,
)

# pydantic_ai_slim/pydantic_ai/models/test.py:42-45
class _WrappedTextOutput:
    """A private wrapper class to tag an output that came from the custom_output_text field."""

    value: str | None

# pydantic_ai_slim/pydantic_ai/models/wrapper.py:83-85
    def settings(self) -> ModelSettings | None:
        """Get the settings from the wrapped model."""
        return self.wrapped.settings

# pydantic_ai_slim/pydantic_ai/embeddings/wrapper.py:63-65
    def settings(self) -> EmbeddingSettings | None:
        """Get the settings from the wrapped embedding model."""
        return self.wrapped.settings

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py:10-10
from ._toolset import PrefectWrapperToolset

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:117-126
    async def request(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> ModelResponse:
        """Make a model request, wrapped as a Prefect task when in a flow."""
        return await self._wrapped_request.with_options(
            name=f'Model Request: {self.wrapped.model_name}', **self.task_config
        )(messages, model_settings, model_request_parameters)

# pydantic_evals/pydantic_evals/_utils.py:49-75
def get_unwrapped_function_name(func: Callable[..., Any]) -> str:
    """Get the name of a function, unwrapping partials and decorators.

    Args:
        func: The function to get the name of.

    Returns:
        The name of the function.

    Raises:
        AttributeError: If the function doesn't have a __name__ attribute and isn't a method.
    """

    def _unwrap(f: Callable[..., Any]) -> Callable[..., Any]:
        """Unwraps f, also unwrapping partials, for the sake of getting f's name."""
        if isinstance(f, partial):
            return _unwrap(f.func)
        return inspect.unwrap(f)

    try:
        return _unwrap(func).__name__
    except AttributeError as e:
        # Handle instances of types with `__call__` as a method
        if inspect.ismethod(getattr(func, '__call__', None)):
            return f'{type(func).__qualname__}.__call__'
        else:
            raise e

# tests/test_prefect.py:663-666
async def test_prefect_agent_run(allow_model_requests: None) -> None:
    """Test that agent.run() works (auto-wrapped as flow)."""
    result = await simple_prefect_agent.run('What is the capital of Mexico?')
    assert result.output == snapshot('The capital of Mexico is Mexico City.')

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py:50-60
    async def call_tool(
        self,
        name: str,
        tool_args: dict[str, Any],
        ctx: RunContext[AgentDepsT],
        tool: ToolsetTool[AgentDepsT],
    ) -> ToolResult:
        """Call an MCP tool, wrapped as a Prefect task with a descriptive name."""
        return await self._call_tool_task.with_options(name=f'Call MCP Tool: {name}', **self._task_config)(
            name, tool_args, ctx, tool
        )

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py:39-58
    async def call_tool(
        self,
        name: str,
        tool_args: dict[str, Any],
        ctx: RunContext[AgentDepsT],
        tool: ToolsetTool[AgentDepsT],
    ) -> Any:
        """Call a tool, wrapped as a Prefect task with a descriptive name."""
        # Check if this specific tool has custom config or is disabled
        tool_specific_config = self._tool_task_config.get(name, default_task_config)
        if tool_specific_config is None:
            # None means this tool should not be wrapped as a task
            return await super().call_tool(name, tool_args, ctx, tool)

        # Merge tool-specific config with default config
        merged_config = self._task_config | tool_specific_config

        return await self._call_tool_task.with_options(name=f'Call Tool: {name}', **merged_config)(
            name, tool_args, ctx, tool
        )

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:607-617
class _OpenRouterChoiceDelta(chat_completion_chunk.ChoiceDelta):
    """Wrapped chat completion message with OpenRouter specific attributes."""

    reasoning: str | None = None
    """The reasoning text associated with the message, if any."""

    reasoning_details: list[_OpenRouterReasoningDetail] | None = None
    """The reasoning details associated with the message, if any."""

    annotations: list[_OpenRouterAnnotation] | None = None
    """Annotations associated with the message, supporting both url_citation and file types."""

# pydantic_ai_slim/pydantic_ai/models/openai.py:184-184
    self_harm: _AzureContentFilterResultDetail | None = None

# tests/graph/beta/test_parent_forks.py:157-175
def test_parent_fork_self_loop():
    """Test parent fork identification with a self-loop at the join."""
    join_id = 'J'
    nodes = {'start', 'F', 'A', 'B', 'J', 'end'}
    start_ids = {'start'}
    fork_ids = {'F'}
    edges = {
        'start': ['F'],
        'F': ['A', 'B'],
        'A': ['J'],
        'B': ['J'],
        'J': ['J', 'end'],  # Self-loop
    }

    finder = ParentForkFinder(nodes, start_ids, fork_ids, edges)
    parent_fork = finder.find_parent_fork(join_id)

    # Self-loop means J is on a cycle avoiding F
    assert parent_fork is None

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:384-397
class _OpenRouterCompletionMessage(chat.ChatCompletionMessage):
    """Wrapped chat completion message with OpenRouter specific attributes."""

    reasoning: str | None = None
    """The reasoning text associated with the message, if any."""

    reasoning_details: list[_OpenRouterReasoningDetail] | None = None
    """The reasoning details associated with the message, if any."""

    tool_calls: list[_OpenRouterChatCompletionMessageToolCallUnion] | None = None  # type: ignore[reportIncompatibleVariableOverride]
    """The tool calls generated by the model, such as function calls."""

    annotations: list[_OpenRouterAnnotation] | None = None  # type: ignore[reportIncompatibleVariableOverride]
    """Annotations associated with the message, supporting both url_citation and file types."""

# tests/test_toolsets.py:530-545
async def test_tool_manager_reuse_self():
    """Test the retry logic with failed_tools and for_run_step method."""

    run_context = build_run_context(None, run_step=1)

    tool_manager = await ToolManager[None](FunctionToolset()).for_run_step(run_context)

    same_tool_manager = await tool_manager.for_run_step(ctx=run_context)

    assert tool_manager is same_tool_manager

    step_2_context = build_run_context(None, run_step=2)

    updated_tool_manager = await tool_manager.for_run_step(ctx=step_2_context)

    assert tool_manager != updated_tool_manager

# tests/models/test_fallback.py:592-624
async def test_fallback_model_settings_merge():
    """Test that FallbackModel properly merges model settings from wrapped model and runtime settings."""

    def return_settings(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        return ModelResponse(parts=[TextPart(to_json(info.model_settings).decode())])

    base_model = FunctionModel(return_settings, settings=ModelSettings(temperature=0.1, max_tokens=1024))
    fallback_model = FallbackModel(base_model)

    # Test that base model settings are preserved when no additional settings are provided
    agent = Agent(fallback_model)
    result = await agent.run('Hello')
    assert result.output == IsJson({'max_tokens': 1024, 'temperature': 0.1})

    # Test that runtime model_settings are merged with base settings
    agent_with_settings = Agent(fallback_model, model_settings=ModelSettings(temperature=0.5, parallel_tool_calls=True))
    result = await agent_with_settings.run('Hello')
    expected = {'max_tokens': 1024, 'temperature': 0.5, 'parallel_tool_calls': True}
    assert result.output == IsJson(expected)

    # Test that run-time model_settings override both base and agent settings
    result = await agent_with_settings.run(
        'Hello', model_settings=ModelSettings(temperature=0.9, extra_headers={'runtime_setting': 'runtime_value'})
    )
    expected = {
        'max_tokens': 1024,
        'temperature': 0.9,
        'parallel_tool_calls': True,
        'extra_headers': {
            'runtime_setting': 'runtime_value',
        },
    }
    assert result.output == IsJson(expected)

# tests/test_temporal.py:2977-3008
async def test_temporal_model_request_outside_workflow():
    """Test that TemporalModel.request() falls back to wrapped model outside a workflow.

    When TemporalModel.request() is called directly (not through TemporalAgent.run())
    and not inside a Temporal workflow, it should delegate to the wrapped model's request method.
    """
    test_model = TestModel(custom_output_text='Direct model response')

    temporal_model = TemporalModel(
        test_model,
        activity_name_prefix='test__direct_request',
        activity_config={'start_to_close_timeout': timedelta(seconds=60)},
        deps_type=type(None),
    )

    # Call request() directly - outside a workflow, this should fall back to super().request()
    messages: list[ModelMessage] = [ModelRequest.user_text_prompt('Hello')]
    response = await temporal_model.request(
        messages,
        model_settings=None,
        model_request_parameters=ModelRequestParameters(
            function_tools=[],
            builtin_tools=[],
            output_mode='text',
            allow_text_output=True,
            output_tools=[],
            output_object=None,
        ),
    )

    # Verify response comes from the wrapped TestModel
    assert any(isinstance(part, TextPart) and part.content == 'Direct model response' for part in response.parts)

# tests/test_usage_limits.py:3-3
import operator

# tests/models/test_model_settings.py:46-59
def test_wrapper_model_settings_delegation():
    """Test that WrapperModel correctly delegates settings to wrapped model."""
    # Create a base model with settings
    base_settings = ModelSettings(max_tokens=150, temperature=0.6)
    base_model = TestModel(settings=base_settings)

    # Create wrapper - it should delegate to wrapped model's settings
    wrapper = WrapperModel(base_model)
    assert wrapper.settings == base_settings

    # Test with wrapped model without settings
    base_model_no_settings = TestModel()
    wrapper_no_settings = WrapperModel(base_model_no_settings)
    assert wrapper_no_settings.settings is None

# tests/test_concurrency.py:596-612
    async def test_count_tokens(self):
        """Test that count_tokens delegates to wrapped model with concurrency limiting."""
        from unittest.mock import AsyncMock

        from pydantic_ai.models import ModelRequestParameters
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel
        from pydantic_ai.usage import RequestUsage

        base_model = TestModel()
        # Mock count_tokens to return a value
        base_model.count_tokens = AsyncMock(return_value=RequestUsage())
        model = ConcurrencyLimitedModel(base_model, limiter=5)

        # count_tokens should delegate to wrapped model
        usage = await model.count_tokens([], None, ModelRequestParameters())
        assert usage is not None
        base_model.count_tokens.assert_called_once()

# tests/test_temporal.py:3011-3048
async def test_temporal_model_request_stream_outside_workflow():
    """Test that TemporalModel.request_stream() falls back to wrapped model outside a workflow.

    When TemporalModel.request_stream() is called directly (not through TemporalAgent.run())
    and not inside a Temporal workflow, it should delegate to the wrapped model's request_stream method.
    """
    test_model = TestModel(custom_output_text='Direct stream response')

    temporal_model = TemporalModel(
        test_model,
        activity_name_prefix='test__direct_stream',
        activity_config={'start_to_close_timeout': timedelta(seconds=60)},
        deps_type=type(None),
    )

    # Call request_stream() directly - outside a workflow, this should fall back to super().request_stream()
    messages: list[ModelMessage] = [ModelRequest.user_text_prompt('Hello')]
    async with temporal_model.request_stream(
        messages,
        model_settings=None,
        model_request_parameters=ModelRequestParameters(
            function_tools=[],
            builtin_tools=[],
            output_mode='text',
            allow_text_output=True,
            output_tools=[],
            output_object=None,
        ),
    ) as stream:
        # Consume the stream
        async for _ in stream:
            pass

        # Get the final response
        response = stream.get()

    # Verify response comes from the wrapped TestModel
    assert any(isinstance(part, TextPart) and part.content == 'Direct stream response' for part in response.parts)

# pydantic_ai_slim/pydantic_ai/_run_context.py:51-51
    retries: dict[str, int] = field(default_factory=dict[str, int])