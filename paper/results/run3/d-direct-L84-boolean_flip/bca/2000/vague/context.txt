## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

## pydantic_ai_slim/pydantic_ai/messages.py

    def format(self) -> str:
        """The file format."""
        raise NotImplementedError

## pydantic_ai_slim/pydantic_ai/profiles/google.py

class GoogleJsonSchemaTransformer(JsonSchemaTransformer):
    """Transforms the JSON Schema from Pydantic to be suitable for Gemini.

    Gemini supports [a subset of OpenAPI v3.0.3](https://ai.google.dev/gemini-api/docs/function-calling#function_declarations).
    """

    def transform(self, schema: JsonSchema) -> JsonSchema:
        # Remove properties not supported by Gemini
        schema.pop('$schema', None)
        if (const := schema.pop('const', None)) is not None:
            # Gemini doesn't support const, but it does support enum with a single value
            schema['enum'] = [const]
            # If type is not present, infer it from the const value for Gemini API compatibility
            if 'type' not in schema:
                if isinstance(const, str):
                    schema['type'] = 'string'
                elif isinstance(const, bool):
                    # bool must be checked before int since bool is a subclass of int in Python
                    schema['type'] = 'boolean'
                elif isinstance(const, int):
                    schema['type'] = 'integer'
                elif isinstance(const, float):
                    schema['type'] = 'number'
        schema.pop('discriminator', None)
        schema.pop('examples', None)

        # Remove 'title' due to https://github.com/googleapis/python-genai/issues/1732
        schema.pop('title', None)

        type_ = schema.get('type')
        if type_ == 'string' and (fmt := schema.pop('format', None)):
            description = schema.get('description')
            if description:
                schema['description'] = f'{description} (format: {fmt})'
            else:
                schema['description'] = f'Format: {fmt}'

        # Note: exclusiveMinimum/exclusiveMaximum are NOT yet supported
        schema.pop('exclusiveMinimum', None)
        schema.pop('exclusiveMaximum', None)

        return schema

## tests/evals/test_report_evaluators.py

def _make_report(cases: list[ReportCase]) -> EvaluationReport:
    return EvaluationReport(name='test', cases=cases)

## tests/models/test_mcp_sampling.py

def test_assistant_text_history():
    result = CreateMessageResult(
        role='assistant', content=TextContent(type='text', text='text content'), model='test-model'
    )
    create_message = AsyncMock(return_value=result)
    agent = Agent(model=MCPSamplingModel(fake_session(create_message)), instructions='testing')

    result = agent.run_sync('1')
    result = agent.run_sync('2', message_history=result.all_messages())

    assert result.output == snapshot('text content')
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='1', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsDatetime(),
                instructions='testing',
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='text content')],
                model_name='test-model',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[UserPromptPart(content='2', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsDatetime(),
                instructions='testing',
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='text content')],
                model_name='test-model',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

def test_assistant_text_history_complex():
    history = [
        ModelRequest(
            parts=[
                UserPromptPart(content='1'),
                UserPromptPart(content=['a string', BinaryContent(data=b'data', media_type='image/jpeg')]),
                SystemPromptPart(content='system content'),
            ],
            timestamp=IsDatetime(),
        ),
        ModelResponse(
            parts=[TextPart(content='text content')],
            model_name='test-model',
        ),
    ]

    result = CreateMessageResult(
        role='assistant', content=TextContent(type='text', text='text content'), model='test-model'
    )
    create_message = AsyncMock(return_value=result)
    agent = Agent(model=MCPSamplingModel(fake_session(create_message)))
    result = agent.run_sync('1', message_history=history)
    assert result.output == snapshot('text content')

## tests/models/test_xai.py

async def test_xai_reorders_tool_return_parts_by_tool_call_id(allow_model_requests: None):
    response = create_response(
        content='done',
        usage=create_usage(prompt_tokens=20, completion_tokens=5),
    )
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))

    messages: list[ModelMessage] = [
        ModelRequest(parts=[UserPromptPart(content='Run tools')]),
        ModelResponse(
            parts=[
                # Deliberately non-alphabetical order to ensure we don't sort tool results by name/content.
                ToolCallPart(tool_name='tool_a', args='{}', tool_call_id='tool_a'),
                ToolCallPart(tool_name='tool_c', args='{}', tool_call_id='tool_c'),
                ToolCallPart(tool_name='tool_b', args='{}', tool_call_id='tool_b'),
            ],
            finish_reason='tool_call',
        ),
        # Intentionally shuffled: xAI expects tool results in the order the calls were requested.
        ModelRequest(
            parts=[
                ToolReturnPart(tool_name='tool_b', content='tool_b', tool_call_id='tool_b'),
                ToolReturnPart(tool_name='tool_a', content='tool_a', tool_call_id='tool_a'),
                ToolReturnPart(tool_name='tool_c', content='tool_c', tool_call_id='tool_c'),
            ]
        ),
    ]

    await m.request(messages, model_settings=None, model_request_parameters=ModelRequestParameters())

    assert get_mock_chat_create_kwargs(mock_client) == snapshot(
        [
            {
                'model': XAI_NON_REASONING_MODEL,
                'messages': [
                    {'content': [{'text': 'Run tools'}], 'role': 'ROLE_USER'},
                    {
                        'content': [{'text': ''}],
                        'role': 'ROLE_ASSISTANT',
                        'tool_calls': [
                            {
                                'id': 'tool_a',
                                'type': 'TOOL_CALL_TYPE_CLIENT_SIDE_TOOL',
                                'status': 'TOOL_CALL_STATUS_COMPLETED',
                                'function': {'name': 'tool_a', 'arguments': '{}'},
                            },
                            {
                                'id': 'tool_c',
                                'type': 'TOOL_CALL_TYPE_CLIENT_SIDE_TOOL',
                                'status': 'TOOL_CALL_STATUS_COMPLETED',
                                'function': {'name': 'tool_c', 'arguments': '{}'},
                            },
                            {
                                'id': 'tool_b',
                                'type': 'TOOL_CALL_TYPE_CLIENT_SIDE_TOOL',
                                'status': 'TOOL_CALL_STATUS_COMPLETED',
                                'function': {'name': 'tool_b', 'arguments': '{}'},
                            },
                        ],
                    },
                    {'content': [{'text': 'tool_a'}], 'role': 'ROLE_TOOL'},
                    {'content': [{'text': 'tool_c'}], 'role': 'ROLE_TOOL'},
                    {'content': [{'text': 'tool_b'}], 'role': 'ROLE_TOOL'},
                ],
                'tools': None,
                'tool_choice': None,
                'response_format': None,
                'use_encrypted_content': False,
                'include': [],
            }
        ]
    )
