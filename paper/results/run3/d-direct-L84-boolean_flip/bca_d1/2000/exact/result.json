{
  "task_id": "d-direct-L84-boolean_flip",
  "method": "bca_d1",
  "budget": 2000,
  "query_type": "exact",
  "tokens_used": 1975,
  "symbols_selected": 6,
  "files_included": 5,
  "assembly_time_ms": 157.9,
  "llm_time_ms": 7551.9,
  "llm_input_tokens": 1753,
  "llm_output_tokens": 372,
  "tests_passed": false,
  "test_output": "yncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))",
  "patch": "FILE: pydantic_ai_slim/pydantic_ai/models/anthropic.py\nSEARCH:\n    def prepare_request(\n        self, model_settings: ModelSettings | None, model_request_parameters: ModelRequestParameters\n    ) -> tuple[ModelSettings | None, ModelRequestParameters]:\n        settings = merge_model_settings(self.settings, model_settings)\n        if (\n            model_request_parameters.output_tools\n            and settings\n            and (thinking := settings.get('anthropic_thinking'))\n            and thinking.get('type') in ('enabled', 'adaptive')\n        ):\nREPLACE:\n    def prepare_request(\n        self, model_settings: ModelSettings | None, model_request_parameters: ModelRequestParameters\n    ) -> tuple[ModelSettings | None, ModelRequestParameters]:\n        settings = merge_model_settings(self.settings, model_settings)\n        if (\n            model_request_parameters.output_tools\n            and settings\n            and (thinking := settings.get('anthropic_thinking'))\n            and thinking.get('type') in ('enabled', 'adaptive')\n        ):\n            if model_request_parameters.output_mode == 'auto':\n                output_mode = 'native' if self.profile.supports_json_schema_output else 'prompted'\n                model_request_parameters = replace(model_request_parameters, output_mode=output_mode)\n            elif (\n                model_request_parameters.output_mode == 'tool' and not model_request_parameters.allow_text_output\n            ):  # pragma: no branch\n                # This would result in `tool_choice=required`, which Anthropic does not support with thinking.\n                suggested_output_type = 'NativeOutput' if self.profile.supports_json_schema_output else 'PromptedOutput'\n                raise UserError(\n                    f'Anthropic does not support thinking and output tools at the same time. Use `output_type={suggested_output_type}(...)` instead.'\n                )\n",
  "error": "",
  "test_time_ms": 1114.2,
  "failure_mode": "test_fail",
  "target_file_hit": false,
  "target_symbol_hit": true,
  "context_patch_overlap": 0.2,
  "patch_files_changed": 1,
  "patch_lines_changed": 21,
  "edit_distance_lines": -1,
  "entity_count_extracted": 4,
  "entity_count_mapped": 90,
  "query_identifier_density": 0.2105,
  "seed_symbol_keys": [
    "pydantic_ai_slim/pydantic_ai/models/wrapper.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/function.py::ModelRequestParameters",
    "tests/models/test_bedrock.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py::ModelRequestParameters",
    "tests/models/test_fallback.py::ModelRequestParameters",
    "tests/models/test_gemini.py::ModelRequestParameters",
    "tests/models/test_xai.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/__init__.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/openai.py::ModelRequestParameters",
    "tests/models/test_model_settings.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/fallback.py::ModelRequestParameters",
    "tests/test_temporal.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/cerebras.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/mistral.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/concurrency.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/huggingface.py::ModelRequestParameters",
    "tests/models/test_google.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/cohere.py::ModelRequestParameters",
    "tests/models/test_openai.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/outlines.py::ModelRequestParameters",
    "tests/models/test_model_request_parameters.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/bedrock.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/anthropic.py::ModelRequestParameters",
    "tests/models/test_instrumented.py::ModelRequestParameters",
    "tests/models/test_anthropic.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/openrouter.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/groq.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/xai.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/gemini.py::ModelRequestParameters",
    "tests/models/test_mistral.py::ModelRequestParameters",
    "tests/evals/test_evaluators.py::ModelRequestParameters",
    "tests/test_direct.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/google.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/test.py::ModelRequestParameters",
    "tests/models/test_openrouter.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py::_RequestParams.model_request_parameters",
    "pydantic_ai_slim/pydantic_ai/models/__init__.py::StreamedResponse.model_request_parameters",
    "pydantic_ai_slim/pydantic_ai/models/function.py::AgentInfo.model_request_parameters",
    "pydantic_ai_slim/pydantic_ai/direct.py::_annotations",
    "pydantic_ai_slim/pydantic_ai/direct.py::queue",
    "pydantic_ai_slim/pydantic_ai/direct.py::threading",
    "pydantic_ai_slim/pydantic_ai/direct.py::Iterator",
    "pydantic_ai_slim/pydantic_ai/direct.py::Sequence",
    "pydantic_ai_slim/pydantic_ai/direct.py::AbstractAsyncContextManager",
    "pydantic_ai_slim/pydantic_ai/direct.py::dataclass",
    "pydantic_ai_slim/pydantic_ai/direct.py::field",
    "pydantic_ai_slim/pydantic_ai/direct.py::datetime",
    "pydantic_ai_slim/pydantic_ai/direct.py::TracebackType",
    "pydantic_ai_slim/pydantic_ai/direct.py::RequestUsage",
    "pydantic_ai_slim/pydantic_ai/direct.py::_get_event_loop",
    "pydantic_ai_slim/pydantic_ai/direct.py::agent",
    "pydantic_ai_slim/pydantic_ai/direct.py::messages",
    "pydantic_ai_slim/pydantic_ai/direct.py::models",
    "pydantic_ai_slim/pydantic_ai/direct.py::settings",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponse",
    "pydantic_ai_slim/pydantic_ai/direct.py::instrumented_models",
    "pydantic_ai_slim/pydantic_ai/direct.py::__all__",
    "pydantic_ai_slim/pydantic_ai/direct.py::STREAM_INITIALIZATION_TIMEOUT",
    "pydantic_ai_slim/pydantic_ai/direct.py::model_request",
    "pydantic_ai_slim/pydantic_ai/direct.py::model_request_sync",
    "pydantic_ai_slim/pydantic_ai/direct.py::model_request_stream",
    "pydantic_ai_slim/pydantic_ai/direct.py::model_request_stream_sync",
    "pydantic_ai_slim/pydantic_ai/direct.py::_prepare_model",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._async_stream_cm",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._queue",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._thread",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._stream_response",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._exception",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._context_entered",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._stream_ready",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.__enter__",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.__exit__",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.__iter__",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.__repr__",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.__str__",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._check_context_manager_usage",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._ensure_stream_ready",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._start_producer",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._async_producer",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync._cleanup",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.get",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.response",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.usage",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.model_name",
    "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.timestamp"
  ],
  "mutation_symbol_key": "pydantic_ai_slim/pydantic_ai/direct.py::model_request",
  "min_hops_seed_to_mutation": 0,
  "median_hops_seed_to_mutation": 2.0,
  "bca_closure_added_symbols": 0,
  "bca_closure_added_tokens": 0,
  "bca_frontier_visited": 55,
  "context_symbol_keys": [
    "pydantic_ai_slim/pydantic_ai/models/__init__.py::ModelRequestParameters",
    "pydantic_ai_slim/pydantic_ai/models/__init__.py::ModelRequestParameters.tool_defs",
    "pydantic_ai_slim/pydantic_ai/models/__init__.py::ModelRequestParameters.prompted_output_instructions",
    "pydantic_ai_slim/pydantic_ai/models/__init__.py::Model",
    "pydantic_ai_slim/pydantic_ai/models/__init__.py::Model.prepare_request",
    "pydantic_ai_slim/pydantic_ai/models/anthropic.py::AnthropicModel.prepare_request",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::MODEL_SETTING_ATTRIBUTES",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::GEN_AI_SYSTEM_ATTRIBUTE",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::GEN_AI_REQUEST_MODEL_ATTRIBUTE",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::GEN_AI_PROVIDER_NAME_ATTRIBUTE",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::_build_tool_definitions",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::InstrumentedModel._instrument",
    "pydantic_ai_slim/pydantic_ai/models/instrumented.py::CostCalculationFailedWarning"
  ],
  "mutation_symbol_lines": 50,
  "mutation_symbol_kind": "function",
  "mutation_file_symbols": 48,
  "graph_node_count": 18010,
  "retrieval_top1_top2_gap": 0.207,
  "retrieval_softmax_entropy": 2.5124,
  "retrieval_softmax_tau": 1.016,
  "retrieval_effective_candidates": 5.71,
  "retrieval_top5_ratio": 0.6798,
  "retrieval_within95_count": 1,
  "retrieval_scored_symbols": 6,
  "retrieval_top1_score": 1.23,
  "retrieval_top5_mean_score": 0.8362,
  "retrieval_budget_utilization": 0.9875,
  "retrieval_file_concentration": 0.0,
  "repo_name": "pydantic-ai",
  "category": "direct_api",
  "mutation_type": "boolean_flip",
  "source": "discovered"
}