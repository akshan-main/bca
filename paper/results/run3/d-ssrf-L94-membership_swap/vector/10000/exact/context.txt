# pydantic_ai_slim/pydantic_ai/_ssrf.py:23-37
_PRIVATE_NETWORKS: tuple[ipaddress.IPv4Network | ipaddress.IPv6Network, ...] = (
    # IPv4 private ranges
    ipaddress.IPv4Network('127.0.0.0/8'),  # Loopback
    ipaddress.IPv4Network('10.0.0.0/8'),  # Private
    ipaddress.IPv4Network('172.16.0.0/12'),  # Private
    ipaddress.IPv4Network('192.168.0.0/16'),  # Private
    ipaddress.IPv4Network('169.254.0.0/16'),  # Link-local (includes cloud metadata)
    ipaddress.IPv4Network('0.0.0.0/8'),  # "This" network
    ipaddress.IPv4Network('100.64.0.0/10'),  # CGNAT (RFC 6598), includes Alibaba Cloud metadata
    # IPv6 private ranges
    ipaddress.IPv6Network('::1/128'),  # Loopback
    ipaddress.IPv6Network('fe80::/10'),  # Link-local
    ipaddress.IPv6Network('fc00::/7'),  # Unique local address
    ipaddress.IPv6Network('2002::/16'),  # 6to4 (can embed private IPv4 addresses)
)

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:105-110
    async def __aexit__(self, *args: Any) -> bool | None:
        """Exit the toolset context.

        This is where you can tear down network connections in a concrete implementation.
        """
        return None

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:98-103
    async def __aenter__(self) -> Self:
        """Enter the toolset context.

        This is where you can set up network connections in a concrete implementation.
        """
        return self

# pydantic_ai_slim/pydantic_ai/models/__init__.py:867-916
    def _get_instructions(
        messages: Sequence[ModelMessage], model_request_parameters: ModelRequestParameters | None = None
    ) -> str | None:
        """Get instructions from the first ModelRequest found when iterating messages in reverse.

        In the case that a "mock" request was generated to include a tool-return part for a result tool,
        we want to use the instructions from the second-to-most-recent request (which should correspond to the
        original request that generated the response that resulted in the tool-return part).
        """
        instructions = None

        last_two_requests: list[ModelRequest] = []
        for message in reversed(messages):
            if isinstance(message, ModelRequest):
                last_two_requests.append(message)
                if len(last_two_requests) == 2:
                    break
                if message.instructions is not None:
                    instructions = message.instructions
                    break

        # If we don't have two requests, and we didn't already return instructions, there are definitely not any:
        if instructions is None and len(last_two_requests) == 2:
            most_recent_request = last_two_requests[0]
            second_most_recent_request = last_two_requests[1]

            # If we've gotten this far and the most recent request consists of only tool-return parts or retry-prompt parts,
            # we use the instructions from the second-to-most-recent request. This is necessary because when handling
            # result tools, we generate a "mock" ModelRequest with a tool-return part for it, and that ModelRequest will not
            # have the relevant instructions from the agent.

            # While it's possible that you could have a message history where the most recent request has only tool returns,
            # I believe there is no way to achieve that would _change_ the instructions without manually crafting the most
            # recent message. That might make sense in principle for some usage pattern, but it's enough of an edge case
            # that I think it's not worth worrying about, since you can work around this by inserting another ModelRequest
            # with no parts at all immediately before the request that has the tool calls (that works because we only look
            # at the two most recent ModelRequests here).

            # If you have a use case where this causes pain, please open a GitHub issue and we can discuss alternatives.

            if all(p.part_kind == 'tool-return' or p.part_kind == 'retry-prompt' for p in most_recent_request.parts):
                instructions = second_most_recent_request.instructions

        if model_request_parameters and (output_instructions := model_request_parameters.prompted_output_instructions):
            if instructions:
                instructions = '\n\n'.join([instructions, output_instructions])
            else:
                instructions = output_instructions

        return instructions

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:6-6
from typing import Any, Literal

# examples/pydantic_ai_examples/chat_app.py:20-20
from typing import Annotated, Any, Literal, TypeVar

# examples/pydantic_ai_examples/slack_lead_qualifier/app.py:1-1
from typing import Any

# examples/pydantic_ai_examples/slack_lead_qualifier/modal.py:1-1
from typing import Any

# examples/pydantic_ai_examples/slack_lead_qualifier/models.py:1-1
from typing import Annotated, Any

# examples/pydantic_ai_examples/slack_lead_qualifier/slack.py:2-2
from typing import Any

# examples/pydantic_ai_examples/sql_gen.py:19-19
from typing import Annotated, Any, TypeAlias

# examples/pydantic_ai_examples/weather_agent.py:16-16
from typing import Any

# pydantic_ai_slim/pydantic_ai/_a2a.py:9-9
from typing import Any, Generic, TypeVar

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:14-14
from typing import TYPE_CHECKING, Any, Generic, Literal, TypeGuard, cast

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:11-11
from typing import Any

# pydantic_ai_slim/pydantic_ai/_function_schema.py:12-12
from typing import TYPE_CHECKING, Any, Concatenate, cast, get_origin

# pydantic_ai_slim/pydantic_ai/_griffe.py:8-8
from typing import TYPE_CHECKING, Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/_json_schema.py:7-7
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/_output.py:9-9
from typing import TYPE_CHECKING, Any, Generic, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:18-18
from typing import Any, TypeVar

# pydantic_ai_slim/pydantic_ai/_run_context.py:8-8
from typing import TYPE_CHECKING, Any, Generic

# pydantic_ai_slim/pydantic_ai/_system_prompt.py:6-6
from typing import Any, Generic, cast

# pydantic_ai_slim/pydantic_ai/_tool_manager.py:8-8
from typing import Any, Generic, Literal

# pydantic_ai_slim/pydantic_ai/_utils.py:16-26
from typing import (
    TYPE_CHECKING,
    Any,
    Generic,
    TypeAlias,
    TypeGuard,
    TypeVar,
    get_args,
    get_origin,
    overload,
)

# pydantic_ai_slim/pydantic_ai/ag_ui.py:12-12
from typing import Any

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:12-12
from typing import TYPE_CHECKING, Any, ClassVar, overload

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:9-9
from typing import TYPE_CHECKING, Any, Generic, TypeAlias, cast, overload

# pydantic_ai_slim/pydantic_ai/agent/wrapper.py:5-5
from typing import Any, overload

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:6-6
from typing import Annotated, Any, Literal, Union

# pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py:6-6
from typing_extensions import Any, TypedDict

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:11-11
from typing_extensions import Any, TypedDict

# pydantic_ai_slim/pydantic_ai/common_tools/tavily.py:5-5
from typing_extensions import Any, TypedDict

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py:5-5
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp.py:5-5
from typing import TYPE_CHECKING, Any

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:6-6
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py:6-6
from typing import Any, overload

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:2-2
from typing import Any, TypeGuard

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py:4-4
from typing import TYPE_CHECKING, Any

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:6-6
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py:6-6
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py:8-8
from typing import Any, Literal, overload

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_dynamic_toolset.py:5-5
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_function_toolset.py:4-4
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_mcp.py:5-5
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:9-9
from typing import Any, cast

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:6-6
from typing import Annotated, Any, Literal

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_workflow.py:2-2
from typing import Any

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:5-5
from typing import Any, ClassVar, Literal, get_args

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:9-9
from typing import TYPE_CHECKING, Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:3-3
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:8-8
from typing import Any

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:4-4
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/embeddings/sentence_transformers.py:6-6
from typing import Any, cast

# pydantic_ai_slim/pydantic_ai/exceptions.py:5-5
from typing import TYPE_CHECKING, Any

# pydantic_ai_slim/pydantic_ai/ext/aci.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/ext/langchain.py:3-3
from typing import Any, Protocol

# pydantic_ai_slim/pydantic_ai/format_prompt.py:8-8
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/mcp.py:14-14
from typing import Annotated, Any, overload

# pydantic_ai_slim/pydantic_ai/messages.py:14-14
from typing import TYPE_CHECKING, Annotated, Any, Literal, TypeAlias, cast, overload

# pydantic_ai_slim/pydantic_ai/models/__init__.py:17-17
from typing import Any, Generic, Literal, TypeVar, get_args, overload

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:8-8
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:10-10
from typing import TYPE_CHECKING, Any, Generic, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/cerebras.py:6-6
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:8-8
from typing import Any

# pydantic_ai_slim/pydantic_ai/models/fallback.py:7-7
from typing import TYPE_CHECKING, Any

# pydantic_ai_slim/pydantic_ai/models/function.py:10-10
from typing import Any, TypeAlias

# pydantic_ai_slim/pydantic_ai/models/gemini.py:16-16
from typing import Annotated, Any, Literal, Protocol, cast

# pydantic_ai_slim/pydantic_ai/models/google.py:9-9
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/groq.py:7-7
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:7-7
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:9-9
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py:6-6
from typing import TYPE_CHECKING, Any, cast

# pydantic_ai_slim/pydantic_ai/models/mistral.py:7-7
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/models/openai.py:12-12
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:5-5
from typing import Annotated, Any, Literal, TypeAlias, cast

# pydantic_ai_slim/pydantic_ai/models/outlines.py:13-13
from typing import TYPE_CHECKING, Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/models/test.py:9-9
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/models/wrapper.py:7-7
from typing import Any

# pydantic_ai_slim/pydantic_ai/models/xai.py:9-9
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/output.py:5-5
from typing import Any, Generic, Literal

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:7-7
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/providers/__init__.py:9-9
from typing import Any, Generic, TypeVar

# pydantic_ai_slim/pydantic_ai/providers/bedrock.py:7-7
from typing import Any, Literal, overload

# pydantic_ai_slim/pydantic_ai/providers/gateway.py:8-8
from typing import TYPE_CHECKING, Any, Literal, overload

# pydantic_ai_slim/pydantic_ai/providers/outlines.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/providers/sentence_transformers.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/result.py:7-7
from typing import TYPE_CHECKING, Any, Generic, cast, overload

# pydantic_ai_slim/pydantic_ai/retries.py:39-39
from typing import TYPE_CHECKING, Any, cast

# pydantic_ai_slim/pydantic_ai/run.py:7-7
from typing import TYPE_CHECKING, Any, Generic, Literal, overload

# pydantic_ai_slim/pydantic_ai/tools.py:5-5
from typing import Annotated, Any, Concatenate, Generic, Literal, TypeAlias, cast

# pydantic_ai_slim/pydantic_ai/toolsets/_dynamic.py:5-5
from typing import Any, TypeAlias

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:6-6
from typing import TYPE_CHECKING, Any, Generic, Literal, Protocol

# pydantic_ai_slim/pydantic_ai/toolsets/approval_required.py:5-5
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/combined.py:8-8
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/external.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py:8-8
from typing import TYPE_CHECKING, Any, Literal

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:5-5
from typing import Any, overload

# pydantic_ai_slim/pydantic_ai/toolsets/prefixed.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/renamed.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/wrapper.py:5-5
from typing import Any

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:9-17
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Generic,
    Protocol,
    cast,
    runtime_checkable,
)

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:7-7
from typing import TYPE_CHECKING, Any, Generic, Literal, TypeAlias, TypeVar, cast

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter.py:8-12
from typing import (
    TYPE_CHECKING,
    Any,
    cast,
)

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/app.py:7-7
from typing import Any, Generic

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter.py:10-10
from typing import TYPE_CHECKING, Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:7-7
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_utils.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:8-8
from typing import Annotated, Any, Literal

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:8-8
from typing import Annotated, Any, Literal

# pydantic_ai_slim/pydantic_ai/usage.py:6-6
from typing import Annotated, Any

# pydantic_evals/pydantic_evals/_utils.py:10-10
from typing import TYPE_CHECKING, Any, TypeVar

# pydantic_evals/pydantic_evals/dataset.py:24-24
from typing import TYPE_CHECKING, Any, Generic, Literal, Union, cast

# pydantic_evals/pydantic_evals/evaluators/_base.py:7-7
from typing import Any

# pydantic_evals/pydantic_evals/evaluators/_run_evaluator.py:5-5
from typing import TYPE_CHECKING, Any

# pydantic_evals/pydantic_evals/evaluators/common.py:5-5
from typing import Any, Literal, cast

# pydantic_evals/pydantic_evals/evaluators/context.py:10-10
from typing import Any, Generic

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:7-7
from typing import Any, Generic, cast

# pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py:5-5
from typing import Any

# pydantic_evals/pydantic_evals/evaluators/report_common.py:4-4
from typing import Any, Literal, cast

# pydantic_evals/pydantic_evals/evaluators/report_evaluator.py:7-7
from typing import TYPE_CHECKING, Any, Generic, cast

# pydantic_evals/pydantic_evals/evaluators/spec.py:5-5
from typing import TYPE_CHECKING, Any, cast

# pydantic_evals/pydantic_evals/generation.py:11-11
from typing import Any

# pydantic_evals/pydantic_evals/otel/span_tree.py:9-9
from typing import TYPE_CHECKING, Any

# pydantic_evals/pydantic_evals/reporting/__init__.py:7-7
from typing import Any, Generic, Literal, Protocol, cast

# pydantic_graph/pydantic_graph/_utils.py:10-10
from typing import TYPE_CHECKING, Any, TypeAlias, TypeVar, get_args, get_origin

# pydantic_graph/pydantic_graph/beta/decision.py:13-13
from typing import TYPE_CHECKING, Any, Generic, get_origin

# pydantic_graph/pydantic_graph/beta/graph.py:14-14
from typing import TYPE_CHECKING, Any, Generic, Literal, TypeGuard, cast, get_args, get_origin, overload

# pydantic_graph/pydantic_graph/beta/graph_builder.py:15-15
from typing import Any, Generic, Literal, cast, get_origin, get_type_hints, overload

# pydantic_graph/pydantic_graph/beta/join.py:14-14
from typing import Any, Generic, Literal, cast, overload

# pydantic_graph/pydantic_graph/beta/node_types.py:10-10
from typing import Any, TypeGuard

# pydantic_graph/pydantic_graph/beta/paths.py:13-13
from typing import TYPE_CHECKING, Any, Generic, get_origin

# pydantic_graph/pydantic_graph/beta/step.py:12-12
from typing import Any, Generic, Protocol, cast, get_origin, overload

# pydantic_graph/pydantic_graph/beta/util.py:8-8
from typing import Any, Generic, cast, get_args, get_origin

# pydantic_graph/pydantic_graph/graph.py:10-10
from typing import Any, Generic, cast, overload

# pydantic_graph/pydantic_graph/mermaid.py:8-8
from typing import TYPE_CHECKING, Annotated, Any, Literal, TypeAlias

# pydantic_graph/pydantic_graph/nodes.py:7-7
from typing import Any, ClassVar, Generic, get_origin, get_type_hints

# pydantic_graph/pydantic_graph/persistence/__init__.py:7-7
from typing import TYPE_CHECKING, Annotated, Any, Generic, Literal

# pydantic_graph/pydantic_graph/persistence/_utils.py:8-8
from typing import Annotated, Any, Union

# pydantic_graph/pydantic_graph/persistence/file.py:9-9
from typing import Any

# pydantic_graph/pydantic_graph/persistence/in_mem.py:13-13
from typing import Any

# tests/conftest.py:17-17
from typing import TYPE_CHECKING, Any, TypeAlias, cast

# tests/evals/test_dataset.py:7-7
from typing import Any, Literal

# tests/evals/test_evaluator_base.py:6-6
from typing import TYPE_CHECKING, Any

# tests/evals/test_evaluator_common.py:4-4
from typing import TYPE_CHECKING, Any

# tests/evals/test_evaluator_context.py:3-3
from typing import Any

# tests/evals/test_evaluators.py:4-4
from typing import Any, cast

# tests/evals/test_report_evaluators.py:6-6
from typing import Any

# tests/evals/test_reports.py:3-3
from typing import Any

# tests/evals/test_utils.py:7-7
from typing import Any

# tests/example_modules/bank_database.py:5-5
from typing import Any

# tests/example_modules/fake_database.py:4-4
from typing import Any

# tests/example_modules/mcp_server.py:1-1
from typing import Any

# tests/example_modules/weather_service.py:4-4
from typing import Any

# tests/ext/test_langchain.py:2-2
from typing import Any

# tests/graph/beta/test_edge_cases.py:6-6
from typing import Any

# tests/graph/beta/test_graph_iteration.py:6-6
from typing import Any

# tests/graph/beta/test_node_and_step.py:3-3
from typing import Any

# tests/graph/beta/test_v1_v2_integration.py:6-6
from typing import Annotated, Any

# tests/json_body_serializer.py:7-7
from typing import TYPE_CHECKING, Any

# tests/mcp_server.py:3-3
from typing import Any

# tests/models/mock_async_stream.py:12-12
from typing import Any, Generic, TypeVar

# tests/models/mock_openai.py:6-6
from typing import Any, cast

# tests/models/mock_xai.py:13-13
from typing import Any, cast

# tests/models/test_anthropic.py:11-11
from typing import Annotated, Any, TypeVar, cast

# tests/models/test_bedrock.py:5-5
from typing import Any

# tests/models/test_cerebras.py:3-3
from typing import Any, cast

# tests/models/test_cohere.py:7-7
from typing import Any, cast

# tests/models/test_fallback.py:7-7
from typing import Any, Literal

# tests/models/test_google.py:12-12
from typing import Any

# tests/models/test_groq.py:9-9
from typing import Any, Literal, cast

# tests/models/test_huggingface.py:8-8
from typing import Any, Literal, cast

# tests/models/test_mcp_sampling.py:3-3
from typing import Any

# tests/models/test_mistral.py:8-8
from typing import Any, cast

# tests/models/test_model_names.py:4-4
from typing import Any, Literal, get_args

# tests/models/test_model_test.py:9-9
from typing import Annotated, Any, Literal

# tests/models/test_openai.py:10-10
from typing import Annotated, Any, Literal, cast

# tests/models/test_outlines.py:12-12
from typing import Any

# tests/models/test_xai.py:22-22
from typing import Any

# tests/models/xai_proto_cassettes.py:38-38
from typing import Any, Literal, Protocol, cast

# tests/parts_from_messages.py:1-1
from typing import Any

# tests/providers/test_gateway.py:3-3
from typing import Any, Literal

# tests/providers/test_provider_names.py:4-4
from typing import Any

# tests/test_ag_ui.py:10-10
from typing import Any

# tests/test_agent.py:9-9
from typing import Any, Generic, Literal, TypeVar, Union

# tests/test_cli.py:5-5
from typing import Any

# tests/test_concurrency.py:7-7
from typing import TYPE_CHECKING, Any

# tests/test_dbos.py:12-12
from typing import Any, Literal

# tests/test_embeddings.py:7-7
from typing import Any, get_args

# tests/test_examples.py:13-13
from typing import Any, cast

# tests/test_exceptions.py:4-4
from typing import Any

# tests/test_fastmcp.py:8-8
from typing import Any

# tests/test_format_as_xml.py:7-7
from typing import Any

# tests/test_function_schema.py:2-2
from typing import Any

# tests/test_history_processor.py:2-2
from typing import Any

# tests/test_json_body_serializer.py:1-1
from typing import Any

# tests/test_json_schema.py:6-6
from typing import Any

# tests/test_logfire.py:5-5
from typing import Any, Literal

# tests/test_mcp.py:9-9
from typing import Any

# tests/test_parts_manager.py:4-4
from typing import Any

# tests/test_streaming.py:10-10
from typing import Any

# tests/test_temporal.py:10-10
from typing import Any, Literal

# tests/test_tools.py:5-5
from typing import Annotated, Any, Literal

# tests/test_toolsets.py:6-6
from typing import Any, TypeVar

# tests/test_ui.py:6-6
from typing import Any

# tests/test_ui_web.py:8-8
from typing import Any

# tests/test_vercel_ai.py:5-5
from typing import Any, cast

# tests/typed_agent.py:8-8
from typing import Any, TypeAlias

# tests/typed_deps.py:2-2
from typing import Any

# tests/typed_graph.py:4-4
from typing import Any

# pydantic_evals/pydantic_evals/otel/span_tree.py:487-489
    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:76-83
    def id(self) -> str | None:
        """An ID for the toolset that is unique among all toolsets registered with the same agent.

        If you're implementing a concrete implementation that users can instantiate more than once, you should let them optionally pass a custom ID to the constructor and return that here.

        A toolset needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the toolset's activities within the workflow.
        """
        raise NotImplementedError()

# pydantic_ai_slim/pydantic_ai/messages.py:681-703
class ToolReturn:
    """A structured return value for tools that need to provide both a return value and custom content to the model.

    This class allows tools to return complex responses that include:
    - A return value for actual tool return
    - Custom content (including multi-modal content) to be sent to the model as a UserPromptPart
    - Optional metadata for application use
    """

    return_value: ToolReturnContent
    """The return value to be used in the tool response."""

    _: KW_ONLY

    content: str | Sequence[UserContent] | None = None
    """The content to be sent to the model as a UserPromptPart."""

    metadata: Any = None
    """Additional data that can be accessed programmatically by the application but is not sent to the LLM."""

    kind: Literal['tool-return'] = 'tool-return'

    __repr__ = _utils.dataclasses_no_defaults_repr

# tests/test_ssrf.py:694-703
    async def test_multiple_ips_with_any_private_blocked(self) -> None:
        """Test that if any IP in the resolution is private, request is blocked."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            # DNS returns multiple IPs, one of which is private
            mock_executor.return_value = [
                (2, 1, 6, '', ('8.8.8.8', 0)),  # Public
                (10, 1, 6, '', ('::1', 0)),  # Private IPv6 loopback
            ]
            with pytest.raises(ValueError, match='Access to private/internal IP address'):
                await validate_and_resolve_url('http://attacker.com/path', allow_local=False)

# pydantic_ai_slim/pydantic_ai/messages.py:906-924
class BuiltinToolReturnPart(BaseToolReturnPart):
    """A tool return message from a built-in tool."""

    _: KW_ONLY

    provider_name: str | None = None
    """The name of the provider that generated the response.

    Required to be set when `provider_details` is set.
    """

    provider_details: dict[str, Any] | None = None
    """Additional data returned by the provider that can't be mapped to standard fields.

    This is used for data that is required to be sent back to APIs, as well as data users may want to access programmatically.
    When this field is set, `provider_name` is required to identify the provider that generated this data."""

    part_kind: Literal['builtin-tool-return'] = 'builtin-tool-return'
    """Part type identifier, this is available on all parts as a discriminator."""

# pydantic_ai_slim/pydantic_ai/mcp.py:868-934
    def __init__(
        self,
        command: str,
        args: Sequence[str],
        *,
        env: dict[str, str] | None = None,
        cwd: str | Path | None = None,
        tool_prefix: str | None = None,
        log_level: mcp_types.LoggingLevel | None = None,
        log_handler: LoggingFnT | None = None,
        timeout: float = 5,
        read_timeout: float = 5 * 60,
        process_tool_call: ProcessToolCallback | None = None,
        allow_sampling: bool = True,
        sampling_model: models.Model | None = None,
        max_retries: int = 1,
        elicitation_callback: ElicitationFnT | None = None,
        cache_tools: bool = True,
        cache_resources: bool = True,
        id: str | None = None,
        client_info: mcp_types.Implementation | None = None,
    ):
        """Build a new MCP server.

        Args:
            command: The command to run.
            args: The arguments to pass to the command.
            env: The environment variables to set in the subprocess.
            cwd: The working directory to use when spawning the process.
            tool_prefix: A prefix to add to all tools that are registered with the server.
            log_level: The log level to set when connecting to the server, if any.
            log_handler: A handler for logging messages from the server.
            timeout: The timeout in seconds to wait for the client to initialize.
            read_timeout: Maximum time in seconds to wait for new messages before timing out.
            process_tool_call: Hook to customize tool calling and optionally pass extra metadata.
            allow_sampling: Whether to allow MCP sampling through this client.
            sampling_model: The model to use for sampling.
            max_retries: The maximum number of times to retry a tool call.
            elicitation_callback: Callback function to handle elicitation requests from the server.
            cache_tools: Whether to cache the list of tools.
                See [`MCPServer.cache_tools`][pydantic_ai.mcp.MCPServer.cache_tools].
            cache_resources: Whether to cache the list of resources.
                See [`MCPServer.cache_resources`][pydantic_ai.mcp.MCPServer.cache_resources].
            id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.
            client_info: Information describing the MCP client implementation.
        """
        self.command = command
        self.args = args
        self.env = env
        self.cwd = cwd

        super().__init__(
            tool_prefix,
            log_level,
            log_handler,
            timeout,
            read_timeout,
            process_tool_call,
            allow_sampling,
            sampling_model,
            max_retries,
            elicitation_callback,
            cache_tools,
            cache_resources,
            id=id,
            client_info=client_info,
        )

# pydantic_ai_slim/pydantic_ai/models/openai.py:1030-1037
        def _map_response_builtin_part(self, item: BuiltinToolCallPart | BuiltinToolReturnPart) -> None:
            """Maps a built-in tool call or return part to the response context.

            This method serves as a hook that can be overridden by subclasses
            to implement custom logic for handling built-in tool parts.
            """
            # OpenAI doesn't return built-in tool calls
            pass

# pydantic_evals/pydantic_evals/evaluators/common.py:63-128
class Contains(Evaluator[object, object, object]):
    """Check if the output contains the expected output.

    For strings, checks if expected_output is a substring of output.
    For lists/tuples, checks if expected_output is in output.
    For dicts, checks if all key-value pairs in expected_output are in output.

    Note: case_sensitive only applies when both the value and output are strings.
    """

    value: Any
    case_sensitive: bool = True
    as_strings: bool = False
    evaluation_name: str | None = field(default=None)

    def evaluate(
        self,
        ctx: EvaluatorContext[object, object, object],
    ) -> EvaluationReason:
        # Convert objects to strings if requested
        failure_reason: str | None = None
        as_strings = self.as_strings or (isinstance(self.value, str) and isinstance(ctx.output, str))
        if as_strings:
            output_str = str(ctx.output)
            expected_str = str(self.value)

            if not self.case_sensitive:
                output_str = output_str.lower()
                expected_str = expected_str.lower()

            failure_reason: str | None = None
            if expected_str not in output_str:
                output_trunc = _truncated_repr(output_str, max_length=100)
                expected_trunc = _truncated_repr(expected_str, max_length=100)
                failure_reason = f'Output string {output_trunc} does not contain expected string {expected_trunc}'
            return EvaluationReason(value=failure_reason is None, reason=failure_reason)

        try:
            # Handle different collection types
            if isinstance(ctx.output, dict):
                if isinstance(self.value, dict):
                    # Cast to Any to avoid type checking issues
                    output_dict = cast(dict[Any, Any], ctx.output)  # pyright: ignore[reportUnknownMemberType]
                    expected_dict = cast(dict[Any, Any], self.value)  # pyright: ignore[reportUnknownMemberType]
                    for k in expected_dict:
                        if k not in output_dict:
                            k_trunc = _truncated_repr(k, max_length=30)
                            failure_reason = f'Output dictionary does not contain expected key {k_trunc}'
                            break
                        elif output_dict[k] != expected_dict[k]:
                            k_trunc = _truncated_repr(k, max_length=30)
                            output_v_trunc = _truncated_repr(output_dict[k], max_length=100)
                            expected_v_trunc = _truncated_repr(expected_dict[k], max_length=100)
                            failure_reason = f'Output dictionary has different value for key {k_trunc}: {output_v_trunc} != {expected_v_trunc}'
                            break
                else:
                    if self.value not in ctx.output:  # pyright: ignore[reportUnknownMemberType]
                        output_trunc = _truncated_repr(ctx.output, max_length=200)  # pyright: ignore[reportUnknownMemberType]
                        failure_reason = f'Output {output_trunc} does not contain provided value as a key'
            elif self.value not in ctx.output:  # pyright: ignore[reportOperatorIssue]  # will be handled by except block
                output_trunc = _truncated_repr(ctx.output, max_length=200)
                failure_reason = f'Output {output_trunc} does not contain provided value'
        except (TypeError, ValueError) as e:
            failure_reason = f'Containment check failed: {e}'

        return EvaluationReason(value=failure_reason is None, reason=failure_reason)

# pydantic_ai_slim/pydantic_ai/messages.py:553-566
    def identifier(self) -> str:
        """Identifier for the binary content, such as a unique ID.

        This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,
        and the tool can look up the file in question by iterating over the message history and finding the matching `BinaryContent`.

        This identifier is only automatically passed to the model when the `BinaryContent` is returned by a tool.
        If you're passing the `BinaryContent` as a user message, it's up to you to include a separate text part with the identifier,
        e.g. "This is file <identifier>:" preceding the `BinaryContent`.

        It's also included in inline-text delimiters for providers that require inlining text documents, so the model can
        distinguish multiple files.
        """
        return self._identifier or _multi_modal_content_identifier(self.data)

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:146-162
    def get_default_evaluation_name(self) -> str:
        """Return the default name to use in reports for the output of this evaluator.

        By default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used.
        Otherwise, the serialization name of the evaluator (which is usually the class name) will be used.

        This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.

        Note that evaluators that return a mapping of results will always use the keys of that mapping as the names
        of the associated evaluation results.
        """
        evaluation_name = getattr(self, 'evaluation_name', None)
        if isinstance(evaluation_name, str):
            # If the evaluator has an attribute `name` of type string, use that
            return evaluation_name

        return self.get_serialization_name()

# pydantic_ai_slim/pydantic_ai/models/mistral.py:718-745
    def _validate_required_json_schema(json_dict: dict[str, Any], json_schema: dict[str, Any]) -> bool:
        """Validate that all required parameters in the JSON schema are present in the JSON dictionary."""
        required_params = json_schema.get('required', [])
        properties = json_schema.get('properties', {})

        for param in required_params:
            if param not in json_dict:
                return False

            param_schema = properties.get(param, {})
            param_type = param_schema.get('type')
            param_items_type = param_schema.get('items', {}).get('type')

            if param_type == 'array' and param_items_type:
                if not isinstance(json_dict[param], list):
                    return False
                for item in json_dict[param]:
                    if not isinstance(item, VALID_JSON_TYPE_MAPPING[param_items_type]):
                        return False
            elif param_type and not isinstance(json_dict[param], VALID_JSON_TYPE_MAPPING[param_type]):
                return False

            if isinstance(json_dict[param], dict) and 'properties' in param_schema:
                nested_schema = param_schema
                if not MistralStreamedResponse._validate_required_json_schema(json_dict[param], nested_schema):
                    return False

        return True

# pydantic_ai_slim/pydantic_ai/exceptions.py:89-101
class ApprovalRequired(Exception):
    """Exception to raise when a tool call requires human-in-the-loop approval.

    See [tools docs](../deferred-tools.md#human-in-the-loop-tool-approval) for more information.

    Args:
        metadata: Optional dictionary of metadata to attach to the deferred tool call.
            This metadata will be available in `DeferredToolRequests.metadata` keyed by `tool_call_id`.
    """

    def __init__(self, metadata: dict[str, Any] | None = None):
        self.metadata = metadata
        super().__init__()

# pydantic_graph/pydantic_graph/nodes.py:49-65
    async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:
        """Run the node.

        This is an abstract method that must be implemented by subclasses.

        !!! note "Return types used at runtime"
            The return type of this method are read by `pydantic_graph` at runtime and used to define which
            nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)
            and enforced when running the graph.

        Args:
            ctx: The graph context.

        Returns:
            The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.
        """
        ...

# tests/models/mock_async_stream.py:22-63
class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:449-649
    async def run_stream(  # noqa: C901
        self,
        user_prompt: str | Sequence[_messages.UserContent] | None = None,
        *,
        output_type: OutputSpec[RunOutputDataT] | None = None,
        message_history: Sequence[_messages.ModelMessage] | None = None,
        deferred_tool_results: DeferredToolResults | None = None,
        model: models.Model | models.KnownModelName | str | None = None,
        instructions: Instructions[AgentDepsT] = None,
        deps: AgentDepsT = None,
        model_settings: ModelSettings | None = None,
        usage_limits: _usage.UsageLimits | None = None,
        usage: _usage.RunUsage | None = None,
        metadata: AgentMetadata[AgentDepsT] | None = None,
        infer_name: bool = True,
        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,
        builtin_tools: Sequence[AbstractBuiltinTool | BuiltinToolFunc[AgentDepsT]] | None = None,
        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,
    ) -> AsyncIterator[result.StreamedRunResult[AgentDepsT, Any]]:
        """Run the agent with a user prompt in async streaming mode.

        This method builds an internal agent graph (using system prompts, tools and output schemas) and then
        runs the graph until the model produces output matching the `output_type`, for example text or structured data.
        At this point, a streaming run result object is yielded from which you can stream the output as it comes in,
        and -- once this output has completed streaming -- get the complete output, message history, and usage.

        As this method will consider the first output matching the `output_type` to be the final output,
        it will stop running the agent graph and will not execute any tool calls made by the model after this "final" output.
        If you want to always run the agent graph to completion and stream events and output at the same time,
        use [`agent.run()`][pydantic_ai.agent.AbstractAgent.run] with an `event_stream_handler` or [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] instead.

        Example:
        ```python
        from pydantic_ai import Agent

        agent = Agent('openai:gpt-5.2')

        async def main():
            async with agent.run_stream('What is the capital of the UK?') as response:
                print(await response.get_output())
                #> The capital of the UK is London.
        ```

        Args:
            user_prompt: User input to start/continue the conversation.
            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
                output validators since output validators would expect an argument that matches the agent's output type.
            message_history: History of the conversation so far.
            deferred_tool_results: Optional results for deferred tool calls in the message history.
            model: Optional model to use for this run, required if `model` was not set when creating the agent.
            instructions: Optional additional instructions to use for this run.
            deps: Optional dependencies to use for this run.
            model_settings: Optional settings to use for this model's request.
            usage_limits: Optional limits on model request count or token usage.
            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
            metadata: Optional metadata to attach to this run. Accepts a dictionary or a callable taking
                [`RunContext`][pydantic_ai.tools.RunContext]; merged with the agent's configured metadata.
            infer_name: Whether to try to infer the agent name from the call frame if it's not set.
            toolsets: Optional additional toolsets for this run.
            builtin_tools: Optional additional builtin tools for this run.
            event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.
                It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.
                Note that it does _not_ receive any events after the final result is found.

        Returns:
            The result of the run.
        """
        if infer_name and self.name is None:
            # f_back because `asynccontextmanager` adds one frame
            if frame := inspect.currentframe():  # pragma: no branch
                self._infer_name(frame.f_back)

        event_stream_handler = event_stream_handler or self.event_stream_handler

        yielded = False
        async with self.iter(
            user_prompt,
            output_type=output_type,
            message_history=message_history,
            deferred_tool_results=deferred_tool_results,
            model=model,
            deps=deps,
            instructions=instructions,
            model_settings=model_settings,
            usage_limits=usage_limits,
            usage=usage,
            metadata=metadata,
            infer_name=False,
            toolsets=toolsets,
            builtin_tools=builtin_tools,
        ) as agent_run:
            first_node = agent_run.next_node  # start with the first node
            assert isinstance(first_node, _agent_graph.UserPromptNode)  # the first node should be a user prompt node
            node = first_node
            while True:
                graph_ctx = agent_run.ctx
                if self.is_model_request_node(node):
                    async with node.stream(graph_ctx) as stream:
                        final_result_event = None

                        async def stream_to_final(
                            stream: AgentStream,
                        ) -> AsyncIterator[_messages.ModelResponseStreamEvent]:
                            nonlocal final_result_event
                            async for event in stream:
                                yield event
                                if isinstance(event, _messages.FinalResultEvent):
                                    final_result_event = event
                                    break

                        if event_stream_handler is not None:
                            await event_stream_handler(
                                _agent_graph.build_run_context(graph_ctx), stream_to_final(stream)
                            )
                        else:
                            async for _ in stream_to_final(stream):
                                pass

                        if final_result_event is not None:
                            final_result = FinalResult(
                                None, final_result_event.tool_name, final_result_event.tool_call_id
                            )
                            if yielded:
                                raise exceptions.AgentRunError('Agent run produced final results')  # pragma: no cover
                            yielded = True

                            messages = graph_ctx.state.message_history.copy()

                            async def on_complete() -> None:
                                """Called when the stream has completed.

                                The model response will have been added to messages by now
                                by `StreamedRunResult._marked_completed`.
                                """
                                nonlocal final_result
                                final_result = FinalResult(
                                    await stream.get_output(), final_result.tool_name, final_result.tool_call_id
                                )

                                # When we get here, the `ModelRequestNode` has completed streaming after the final result was found.
                                # When running an agent with `agent.run`, we'd then move to `CallToolsNode` to execute the tool calls and
                                # find the final result.
                                # We also want to execute tool calls (in case `agent.end_strategy == 'exhaustive'`) here, but
                                # we don't want to use run the `CallToolsNode` logic to determine the final output, as it would be
                                # wasteful and could produce a different result (e.g. when text output is followed by tool calls).
                                # So we call `process_tool_calls` directly and then end the run with the found final result.

                                parts: list[_messages.ModelRequestPart] = []
                                async for _event in _agent_graph.process_tool_calls(
                                    tool_manager=graph_ctx.deps.tool_manager,
                                    tool_calls=stream.response.tool_calls,
                                    tool_call_results=None,
                                    tool_call_metadata=None,
                                    final_result=final_result,
                                    ctx=graph_ctx,
                                    output_parts=parts,
                                ):
                                    pass

                                # To allow this message history to be used in a future run without dangling tool calls,
                                # append a new ModelRequest using the tool returns and retries
                                if parts:
                                    messages.append(
                                        _messages.ModelRequest(
                                            parts, run_id=graph_ctx.state.run_id, timestamp=_utils.now_utc()
                                        )
                                    )

                                await agent_run.next(_agent_graph.SetFinalResult(final_result))

                            yield StreamedRunResult(
                                messages,
                                graph_ctx.deps.new_message_index,
                                stream,
                                on_complete,
                            )
                            break
                elif self.is_call_tools_node(node) and event_stream_handler is not None:
                    async with node.stream(agent_run.ctx) as stream:
                        await event_stream_handler(_agent_graph.build_run_context(agent_run.ctx), stream)

                next_node = await agent_run.next(node)
                if isinstance(next_node, End) and agent_run.result is not None:
                    # A final output could have been produced by the CallToolsNode rather than the ModelRequestNode,
                    # if a tool function raised CallDeferred or ApprovalRequired.
                    # In this case there's no response to stream, but we still let the user access the output etc as normal.
                    yield StreamedRunResult(
                        graph_ctx.state.message_history,
                        graph_ctx.deps.new_message_index,
                        run_result=agent_run.result,
                    )
                    yielded = True
                    break
                if not isinstance(next_node, _agent_graph.AgentNode):
                    raise exceptions.AgentRunError(  # pragma: no cover
                        'Should have produced a StreamedRunResult before getting here'
                    )
                node = cast(_agent_graph.AgentNode[Any, Any], next_node)

        if not yielded:
            raise exceptions.AgentRunError('Agent run finished without producing a final result')  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/app.py:42-148
    def __init__(
        self,
        agent: AbstractAgent[AgentDepsT, OutputDataT],
        *,
        # AGUIAdapter.dispatch_request parameters
        output_type: OutputSpec[Any] | None = None,
        message_history: Sequence[ModelMessage] | None = None,
        deferred_tool_results: DeferredToolResults | None = None,
        model: Model | KnownModelName | str | None = None,
        deps: AgentDepsT = None,
        model_settings: ModelSettings | None = None,
        usage_limits: UsageLimits | None = None,
        usage: RunUsage | None = None,
        infer_name: bool = True,
        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,
        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,
        on_complete: OnCompleteFunc[Any] | None = None,
        # Starlette parameters
        debug: bool = False,
        routes: Sequence[BaseRoute] | None = None,
        middleware: Sequence[Middleware] | None = None,
        exception_handlers: Mapping[Any, ExceptionHandler] | None = None,
        on_startup: Sequence[Callable[[], Any]] | None = None,
        on_shutdown: Sequence[Callable[[], Any]] | None = None,
        lifespan: Lifespan[Self] | None = None,
    ) -> None:
        """An ASGI application that handles every request by running the agent and streaming the response.

        Note that the `deps` will be the same for each request, with the exception of the frontend state that's
        injected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ui.StateHandler] protocol.
        To provide different `deps` for each request (e.g. based on the authenticated user),
        use [`AGUIAdapter.run_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.run_stream] or
        [`AGUIAdapter.dispatch_request()`][pydantic_ai.ui.ag_ui.AGUIAdapter.dispatch_request] instead.

        Args:
            agent: The agent to run.

            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has
                no output validators since output validators would expect an argument that matches the agent's
                output type.
            message_history: History of the conversation so far.
            deferred_tool_results: Optional results for deferred tool calls in the message history.
            model: Optional model to use for this run, required if `model` was not set when creating the agent.
            deps: Optional dependencies to use for this run.
            model_settings: Optional settings to use for this model's request.
            usage_limits: Optional limits on model request count or token usage.
            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
            infer_name: Whether to try to infer the agent name from the call frame if it's not set.
            toolsets: Optional additional toolsets for this run.
            builtin_tools: Optional additional builtin tools for this run.
            on_complete: Optional callback function called when the agent run completes successfully.
                The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.

            debug: Boolean indicating if debug tracebacks should be returned on errors.
            routes: A list of routes to serve incoming HTTP and WebSocket requests.
            middleware: A list of middleware to run for every request. A starlette application will always
                automatically include two middleware classes. `ServerErrorMiddleware` is added as the very
                outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.
                `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled
                exception cases occurring in the routing or endpoints.
            exception_handlers: A mapping of either integer status codes, or exception class types onto
                callables which handle the exceptions. Exception handler callables should be of the form
                `handler(request, exc) -> response` and may be either standard functions, or async functions.
            on_startup: A list of callables to run on application startup. Startup handler callables do not
                take any arguments, and may be either standard functions, or async functions.
            on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do
                not take any arguments, and may be either standard functions, or async functions.
            lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.
                This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or
                the other, not both.
        """
        super().__init__(
            debug=debug,
            routes=routes,
            middleware=middleware,
            exception_handlers=exception_handlers,
            on_startup=on_startup,
            on_shutdown=on_shutdown,
            lifespan=lifespan,
        )

        async def run_agent(request: Request) -> Response:
            """Endpoint to run the agent with the provided input data."""
            # `dispatch_request` will store the frontend state from the request on `deps.state` (if it implements the `StateHandler` protocol),
            # so we need to copy the deps to avoid different requests mutating the same deps object.
            nonlocal deps
            if isinstance(deps, StateHandler):  # pragma: no branch
                deps = replace(deps)

            return await AGUIAdapter[AgentDepsT, OutputDataT].dispatch_request(
                request,
                agent=agent,
                output_type=output_type,
                message_history=message_history,
                deferred_tool_results=deferred_tool_results,
                model=model,
                deps=deps,
                model_settings=model_settings,
                usage_limits=usage_limits,
                usage=usage,
                infer_name=infer_name,
                toolsets=toolsets,
                builtin_tools=builtin_tools,
                on_complete=on_complete,
            )

        self.router.add_route('/', run_agent, methods=['POST'])

# pydantic_graph/pydantic_graph/beta/util.py:16-31
class TypeExpression(Generic[T]):
    """A workaround for type checker limitations when using complex type expressions.

        This class serves as a wrapper for types that cannot normally be used in positions
    requiring `type[T]`, such as `Any`, `Union[...]`, or `Literal[...]`. It provides a
        way to pass these complex type expressions to functions expecting concrete types.

    Example:
            Instead of `output_type=Union[str, int]` (which may cause type errors),
            use `output_type=TypeExpression[Union[str, int]]`.

    Note:
            This is a workaround for the lack of TypeForm in the Python type system.
    """

    pass

# pydantic_ai_slim/pydantic_ai/embeddings/test.py:65-85
    def __init__(
        self,
        model_name: str = 'test',
        *,
        provider_name: str = 'test',
        dimensions: int = 8,
        settings: EmbeddingSettings | None = None,
    ):
        """Initialize the test embedding model.

        Args:
            model_name: The model name to report in results.
            provider_name: The provider name to report in results.
            dimensions: The number of dimensions for the generated embeddings.
            settings: Optional default settings for the model.
        """
        self._model_name = model_name
        self._provider_name = provider_name
        self._dimensions = dimensions
        self.last_settings = None
        super().__init__(settings=settings)

# pydantic_evals/pydantic_evals/reporting/__init__.py:682-688
class RenderValueConfig(TypedDict, total=False):
    """A configuration for rendering a values in an Evaluation report."""

    value_formatter: str | Callable[[Any], str]
    diff_checker: Callable[[Any, Any], bool] | None
    diff_formatter: Callable[[Any, Any], str | None] | None
    diff_style: str

# tests/test_ssrf.py:365-370
    async def test_literal_ip_address_in_url(self) -> None:
        """Test handling of literal IP addresses in URLs."""
        # Public IP - should work
        resolved = await validate_and_resolve_url('http://8.8.8.8/path', allow_local=False)
        assert resolved.resolved_ip == '8.8.8.8'
        assert resolved.hostname == '8.8.8.8'

# tests/test_temporal.py:2579-2579
    values: list[int] = field(default_factory=list[int])