# pydantic_ai_slim/pydantic_ai/_agent_graph.py:60-60
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/_output.py:41-41
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/_utils.py:136-136
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:99-99
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:59-59
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:156-156
T = typing.TypeVar('T')

# pydantic_ai_slim/pydantic_ai/output.py:35-35
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/result.py:42-42
T = TypeVar('T')

# pydantic_evals/pydantic_evals/_utils.py:34-34
T = TypeVar('T')

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:56-56
T = TypeVar('T')

# pydantic_evals/pydantic_evals/reporting/__init__.py:934-934
T = TypeVar('T')

# pydantic_graph/pydantic_graph/_utils.py:126-126
T = TypeVar('T')

# pydantic_graph/pydantic_graph/beta/decision.py:36-36
T = TypeVar('T', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/graph_builder.py:61-61
T = TypeVar('T', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/join.py:25-25
T = TypeVar('T', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/parent_forks.py:29-29
T = TypeVar('T', bound=Hashable, infer_variance=True, default=str)

# pydantic_graph/pydantic_graph/beta/paths.py:26-26
T = TypeVar('T')

# pydantic_graph/pydantic_graph/beta/util.py:12-12
T = TypeVar('T', infer_variance=True)

# tests/models/mock_async_stream.py:18-18
T = TypeVar('T')

# tests/models/test_anthropic.py:127-127
T = TypeVar('T')

# tests/test_agent.py:108-108
T = TypeVar('T')

# tests/test_toolsets.py:34-34
T = TypeVar('T')

# pydantic_graph/pydantic_graph/beta/graph_builder.py:469-489
    def match(
        self,
        source: TypeOrTypeExpression[SourceT],
        *,
        matches: Callable[[Any], bool] | None = None,
    ) -> DecisionBranchBuilder[StateT, DepsT, SourceT, SourceT, Never]:
        """Create a decision branch matcher.

        Args:
            source: The type or type expression to match against
            matches: Optional custom matching function

        Returns:
            A DecisionBranchBuilder for constructing the branch
        """
        # Note, the following node_id really is just a placeholder and shouldn't end up in the final graph
        # This is why we don't expose a way for end users to override the value used here.
        node_id = NodeID(generate_placeholder_node_id('match_decision'))
        decision = Decision[StateT, DepsT, Never](id=node_id, branches=[], note=None)
        new_path_builder = PathBuilder[StateT, DepsT, SourceT](working_items=[])
        return DecisionBranchBuilder(decision=decision, source=source, matches=matches, path_builder=new_path_builder)

# tests/graph/test_mermaid.py:431-441
def test_edge_union():
    """Test that a union of things annotated with an Edge doesn't raise a TypeError.

    This is important because such unions may occur as a return type for a graph, and needs to be evaluated when
    generating a mermaid diagram.
    """
    # This would raise an error on 3.10 if Edge was not hashable:
    edges_union = Union[  # noqa: UP007
        Annotated[End[None], Edge(label='first label')], Annotated[End[None], Edge(label='second label')]
    ]
    assert edges_union

# pydantic_graph/pydantic_graph/beta/join.py:106-109
def reduce_list_append(current: list[T], inputs: T) -> list[T]:
    """A reducer that appends to a list."""
    current.append(inputs)
    return current

# pydantic_evals/pydantic_evals/_utils.py:37-46
def is_set(t_or_unset: T | Unset) -> TypeIs[T]:
    """Check if a value is set (not the UNSET singleton).

    Args:
        t_or_unset: The value to check, which may be the UNSET singleton or a regular value.

    Returns:
        True if the value is not UNSET, narrowing the type to T in a type-aware way.
    """
    return t_or_unset is not UNSET

# pydantic_graph/pydantic_graph/beta/join.py:112-115
def reduce_list_extend(current: list[T], inputs: Iterable[T]) -> list[T]:
    """A reducer that extends a list."""
    current.extend(inputs)
    return current

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:1461-1471
    def _get_deps(self: Agent[T, OutputDataT], deps: T) -> T:
        """Get deps for a run.

        If we've overridden deps via `_override_deps`, use that, otherwise use the deps passed to the call.

        We could do runtime type checking of deps against `self._deps_type`, but that's a slippery slope.
        """
        if some_deps := self._override_deps.get():
            return some_deps.value
        else:
            return deps

# tests/test_temporal.py:2579-2579
    values: list[int] = field(default_factory=list[int])

# tests/test_streaming.py:1082-1105
    async def test_output_function_structured_double_get_output(self):
        """Test that calling `get_output()` twice works correctly.

        The first `get_output()` should do validation and cache the result.
        The second `get_output()` should return cached results without re-validation.
        """
        call_log: list[tuple[Foo, bool]] = []

        def process_foo(ctx: RunContext[None], foo: Foo) -> Foo:
            call_log.append((foo, ctx.partial_output))
            return Foo(a=foo.a * 2, b=foo.b.upper())

        async def sf(_: list[ModelMessage], info: AgentInfo) -> AsyncIterator[DeltaToolCalls]:
            assert info.output_tools is not None
            yield {0: DeltaToolCall(name=info.output_tools[0].name, json_args='{"a": 21, "b": "foo"}')}

        agent = Agent(FunctionModel(stream_function=sf), output_type=ToolOutput(process_foo, name='my_output'))

        async with agent.run_stream('test') as result:
            output1 = await result.get_output()
            output2 = await result.get_output()

        assert output1 == output2 == Foo(a=42, b='FOO')
        assert call_log == snapshot([(Foo(a=21, b='foo'), False)])

# examples/pydantic_ai_examples/evals/agent.py:12-20
class TimeRangeDeps:
    """Dependencies for the time range inference agent.

    While we could just get the current time using datetime.now() directly in the tools or system prompt, passing it
    via deps makes it easier to use a repeatable value during testing. While there are packages like `time-machine`
    that can do this for you, that kind of monkey-patching approach can become unwieldy as things get more complex.
    """

    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

# tests/graph/beta/test_broadcast_and_spread.py:17-17
    values: list[int] = field(default_factory=list[int])

# pydantic_graph/pydantic_graph/exceptions.py:59-62
    def check(cls, status: 'SnapshotStatus') -> None:
        """Check if the status is valid."""
        if status not in {'created', 'pending'}:
            raise cls(status)

# pydantic_evals/pydantic_evals/_utils.py:49-75
def get_unwrapped_function_name(func: Callable[..., Any]) -> str:
    """Get the name of a function, unwrapping partials and decorators.

    Args:
        func: The function to get the name of.

    Returns:
        The name of the function.

    Raises:
        AttributeError: If the function doesn't have a __name__ attribute and isn't a method.
    """

    def _unwrap(f: Callable[..., Any]) -> Callable[..., Any]:
        """Unwraps f, also unwrapping partials, for the sake of getting f's name."""
        if isinstance(f, partial):
            return _unwrap(f.func)
        return inspect.unwrap(f)

    try:
        return _unwrap(func).__name__
    except AttributeError as e:
        # Handle instances of types with `__call__` as a method
        if inspect.ismethod(getattr(func, '__call__', None)):
            return f'{type(func).__qualname__}.__call__'
        else:
            raise e

# pydantic_graph/pydantic_graph/beta/parent_forks.py:199-232
    def _get_upstream_nodes_if_parent(self, join_id: T, fork_id: T) -> set[T] | None:
        """Check if a fork is a valid parent and return upstream nodes.

        Tests whether the given fork can serve as a parent fork for the join by checking
        for cycles that bypass the fork. If valid, returns all nodes that can reach the
        join without going through the fork.

        Args:
            join_id: The join node being analyzed.
            fork_id: The potential parent fork to test.

        Returns:
            The set of node IDs upstream of the join (excluding the fork) if the fork is
            a valid parent, or None if a cycle exists that bypasses the fork (making it
            invalid as a parent fork).

        Note:
            If, in the graph with fork_id removed, a path exists that starts and ends at
            the join (i.e., join is on a cycle avoiding the fork), we return None because
            the fork would not be a valid "parent fork".
        """
        upstream: set[T] = set()
        stack = [join_id]
        while stack:
            v = stack.pop()
            for p in self._predecessors[v]:
                if p == fork_id:
                    continue
                if p == join_id:
                    return None  # J sits on a cycle w/out the specified node
                if p not in upstream:
                    upstream.add(p)
                    stack.append(p)
        return upstream

# pydantic_graph/pydantic_graph/beta/join.py:144-147
    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_evals/pydantic_evals/_utils.py:20-28
class Unset:
    """A singleton to represent an unset value.

    Used to distinguish between explicitly set `None` values and values that were never set.

    Copied from pydantic_ai/_utils.py.
    """

    pass

# tests/conftest.py:147-149
    def set(self, name: str, value: str) -> None:
        self.envars[name] = os.getenv(name)
        os.environ[name] = value

# pydantic_ai_slim/pydantic_ai/tools.py:235-235
A = TypeVar('A')