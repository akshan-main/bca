# pydantic_ai_slim/pydantic_ai/tools.py:235-235
A = TypeVar('A')

# tests/typed_graph.py:90-94
class A(BaseNode[MyState, MyDeps]):
    async def run(self, ctx: GraphRunContext[MyState, MyDeps]) -> B:
        assert ctx.state.x == 1
        assert ctx.deps.y == 'y'
        return B()

# pydantic_ai_slim/pydantic_ai/concurrency.py:276-295
def normalize_to_limiter(
    limit: AnyConcurrencyLimit,
    *,
    name: str | None = None,
) -> AbstractConcurrencyLimiter | None:
    """Normalize a concurrency limit configuration to an AbstractConcurrencyLimiter.

    Args:
        limit: The concurrency limit configuration.
        name: Optional name for the limiter if one is created.

    Returns:
        An AbstractConcurrencyLimiter if limit is not None, otherwise None.
    """
    if limit is None:
        return None
    elif isinstance(limit, AbstractConcurrencyLimiter):
        return limit
    else:
        return ConcurrencyLimiter.from_limit(limit, name=name)

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:24-51
def default_render_number(value: float | int) -> str:
    """The default logic for formatting numerical values in an Evaluation report.

    * If the value is an integer, format it as an integer.
    * If the value is a float, include at least one decimal place and at least 3 significant figures.
    """
    # If it's an int, just return its string representation.
    if isinstance(value, int):
        return f'{value:,d}'

    abs_val = abs(value)

    # Special case for zero:
    if abs_val == 0:
        return f'{value:,.{VALUE_SIG_FIGS}f}'

    if abs_val >= 1:
        # Count the digits in the integer part.
        digits = math.floor(math.log10(abs_val)) + 1
        # Number of decimals: at least one, and enough to reach 4 significant figures.
        decimals = max(1, VALUE_SIG_FIGS - digits)
    else:
        # For numbers between 0 and 1, determine the exponent.
        # For example: 0.1 -> log10(0.1) = -1, so we want -(-1) + 3 = 4 decimals.
        exponent = math.floor(math.log10(abs_val))
        decimals = -exponent + VALUE_SIG_FIGS - 1  # because the first nonzero digit is in the 10^exponent place.

    return f'{value:,.{decimals}f}'

# tests/test_validation_context.py:22-27
class Value(BaseModel):
    x: int

    @field_validator('x')
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:60-60
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/_output.py:41-41
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/_utils.py:136-136
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:99-99
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:59-59
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:156-156
T = typing.TypeVar('T')

# pydantic_ai_slim/pydantic_ai/output.py:35-35
T = TypeVar('T')

# pydantic_ai_slim/pydantic_ai/result.py:42-42
T = TypeVar('T')

# pydantic_evals/pydantic_evals/_utils.py:34-34
T = TypeVar('T')

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:56-56
T = TypeVar('T')

# pydantic_evals/pydantic_evals/reporting/__init__.py:934-934
T = TypeVar('T')

# pydantic_graph/pydantic_graph/_utils.py:126-126
T = TypeVar('T')

# pydantic_graph/pydantic_graph/beta/decision.py:36-36
T = TypeVar('T', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/graph_builder.py:61-61
T = TypeVar('T', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/join.py:25-25
T = TypeVar('T', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/parent_forks.py:29-29
T = TypeVar('T', bound=Hashable, infer_variance=True, default=str)

# pydantic_graph/pydantic_graph/beta/paths.py:26-26
T = TypeVar('T')

# pydantic_graph/pydantic_graph/beta/util.py:12-12
T = TypeVar('T', infer_variance=True)

# tests/models/mock_async_stream.py:18-18
T = TypeVar('T')

# tests/models/test_anthropic.py:127-127
T = TypeVar('T')

# tests/test_agent.py:108-108
T = TypeVar('T')

# tests/test_toolsets.py:34-34
T = TypeVar('T')

# pydantic_evals/pydantic_evals/evaluators/spec.py:124-142
    def enforce_one_key(cls, value: str | dict[str, Any]) -> Any:
        """Enforce that the root value has exactly one key (the evaluator name) when it is a dict.

        Args:
            value: The value to validate.

        Returns:
            The validated value.

        Raises:
            ValueError: If the value is a dict with multiple keys.
        """
        if isinstance(value, str):
            return value
        if len(value) != 1:
            raise ValueError(
                f'Expected a single key containing the Evaluator class name, found keys {list(value.keys())}'
            )
        return value

# pydantic_evals/pydantic_evals/_utils.py:37-46
def is_set(t_or_unset: T | Unset) -> TypeIs[T]:
    """Check if a value is set (not the UNSET singleton).

    Args:
        t_or_unset: The value to check, which may be the UNSET singleton or a regular value.

    Returns:
        True if the value is not UNSET, narrowing the type to T in a type-aware way.
    """
    return t_or_unset is not UNSET

# tests/test_agent.py:150-150
    a: int

# tests/test_agent_output_schemas.py:24-24
    a: list[Bar]

# tests/test_streaming.py:65-65
    a: int

# tests/typed_agent.py:154-154
    a: int

# pydantic_ai_slim/pydantic_ai/_utils.py:305-372
class PeekableAsyncStream(Generic[T]):
    """Wraps an async iterable of type T and allows peeking at the *next* item without consuming it.

    We only buffer one item at a time (the next item). Once that item is yielded, it is discarded.
    This is a single-pass stream.
    """

    def __init__(self, source: AsyncIterable[T]):
        self._source = source
        self._source_iter: AsyncIterator[T] | None = None
        self._buffer: T | Unset = UNSET
        self._exhausted = False

    async def peek(self) -> T | Unset:
        """Returns the next item that would be yielded without consuming it.

        Returns None if the stream is exhausted.
        """
        if self._exhausted:
            return UNSET

        # If we already have a buffered item, just return it.
        if not isinstance(self._buffer, Unset):
            return self._buffer

        # Otherwise, we need to fetch the next item from the underlying iterator.
        if self._source_iter is None:
            self._source_iter = aiter(self._source)

        try:
            self._buffer = await anext(self._source_iter)
        except StopAsyncIteration:
            self._exhausted = True
            return UNSET

        return self._buffer

    async def is_exhausted(self) -> bool:
        """Returns True if the stream is exhausted, False otherwise."""
        return isinstance(await self.peek(), Unset)

    def __aiter__(self) -> AsyncIterator[T]:
        # For a single-pass iteration, we can return self as the iterator.
        return self

    async def __anext__(self) -> T:
        """Yields the buffered item if present, otherwise fetches the next item from the underlying source.

        Raises StopAsyncIteration if the stream is exhausted.
        """
        if self._exhausted:
            raise StopAsyncIteration

        # If we have a buffered item, yield it.
        if not isinstance(self._buffer, Unset):
            item = self._buffer
            self._buffer = UNSET
            return item

        # Otherwise, fetch the next item from the source.
        if self._source_iter is None:
            self._source_iter = aiter(self._source)

        try:
            return await anext(self._source_iter)
        except StopAsyncIteration:
            self._exhausted = True
            raise

# pydantic_evals/pydantic_evals/evaluators/common.py:73-73
    value: Any

# pydantic_graph/pydantic_graph/beta/graph.py:86-87
    def value(self) -> OutputT:
        return self._value

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:73-73
    value: EvaluationScalarT

# pydantic_ai_slim/pydantic_ai/_utils.py:143-143
    value: T

# pydantic_graph/pydantic_graph/beta/util.py:66-66
    value: T

# tests/graph/beta/test_graph_edge_cases.py:20-20
    value: int = 0

# tests/graph/beta/test_paths.py:27-27
    value: int = 0

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:44-44
    value: EvaluationScalar

# tests/graph/test_mermaid.py:431-441
def test_edge_union():
    """Test that a union of things annotated with an Edge doesn't raise a TypeError.

    This is important because such unions may occur as a return type for a graph, and needs to be evaluated when
    generating a mermaid diagram.
    """
    # This would raise an error on 3.10 if Edge was not hashable:
    edges_union = Union[  # noqa: UP007
        Annotated[End[None], Edge(label='first label')], Annotated[End[None], Edge(label='second label')]
    ]
    assert edges_union

# pydantic_evals/pydantic_evals/reporting/analyses.py:65-65
    value: float | int

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:258-350
    def handle_tool_call_delta(
        self,
        *,
        vendor_part_id: Hashable | None,
        tool_name: str | None = None,
        args: str | dict[str, Any] | None = None,
        tool_call_id: str | None = None,
        provider_name: str | None = None,
        provider_details: dict[str, Any] | None = None,
    ) -> ModelResponseStreamEvent | None:
        """Handle or update a tool call, creating or updating a `ToolCallPart`, `BuiltinToolCallPart`, or `ToolCallPartDelta`.

        Managed items remain as `ToolCallPartDelta`s until they have at least a tool_name, at which
        point they are upgraded to `ToolCallPart`s.

        If `vendor_part_id` is None, updates the latest matching ToolCallPart (or ToolCallPartDelta)
        if any. Otherwise, a new part (or delta) may be created.

        Args:
            vendor_part_id: The ID the vendor uses for this tool call.
                If None, the latest matching tool call may be updated.
            tool_name: The name of the tool. If None, the manager does not enforce
                a name match when `vendor_part_id` is None.
            args: Arguments for the tool call, either as a string, a dictionary of key-value pairs, or None.
            tool_call_id: An optional string representing an identifier for this tool call.
            provider_name: An optional provider name for the tool call part.
            provider_details: An optional dictionary of provider-specific details for the tool call part.

        Returns:
            - A `PartStartEvent` if a new ToolCallPart or BuiltinToolCallPart is created.
            - A `PartDeltaEvent` if an existing part is updated.
            - `None` if no new event is emitted (e.g., the part is still incomplete).

        Raises:
            UnexpectedModelBehavior: If attempting to apply a tool call delta to a part that is not
                a ToolCallPart, BuiltinToolCallPart, or ToolCallPartDelta.
        """
        existing_matching_part_and_index: tuple[ToolCallPartDelta | ToolCallPart | BuiltinToolCallPart, int] | None = (
            None
        )

        if vendor_part_id is None:
            # vendor_part_id is None, so check if the latest part is a matching tool call or delta to update
            # When the vendor_part_id is None, if the tool_name is _not_ None, assume this should be a new part rather
            # than a delta on an existing one. We can change this behavior in the future if necessary for some model.
            if tool_name is None:
                existing_matching_part_and_index = self._latest_part_if_of_type(
                    ToolCallPart, BuiltinToolCallPart, ToolCallPartDelta
                )
        else:
            # vendor_part_id is provided, so look up the corresponding part or delta
            part_index = self._vendor_id_to_part_index.get(vendor_part_id)
            if part_index is not None:
                existing_part = self._parts[part_index]
                if not isinstance(existing_part, ToolCallPartDelta | ToolCallPart | BuiltinToolCallPart):
                    raise UnexpectedModelBehavior(f'Cannot apply a tool call delta to {existing_part=}')
                existing_matching_part_and_index = existing_part, part_index

        if existing_matching_part_and_index is None:
            # No matching part/delta was found, so create a new ToolCallPartDelta (or ToolCallPart if fully formed)
            delta = ToolCallPartDelta(
                tool_name_delta=tool_name,
                args_delta=args,
                tool_call_id=tool_call_id,
                provider_name=provider_name,
                provider_details=provider_details,
            )
            part = delta.as_part() or delta
            new_part_index = self._append_part(part, vendor_part_id)
            # Only emit a PartStartEvent if we have enough information to produce a full ToolCallPart
            if isinstance(part, ToolCallPart | BuiltinToolCallPart):
                return PartStartEvent(index=new_part_index, part=part)
        else:
            # Update the existing part or delta with the new information
            existing_part, part_index = existing_matching_part_and_index
            delta = ToolCallPartDelta(
                tool_name_delta=tool_name,
                args_delta=args,
                tool_call_id=tool_call_id,
                provider_name=self._resolve_provider_name(existing_part, provider_name),
                provider_details=provider_details,
            )
            updated_part = delta.apply(existing_part)
            self._parts[part_index] = updated_part
            if isinstance(updated_part, ToolCallPart | BuiltinToolCallPart):
                if isinstance(existing_part, ToolCallPartDelta):
                    # We just upgraded a delta to a full part, so emit a PartStartEvent
                    return PartStartEvent(index=part_index, part=updated_part)
                else:
                    # We updated an existing part, so emit a PartDeltaEvent
                    if updated_part.tool_call_id and not delta.tool_call_id:
                        delta = replace(delta, tool_call_id=updated_part.tool_call_id)
                    return PartDeltaEvent(index=part_index, delta=delta)

# pydantic_evals/pydantic_evals/evaluators/common.py:32-32
    value: Any

# tests/graph/beta/test_joins_and_reducers.py:24-24
    value: int = 0

# tests/graph/beta/test_decisions.py:468-524
async def test_match_node():
    """Test using match_node() with BaseNode types in decisions.

    match_node() is designed for exhaustive matching of BaseNode return types
    in decision branches. Unlike match().to(), it doesn't require a .to() call
    since the destination is the BaseNode class itself.

    This is only necessary if you have a step that might return a v1-style node _or_ an
    arbitrary output that you want to route to another node using the builder API.
    """
    g = GraphBuilder(state_type=DecisionState, input_type=int, output_type=str)

    @dataclass
    class NodeStep(BaseNode[DecisionState, None, str]):
        value: int

        async def run(self, ctx: GraphRunContext[DecisionState, None]) -> End[str]:
            ctx.state.path_taken = 'path_a'
            return End(f'Path A: {self.value}')

    @g.step
    async def regular_step(ctx: StepContext[DecisionState, None, int]):
        ctx.state.path_taken = 'path_b'
        return f'Path B: {ctx.inputs}'

    @g.step
    async def route_to_node(ctx: StepContext[DecisionState, None, int]) -> NodeStep | int:
        # Route based on input value
        if ctx.inputs < 10:
            return NodeStep(ctx.inputs)
        else:
            return ctx.inputs

    # Use match_node to create decision branches for BaseNode types
    # Note: match_node doesn't require .to() - the node type IS the destination
    g.add(
        g.node(NodeStep),
        g.edge_from(g.start_node).to(route_to_node),
        g.edge_from(route_to_node).to(
            g.decision().branch(g.match_node(NodeStep)).branch(g.match(int).to(regular_step))
        ),
        g.edge_from(regular_step).to(g.end_node),
    )

    graph = g.build()

    # Test path A (value < 10)
    state_a = DecisionState()
    result_a = await graph.run(state=state_a, inputs=5)
    assert result_a == 'Path A: 5'
    assert state_a.path_taken == 'path_a'

    # Test path B (value >= 10)
    state_b = DecisionState()
    result_b = await graph.run(state=state_b, inputs=15)
    assert result_b == 'Path B: 15'
    assert state_b.path_taken == 'path_b'

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:42-45
    value: Any = Field(
        default=None,
        description='The value to apply (for add, replace operations)',
    )

# pydantic_ai_slim/pydantic_ai/models/test.py:52-52
    value: dict[str, Any] | None

# tests/test_agent.py:3308-3308
    value: str

# tests/test_streaming.py:1141-1141
    value: str

# pydantic_ai_slim/pydantic_ai/models/test.py:45-45
    value: str | None

# tests/graph/beta/test_decisions.py:20-20
    value: int = 0

# tests/graph/beta/test_edge_cases.py:19-19
    value: int = 0

# tests/test_agent.py:114-114
    value: T

# tests/models/test_openai.py:1668-1668
    a = 'a'

# tests/typed_deps.py:11-11
    a: int

# tests/graph/beta/test_edge_labels.py:17-17
    value: int = 0

# tests/test_ag_ui.py:158-158
    value: int = 0

# tests/test_prefect.py:1195-1195
    value: str

# tests/models/test_gemini_vertex.py:6-6
from inline_snapshot import Is, snapshot

# tests/models/test_google.py:16-16
from inline_snapshot import Is, snapshot

# pydantic_ai_slim/pydantic_ai/models/gemini.py:984-997
def _ensure_decodeable(content: bytearray) -> bytearray:
    """Trim any invalid unicode point bytes off the end of a bytearray.

    This is necessary before attempting to parse streaming JSON bytes.

    This is a temporary workaround until https://github.com/pydantic/pydantic-core/issues/1633 is resolved
    """
    try:
        content.decode()
    except UnicodeDecodeError as e:
        # e.start marks the start of the invalid decoded bytes, so cut up to before the first invalid byte
        return content[: e.start]
    else:
        return content

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:54-59
def default_render_percentage(value: float) -> str:
    """The default logic for formatting percentages in an Evaluation report.

    * Include at least one decimal place and at least 3 significant figures.
    """
    return f'{value:,.{VALUE_SIG_FIGS - 2}%}'

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:33-35
    def validate_python(
        self, input: Any, *, allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False, **kwargs: Any
    ) -> Any: ...

# pydantic_ai_slim/pydantic_ai/exceptions.py:230-231
class IncompleteToolCall(UnexpectedModelBehavior):
    """Error raised when a model stops due to token limit while emitting a tool call."""

# tests/test_validation_context.py:23-23
    x: int