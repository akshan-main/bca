## docs/.hooks/snippets.py

class ParsedFile:
    lines: list[str]
    sections: dict[str, list[LineRange]]
    lines_mapping: dict[int, int]

    def render(self, fragment_sections: list[str], highlight_sections: list[str]) -> RenderedSnippet:
        fragment_ranges: list[LineRange] = []
        if fragment_sections:
            for k in fragment_sections:
                if k not in self.sections:
                    raise ValueError(f'Unrecognized fragment section: {k!r} (expected {list(self.sections)})')
                fragment_ranges.extend(self.sections[k])
            fragment_ranges = LineRange.merge(fragment_ranges)
        else:
            fragment_ranges = [LineRange(0, len(self.lines))]

        highlight_ranges: list[LineRange] = []
        for k in highlight_sections:
            if k not in self.sections:
                raise ValueError(f'Unrecognized highlight section: {k!r} (expected {list(self.sections)})')
            highlight_ranges.extend(self.sections[k])
        highlight_ranges = LineRange.merge(highlight_ranges)

        rendered_highlight_ranges = list[LineRange]()
        rendered_lines: list[str] = []
        last_end_line = 1
        current_line = 0
        for fragment_range in fragment_ranges:
            if fragment_range.start_line > last_end_line:
                if current_line == 0:
                    rendered_lines.append('...\n')
                else:
                    rendered_lines.append('\n...\n')

                current_line += 1
            fragment_highlight_ranges = fragment_range.intersection(highlight_ranges)
            for fragment_highlight_range in fragment_highlight_ranges:
                rendered_highlight_ranges.append(
                    LineRange(
                        fragment_highlight_range.start_line - fragment_range.start_line + current_line,
                        fragment_highlight_range.end_line - fragment_range.start_line + current_line,
                    )
                )

            for i in range(fragment_range.start_line, fragment_range.end_line):
                rendered_lines.append(self.lines[i])
                current_line += 1
            last_end_line = fragment_range.end_line

        if last_end_line < len(self.lines):
            rendered_lines.append('\n...')

        original_range = LineRange(
            self.lines_mapping[fragment_ranges[0].start_line],
            self.lines_mapping[fragment_ranges[-1].end_line - 1] + 1,
        )
        return RenderedSnippet('\n'.join(rendered_lines), LineRange.merge(rendered_highlight_ranges), original_range)

    def render(self, fragment_sections: list[str], highlight_sections: list[str]) -> RenderedSnippet:
        fragment_ranges: list[LineRange] = []
        if fragment_sections:
            for k in fragment_sections:
                if k not in self.sections:
                    raise ValueError(f'Unrecognized fragment section: {k!r} (expected {list(self.sections)})')
                fragment_ranges.extend(self.sections[k])
            fragment_ranges = LineRange.merge(fragment_ranges)
        else:
            fragment_ranges = [LineRange(0, len(self.lines))]

        highlight_ranges: list[LineRange] = []
        for k in highlight_sections:
            if k not in self.sections:
                raise ValueError(f'Unrecognized highlight section: {k!r} (expected {list(self.sections)})')
            highlight_ranges.extend(self.sections[k])
        highlight_ranges = LineRange.merge(highlight_ranges)

        rendered_highlight_ranges = list[LineRange]()
        rendered_lines: list[str] = []
        last_end_line = 1
        current_line = 0
        for fragment_range in fragment_ranges:
            if fragment_range.start_line > last_end_line:
                if current_line == 0:
                    rendered_lines.append('...\n')
                else:
                    rendered_lines.append('\n...\n')

                current_line += 1
            fragment_highlight_ranges = fragment_range.intersection(highlight_ranges)
            for fragment_highlight_range in fragment_highlight_ranges:
                rendered_highlight_ranges.append(
                    LineRange(
                        fragment_highlight_range.start_line - fragment_range.start_line + current_line,
                        fragment_highlight_range.end_line - fragment_range.start_line + current_line,
                    )
                )

            for i in range(fragment_range.start_line, fragment_range.end_line):
                rendered_lines.append(self.lines[i])
                current_line += 1
            last_end_line = fragment_range.end_line

        if last_end_line < len(self.lines):
            rendered_lines.append('\n...')

        original_range = LineRange(
            self.lines_mapping[fragment_ranges[0].start_line],
            self.lines_mapping[fragment_ranges[-1].end_line - 1] + 1,
        )
        return RenderedSnippet('\n'.join(rendered_lines), LineRange.merge(rendered_highlight_ranges), original_range)

## examples/pydantic_ai_examples/question_graph.py

from pydantic_graph import (
    BaseNode,
    End,
    Graph,
    GraphRunContext,
)

class Ask(BaseNode[QuestionState]):
    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:
        result = await ask_agent.run(
            'Ask a simple question with a single correct answer.',
            message_history=ctx.state.ask_agent_messages,
        )
        ctx.state.ask_agent_messages += result.all_messages()
        ctx.state.question = result.output
        return Answer(result.output)

    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:
        result = await ask_agent.run(
            'Ask a simple question with a single correct answer.',
            message_history=ctx.state.ask_agent_messages,
        )
        ctx.state.ask_agent_messages += result.all_messages()
        ctx.state.question = result.output
        return Answer(result.output)

## examples/pydantic_ai_examples/slack_lead_qualifier/store.py

class AnalysisStore:
    @classmethod
    @logfire.instrument('Add analysis to store')
    async def add(cls, analysis: Analysis):
        await cls._get_store().put.aio(analysis.profile.email, analysis.model_dump())

    @classmethod
    @logfire.instrument('List analyses from store')
    async def list(cls) -> list[Analysis]:
        return [
            Analysis.model_validate(analysis)
            async for analysis in cls._get_store().values.aio()
        ]

    @classmethod
    @logfire.instrument('Clear analyses from store')
    async def clear(cls):
        await cls._get_store().clear.aio()

    @classmethod
    def _get_store(cls) -> modal.Dict:
        return modal.Dict.from_name('analyses', create_if_missing=True)  # type: ignore ### [/analysis_store]

    async def add(cls, analysis: Analysis):
        await cls._get_store().put.aio(analysis.profile.email, analysis.model_dump())

## pydantic_ai_slim/pydantic_ai/_a2a.py

from typing import Any, Generic, TypeVar

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    async def stream(
        self,
        ctx: GraphRunContext[GraphAgentState, GraphAgentDeps[DepsT, T]],
    ) -> AsyncIterator[result.AgentStream[DepsT, T]]:
        assert not self._did_stream, 'stream() should only be called once per node'

        model_settings, model_request_parameters, message_history, run_context = await self._prepare_request(ctx)
        with set_current_run_context(run_context):
            async with ctx.deps.model.request_stream(
                message_history, model_settings, model_request_parameters, run_context
            ) as streamed_response:
                self._did_stream = True
                ctx.state.usage.requests += 1
                agent_stream = result.AgentStream[DepsT, T](
                    _raw_stream_response=streamed_response,
                    _output_schema=ctx.deps.output_schema,
                    _model_request_parameters=model_request_parameters,
                    _output_validators=ctx.deps.output_validators,
                    _run_ctx=build_run_context(ctx),
                    _usage_limits=ctx.deps.usage_limits,
                    _tool_manager=ctx.deps.tool_manager,
                    _metadata_getter=lambda: ctx.state.metadata,
                )
                yield agent_stream
                # In case the user didn't manually consume the full stream, ensure it is fully consumed here,
                # otherwise usage won't be properly counted:
                async for _ in agent_stream:
                    pass

        model_response = streamed_response.get()

        self._finish_handling(ctx, model_response)
        assert self._result is not None  # this should be set by the previous line

def build_run_context(ctx: GraphRunContext[GraphAgentState, GraphAgentDeps[DepsT, Any]]) -> RunContext[DepsT]:
    """Build a `RunContext` object from the current agent graph run context."""
    run_context = RunContext[DepsT](
        deps=ctx.deps.user_deps,
        model=ctx.deps.model,
        usage=ctx.state.usage,
        prompt=ctx.deps.prompt,
        messages=ctx.state.message_history,
        validation_context=None,
        tracer=ctx.deps.tracer,
        trace_include_content=ctx.deps.instrumentation_settings is not None
        and ctx.deps.instrumentation_settings.include_content,
        instrumentation_version=ctx.deps.instrumentation_settings.version
        if ctx.deps.instrumentation_settings
        else DEFAULT_INSTRUMENTATION_VERSION,
        run_step=ctx.state.run_step,
        run_id=ctx.state.run_id,
        metadata=ctx.state.metadata,
    )
    validation_context = build_validation_context(ctx.deps.validation_context, run_context)
    run_context = replace(run_context, validation_context=validation_context)
    return run_context

## pydantic_ai_slim/pydantic_ai/_utils.py

    async def __anext__(self) -> T:
        """Yields the buffered item if present, otherwise fetches the next item from the underlying source.

        Raises StopAsyncIteration if the stream is exhausted.
        """
        if self._exhausted:
            raise StopAsyncIteration

        # If we have a buffered item, yield it.
        if not isinstance(self._buffer, Unset):
            item = self._buffer
            self._buffer = UNSET
            return item

        # Otherwise, fetch the next item from the source.
        if self._source_iter is None:
            self._source_iter = aiter(self._source)

        try:
            return await anext(self._source_iter)
        except StopAsyncIteration:
            self._exhausted = True
            raise

## pydantic_ai_slim/pydantic_ai/result.py

    async def stream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:
        async for output in self.stream_output(debounce_by=debounce_by):
            yield output

## pydantic_ai_slim/pydantic_ai/ui/_event_stream.py

    async def handle_text_end(self, part: TextPart, followed_by_text: bool = False) -> AsyncIterator[EventT]:
        """Handle the end of a `TextPart`.

        Args:
            part: The text part.
            followed_by_text: Whether the part is directly followed by another text part. In this case, you may not want to yield a "text-end" event yet.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

    async def handle_thinking_end(
        self, part: ThinkingPart, followed_by_thinking: bool = False
    ) -> AsyncIterator[EventT]:
        """Handle the end of a `ThinkingPart`.

        Args:
            part: The thinking part.
            followed_by_thinking: Whether the part is directly followed by another thinking part. In this case, you may not want to yield a "thinking-end" event yet.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

## pydantic_evals/pydantic_evals/reporting/__init__.py

    value_formatter: str | Callable[[Any], str]

    diff_checker: Callable[[Any, Any], bool] | None

class _ValueRenderer:
    value_formatter: str | Callable[[Any], str] = '{}'
    diff_checker: Callable[[Any, Any], bool] | None = lambda x, y: x != y
    diff_formatter: Callable[[Any, Any], str | None] | None = None
    diff_style: str = 'magenta'

    @staticmethod
    def from_config(config: RenderValueConfig) -> _ValueRenderer:
        return _ValueRenderer(
            value_formatter=config.get('value_formatter', '{}'),
            diff_checker=config.get('diff_checker', lambda x, y: x != y),
            diff_formatter=config.get('diff_formatter'),
            diff_style=config.get('diff_style', 'magenta'),
        )

    def render_value(self, name: str | None, v: Any) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

    def render_diff(self, name: str | None, old: Any | None, new: Any | None) -> str:
        old_str = self._get_value_str(old) or MISSING_VALUE_STR
        new_str = self._get_value_str(new) or MISSING_VALUE_STR
        if old_str == new_str:
            result = old_str
        else:
            result = f'{old_str} → {new_str}'

            has_diff = self.diff_checker and self.diff_checker(old, new)
            if has_diff:  # pragma: no branch
                # If there is a diff, make the name bold and compute the diff_str
                name = name and f'[bold]{name}[/]'
                diff_str = self.diff_formatter and self.diff_formatter(old, new)
                if diff_str:  # pragma: no cover
                    result += f' ({diff_str})'
                result = f'[{self.diff_style}]{result}[/]'

        # Add the name
        if name:
            result = f'{name}: {result}'

        return result

    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

    diff_checker: Callable[[Any, Any], bool] | None = lambda x, y: x != y

    def from_config(config: RenderValueConfig) -> _ValueRenderer:
        return _ValueRenderer(
            value_formatter=config.get('value_formatter', '{}'),
            diff_checker=config.get('diff_checker', lambda x, y: x != y),
            diff_formatter=config.get('diff_formatter'),
            diff_style=config.get('diff_style', 'magenta'),
        )

    def render_value(self, name: str | None, v: Any) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

    def render_diff(self, name: str | None, old: Any | None, new: Any | None) -> str:
        old_str = self._get_value_str(old) or MISSING_VALUE_STR
        new_str = self._get_value_str(new) or MISSING_VALUE_STR
        if old_str == new_str:
            result = old_str
        else:
            result = f'{old_str} → {new_str}'

            has_diff = self.diff_checker and self.diff_checker(old, new)
            if has_diff:  # pragma: no branch
                # If there is a diff, make the name bold and compute the diff_str
                name = name and f'[bold]{name}[/]'
                diff_str = self.diff_formatter and self.diff_formatter(old, new)
                if diff_str:  # pragma: no cover
                    result += f' ({diff_str})'
                result = f'[{self.diff_style}]{result}[/]'

        # Add the name
        if name:
            result = f'{name}: {result}'

        return result

    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

    def render_value(self, name: str | None, v: float | int) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

    def render_value(self, name: str | None, v: T_contra) -> str: ...  # pragma: no branch

## pydantic_graph/pydantic_graph/beta/decision.py

    def map(
        self: DecisionBranchBuilder[StateT, DepsT, Iterable[T], SourceT, HandledT]
        | DecisionBranchBuilder[StateT, DepsT, AsyncIterable[T], SourceT, HandledT],
        *,
        fork_id: str | None = None,
        downstream_join_id: str | None = None,
    ) -> DecisionBranchBuilder[StateT, DepsT, T, SourceT, HandledT]:
        """Spread the branch's output.

        To do this, the current output must be iterable, and any subsequent steps in the path being built for this
        branch will be applied to each item of the current output in parallel.

        Args:
            fork_id: Optional ID for the fork, defaults to a generated value
            downstream_join_id: Optional ID of a downstream join node which is involved when mapping empty iterables

        Returns:
            A new DecisionBranchBuilder where mapping is performed prior to generating the final output.
        """
        return DecisionBranchBuilder(
            decision=self._decision,
            source=self._source,
            matches=self._matches,
            path_builder=self._path_builder.map(fork_id=fork_id, downstream_join_id=downstream_join_id),
        )

## pydantic_graph/pydantic_graph/beta/graph.py

    _value: OutputT

    def render(self, *, title: str | None = None, direction: StateDiagramDirection | None = None) -> str:
        """Render the graph as a Mermaid diagram string.

        Args:
            title: Optional title for the diagram
            direction: Optional direction for the diagram layout

        Returns:
            A string containing the Mermaid diagram representation
        """
        from pydantic_graph.beta.mermaid import build_mermaid_graph

        return build_mermaid_graph(self.nodes, self.edges_by_source).render(title=title, direction=direction)

    iter_stream_sender: MemoryObjectSendStream[_GraphTaskResult] = field(init=False)

## pydantic_graph/pydantic_graph/beta/mermaid.py

class MermaidGraph:
    """A mermaid graph."""

    nodes: list[MermaidNode]
    edges: list[MermaidEdge]

    title: str | None = None
    direction: StateDiagramDirection | None = None

    def render(
        self,
        direction: StateDiagramDirection | None = None,
        title: str | None = None,
        edge_labels: bool = True,
    ):
        lines: list[str] = []
        if title:
            lines = ['---', f'title: {title}', '---']
        lines.append('stateDiagram-v2')
        if direction is not None:
            lines.append(f'  direction {direction}')

        nodes, edges = _topological_sort(self.nodes, self.edges)
        for node in nodes:
            # List all nodes in order they were created
            node_lines: list[str] = []
            if node.kind == 'start' or node.kind == 'end':
                pass  # Start and end nodes use special [*] syntax in edges
            elif node.kind == 'step':
                line = f'  {node.id}'
                if node.label:
                    line += f': {node.label}'
                node_lines.append(line)
            elif node.kind == 'join':
                node_lines = [f'  state {node.id} <<join>>']
            elif node.kind == 'broadcast' or node.kind == 'map':
                node_lines = [f'  state {node.id} <<fork>>']
            elif node.kind == 'decision':
                node_lines = [f'  state {node.id} <<choice>>']
                if node.note:
                    node_lines.append(f'  note right of {node.id}\n    {node.note}\n  end note')
            else:  # pragma: no cover
                assert_never(node.kind)
            lines.extend(node_lines)

        lines.append('')

        for edge in edges:
            # Use special [*] syntax for start/end nodes
            render_start_id = '[*]' if edge.start_id == StartNode.id else edge.start_id
            render_end_id = '[*]' if edge.end_id == EndNode.id else edge.end_id
            edge_line = f'  {render_start_id} --> {render_end_id}'
            if edge.label and edge_labels:
                edge_line += f': {edge.label}'
            lines.append(edge_line)

        return '\n'.join(lines)

    def render(
        self,
        direction: StateDiagramDirection | None = None,
        title: str | None = None,
        edge_labels: bool = True,
    ):
        lines: list[str] = []
        if title:
            lines = ['---', f'title: {title}', '---']
        lines.append('stateDiagram-v2')
        if direction is not None:
            lines.append(f'  direction {direction}')

        nodes, edges = _topological_sort(self.nodes, self.edges)
        for node in nodes:
            # List all nodes in order they were created
            node_lines: list[str] = []
            if node.kind == 'start' or node.kind == 'end':
                pass  # Start and end nodes use special [*] syntax in edges
            elif node.kind == 'step':
                line = f'  {node.id}'
                if node.label:
                    line += f': {node.label}'
                node_lines.append(line)
            elif node.kind == 'join':
                node_lines = [f'  state {node.id} <<join>>']
            elif node.kind == 'broadcast' or node.kind == 'map':
                node_lines = [f'  state {node.id} <<fork>>']
            elif node.kind == 'decision':
                node_lines = [f'  state {node.id} <<choice>>']
                if node.note:
                    node_lines.append(f'  note right of {node.id}\n    {node.note}\n  end note')
            else:  # pragma: no cover
                assert_never(node.kind)
            lines.extend(node_lines)

        lines.append('')

        for edge in edges:
            # Use special [*] syntax for start/end nodes
            render_start_id = '[*]' if edge.start_id == StartNode.id else edge.start_id
            render_end_id = '[*]' if edge.end_id == EndNode.id else edge.end_id
            edge_line = f'  {render_start_id} --> {render_end_id}'
            if edge.label and edge_labels:
                edge_line += f': {edge.label}'
            lines.append(edge_line)

        return '\n'.join(lines)

def _topological_sort(
    nodes: list[MermaidNode], edges: list[MermaidEdge]
) -> tuple[list[MermaidNode], list[MermaidEdge]]:
    """Sort nodes and edges in a logical topological order.

    Uses BFS from the start node to assign depths, then sorts:
    - Nodes by their distance from start
    - Edges by the distance of their source and target nodes
    """
    # Build adjacency list for BFS
    adjacency: dict[str, list[str]] = defaultdict(list)
    for edge in edges:
        adjacency[edge.start_id].append(edge.end_id)

    # BFS to assign depth to each node (distance from start)
    depths: dict[str, int] = {}
    queue: list[tuple[str, int]] = [(StartNode.id, 0)]
    depths[StartNode.id] = 0

    while queue:
        node_id, depth = queue.pop(0)
        for next_id in adjacency[node_id]:
            if next_id not in depths:  # pragma: no branch
                depths[next_id] = depth + 1
                queue.append((next_id, depth + 1))

    # Sort nodes by depth (distance from start), then by id for stability
    # Nodes not reachable from start get infinity depth (sorted to end)
    sorted_nodes = sorted(nodes, key=lambda n: (depths.get(n.id, float('inf')), n.id))

    # Sort edges by source depth, then target depth
    # This ensures edges closer to start come first, edges closer to end come last
    sorted_edges = sorted(
        edges,
        key=lambda e: (
            depths.get(e.start_id, float('inf')),
            depths.get(e.end_id, float('inf')),
            e.start_id,
            e.end_id,
        ),
    )

    return sorted_nodes, sorted_edges

## pydantic_graph/pydantic_graph/nodes.py

class End(Generic[RunEndT]):
    """Type to return from a node to signal the end of the graph."""

    data: RunEndT
    """Data to return from the graph."""

    def deep_copy_data(self) -> End[RunEndT]:
        """Returns a deep copy of the end of the run."""
        if self.data is None:
            return self
        else:
            end = End(copy.deepcopy(self.data))
            end.set_snapshot_id(self.get_snapshot_id())
            return end

    def get_snapshot_id(self) -> str:
        if snapshot_id := getattr(self, '__snapshot_id', None):
            return snapshot_id
        else:
            self.__dict__['__snapshot_id'] = snapshot_id = generate_snapshot_id('end')
            return snapshot_id

    def set_snapshot_id(self, set_id: str) -> None:
        self.__dict__['__snapshot_id'] = set_id

## tests/graph/beta/test_edge_cases.py

async def test_graph_with_no_steps():
    """Test a graph with no intermediate steps (direct start to end)."""
    g = GraphBuilder(input_type=int, output_type=int)

    g.add(g.edge_from(g.start_node).to(g.end_node))

    graph = g.build()
    result = await graph.run(inputs=42)
    assert result == 42

## tests/graph/beta/test_graph_execution.py

class ExecutionState:
    log: list[str] = field(default_factory=list[str])
    counter: int = 0

async def test_map_to_end_node_cancels_pending():
    """Test that mapping directly to end_node cancels pending tasks"""
    import asyncio

    g = GraphBuilder(state_type=ExecutionState, output_type=int)

    @g.step
    async def generate(ctx: StepContext[ExecutionState, None, None]) -> list[int]:
        return [1, 2, 3, 4, 5]

    @g.step
    async def early_exit(ctx: StepContext[ExecutionState, None, int]) -> int:
        # First item returns immediately
        if ctx.inputs == 1:
            return ctx.inputs
        # Others would take longer
        await asyncio.sleep(1)
        return ctx.inputs  # pragma: no cover

    g.add(
        g.edge_from(g.start_node).to(generate),
        g.edge_from(generate).map().to(early_exit),
        g.edge_from(early_exit).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=ExecutionState())
    # Should complete quickly with the first result
    assert result in [1, 2, 3, 4, 5]

## tests/graph/beta/test_mermaid_rendering.py

async def test_render_with_direction():
    """Test rendering with explicit direction"""
    g = GraphBuilder(state_type=SimpleState, output_type=int)

    @g.step
    async def step(ctx: StepContext[SimpleState, None, None]) -> int:
        return 1  # pragma: no cover

    g.add(
        g.edge_from(g.start_node).to(step),
        g.edge_from(step).transform(lambda ctx: ctx.inputs * 2).to(g.end_node),
    )

    graph = g.build()

    # Test left-to-right direction
    mermaid_lr = graph.render(direction='LR')
    assert 'direction LR' in mermaid_lr

    # Test right-to-left direction
    mermaid_rl = graph.render(direction='RL')
    assert 'direction RL' in mermaid_rl

## tests/models/mock_async_stream.py

class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

## tests/models/mock_xai.py

    def stream(self) -> MockAsyncStream[tuple[chat_types.Response, Any]]:
        """Mock the stream() method for streaming responses."""
        assert self.stream_data is not None, 'you can only use stream() if stream_data is provided'

        data = list(self.stream_data[self.index])
        self.parent.index += 1

        return MockAsyncStream(iter(data))

## tests/models/test_download_item.py

async def test_download_item_application_octet_stream(disable_ssrf_protection_for_vcr: None) -> None:
    downloaded_item = await download_item(
        VideoUrl(
            url='https://raw.githubusercontent.com/pydantic/pydantic-ai/refs/heads/main/tests/assets/small_video.mp4'
        ),
        data_format='bytes',
    )
    assert downloaded_item['data_type'] == 'video/mp4'
    assert downloaded_item['data'] == IsInstance(bytes)

## tests/models/xai_proto_cassettes.py

    def stream(self) -> Any:
        if self._expected_type != 'stream':
            raise RuntimeError(
                f'Cassette expects a sample() call at interaction {self._client.interaction_idx}, '
                f'but stream() was called.'
            )
        interaction = self._client.next_interaction()

        async def _aiter():
            if not isinstance(interaction, StreamInteraction):  # pragma: no cover
                raise RuntimeError(f'Expected StreamInteraction, got {type(interaction).__name__}')

            # Reconstruct the aggregated response by applying each chunk, mirroring the SDK behavior.
            aggregated = chat_types.Response(chat_pb2.GetChatCompletionResponse(), index=None)
            for chunk_bytes in interaction.chunks_raw:
                chunk_proto = chat_pb2.GetChatCompletionChunk()
                chunk_proto.ParseFromString(chunk_bytes)
                aggregated.process_chunk(chunk_proto)
                yield aggregated, chat_types.Chunk(chunk_proto, index=None)

        return _aiter()

## tests/test_dbos.py

async def test_dbos_model_stream_direct(allow_model_requests: None, dbos: DBOS):
    @DBOS.workflow()
    async def run_model_stream():
        messages: list[ModelMessage] = [ModelRequest.user_text_prompt('What is the capital of Mexico?')]
        async with model_request_stream(complex_dbos_agent.model, messages) as stream:
            async for _ in stream:
                pass

    with workflow_raises(
        AssertionError,
        snapshot(
            'A DBOS model cannot be used with `pydantic_ai.direct.model_request_stream()` as it requires a `run_context`. Set an `event_stream_handler` on the agent and use `agent.run()` instead.'
        ),
    ):
        await run_model_stream()

## tests/test_temporal.py

class DirectStreamWorkflow:
    @workflow.run
    async def run(self, prompt: str) -> str:
        messages: list[ModelMessage] = [ModelRequest.user_text_prompt(prompt)]
        async with model_request_stream(complex_temporal_agent.model, messages) as stream:
            async for _ in stream:
                pass
        return 'done'  # pragma: no cover
