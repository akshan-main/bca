# clai/update_readme.py:4-4
from pathlib import Path

# docs/.hooks/algolia.py:6-6
from pathlib import Path

# docs/.hooks/main.py:6-6
from pathlib import Path

# docs/.hooks/snippets.py:5-5
from pathlib import Path

# docs/.hooks/test_snippets.py:6-6
from pathlib import Path

# examples/pydantic_ai_examples/__main__.py:20-20
from pathlib import Path

# examples/pydantic_ai_examples/chat_app.py:19-19
from pathlib import Path

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:2-2
from pathlib import Path

# examples/pydantic_ai_examples/evals/example_02_add_custom_evaluators.py:1-1
from pathlib import Path

# examples/pydantic_ai_examples/evals/example_03_unit_testing.py:1-1
from pathlib import Path

# examples/pydantic_ai_examples/evals/example_04_compare_models.py:1-1
from pathlib import Path

# examples/pydantic_ai_examples/question_graph.py:11-11
from pathlib import Path

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:10-10
from pathlib import Path

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:11-11
from pathlib import Path

# pydantic_ai_slim/pydantic_ai/mcp.py:13-13
from pathlib import Path

# pydantic_ai_slim/pydantic_ai/messages.py:13-13
from pathlib import Path

# pydantic_ai_slim/pydantic_ai/providers/google_vertex.py:6-6
from pathlib import Path

# pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py:7-7
from pathlib import Path

# pydantic_ai_slim/pydantic_ai/ui/_web/app.py:6-6
from pathlib import Path

# pydantic_evals/pydantic_evals/_utils.py:9-9
from pathlib import Path

# pydantic_evals/pydantic_evals/dataset.py:23-23
from pathlib import Path

# pydantic_evals/pydantic_evals/generation.py:10-10
from pathlib import Path

# pydantic_graph/pydantic_graph/beta/decision.py:19-19
from pydantic_graph.beta.paths import Path, PathBuilder, TransformFunction

# pydantic_graph/pydantic_graph/beta/graph.py:33-40
from pydantic_graph.beta.paths import (
    BroadcastMarker,
    DestinationMarker,
    LabelMarker,
    MapMarker,
    Path,
    TransformMarker,
)

# pydantic_graph/pydantic_graph/beta/graph_builder.py:38-46
from pydantic_graph.beta.paths import (
    BroadcastMarker,
    DestinationMarker,
    EdgePath,
    EdgePathBuilder,
    MapMarker,
    Path,
    PathBuilder,
)

# pydantic_graph/pydantic_graph/beta/mermaid.py:14-14
from pydantic_graph.beta.paths import BroadcastMarker, DestinationMarker, LabelMarker, MapMarker, Path

# pydantic_graph/pydantic_graph/graph.py:9-9
from pathlib import Path

# pydantic_graph/pydantic_graph/mermaid.py:6-6
from pathlib import Path

# pydantic_graph/pydantic_graph/persistence/file.py:7-7
from pathlib import Path

# scripts/check_cassettes.py:16-16
from pathlib import Path

# tests/conftest.py:15-15
from pathlib import Path

# tests/evals/test_dataset.py:6-6
from pathlib import Path

# tests/graph/beta/test_paths.py:11-20
from pydantic_graph.beta.paths import (
    BroadcastMarker,
    DestinationMarker,
    LabelMarker,
    MapMarker,
    Path,
    PathBuilder,
    PathItem,
    TransformMarker,
)

# tests/graph/test_file_persistence.py:5-5
from pathlib import Path

# tests/graph/test_mermaid.py:7-7
from pathlib import Path

# tests/import_examples.py:9-9
from pathlib import Path

# tests/mcp_server.py:2-2
from pathlib import Path

# tests/models/test_outlines.py:11-11
from pathlib import Path

# tests/models/xai_proto_cassettes.py:37-37
from pathlib import Path

# tests/providers/test_google_vertex.py:7-7
from pathlib import Path

# tests/test_examples.py:12-12
from pathlib import Path

# tests/test_fastmcp.py:6-6
from pathlib import Path

# tests/test_mcp.py:8-8
from pathlib import Path

# tests/test_messages.py:3-3
from pathlib import Path

# tests/test_ui_web.py:7-7
from pathlib import Path

# pydantic_graph/pydantic_graph/beta/paths.py:278-278
    path: Path

# examples/pydantic_ai_examples/rag.py:171-171
    path: str

# pydantic_graph/pydantic_graph/beta/decision.py:116-116
    path: Path

# pydantic_ai_slim/pydantic_ai/_ssrf.py:70-70
    path: str

# docs/.hooks/snippets.py:13-13
    path: str

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:41-41
    path: str = Field(description='JSON Pointer (RFC 6901) to the target location')

# pydantic_graph/pydantic_graph/beta/paths.py:130-159
class Path:
    """A sequence of path items defining data flow through the graph.

    Paths represent the route that data takes through the graph, including
    transformations, forks, and routing decisions.
    """

    items: list[PathItem]
    """The sequence of path items that define this path."""

    @property
    def last_fork(self) -> BroadcastMarker | MapMarker | None:
        """Get the most recent fork or map marker in this path.

        Returns:
            The last BroadcastMarker or MapMarker in the path, or None if no forks exist
        """
        for item in reversed(self.items):
            if isinstance(item, BroadcastMarker | MapMarker):
                return item
        return None

    @property
    def next_path(self) -> Path:
        """Create a new path with the first item removed.

        Returns:
            A new Path with all items except the first one
        """
        return Path(self.items[1:])

# pydantic_graph/pydantic_graph/beta/paths.py:153-159
    def next_path(self) -> Path:
        """Create a new path with the first item removed.

        Returns:
            A new Path with all items except the first one
        """
        return Path(self.items[1:])

# pydantic_evals/pydantic_evals/dataset.py:897-924
def _get_relative_path_reference(target: Path, source: Path, _prefix: str = '') -> Path:  # pragma: no cover
    """Get a relative path reference from source to target.

    Recursively resolve a relative path to target from source, adding '..' as needed.
    This is useful for creating a relative path reference from a source file to a target file.

    Args:
        target: The target path to reference.
        source: The source path to reference from.
        _prefix: Internal prefix used during recursion.

    Returns:
        A Path object representing the relative path from source to target.

    Example:
        If source is '/a/b/c.py' and target is '/a/d/e.py', the relative path reference
        would be '../../d/e.py'.
    """
    # Recursively resolve a relative path to target from source, adding '..' as needed.
    # This is useful for creating a relative path reference from a source file to a target file.
    # For example, if source is '/a/b/c.py' and target is '/a/d/e.py', the relative path reference
    # would be '../../d/e.py'.
    if not target.is_absolute():
        target = target.resolve()
    try:
        return Path(f'{_prefix}{Path(target).relative_to(source)}')
    except ValueError:
        return _get_relative_path_reference(target, source.parent, _prefix=f'{_prefix}../')

# pydantic_ai_slim/pydantic_ai/providers/gateway.py:212-219
def _merge_url_path(base_url: str, path: str) -> str:
    """Merge a base URL and a path.

    Args:
        base_url: The base URL to merge.
        path: The path to merge.
    """
    return base_url.rstrip('/') + '/' + path.lstrip('/')

# pydantic_graph/pydantic_graph/beta/paths.py:141-150
    def last_fork(self) -> BroadcastMarker | MapMarker | None:
        """Get the most recent fork or map marker in this path.

        Returns:
            The last BroadcastMarker or MapMarker in the path, or None if no forks exist
        """
        for item in reversed(self.items):
            if isinstance(item, BroadcastMarker | MapMarker):
                return item
        return None

# tests/test_usage_limits.py:3-3
import operator

# pydantic_graph/pydantic_graph/beta/graph_builder.py:1041-1055
def _update_path_with_id_remapping(path: Path, node_id_remapping: dict[NodeID, NodeID]) -> Path:
    # Note: we have already deepcopied the node provided to this function so it should be okay to make mutations,
    # this could change if we change the code surrounding the code paths leading to this function call though.
    for item in path.items:
        if isinstance(item, MapMarker):
            downstream_join_id = item.downstream_join_id
            if downstream_join_id is not None:
                item.downstream_join_id = JoinID(node_id_remapping.get(downstream_join_id, downstream_join_id))
            item.fork_id = ForkID(node_id_remapping.get(item.fork_id, item.fork_id))
        elif isinstance(item, BroadcastMarker):
            item.fork_id = ForkID(node_id_remapping.get(item.fork_id, item.fork_id))
            item.paths = [_update_path_with_id_remapping(p, node_id_remapping) for p in item.paths]
        elif isinstance(item, DestinationMarker):
            item.destination_id = node_id_remapping.get(item.destination_id, item.destination_id)
    return path

# pydantic_evals/pydantic_evals/dataset.py:728-775
    def to_file(
        self,
        path: Path | str,
        fmt: Literal['yaml', 'json'] | None = None,
        schema_path: Path | str | None = DEFAULT_SCHEMA_PATH_TEMPLATE,
        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
        custom_report_evaluator_types: Sequence[type[ReportEvaluator[InputsT, OutputT, MetadataT]]] = (),
    ):
        """Save the dataset to a file.

        Args:
            path: Path to save the dataset to.
            fmt: Format to use. If None, the format will be inferred from the file extension.
                Must be either 'yaml' or 'json'.
            schema_path: Path to save the JSON schema to. If None, no schema will be saved.
                Can be a string template with {stem} which will be replaced with the dataset filename stem.
            custom_evaluator_types: Custom evaluator classes to include in the schema.
            custom_report_evaluator_types: Custom report evaluator classes to include in the schema.
        """
        path = Path(path)
        fmt = self._infer_fmt(path, fmt)

        schema_ref: str | None = None
        if schema_path is not None:  # pragma: no branch
            if isinstance(schema_path, str):  # pragma: no branch
                schema_path = Path(schema_path.format(stem=path.stem))

            if not schema_path.is_absolute():
                schema_ref = str(schema_path)
                schema_path = path.parent / schema_path
            elif schema_path.is_relative_to(path):  # pragma: no cover
                schema_ref = str(_get_relative_path_reference(schema_path, path))
            else:  # pragma: no cover
                schema_ref = str(schema_path)
            self._save_schema(schema_path, custom_evaluator_types, custom_report_evaluator_types)

        context: dict[str, Any] = {'use_short_form': True}
        if fmt == 'yaml':
            dumped_data = self.model_dump(mode='json', by_alias=True, context=context)
            content = yaml.dump(dumped_data, sort_keys=False)
            if schema_ref:  # pragma: no branch
                yaml_language_server_line = f'{_YAML_SCHEMA_LINE_PREFIX}{schema_ref}'
                content = f'{yaml_language_server_line}\n{content}'
            path.write_text(content, encoding='utf-8')
        else:
            context['$schema'] = schema_ref
            json_data = self.model_dump_json(indent=2, by_alias=True, context=context)
            path.write_text(json_data + '\n', encoding='utf-8')

# tests/models/xai_proto_cassettes.py:499-500
    def dump(self, path: Path) -> None:
        self.cassette.dump(path)

# tests/models/xai_proto_cassettes.py:189-241
    def dump(self, path: Path) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        interactions_data: list[dict[str, Any]] = []

        for interaction in self.interactions:
            if isinstance(interaction, SampleInteraction):
                # Build request_sample: json first (if present), then raw
                req: dict[str, Any] = {}
                if interaction.request_json:
                    req['json'] = interaction.request_json
                req['raw'] = interaction.request_raw

                # Build response_sample: json first (if present), then raw
                resp: dict[str, Any] = {}
                if interaction.response_json:
                    resp['json'] = interaction.response_json
                resp['raw'] = interaction.response_raw

                interactions_data.append(
                    {
                        'request_sample': req,
                        'response_sample': resp,
                    }
                )

            elif isinstance(interaction, StreamInteraction):
                # Build request_stream: json first (if present), then raw
                req = {}
                if interaction.request_json:
                    req['json'] = interaction.request_json
                req['raw'] = interaction.request_raw

                # Build response_stream: chunks_json first (if present), then chunks_raw
                resp = {}
                if interaction.chunks_json:
                    resp['chunks_json'] = interaction.chunks_json
                resp['chunks_raw'] = interaction.chunks_raw

                interactions_data.append(
                    {
                        'request_stream': req,
                        'response_stream': resp,
                    }
                )

        data: dict[str, Any] = {
            'version': self.version,
            'interactions': interactions_data,
        }
        path.write_text(
            yaml.safe_dump(data, sort_keys=False, allow_unicode=True),
            encoding='utf-8',
        )

# pydantic_graph/pydantic_graph/beta/paths.py:283-288
    def __init__(
        self, sources: Sequence[SourceNode[StateT, DepsT, Any]], path: Path, destinations: list[AnyDestinationNode]
    ):
        self._sources = sources
        self.path = path
        self.destinations = destinations

# pydantic_evals/pydantic_evals/dataset.py:828-845
    def _save_schema(
        cls,
        path: Path | str,
        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
        custom_report_evaluator_types: Sequence[type[ReportEvaluator[InputsT, OutputT, MetadataT]]] = (),
    ):
        """Save the JSON schema for this dataset type to a file.

        Args:
            path: Path to save the schema to.
            custom_evaluator_types: Custom evaluator classes to include in the schema.
            custom_report_evaluator_types: Custom report evaluator classes to include in the schema.
        """
        path = Path(path)
        json_schema = cls.model_json_schema_with_evaluators(custom_evaluator_types, custom_report_evaluator_types)
        schema_content = to_json(json_schema, indent=2).decode() + '\n'
        if not path.exists() or path.read_text(encoding='utf-8') != schema_content:  # pragma: no branch
            path.write_text(schema_content, encoding='utf-8')

# pydantic_graph/pydantic_graph/beta/paths.py:308-318
    def __init__(
        self, sources: Sequence[SourceNode[StateT, DepsT, Any]], path_builder: PathBuilder[StateT, DepsT, OutputT]
    ):
        """Initialize an edge path builder.

        Args:
            sources: The source nodes for this edge path
            path_builder: The path builder for defining the data flow
        """
        self.sources = sources
        self._path_builder = path_builder

# pydantic_evals/pydantic_evals/dataset.py:859-881
    def _infer_fmt(cls, path: Path, fmt: Literal['yaml', 'json'] | None) -> Literal['yaml', 'json']:
        """Infer the format to use for a file based on its extension.

        Args:
            path: The path to infer the format for.
            fmt: The explicitly provided format, if any.

        Returns:
            The inferred format ('yaml' or 'json').

        Raises:
            ValueError: If the format cannot be inferred from the file extension.
        """
        if fmt is not None:
            return fmt
        suffix = path.suffix.lower()
        if suffix in {'.yaml', '.yml'}:
            return 'yaml'
        elif suffix == '.json':
            return 'json'
        raise ValueError(
            f'Could not infer format for filename {path.name!r}. Use the `fmt` argument to specify the format.'
        )

# pydantic_ai_slim/pydantic_ai/ui/_web/app.py:50-107
async def _get_ui_html(html_source: str | Path | None = None) -> bytes:
    """Get UI HTML content from the specified source or default CDN.

    When html_source is provided, it is used directly.
    When html_source is None, fetches from the default CDN.

    Args:
        html_source: Path or URL for the chat UI HTML. Can be:
            - None: Uses the default CDN (cached locally)
            - A Path instance: Reads from the local file
            - A URL (http:// or https://): Fetches from the URL
            - A file path string: Reads from the local file
    """
    # Use default CDN with caching
    if html_source is None:
        cache_dir = _get_cache_dir()
        cache_file = cache_dir / f'{CHAT_UI_VERSION}.html'

        if cache_file.exists():
            return cache_file.read_bytes()

        async with httpx.AsyncClient() as client:
            response = await client.get(DEFAULT_HTML_URL)
            response.raise_for_status()
            content = response.content

        cache_file.write_bytes(content)
        return content

    # Handle Path instances
    if isinstance(html_source, Path):
        html_source = html_source.expanduser()
        if html_source.is_file():
            return html_source.read_bytes()
        raise FileNotFoundError(f'Local UI file not found: {html_source}')

    # Handle URLs with filesystem caching
    if html_source.startswith(('http://', 'https://')):
        cache_dir = _get_cache_dir()
        url_hash = hashlib.sha256(html_source.encode()).hexdigest()[:16]
        cache_file = cache_dir / f'url_{url_hash}.html'

        if cache_file.exists():
            return cache_file.read_bytes()

        async with httpx.AsyncClient() as client:
            response = await client.get(html_source)
            response.raise_for_status()
            content = response.content

        cache_file.write_bytes(content)
        return content

    # Handle local file paths (strings)
    local_path = Path(html_source).expanduser()
    if local_path.is_file():
        return local_path.read_bytes()
    raise FileNotFoundError(f'Local UI file not found: {html_source}')

# pydantic_graph/pydantic_graph/mermaid.py:198-221
def save_image(
    path: Path | str,
    graph: Graph[Any, Any, Any],
    /,
    **kwargs: Unpack[MermaidConfig],
) -> None:
    """Generate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink) and save it to a local file.

    Args:
        path: The path to save the image to.
        graph: The graph to generate the image for.
        **kwargs: Additional parameters to configure mermaid chart generation.
    """
    if isinstance(path, str):
        path = Path(path)

    if 'image_type' not in kwargs:
        ext = path.suffix.lower()[1:]
        # no need to check for .jpeg/.jpg, as it is the default
        if ext in ('png', 'webp', 'svg', 'pdf'):
            kwargs['image_type'] = ext

    image_data = request_image(graph, **kwargs)
    path.write_bytes(image_data)

# tests/models/xai_proto_cassettes.py:160-187
    def load(cls, path: Path) -> XaiProtoCassette:
        data = yaml.safe_load(path.read_text(encoding='utf-8'))

        interactions: list[Interaction] = []
        for item in data.get('interactions', []):
            if 'request_sample' in item:
                req = item['request_sample']
                resp = item['response_sample']
                interactions.append(
                    SampleInteraction(
                        request_raw=req['raw'],
                        response_raw=resp['raw'],
                        request_json=req.get('json'),
                        response_json=resp.get('json'),
                    )
                )
            elif 'request_stream' in item:
                req = item['request_stream']
                resp = item['response_stream']
                interactions.append(
                    StreamInteraction(
                        request_raw=req['raw'],
                        chunks_raw=resp['chunks_raw'],
                        request_json=req.get('json'),
                        chunks_json=resp.get('chunks_json'),
                    )
                )
        return cls(interactions=interactions)

# pydantic_graph/pydantic_graph/beta/paths.py:137-137
    items: list[PathItem]

# pydantic_graph/pydantic_graph/beta/paths.py:204-217
    def broadcast(self, forks: Sequence[Path], /, *, fork_id: str | None = None) -> Path:
        """Create a fork that broadcasts data to multiple parallel paths.

        Args:
            forks: The sequence of paths to run in parallel
            fork_id: Optional ID for the fork, defaults to a generated value

        Returns:
            A complete Path that forks to the specified parallel paths
        """
        next_item = BroadcastMarker(
            paths=forks, fork_id=ForkID(NodeID(fork_id or generate_placeholder_node_id('broadcast')))
        )
        return Path(items=[*self.working_items, next_item])

# pydantic_graph/pydantic_graph/beta/paths.py:61-69
class TransformMarker:
    """A marker indicating a data transformation step in a path.

    Transform markers wrap step functions that modify data as it flows
    through the graph path.
    """

    transform: TransformFunction[Any, Any, Any, Any]
    """The step function that performs the transformation."""

# examples/pydantic_ai_examples/__main__.py:49-63
def copy_to(this_dir: Path, dst: Path):
    if dst.exists():
        print(f'Error: destination path "{dst}" already exists', file=sys.stderr)
        sys.exit(1)

    dst.mkdir(parents=True)

    count = 0
    for file in this_dir.glob('*.*'):
        with open(file, 'rb') as src_file:
            with open(dst / file.name, 'wb') as dst_file:
                dst_file.write(src_file.read())
        count += 1

    print(f'Copied {count} example files to "{dst}"')

# pydantic_evals/pydantic_evals/dataset.py:539-577
    def from_file(
        cls,
        path: Path | str,
        fmt: Literal['yaml', 'json'] | None = None,
        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
        custom_report_evaluator_types: Sequence[type[ReportEvaluator[InputsT, OutputT, MetadataT]]] = (),
    ) -> Self:
        """Load a dataset from a file.

        Args:
            path: Path to the file to load.
            fmt: Format of the file. If None, the format will be inferred from the file extension.
                Must be either 'yaml' or 'json'.
            custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.
                These are additional evaluators beyond the default ones.
            custom_report_evaluator_types: Custom report evaluator classes to use when deserializing the dataset.
                These are additional report evaluators beyond the default ones.

        Returns:
            A new Dataset instance loaded from the file.

        Raises:
            ValidationError: If the file cannot be parsed as a valid dataset.
            ValueError: If the format cannot be inferred from the file extension.
        """
        path = Path(path)
        fmt = cls._infer_fmt(path, fmt)

        raw = Path(path).read_text(encoding='utf-8')
        try:
            return cls.from_text(
                raw,
                fmt=fmt,
                custom_evaluator_types=custom_evaluator_types,
                custom_report_evaluator_types=custom_report_evaluator_types,
                default_name=path.stem,
            )
        except ValidationError as e:  # pragma: no cover
            raise ValueError(f'{path} contains data that does not match the schema for {cls.__name__}:\n{e}.') from e

# tests/test_examples.py:279-280
def custom_include_print(path: Path, frame: FrameInfo, args: Sequence[Any]) -> bool:
    return path.samefile(frame.filename) or frame.filename.endswith('test_examples.py')

# tests/models/xai_proto_cassettes.py:299-300
    def from_path(cls, path: Path) -> XaiProtoCassetteClient:
        return cls(cassette=XaiProtoCassette.load(path))

# pydantic_graph/pydantic_graph/beta/paths.py:114-122
class DestinationMarker:
    """A marker indicating the target destination node for a path.

    Destination markers specify where data should be routed at the end
    of a path execution.
    """

    destination_id: NodeID
    """The unique identifier of the destination node."""

# pydantic_graph/pydantic_graph/beta/paths.py:269-292
class EdgePath(Generic[StateT, DepsT]):
    """A complete edge connecting source nodes to destinations via a path.

    EdgePath represents a complete connection in the graph, specifying the
    source nodes, the path that data follows, and the destination nodes.
    """

    _sources: Sequence[SourceNode[StateT, DepsT, Any]]
    """The source nodes that provide data to this edge."""
    path: Path
    """The path that data follows through the graph."""
    destinations: list[AnyDestinationNode]
    """The destination nodes that can be referenced by DestinationMarker in the path."""

    def __init__(
        self, sources: Sequence[SourceNode[StateT, DepsT, Any]], path: Path, destinations: list[AnyDestinationNode]
    ):
        self._sources = sources
        self.path = path
        self.destinations = destinations

    @property
    def sources(self) -> Sequence[SourceNode[StateT, DepsT, Any]]:
        return self._sources

# pydantic_graph/pydantic_graph/beta/graph.py:857-873
    def _handle_path(self, path: Path, inputs: Any, fork_stack: ForkStack) -> Sequence[GraphTask]:
        if not path.items:
            return []  # pragma: no cover

        item = path.items[0]
        assert not isinstance(item, MapMarker | BroadcastMarker), (
            'These markers should be removed from paths during graph building'
        )
        if isinstance(item, DestinationMarker):
            return [GraphTask(item.destination_id, inputs, fork_stack, self.get_next_task_id())]
        elif isinstance(item, TransformMarker):
            inputs = item.transform(StepContext(state=self.state, deps=self.deps, inputs=inputs))
            return self._handle_path(path.next_path, inputs, fork_stack)
        elif isinstance(item, LabelMarker):
            return self._handle_path(path.next_path, inputs, fork_stack)
        else:
            assert_never(item)

# pydantic_graph/pydantic_graph/beta/graph_builder.py:444-455
    def edge_from(self, *sources: Source[SourceOutputT]) -> EdgePathBuilder[StateT, DepsT, SourceOutputT]:
        """Create an edge path builder starting from the given source nodes.

        Args:
            *sources: The source nodes to start the edge path from

        Returns:
            An EdgePathBuilder for constructing the complete edge path
        """
        return EdgePathBuilder[StateT, DepsT, SourceOutputT](
            sources=sources, path_builder=PathBuilder(working_items=[])
        )

# pydantic_graph/pydantic_graph/graph.py:435-456
    def mermaid_save(
        self, path: Path | str, /, *, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig]
    ) -> None:
        """Generate a diagram representing the graph and save it as an image.

        The format and diagram can be customized using `kwargs`,
        see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].

        !!! note "Uses external service"
            This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, `mermaid.ink`
            is a free service not affiliated with Pydantic.

        Args:
            path: The path to save the image to.
            infer_name: Whether to infer the graph name from the calling frame.
            **kwargs: Additional arguments to pass to `mermaid.save_image`.
        """
        if infer_name and self.name is None:
            self._infer_name(inspect.currentframe())
        if 'title' not in kwargs and self.name:
            kwargs['title'] = self.name
        mermaid.save_image(path, self, **kwargs)

# pydantic_evals/pydantic_evals/generation.py:33-85
async def generate_dataset(
    *,
    dataset_type: type[Dataset[InputsT, OutputT, MetadataT]],
    path: Path | str | None = None,
    custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
    model: models.Model | models.KnownModelName = 'openai:gpt-5.2',
    n_examples: int = 3,
    extra_instructions: str | None = None,
) -> Dataset[InputsT, OutputT, MetadataT]:
    """Use an LLM to generate a dataset of test cases, each consisting of input, expected output, and metadata.

    This function creates a properly structured dataset with the specified input, output, and metadata types.
    It uses an LLM to attempt to generate realistic test cases that conform to the types' schemas.

    Args:
        path: Optional path to save the generated dataset. If provided, the dataset will be saved to this location.
        dataset_type: The type of dataset to generate, with the desired input, output, and metadata types.
        custom_evaluator_types: Optional sequence of custom evaluator classes to include in the schema.
        model: The Pydantic AI model to use for generation. Defaults to 'openai:gpt-5.2'.
        n_examples: Number of examples to generate. Defaults to 3.
        extra_instructions: Optional additional instructions to provide to the LLM.

    Returns:
        A properly structured Dataset object with generated test cases.

    Raises:
        ValidationError: If the LLM's response cannot be parsed as a valid dataset.
    """
    output_schema = dataset_type.model_json_schema_with_evaluators(custom_evaluator_types)

    # TODO: Use `output_type=StructuredDict(output_schema)` (and `from_dict` below) once https://github.com/pydantic/pydantic/issues/12145
    # is fixed and `StructuredDict` no longer needs to use `InlineDefsJsonSchemaTransformer`.
    agent = Agent(
        model,
        system_prompt=(
            f'Generate an object that is in compliance with this JSON schema:\n{output_schema}\n\n'
            f'Include {n_examples} example cases.'
            ' You must not include any characters in your response before the opening { of the JSON object, or after the closing }.'
        ),
        output_type=str,
        retries=1,
    )

    result = await agent.run(extra_instructions or 'Please generate the object.')
    output = strip_markdown_fences(result.output)
    try:
        result = dataset_type.from_text(output, fmt='json', custom_evaluator_types=custom_evaluator_types)
    except ValidationError as e:  # pragma: no cover
        print(f'Raw response from model:\n{result.output}')
        raise e
    if path is not None:
        result.to_file(path, custom_evaluator_types=custom_evaluator_types)  # pragma: no cover
    return result

# pydantic_graph/pydantic_graph/beta/paths.py:102-110
class LabelMarker:
    """A marker providing a human-readable label for a path segment.

    Label markers are used for debugging, visualization, and documentation
    purposes to provide meaningful names for path segments.
    """

    label: str
    """The human-readable label for this path segment."""

# tests/test_ui_web.py:543-552
async def test_get_ui_html_local_file_path_instance(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html supports Path instances."""
    # Create a test HTML file
    test_html = b'<html><body>Path Instance UI</body></html>'
    local_file = tmp_path / 'path-ui.html'
    local_file.write_bytes(test_html)

    result = await app_module._get_ui_html(html_source=local_file)  # pyright: ignore[reportPrivateUsage]

    assert result == test_html

# pydantic_graph/pydantic_graph/beta/paths.py:178-202
    def to(
        self,
        destination: DestinationNode[StateT, DepsT, OutputT],
        /,
        *extra_destinations: DestinationNode[StateT, DepsT, OutputT],
        fork_id: str | None = None,
    ) -> Path:
        """Route the path to one or more destination nodes.

        Args:
            destination: The primary destination node
            *extra_destinations: Additional destination nodes (creates a broadcast)
            fork_id: Optional ID for the fork created when multiple destinations are specified

        Returns:
            A complete Path ending at the specified destination(s)
        """
        if extra_destinations:
            next_item = BroadcastMarker(
                paths=[Path(items=[DestinationMarker(d.id)]) for d in (destination,) + extra_destinations],
                fork_id=ForkID(NodeID(fork_id or generate_placeholder_node_id('broadcast'))),
            )
        else:
            next_item = DestinationMarker(destination.id)
        return Path(items=[*self.working_items, next_item])

# tests/test_ui_web.py:566-572
async def test_get_ui_html_source_instance_not_found(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html raises FileNotFoundError for missing Path instances."""
    # Try to use a non-existent Path instance
    nonexistent_path = tmp_path / 'nonexistent-ui.html'

    with pytest.raises(FileNotFoundError, match='Local UI file not found'):
        await app_module._get_ui_html(html_source=nonexistent_path)  # pyright: ignore[reportPrivateUsage]

# pydantic_ai_slim/pydantic_ai/ui/_web/app.py:110-172
def create_web_app(
    agent: Agent[AgentDepsT, OutputDataT],
    models: ModelsParam = None,
    builtin_tools: Sequence[AbstractBuiltinTool] | None = None,
    deps: AgentDepsT = None,
    model_settings: ModelSettings | None = None,
    instructions: str | None = None,
    html_source: str | Path | None = None,
) -> Starlette:
    """Create a Starlette app that serves a web chat UI for the given agent.

    By default, the UI is fetched from a CDN and cached locally. The html_source
    parameter allows overriding this for enterprise environments, offline usage,
    or custom UI builds.

    Args:
        agent: The Pydantic AI agent to serve
        models: Models to make available in the UI. Can be:
            - A sequence of model names/instances (e.g., `['openai:gpt-5', 'anthropic:claude-sonnet-4-5']`)
            - A dict mapping display labels to model names/instances
                (e.g., `{'GPT 5': 'openai:gpt-5', 'Claude': 'anthropic:claude-sonnet-4-5'}`)
            If not provided, the UI will have no model options.
        builtin_tools: Optional list of additional builtin tools to make available in the UI.
            Tools already configured on the agent are always included but won't appear as options.
        deps: Optional dependencies to use for all requests.
        model_settings: Optional settings to use for all model requests.
        instructions: Optional extra instructions to pass to each agent run.
        html_source: Path or URL for the chat UI HTML. Can be:
            - None (default): Fetches from CDN and caches locally
            - A Path instance: Reads from the local file
            - A URL string (http:// or https://): Fetches from the URL
            - A file path string: Reads from the local file

    Returns:
        A configured Starlette application ready to be served
    """
    api_app = create_api_app(
        agent=agent,
        models=models,
        builtin_tools=builtin_tools,
        deps=deps,
        model_settings=model_settings,
        instructions=instructions,
    )

    routes = [Mount('/api', app=api_app)]
    app = Starlette(routes=routes)

    async def index(request: Request) -> Response:
        """Serve the chat UI from filesystem cache or CDN."""
        content = await _get_ui_html(html_source)

        return HTMLResponse(
            content=content,
            headers={
                'Cache-Control': 'public, max-age=3600',
            },
        )

    app.router.add_route('/', index, methods=['GET'])
    app.router.add_route('/{id}', index, methods=['GET'])

    return app

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:96-112
def load_agent(agent_path: str) -> Agent[Any, Any] | None:
    """Load an agent from module path in uvicorn style.

    Args:
        agent_path: Path in format 'module:variable', e.g. 'test_agent:my_agent'

    Returns:
        Agent instance or None if loading fails
    """
    sys.path.insert(0, str(Path.cwd()))
    try:
        obj = _import_string_adapter.validate_python(agent_path)
        if not isinstance(obj, Agent):
            return None
        return obj  # pyright: ignore[reportUnknownVariableType]
    except ValidationError:
        return None

# tests/conftest.py:385-387
def image_content(assets_path: Path) -> BinaryImage:
    image_bytes = assets_path.joinpath('kiwi.jpg').read_bytes()
    return BinaryImage(data=image_bytes, media_type='image/jpeg')

# tests/conftest.py:374-375
def assets_path() -> Path:
    return Path(__file__).parent / 'assets'