## docs/.hooks/snippets.py

def parse_file_sections(file_path: Path) -> ParsedFile:
    """Parse a file and extract sections marked with ### [section] or /// [section]"""
    input_lines = file_path.read_text(encoding='utf-8').splitlines()
    output_lines: list[str] = []
    lines_mapping: dict[int, int] = {}

    sections: dict[str, list[LineRange]] = {}
    section_starts: dict[str, int] = {}

    output_line_no = 0
    for line_no, line in enumerate(input_lines, 1):
        match: re.Match[str] | None = None
        for match in re.finditer(r'\s*(?:###|///)\s*\[([^]]+)]\s*$', line):
            break
        else:
            output_lines.append(line)
            output_line_no += 1
            lines_mapping[output_line_no - 1] = line_no - 1
            continue

        pre_matches_line = line[: match.start()]
        sections_to_start: set[str] = set()
        sections_to_end: set[str] = set()
        for item in match.group(1).split(','):
            if item in sections_to_end or item in sections_to_start:
                raise ValueError(f'Duplicate section reference: {item!r} at {file_path}:{line_no}')
            if item.startswith('/'):
                sections_to_end.add(item[1:])
            else:
                sections_to_start.add(item)

        for section_name in sections_to_start:
            if section_name in section_starts:
                raise ValueError(f'Cannot nest section with the same name {section_name!r} at {file_path}:{line_no}')
            section_starts[section_name] = output_line_no

        for section_name in sections_to_end:
            start_line = section_starts.pop(section_name, None)
            if start_line is None:
                raise ValueError(f'Cannot end unstarted section {section_name!r} at {file_path}:{line_no}')
            if section_name not in sections:
                sections[section_name] = []
            end_line = output_line_no + 1 if pre_matches_line else output_line_no
            sections[section_name].append(LineRange(start_line, end_line))

        if pre_matches_line:
            output_lines.append(pre_matches_line)
            output_line_no += 1
            lines_mapping[output_line_no - 1] = line_no - 1

    if section_starts:
        raise ValueError(f'Some sections were not finished in {file_path}: {list(section_starts)}')

    return ParsedFile(lines=output_lines, sections=sections, lines_mapping=lines_mapping)

## pydantic_ai_slim/pydantic_ai/providers/gateway.py

def _merge_url_path(base_url: str, path: str) -> str:
    """Merge a base URL and a path.

    Args:
        base_url: The base URL to merge.
        path: The path to merge.
    """
    return base_url.rstrip('/') + '/' + path.lstrip('/')

## pydantic_graph/pydantic_graph/beta/graph_builder.py

def _update_path_with_id_remapping(path: Path, node_id_remapping: dict[NodeID, NodeID]) -> Path:
    # Note: we have already deepcopied the node provided to this function so it should be okay to make mutations,
    # this could change if we change the code surrounding the code paths leading to this function call though.
    for item in path.items:
        if isinstance(item, MapMarker):
            downstream_join_id = item.downstream_join_id
            if downstream_join_id is not None:
                item.downstream_join_id = JoinID(node_id_remapping.get(downstream_join_id, downstream_join_id))
            item.fork_id = ForkID(node_id_remapping.get(item.fork_id, item.fork_id))
        elif isinstance(item, BroadcastMarker):
            item.fork_id = ForkID(node_id_remapping.get(item.fork_id, item.fork_id))
            item.paths = [_update_path_with_id_remapping(p, node_id_remapping) for p in item.paths]
        elif isinstance(item, DestinationMarker):
            item.destination_id = node_id_remapping.get(item.destination_id, item.destination_id)
    return path

## pydantic_graph/pydantic_graph/beta/paths.py

PathItem = TypeAliasType('PathItem', TransformMarker | MapMarker | BroadcastMarker | LabelMarker | DestinationMarker)

    def next_path(self) -> Path:
        """Create a new path with the first item removed.

        Returns:
            A new Path with all items except the first one
        """
        return Path(self.items[1:])

## tests/conftest.py

def assets_path() -> Path:
    return Path(__file__).parent / 'assets'

## tests/graph/beta/test_paths.py

async def test_path_next_path():
    """Test Path.next_path removes first item."""
    items: list[PathItem] = [LabelMarker('first'), LabelMarker('second'), DestinationMarker(NodeID('dest'))]
    path = Path(items=items)

    next_path = path.next_path
    assert len(next_path.items) == 2
    assert next_path.items[0] == items[1]
    assert next_path.items[1] == items[2]

## tests/models/xai_proto_cassettes.py

    def from_path(cls, path: Path) -> XaiProtoCassetteClient:
        return cls(cassette=XaiProtoCassette.load(path))

## tests/test_examples.py

def tmp_path_cwd(tmp_path: Path):
    cwd = os.getcwd()

    root_dir = Path(__file__).parent.parent
    for file in (root_dir / 'tests' / 'example_modules').glob('*.py'):
        shutil.copy(file, tmp_path)
    sys.path.append(str(tmp_path))
    os.chdir(tmp_path)

    try:
        yield tmp_path
    finally:
        os.chdir(cwd)
        sys.path.remove(str(tmp_path))

## tests/test_messages.py

def test_binary_content_from_path(tmp_path: Path):
    # test normal file
    test_xml_file = tmp_path / 'test.xml'
    test_xml_file.write_text('<think>about trains</think>', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_xml_file)
    assert binary_content == snapshot(BinaryContent(data=b'<think>about trains</think>', media_type='application/xml'))

    # test non-existent file
    non_existent_file = tmp_path / 'non-existent.txt'
    with pytest.raises(FileNotFoundError, match='File not found:'):
        BinaryContent.from_path(non_existent_file)

    # test file with unknown media type
    test_unknown_file = tmp_path / 'test.unknownext'
    test_unknown_file.write_text('some content', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_unknown_file)
    assert binary_content == snapshot(BinaryContent(data=b'some content', media_type='application/octet-stream'))

    # test string path
    test_txt_file = tmp_path / 'test.txt'
    test_txt_file.write_text('just some text', encoding='utf-8')
    string_path = test_txt_file.as_posix()
    binary_content = BinaryContent.from_path(string_path)  # pyright: ignore[reportArgumentType]
    assert binary_content == snapshot(BinaryContent(data=b'just some text', media_type='text/plain'))

    # test image file
    test_jpg_file = tmp_path / 'test.jpg'
    test_jpg_file.write_bytes(b'\xff\xd8\xff\xe0' + b'0' * 100)  # minimal JPEG header + padding
    binary_content = BinaryContent.from_path(test_jpg_file)
    assert binary_content == snapshot(
        BinaryImage(data=b'\xff\xd8\xff\xe0' + b'0' * 100, media_type='image/jpeg', _identifier='bc8d49')
    )

    # test yaml file
    test_yaml_file = tmp_path / 'config.yaml'
    test_yaml_file.write_text('key: value', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_yaml_file)
    assert binary_content == snapshot(BinaryContent(data=b'key: value', media_type='application/yaml'))

    # test yml file (alternative extension)
    test_yml_file = tmp_path / 'docker-compose.yml'
    test_yml_file.write_text('version: "3"', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_yml_file)
    assert binary_content == snapshot(BinaryContent(data=b'version: "3"', media_type='application/yaml'))

    # test toml file
    test_toml_file = tmp_path / 'pyproject.toml'
    test_toml_file.write_text('[project]\nname = "test"', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_toml_file)
    assert binary_content == snapshot(BinaryContent(data=b'[project]\nname = "test"', media_type='application/toml'))
