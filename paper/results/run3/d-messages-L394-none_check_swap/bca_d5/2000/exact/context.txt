## examples/pydantic_ai_examples/chat_app.py

    timestamp: str

## examples/pydantic_ai_examples/evals/agent.py

    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

    request: _messages.ModelRequest

    model_response: _messages.ModelResponse

## pydantic_ai_slim/pydantic_ai/_otel_messages.py

    type: Literal['text']

## pydantic_ai_slim/pydantic_ai/_utils.py

def is_set(t_or_unset: T | Unset) -> TypeGuard[T]:
    return t_or_unset is not UNSET

def number_to_datetime(x: int | float) -> datetime:
    return _datetime_ta.validate_python(x)

## pydantic_ai_slim/pydantic_ai/messages.py

_mime_types = MimeTypes()

## pydantic_ai_slim/pydantic_ai/models/gemini.py

    response_mime_type: str

## pydantic_ai_slim/pydantic_ai/retries.py

    sleep: Callable[[int | float], None | Awaitable[None]]

## pydantic_ai_slim/pydantic_ai/run.py

    def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:
        """Return all messages from [`all_messages`][pydantic_ai.agent.AgentRun.all_messages] as JSON bytes.

        Returns:
            JSON bytes representing the messages.
        """
        return _messages.ModelMessagesTypeAdapter.dump_json(self.all_messages())

    def new_messages_json(self) -> bytes:
        """Return new messages from [`new_messages`][pydantic_ai.agent.AgentRun.new_messages] as JSON bytes.

        Returns:
            JSON bytes representing the new messages.
        """
        return _messages.ModelMessagesTypeAdapter.dump_json(self.new_messages())

    def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:
        """Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.

        Args:
            output_tool_return_content: The return content of the tool call to set in the last message.
                This provides a convenient way to modify the content of the output tool call if you want to continue
                the conversation and want to set the response to the output tool call. If `None`, the last message will
                not be modified.

        Returns:
            JSON bytes representing the messages.
        """
        return _messages.ModelMessagesTypeAdapter.dump_json(
            self.all_messages(output_tool_return_content=output_tool_return_content)
        )

## pydantic_ai_slim/pydantic_ai/ui/_adapter.py

    def messages(self) -> list[ModelMessage]:
        """Pydantic AI messages from the protocol-specific run input."""
        raise NotImplementedError

## pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter.py

    def messages(self) -> list[ModelMessage]:
        """Pydantic AI messages from the AG-UI run input."""
        return self.load_messages(self.run_input.messages)

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter.py

    def messages(self) -> list[ModelMessage]:
        """Pydantic AI messages from the Vercel AI run input."""
        return self.load_messages(self.run_input.messages)

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## pydantic_graph/pydantic_graph/beta/util.py

class Some(Generic[T]):
    """Container for explicitly present values in Maybe type pattern.

    This class represents a value that is definitely present, as opposed to None.
    It's part of the Maybe pattern, similar to Option/Maybe in functional programming,
    allowing distinction between "no value" (None) and "value is None" (Some(None)).
    """

    value: T
    """The wrapped value."""

## tests/evals/test_report_evaluators.py

def _make_report(cases: list[ReportCase]) -> EvaluationReport:
    return EvaluationReport(name='test', cases=cases)

## tests/graph/beta/test_broadcast_and_spread.py

class CounterState:
    values: list[int] = field(default_factory=list[int])

    values: list[int] = field(default_factory=list[int])

## tests/graph/beta/test_edge_labels.py

class LabelState:
    value: int = 0

## tests/graph/beta/test_graph_edge_cases.py

class MyState:
    value: int = 0

## tests/graph/beta/test_graph_iteration.py

class IterState:
    counter: int = 0

## tests/graph/beta/test_v1_v2_integration.py

class IntegrationState:
    log: list[str] = field(default_factory=list[str])

## tests/models/test_anthropic.py

    def messages(self) -> Any:
        return type('Messages', (), {'create': self.messages_create, 'count_tokens': self.messages_count_tokens})

async def test_anthropic_cache_messages_with_custom_ttl(allow_model_requests: None):
    """Test that anthropic_cache_messages supports custom TTL values."""
    c = completion_message(
        [BetaTextBlock(text='Response', type='text')],
        usage=BetaUsage(input_tokens=10, output_tokens=5),
    )
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(
        m,
        system_prompt='System instructions.',
        model_settings=AnthropicModelSettings(
            anthropic_cache_messages='1h',  # Custom 1h TTL
        ),
    )

    await agent.run('User message')

    # Verify use 1h TTL
    completion_kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    messages = completion_kwargs['messages']

    assert messages[-1]['content'][-1]['cache_control'] == snapshot({'type': 'ephemeral', 'ttl': '1h'})

async def test_image_url_input_invalid_mime_type(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    result = await agent.run(
        [
            'What animal is this?',
            ImageUrl(
                url='https://lh3.googleusercontent.com/proxy/YngsuS8jQJysXxeucAgVBcSgIdwZlSQ-HvsNxGjHS0SrUKXI161bNKh6SOcMsNUGsnxoOrS3AYX--MT4T3S3SoCgSD1xKrtBwwItcgexaX_7W-qHo-VupmYgjjzWO-BuORLp9-pj8Kjr'
            ),
        ]
    )
    assert result.output == snapshot(
        'This is a Great Horned Owl (Bubo virginianus), a large and powerful owl species native to the Americas. The image shows the owl perched on a log or branch, surrounded by soft yellow and green vegetation. The owl has distinctive ear tufts (the "horns"), large yellow eyes, and a mottled gray-brown plumage that provides excellent camouflage in woodland and grassland environments. Great Horned Owls are known for their impressive size, sharp talons, and nocturnal hunting habits. They are formidable predators that can hunt animals as large as skunks, rabbits, and even other birds of prey.'
    )

## tests/models/test_groq.py

def text_chunk(text: str, finish_reason: FinishReason | None = None) -> chat.ChatCompletionChunk:
    return chunk([ChoiceDelta(content=text, role='assistant')], finish_reason=finish_reason)

## tests/models/test_model_function.py

async def test_pass_both():
    Agent(FunctionModel(return_last, stream_function=stream_text_function))

## tests/models/test_model_names.py

    object: Literal['model']

## tests/parts_from_messages.py

def part_types_from_messages(messages: list[ModelMessage]) -> list[Any]:
    """Utility function used when you are not interested in the content of the messages, but only that the part is there.

    As an example, the following messages:

    ```python
    [
        ModelRequest(parts=[UserPromptPart(content='')], kind='request'),
        ModelResponse(parts=[TextPart(content='')], kind='response'),
    ]
    ```

    Will return:

    ```python
    [
        [UserPromptPart],
        [TextPart],
    ]
    ```

    So each list represents either `ModelRequest` or `ModelResponse` and the parts that are present in the message.
    """
    parts: list[Any] = []
    for message in messages:
        message_parts: list[type[ModelResponsePart | ModelRequestPart]] = []
        for part in message.parts:
            message_parts.append(type(part))
        parts.append(message_parts)
    return parts

## tests/test_ag_ui.py

def uuid_str() -> str:
    """Generate a random UUID string."""
    return uuid.uuid4().hex

async def test_empty_messages() -> None:
    """Test handling of empty messages."""

    async def stream_function(
        messages: list[ModelMessage], agent_info: AgentInfo
    ) -> AsyncIterator[str]:  # pragma: no cover
        raise NotImplementedError
        yield 'no messages'

    agent = Agent(
        model=FunctionModel(stream_function=stream_function),
    )

    run_input = create_input()
    events = await run_and_collect_events(agent, run_input)

    assert events == snapshot(
        [
            {
                'type': 'RUN_STARTED',
                'timestamp': IsInt(),
                'threadId': IsStr(),
                'runId': IsStr(),
            },
            {
                'type': 'RUN_ERROR',
                'timestamp': IsInt(),
                'message': 'No message history, user prompt, or instructions provided',
            },
        ]
    )

## tests/test_agent.py

class UserContext:
    location: str | None

## tests/test_ui.py

    def messages(self) -> list[ModelMessage]:
        return self.load_messages(self.run_input.messages)
