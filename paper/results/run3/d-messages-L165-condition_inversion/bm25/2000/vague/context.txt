# tests/evals/test_multi_run.py:113-137
async def test_repeat_with_unnamed_cases():
    """repeat should work with cases that don't have explicit names."""

    async def task(inputs: str) -> str:
        return inputs.upper()

    dataset = Dataset(
        cases=[
            Case(inputs='hello'),
            Case(inputs='world'),
        ]
    )
    report = await dataset.evaluate(task, name='test', progress=False, repeat=2)

    assert len(report.cases) == 4
    case_names = sorted(c.name for c in report.cases)
    assert case_names == sorted(
        [
            'Case 1 [1/2]',
            'Case 1 [2/2]',
            'Case 2 [1/2]',
            'Case 2 [2/2]',
        ]
    )
    assert all(c.source_case_name is not None for c in report.cases)

# pydantic_evals/pydantic_evals/dataset.py:230-230
    cases: list[Case[InputsT, OutputT, MetadataT]]

# pydantic_evals/pydantic_evals/dataset.py:105-105
    cases: list[_CaseModel[InputsT, OutputT, MetadataT]]

# tests/evals/test_multi_run.py:72-110
async def test_repeat_3_produces_3x_cases():
    """repeat=3 should produce 3x cases, each with run-indexed names and source_case_name set."""
    call_count = 0

    async def task(inputs: str) -> str:
        nonlocal call_count
        call_count += 1
        return inputs.upper()

    dataset = Dataset(
        cases=[
            Case(name='case1', inputs='hello'),
            Case(name='case2', inputs='world'),
        ]
    )
    report = await dataset.evaluate(task, name='test', progress=False, repeat=3)

    assert call_count == 6  # 2 cases * 3 repeats
    assert len(report.cases) == 6

    # Check naming
    case_names = sorted(c.name for c in report.cases)
    assert case_names == sorted(
        [
            'case1 [1/3]',
            'case1 [2/3]',
            'case1 [3/3]',
            'case2 [1/3]',
            'case2 [2/3]',
            'case2 [3/3]',
        ]
    )

    # Check source_case_name
    assert all(c.source_case_name is not None for c in report.cases)
    case1_runs = [c for c in report.cases if c.source_case_name == 'case1']
    case2_runs = [c for c in report.cases if c.source_case_name == 'case2']
    assert len(case1_runs) == 3
    assert len(case2_runs) == 3

# tests/evals/test_dataset.py:89-103
def example_cases() -> list[Case[TaskInput, TaskOutput, TaskMetadata]]:
    return [
        Case(
            name='case1',
            inputs=TaskInput(query='What is 2+2?'),
            expected_output=TaskOutput(answer='4'),
            metadata=TaskMetadata(difficulty='easy'),
        ),
        Case(
            name='case2',
            inputs=TaskInput(query='What is the capital of France?'),
            expected_output=TaskOutput(answer='Paris'),
            metadata=TaskMetadata(difficulty='medium', category='geography'),
        ),
    ]

# tests/graph/beta/test_edge_cases.py:18-20
class EdgeCaseState:
    value: int = 0
    error_raised: bool = False

# pydantic_evals/pydantic_evals/reporting/__init__.py:305-305
    cases: list[ReportCase[InputsT, OutputT, MetadataT]]

# tests/evals/test_dataset.py:1386-1409
async def test_unnamed_cases():
    dataset = Dataset[TaskInput, TaskOutput, TaskMetadata](
        cases=[
            Case(
                name=None,
                inputs=TaskInput(query='What is 1+1?'),
            ),
            Case(
                name='My Case',
                inputs=TaskInput(query='What is 2+2?'),
            ),
            Case(
                name=None,
                inputs=TaskInput(query='What is 1+2?'),
            ),
        ]
    )

    async def task(inputs: TaskInput) -> TaskOutput:
        return TaskOutput(answer='4')

    result = await dataset.evaluate(task)
    assert [case.name for case in dataset.cases] == [None, 'My Case', None]
    assert [case.name for case in result.cases] == ['Case 1', 'My Case', 'Case 3']

# pydantic_evals/pydantic_evals/reporting/__init__.py:1400-1404
    def _all_cases(self, report: EvaluationReport, baseline: EvaluationReport | None) -> list[ReportCase]:
        if not baseline:
            return report.cases
        else:
            return report.cases + self._baseline_cases_to_include(report, baseline)

# pydantic_evals/pydantic_evals/dataset.py:84-84
_REPORT_CASES_ADAPTER = TypeAdapter(list[ReportCase])

# tests/profiles/test_openai.py:31-55
SAMPLING_PARAMS_CASES = [
    # o-series: reasoning enabled, no effort_none
    SamplingParamsCase(model='o1', supports_reasoning=True),
    SamplingParamsCase(model='o1-mini', supports_reasoning=True),
    SamplingParamsCase(model='o3', supports_reasoning=True),
    SamplingParamsCase(model='o3-mini', supports_reasoning=True),
    SamplingParamsCase(model='o4-mini', supports_reasoning=True),
    # gpt-5 (not 5.1+): reasoning enabled, no effort_none
    SamplingParamsCase(model='gpt-5', supports_reasoning=True),
    SamplingParamsCase(model='gpt-5-pro', supports_reasoning=True),
    SamplingParamsCase(model='gpt-5-turbo', supports_reasoning=True),
    # gpt-5.1+: reasoning + effort_none
    SamplingParamsCase(model='gpt-5.1', supports_reasoning=True, supports_reasoning_effort_none=True),
    SamplingParamsCase(model='gpt-5.1-turbo', supports_reasoning=True, supports_reasoning_effort_none=True),
    SamplingParamsCase(model='gpt-5.1-mini', supports_reasoning=True, supports_reasoning_effort_none=True),
    SamplingParamsCase(model='gpt-5.1-codex-max', supports_reasoning=True, supports_reasoning_effort_none=True),
    SamplingParamsCase(model='gpt-5.2', supports_reasoning=True, supports_reasoning_effort_none=True),
    SamplingParamsCase(model='gpt-5.2-turbo', supports_reasoning=True, supports_reasoning_effort_none=True),
    SamplingParamsCase(model='gpt-5.2-mini', supports_reasoning=True, supports_reasoning_effort_none=True),
    # no reasoning
    SamplingParamsCase(model='gpt-5-chat'),
    SamplingParamsCase(model='gpt-4o'),
    SamplingParamsCase(model='gpt-4o-mini'),
    SamplingParamsCase(model='gpt-4o-2024-08-06'),
]

# pydantic_evals/pydantic_evals/reporting/__init__.py:154-154
    runs: Sequence[ReportCase[InputsT, OutputT, MetadataT]]

# pydantic_evals/pydantic_evals/reporting/__init__.py:1365-1365
    include_removed_cases: bool

# tests/evals/test_dataset.py:1352-1360
async def test_dataset_evaluate_with_empty_cases(example_dataset: Dataset[TaskInput, TaskOutput, TaskMetadata]):
    """Test evaluating a dataset with no cases."""
    dataset = Dataset(cases=[])

    async def task(inputs: TaskInput) -> TaskOutput:  # pragma: no cover
        return TaskOutput(answer=inputs.query.upper())

    report = await dataset.evaluate(task)
    assert len(report.cases) == 0

# pydantic_evals/pydantic_evals/reporting/__init__.py:1406-1410
    def _baseline_cases_to_include(self, report: EvaluationReport, baseline: EvaluationReport) -> list[ReportCase]:
        if self.include_removed_cases:
            return baseline.cases
        report_case_names = {case.name for case in report.cases}
        return [case for case in baseline.cases if case.name in report_case_names]

# tests/graph/beta/test_edge_cases.py:19-19
    value: int = 0

# tests/evals/test_report_evaluators.py:373-413
async def test_dataset_with_report_evaluators():
    """Integration test: Dataset with report_evaluators runs them after cases."""

    @dataclass
    class LabelEvaluator(Evaluator[TaskInput, str, None]):
        def evaluate(self, ctx: EvaluatorContext[TaskInput, str, None]) -> EvaluatorOutput:
            if ctx.expected_output is not None:
                return ctx.output == ctx.expected_output
            return True  # pragma: no cover

    dataset = Dataset[TaskInput, str, None](
        cases=[
            Case(name='c1', inputs=TaskInput(text='meow'), expected_output='cat'),
            Case(name='c2', inputs=TaskInput(text='woof'), expected_output='dog'),
            Case(name='c3', inputs=TaskInput(text='purr'), expected_output='cat'),
        ],
        evaluators=[LabelEvaluator()],
        report_evaluators=[
            ConfusionMatrixEvaluator(
                predicted_from='output',
                expected_from='expected_output',
                title='Label Confusion',
            ),
        ],
    )

    async def task(inputs: TaskInput) -> str:
        if 'meow' in inputs.text or 'purr' in inputs.text:
            return 'cat'
        return 'dog'

    report = await dataset.evaluate(task, progress=False)

    assert len(report.cases) == 3
    assert len(report.analyses) == 1

    analysis = report.analyses[0]
    assert isinstance(analysis, ConfusionMatrix)
    assert analysis.title == 'Label Confusion'
    assert 'cat' in analysis.class_labels
    assert 'dog' in analysis.class_labels

# tests/graph/beta/test_edge_cases.py:20-20
    error_raised: bool = False

# pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py:2-2
from dataclasses import KW_ONLY, dataclass