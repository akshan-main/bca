## examples/pydantic_ai_examples/chat_app.py

async def index() -> FileResponse:
    return FileResponse((THIS_DIR / 'chat_app.html'), media_type='text/html')

async def main_ts() -> FileResponse:
    """Get the raw typescript code, it's compiled in the browser, forgive me."""
    return FileResponse((THIS_DIR / 'chat_app.ts'), media_type='text/plain')

## examples/pydantic_ai_examples/data_analyst.py

    def get(self, ref: str) -> pd.DataFrame:
        if ref not in self.output:
            raise ModelRetry(
                f'Error: {ref} is not a valid variable reference. Check the previous messages and try again.'
            )
        return self.output[ref]

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

def get_captured_run_messages() -> _RunMessages:
    return _messages_ctx_var.get()

## pydantic_ai_slim/pydantic_ai/agent/__init__.py

    def name(self, value: str | None) -> None:
        """Set the name of the agent, used for logging."""
        self._name = value

    def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:
        """Optional handler for events from the model's streaming response and the agent's execution of tools."""
        return self._event_stream_handler

## pydantic_ai_slim/pydantic_ai/agent/abstract.py

    def is_model_request_node(
        node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],
    ) -> TypeIs[_agent_graph.ModelRequestNode[T, S]]:
        """Check if the node is a `ModelRequestNode`, narrowing the type if it is.

        This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
        """
        return isinstance(node, _agent_graph.ModelRequestNode)

    def is_call_tools_node(
        node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],
    ) -> TypeIs[_agent_graph.CallToolsNode[T, S]]:
        """Check if the node is a `CallToolsNode`, narrowing the type if it is.

        This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
        """
        return isinstance(node, _agent_graph.CallToolsNode)

## pydantic_ai_slim/pydantic_ai/direct.py

    def get(self) -> messages.ModelResponse:
        """Build a ModelResponse from the data received from the stream so far."""
        return self._ensure_stream_ready().get()

## pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py

    def get(self) -> ModelResponse:
        return self.response

    def usage(self) -> RequestUsage:
        return self.response.usage  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py

    async def max_input_tokens(self) -> int | None:
        """Get the maximum number of tokens that can be input to the model."""
        return _MAX_INPUT_TOKENS.get(self._handler.model_name, None)

## pydantic_ai_slim/pydantic_ai/embeddings/cohere.py

    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self.model_name)

## pydantic_ai_slim/pydantic_ai/embeddings/google.py

    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self._model_name)

## pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py

    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self.model_name)

## pydantic_ai_slim/pydantic_ai/messages.py

    def has_content(self) -> bool:
        """Return `True` if the tool return has content."""
        return self.content is not None  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/models/__init__.py

    def usage(self) -> RequestUsage:
        """Get the usage of the response so far. This will not be the final usage until the stream is exhausted."""
        return self._usage

## pydantic_ai_slim/pydantic_ai/models/openai.py

    def base_url(self) -> str:
        return str(self.client.base_url)

    def model_name(self) -> OpenAIModelName:
        """The model name."""
        return self._model_name

    def system(self) -> str:
        """The model provider."""
        return self._provider.name

    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """Return the set of builtin tool types this model can handle."""
        return frozenset({WebSearchTool})

    def system_prompt_role(self) -> OpenAISystemPromptRole | None:
        return OpenAIModelProfile.from_profile(self.profile).openai_system_prompt_role

    def _map_usage(self, response: chat.ChatCompletion) -> usage.RequestUsage:
        return _map_usage(response, self._provider.name, self._provider.base_url, self.model_name)

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:
        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]

    def _map_model_response(self, message: ModelResponse) -> chat.ChatCompletionMessageParam:
        """Hook that determines how `ModelResponse` is mapped into `ChatCompletionMessageParam` objects before sending.

        Subclasses of `OpenAIChatModel` may override this method to provide their own mapping logic.
        """
        return self._MapModelResponseContext(self).map_assistant_message(message)

class OpenAIModel(OpenAIChatModel):
    """Deprecated alias for `OpenAIChatModel`."""

    def _map_usage(self, response: ChatCompletionChunk) -> usage.RequestUsage:
        return _map_usage(response, self._provider_name, self._provider_url, self.model_name)

## pydantic_ai_slim/pydantic_ai/models/openrouter.py

    def _map_finish_reason(  # type: ignore[reportIncompatibleMethodOverride]
        self, key: Literal['stop', 'length', 'tool_calls', 'content_filter', 'error']
    ) -> FinishReason | None:
        return _CHAT_FINISH_REASON_MAP.get(key)

    def _map_finish_reason(  # type: ignore[reportIncompatibleMethodOverride]
        self, key: Literal['stop', 'length', 'tool_calls', 'content_filter', 'error']
    ) -> FinishReason | None:
        return _CHAT_FINISH_REASON_MAP.get(key)

## pydantic_ai_slim/pydantic_ai/models/test.py

    def __init__(self, schema: _utils.ObjectJsonSchema, seed: int = 0):
        self.schema = schema
        self.defs = schema.get('$defs', {})
        self.seed = seed

## pydantic_ai_slim/pydantic_ai/models/xai.py

def _extract_usage(
    response: chat_types.Response,
    model: str,
    provider: str,
    provider_url: str,
) -> RequestUsage:
    """Extract usage information from xAI SDK response.

    Extracts token counts and additional usage details including:
    - reasoning_tokens: Tokens used for model reasoning/thinking
    - cache_read_tokens: Tokens read from prompt cache
    - server_side_tools_used: Count of server-side (built-in) tools executed
    """
    usage_obj = response.usage

    # Build usage data dict with all integer fields for genai-prices extraction
    usage_data: dict[str, int] = {
        'prompt_tokens': usage_obj.prompt_tokens or 0,
        'completion_tokens': usage_obj.completion_tokens or 0,
    }

    # Add reasoning tokens if available (optional attribute)
    if usage_obj.reasoning_tokens:
        usage_data['reasoning_tokens'] = usage_obj.reasoning_tokens

    # Add cached prompt tokens if available (optional attribute)
    if usage_obj.cached_prompt_text_tokens:
        usage_data['cache_read_tokens'] = usage_obj.cached_prompt_text_tokens

    # Aggregate server-side tools used by PydanticAI builtin tool name
    if usage_obj.server_side_tools_used:
        tool_counts: dict[str, int] = defaultdict(int)
        for server_side_tool in usage_obj.server_side_tools_used:
            tool_name = _map_server_side_tools_used_to_name(server_side_tool)
            tool_counts[tool_name] += 1
        # Add each tool as a separate details entry (server_side_tools must be flattened to comply with details being dict[str, int])
        for tool_name, count in tool_counts.items():
            usage_data[f'server_side_tools_{tool_name}'] = count

    # Build details from non-standard fields
    details = {k: v for k, v in usage_data.items() if k not in {'prompt_tokens', 'completion_tokens'}}

    extracted = RequestUsage.extract(
        dict(model=model, usage=usage_data),
        provider=provider,
        provider_url=provider_url,
        provider_fallback='x_ai',  # Pricing file is defined as x_ai.yml
        details=details or None,
    )

    # Ensure token counts are set even if genai-prices extraction failed
    if extracted.input_tokens == 0 and usage_data['prompt_tokens']:
        extracted.input_tokens = usage_data['prompt_tokens']
    if extracted.output_tokens == 0 and usage_data['completion_tokens']:
        extracted.output_tokens = usage_data['completion_tokens']

    return extracted

## pydantic_ai_slim/pydantic_ai/profiles/openai.py

    def __init__(self, schema: JsonSchema, *, strict: bool | None = None):
        super().__init__(schema, strict=strict)
        self.root_ref = schema.get('$ref')

## pydantic_ai_slim/pydantic_ai/providers/openai.py

    def name(self) -> str:
        return 'openai'

    def base_url(self) -> str:
        return str(self.client.base_url)

    def client(self) -> AsyncOpenAI:
        return self._client

    def model_profile(self, model_name: str) -> ModelProfile | None:
        return openai_model_profile(model_name)

## pydantic_ai_slim/pydantic_ai/providers/voyageai.py

    def base_url(self) -> str:
        return self._client._params.get('base_url') or 'https://api.voyageai.com/v1'  # type: ignore

## pydantic_ai_slim/pydantic_ai/result.py

    def get(self) -> _messages.ModelResponse:
        """Get the current state of the response."""
        return self._raw_stream_response.get()

    def usage(self) -> RunUsage:
        """Return the usage of the whole run.

        !!! note
            This won't return the full usage until the stream is finished.
        """
        return self._initial_run_ctx_usage + self._raw_stream_response.usage()

    def timestamp(self) -> datetime:
        """Get the timestamp of the response."""
        return self._raw_stream_response.timestamp

## pydantic_ai_slim/pydantic_ai/run.py

    def _node_to_task(self, node: _agent_graph.AgentNode[AgentDepsT, OutputDataT]) -> GraphTaskRequest:
        return GraphTaskRequest(NodeStep(type(node)).id, inputs=node, fork_stack=())

    def usage(self) -> _usage.RunUsage:
        """Get usage statistics for the run so far, including token usage, model requests, and so on."""
        return self._graph_run.state.usage

    def __repr__(self) -> str:  # pragma: no cover
        result = self._graph_run.output
        result_repr = '<run not finished>' if result is None else repr(result.output)
        return f'<{type(self).__name__} result={result_repr} usage={self.usage()}>'

## pydantic_ai_slim/pydantic_ai/toolsets/_dynamic.py

    def copy(self) -> DynamicToolset[AgentDepsT]:
        """Create a copy of this toolset for use in a new agent run."""
        return DynamicToolset(
            self.toolset_func,
            per_run_step=self.per_run_step,
            id=self._id,
        )

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py

    def _tool_return_output(self, part: BaseToolReturnPart) -> Any:
        output = part.model_response_object()
        # Unwrap the return value from the output dictionary if it exists
        return output.get('return_value', output)

## pydantic_ai_slim/pydantic_ai/usage.py

    input_tokens: Annotated[
        int,
        # `request_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('input_tokens', 'request_tokens')),
    ] = 0

    input_audio_tokens: int = 0

    def request_tokens(self) -> int:
        return self.input_tokens

    def response_tokens(self) -> int:
        return self.output_tokens

    def total_tokens(self) -> int:
        """Sum of `input_tokens + output_tokens`."""
        return self.input_tokens + self.output_tokens

    def __repr__(self):
        kv_pairs = (f'{f.name}={value!r}' for f in fields(self) if (value := getattr(self, f.name)))
        return f'{self.__class__.__qualname__}({", ".join(kv_pairs)})'

    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

    def requests(self):
        return 1

    input_tokens: int = 0

    input_audio_tokens: int = 0

    input_tokens_limit: int | None = None

## pydantic_evals/pydantic_evals/otel/span_tree.py

    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

## pydantic_graph/pydantic_graph/beta/graph.py

    def _handle_non_fork_edges(self, node: AnyNode, inputs: Any, fork_stack: ForkStack) -> Sequence[GraphTask]:
        edges = self.graph.edges_by_source.get(node.id, [])
        assert len(edges) == 1  # this should have already been ensured during graph building
        return self._handle_path(edges[0], inputs, fork_stack)

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## pydantic_graph/pydantic_graph/persistence/_utils.py

    def _node_discriminator(node_data: Any) -> str:
        return node_data.get('node_id')

## tests/conftest.py

    def reset(self) -> None:
        for name, value in self.envars.items():
            if value is None:
                os.environ.pop(name, None)
            else:
                os.environ[name] = value  # pragma: lax no cover

## tests/models/mock_openai.py

    def create_mock_stream(
        cls,
        stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))

## tests/models/test_anthropic.py

def test_usage(
    message_callback: Callable[[], BetaMessage | BetaRawMessageStartEvent | BetaRawMessageDeltaEvent], usage: RunUsage
):
    assert _map_usage(message_callback(), 'anthropic', '', 'unknown') == usage

## tests/models/test_openai.py

def test_init():
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key='foobar'))
    assert m.base_url == 'https://api.openai.com/v1/'
    assert m.client.api_key == 'foobar'
    assert m.model_name == 'gpt-4o'

async def test_max_completion_tokens(allow_model_requests: None, model_name: str, openai_api_key: str):
    m = OpenAIChatModel(model_name, provider=OpenAIProvider(api_key=openai_api_key))
    agent = Agent(m, model_settings=ModelSettings(max_tokens=100))

    result = await agent.run('hello')
    assert result.output == IsStr()

async def test_extra_headers(allow_model_requests: None, openai_api_key: str):
    # This test doesn't do anything, it's just here to ensure that calls with `extra_headers` don't cause errors, including type.
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key=openai_api_key))
    agent = Agent(m, model_settings=OpenAIChatModelSettings(extra_headers={'Extra-Header-Key': 'Extra-Header-Value'}))
    await agent.run('hello')

class MyEnum(Enum):
    a = 'a'
    b = 'b'

def test_openai_model_profile():
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key='foobar'))
    assert isinstance(m.profile, OpenAIModelProfile)

def chunk_with_usage(
    delta: list[ChoiceDelta],
    finish_reason: FinishReason | None = None,
    completion_tokens: int = 1,
    prompt_tokens: int = 2,
    total_tokens: int = 3,
) -> chat.ChatCompletionChunk:
    """Create a chunk with configurable usage stats for testing continuous_usage_stats."""
    return chat.ChatCompletionChunk(
        id='123',
        choices=[
            ChunkChoice(index=index, delta=delta, finish_reason=finish_reason) for index, delta in enumerate(delta)
        ],
        created=1704067200,  # 2024-01-01
        model='gpt-4o-123',
        object='chat.completion.chunk',
        usage=CompletionUsage(
            completion_tokens=completion_tokens, prompt_tokens=prompt_tokens, total_tokens=total_tokens
        ),
    )

## tests/providers/test_deepseek.py

def test_deep_seek_model_profile():
    provider = DeepSeekProvider(api_key='api-key')
    model = OpenAIChatModel('deepseek-r1', provider=provider)
    assert model.profile.json_schema_transformer == OpenAIJsonSchemaTransformer

## tests/providers/test_openai.py

def test_init_with_no_api_key_will_still_setup_client():
    provider = OpenAIProvider(base_url='http://localhost:19434/v1')
    assert provider.base_url == 'http://localhost:19434/v1/'

def test_init_with_non_openai_model():
    provider = OpenAIProvider(base_url='https://example.com/v1/')
    assert provider.base_url == 'https://example.com/v1/'

def test_init_of_openai_with_base_url_and_without_api_key(env: TestEnv):
    env.remove('OPENAI_API_KEY')
    provider = OpenAIProvider(base_url='https://example.com/v1')
    assert provider.client.api_key == 'api-key-not-set'

async def test_init_with_http_client():
    async with httpx.AsyncClient() as http_client:
        provider = OpenAIProvider(http_client=http_client, api_key='foobar')
        assert provider.client._client == http_client  # type: ignore

## tests/test_embeddings.py

    def embedder(self, openai_api_key: str) -> Embedder:
        return Embedder(OpenAIEmbeddingModel('text-embedding-3-small', provider=OpenAIProvider(api_key=openai_api_key)))

## tests/test_mcp.py

def model(openai_api_key: str) -> Model:
    return OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key=openai_api_key))

def agent(model: Model, mcp_server: MCPServerStdio) -> Agent:
    return Agent(model, toolsets=[mcp_server])

## tests/test_usage_limits.py

def test_usage_unknown_provider():
    assert RequestUsage.extract({}, provider='unknown', provider_url='', provider_fallback='') == RequestUsage()
