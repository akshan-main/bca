# pydantic_ai_slim/pydantic_ai/models/__init__.py:1274-1333
async def download_item(
    item: FileUrl,
    data_format: Literal['bytes', 'base64', 'base64_uri', 'text'] = 'bytes',
    type_format: Literal['mime', 'extension'] = 'mime',
) -> DownloadedItem[str] | DownloadedItem[bytes]:
    """Download an item by URL and return the content as a bytes object or a (base64-encoded) string.

    This function includes SSRF (Server-Side Request Forgery) protection:
    - Only http:// and https:// protocols are allowed
    - Private/internal IP addresses are blocked by default
    - Cloud metadata endpoints (169.254.169.254) are always blocked
    - Hostnames are resolved before requests to prevent DNS rebinding

    Set `item.force_download='allow-local'` to allow private IP addresses.

    Args:
        item: The item to download.
        data_format: The format to return the content in:
            - `bytes`: The raw bytes of the content.
            - `base64`: The base64-encoded content.
            - `base64_uri`: The base64-encoded content as a data URI.
            - `text`: The content as a string.
        type_format: The format to return the media type in:
            - `mime`: The media type as a MIME type.
            - `extension`: The media type as an extension.

    Raises:
        UserError: If the URL points to a YouTube video.
        ValueError: If the URL uses an unsupported protocol or targets a private/internal
            IP address (unless allow-local is set).
    """
    if isinstance(item, VideoUrl) and item.is_youtube:
        raise UserError('Downloading YouTube videos is not supported.')

    from .._ssrf import safe_download

    allow_local = item.force_download == 'allow-local'
    response = await safe_download(item.url, allow_local=allow_local)

    if content_type := response.headers.get('content-type'):
        content_type = content_type.split(';')[0]
        if content_type == 'application/octet-stream':
            content_type = None

    media_type = content_type or item.media_type

    data_type = media_type
    if type_format == 'extension':
        data_type = item.format

    data = response.content
    if data_format in ('base64', 'base64_uri'):
        data = base64.b64encode(data).decode('utf-8')
        if data_format == 'base64_uri':
            data = f'data:{media_type};base64,{data}'
        return DownloadedItem[str](data=data, data_type=data_type)
    elif data_format == 'text':
        return DownloadedItem[str](data=data.decode('utf-8'), data_type=data_type)
    else:
        return DownloadedItem[bytes](data=data, data_type=data_type)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:967-967
    blocked: NotRequired[bool]

# pydantic_ai_slim/pydantic_ai/usage.py:113-114
    def requests(self):
        return 1

# pydantic_ai_slim/pydantic_ai/usage.py:176-176
    requests: int = 0

# pydantic_ai_slim/pydantic_ai/_ssrf.py:297-368
async def safe_download(
    url: str,
    allow_local: bool = False,
    max_redirects: int = _MAX_REDIRECTS,
    timeout: int = _DEFAULT_TIMEOUT,
) -> httpx.Response:
    """Download content from a URL with SSRF protection.

    This function:
    1. Validates the URL protocol (only http/https allowed)
    2. Resolves the hostname to IP addresses
    3. Validates that no resolved IP is private (unless allow_local=True)
    4. Always blocks cloud metadata endpoints
    5. Makes the request to the resolved IP with the Host header set
    6. Manually follows redirects, validating each hop

    Args:
        url: The URL to download from.
        allow_local: If True, allows requests to private/internal IP addresses.
                    Cloud metadata endpoints are always blocked regardless.
        max_redirects: Maximum number of redirects to follow (default: 10).
        timeout: Request timeout in seconds (default: 30).

    Returns:
        The httpx.Response object.

    Raises:
        ValueError: If the URL fails SSRF validation or too many redirects occur.
        httpx.HTTPStatusError: If the response has an error status code.
    """
    current_url = url
    redirects_followed = 0

    client = cached_async_http_client(timeout=timeout)
    while True:
        # Validate and resolve the current URL
        resolved = await validate_and_resolve_url(current_url, allow_local)

        # Build URL with resolved IP
        request_url = build_url_with_ip(resolved)

        # For HTTPS, set sni_hostname so TLS uses the original hostname for SNI
        # and certificate validation, even though we're connecting to the resolved IP.
        extensions: dict[str, str] = {}
        if resolved.is_https:
            extensions['sni_hostname'] = resolved.hostname

        # Make request with Host header set to original hostname
        response = await client.get(
            request_url,
            headers={'Host': resolved.hostname},
            extensions=extensions,
            follow_redirects=False,
        )

        # Check if we need to follow a redirect
        if response.is_redirect:
            redirects_followed += 1
            if redirects_followed > max_redirects:
                raise ValueError(f'Too many redirects ({redirects_followed}). Maximum allowed: {max_redirects}')

            # Get redirect location
            location = response.headers.get('location')
            if not location:
                raise ValueError('Redirect response missing Location header')

            current_url = resolve_redirect_url(current_url, location)
            continue

        # Not a redirect, we're done
        response.raise_for_status()
        return response

# examples/pydantic_ai_examples/rag.py:176-180
    def url(self) -> str:
        url_path = re.sub(r'\.md$', '', self.path)
        return (
            f'https://logfire.pydantic.dev/docs/{url_path}/#{slugify(self.title, "-")}'
        )

# pydantic_ai_slim/pydantic_ai/tools.py:281-281
    metadata: dict[str, Any] | None

# pydantic_ai_slim/pydantic_ai/_run_context.py:79-79
    metadata: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:367-367
    url: str

# pydantic_ai_slim/pydantic_ai/messages.py:413-413
    url: str

# pydantic_ai_slim/pydantic_ai/messages.py:261-261
    url: str

# pydantic_ai_slim/pydantic_ai/messages.py:320-320
    url: str

# pydantic_ai_slim/pydantic_ai/messages.py:1347-1347
    metadata: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:387-387
    url: str

# pydantic_ai_slim/pydantic_ai/messages.py:174-174
    url: str

# tests/test_ssrf.py:686-692
    async def test_hostname_resolving_to_cloud_metadata_blocked(self) -> None:
        """Test that a hostname resolving to cloud metadata IP is blocked."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            # Attacker's DNS returns cloud metadata IP
            mock_executor.return_value = [(2, 1, 6, '', ('169.254.169.254', 0))]
            with pytest.raises(ValueError, match='Access to cloud metadata service'):
                await validate_and_resolve_url('http://attacker.com/path', allow_local=True)

# pydantic_ai_slim/pydantic_ai/mcp.py:986-986
    url: str

# pydantic_ai_slim/pydantic_ai/tools.py:523-523
    metadata: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:1049-1049
    metadata: dict[str, Any] | None = None

# pydantic_evals/pydantic_evals/dataset.py:136-136
    metadata: MetadataT | None = None

# pydantic_ai_slim/pydantic_ai/tools.py:231-231
    metadata: dict[str, dict[str, Any]] = field(default_factory=dict[str, dict[str, Any]])

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:89-89
    url: str

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:42-42
    url: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:76-76
    metadata: MetadataT | None

# pydantic_evals/pydantic_evals/evaluators/context.py:59-59
    metadata: MetadataT | None

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:72-72
    url: str

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:59-59
    url: str

# tests/test_ssrf.py:351-356
    async def test_cloud_metadata_always_blocked(self) -> None:
        """Test that cloud metadata IPs are always blocked, even with allow_local=True."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            mock_executor.return_value = [(2, 1, 6, '', ('169.254.169.254', 0))]
            with pytest.raises(ValueError, match='Access to cloud metadata service'):
                await validate_and_resolve_url('http://metadata.google.internal/path', allow_local=True)

# pydantic_ai_slim/pydantic_ai/common_tools/tavily.py:29-29
    url: str

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:176-176
    url: str

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:196-196
    url: str

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:94-94
    metadata: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/tools.py:163-163
    metadata: dict[str, dict[str, Any]] = field(default_factory=dict[str, dict[str, Any]])

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:37-37
    url: NotRequired[str]

# pydantic_ai_slim/pydantic_ai/mcp.py:1035-1113
    def __init__(
        self,
        url: str,
        *,
        headers: dict[str, str] | None = None,
        http_client: httpx.AsyncClient | None = None,
        id: str | None = None,
        tool_prefix: str | None = None,
        log_level: mcp_types.LoggingLevel | None = None,
        log_handler: LoggingFnT | None = None,
        timeout: float = 5,
        read_timeout: float | None = None,
        process_tool_call: ProcessToolCallback | None = None,
        allow_sampling: bool = True,
        sampling_model: models.Model | None = None,
        max_retries: int = 1,
        elicitation_callback: ElicitationFnT | None = None,
        cache_tools: bool = True,
        cache_resources: bool = True,
        client_info: mcp_types.Implementation | None = None,
        **_deprecated_kwargs: Any,
    ):
        """Build a new MCP server.

        Args:
            url: The URL of the endpoint on the MCP server.
            headers: Optional HTTP headers to be sent with each request to the endpoint.
            http_client: An `httpx.AsyncClient` to use with the endpoint.
            id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.
            tool_prefix: A prefix to add to all tools that are registered with the server.
            log_level: The log level to set when connecting to the server, if any.
            log_handler: A handler for logging messages from the server.
            timeout: The timeout in seconds to wait for the client to initialize.
            read_timeout: Maximum time in seconds to wait for new messages before timing out.
            process_tool_call: Hook to customize tool calling and optionally pass extra metadata.
            allow_sampling: Whether to allow MCP sampling through this client.
            sampling_model: The model to use for sampling.
            max_retries: The maximum number of times to retry a tool call.
            elicitation_callback: Callback function to handle elicitation requests from the server.
            cache_tools: Whether to cache the list of tools.
                See [`MCPServer.cache_tools`][pydantic_ai.mcp.MCPServer.cache_tools].
            cache_resources: Whether to cache the list of resources.
                See [`MCPServer.cache_resources`][pydantic_ai.mcp.MCPServer.cache_resources].
            client_info: Information describing the MCP client implementation.
        """
        if 'sse_read_timeout' in _deprecated_kwargs:
            if read_timeout is not None:
                raise TypeError("'read_timeout' and 'sse_read_timeout' cannot be set at the same time.")

            warnings.warn(
                "'sse_read_timeout' is deprecated, use 'read_timeout' instead.", DeprecationWarning, stacklevel=2
            )
            read_timeout = _deprecated_kwargs.pop('sse_read_timeout')

        _utils.validate_empty_kwargs(_deprecated_kwargs)

        if read_timeout is None:
            read_timeout = 5 * 60

        self.url = url
        self.headers = headers
        self.http_client = http_client

        super().__init__(
            tool_prefix=tool_prefix,
            log_level=log_level,
            log_handler=log_handler,
            timeout=timeout,
            read_timeout=read_timeout,
            process_tool_call=process_tool_call,
            allow_sampling=allow_sampling,
            sampling_model=sampling_model,
            max_retries=max_retries,
            elicitation_callback=elicitation_callback,
            cache_tools=cache_tools,
            cache_resources=cache_resources,
            id=id,
            client_info=client_info,
        )

# pydantic_ai_slim/pydantic_ai/messages.py:698-698
    metadata: Any = None

# pydantic_ai_slim/pydantic_ai/messages.py:838-838
    metadata: Any = None

# pydantic_evals/pydantic_evals/reporting/__init__.py:113-113
    metadata: MetadataT | None

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:246-246
    metadata: Any | None = None

# pydantic_evals/pydantic_evals/reporting/__init__.py:149-149
    metadata: MetadataT | None

# pydantic_ai_slim/pydantic_ai/mcp.py:146-146
    metadata: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:144-175
    async def __call__(
        self,
        url: str,
        exclude_source_domain: bool = True,
    ) -> list[ExaSearchResult]:
        """Finds pages similar to the given URL and returns them with content.

        Args:
            url: The URL to find similar pages for.
            exclude_source_domain: Whether to exclude results from the same domain
                as the input URL. Defaults to True.

        Returns:
            Similar pages with text content.
        """
        response = await self.client.find_similar(  # pyright: ignore[reportUnknownMemberType]
            url,
            num_results=self.num_results,
            exclude_source_domain=exclude_source_domain,
            contents={'text': True},
        )

        return [
            ExaSearchResult(
                title=result.title or '',
                url=result.url,
                published_date=result.published_date,
                author=result.author,
                text=result.text or '',
            )
            for result in response.results
        ]

# pydantic_evals/pydantic_evals/dataset.py:94-94
    metadata: MetadataT | None = None

# pydantic_ai_slim/pydantic_ai/run.py:433-435
    def metadata(self) -> dict[str, Any] | None:
        """Metadata associated with this agent run, if configured."""
        return self._state.metadata

# pydantic_ai_slim/pydantic_ai/run.py:289-291
    def metadata(self) -> dict[str, Any] | None:
        """Metadata associated with this agent run, if configured."""
        return self._graph_run.state.metadata

# pydantic_ai_slim/pydantic_ai/result.py:550-557
    def metadata(self) -> dict[str, Any] | None:
        """Metadata associated with this agent run, if configured."""
        if self._run_result is not None:
            return self._run_result.metadata
        elif self._stream_response is not None:
            return self._stream_response.metadata
        else:
            return None

# pydantic_ai_slim/pydantic_ai/mcp.py:868-934
    def __init__(
        self,
        command: str,
        args: Sequence[str],
        *,
        env: dict[str, str] | None = None,
        cwd: str | Path | None = None,
        tool_prefix: str | None = None,
        log_level: mcp_types.LoggingLevel | None = None,
        log_handler: LoggingFnT | None = None,
        timeout: float = 5,
        read_timeout: float = 5 * 60,
        process_tool_call: ProcessToolCallback | None = None,
        allow_sampling: bool = True,
        sampling_model: models.Model | None = None,
        max_retries: int = 1,
        elicitation_callback: ElicitationFnT | None = None,
        cache_tools: bool = True,
        cache_resources: bool = True,
        id: str | None = None,
        client_info: mcp_types.Implementation | None = None,
    ):
        """Build a new MCP server.

        Args:
            command: The command to run.
            args: The arguments to pass to the command.
            env: The environment variables to set in the subprocess.
            cwd: The working directory to use when spawning the process.
            tool_prefix: A prefix to add to all tools that are registered with the server.
            log_level: The log level to set when connecting to the server, if any.
            log_handler: A handler for logging messages from the server.
            timeout: The timeout in seconds to wait for the client to initialize.
            read_timeout: Maximum time in seconds to wait for new messages before timing out.
            process_tool_call: Hook to customize tool calling and optionally pass extra metadata.
            allow_sampling: Whether to allow MCP sampling through this client.
            sampling_model: The model to use for sampling.
            max_retries: The maximum number of times to retry a tool call.
            elicitation_callback: Callback function to handle elicitation requests from the server.
            cache_tools: Whether to cache the list of tools.
                See [`MCPServer.cache_tools`][pydantic_ai.mcp.MCPServer.cache_tools].
            cache_resources: Whether to cache the list of resources.
                See [`MCPServer.cache_resources`][pydantic_ai.mcp.MCPServer.cache_resources].
            id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.
            client_info: Information describing the MCP client implementation.
        """
        self.command = command
        self.args = args
        self.env = env
        self.cwd = cwd

        super().__init__(
            tool_prefix,
            log_level,
            log_handler,
            timeout,
            read_timeout,
            process_tool_call,
            allow_sampling,
            sampling_model,
            max_retries,
            elicitation_callback,
            cache_tools,
            cache_resources,
            id=id,
            client_info=client_info,
        )

# pydantic_ai_slim/pydantic_ai/result.py:145-149
    def metadata(self) -> dict[str, Any] | None:
        """Metadata associated with this agent run, if configured."""
        if self._metadata_getter is not None:
            return self._metadata_getter()
        return self._run_ctx.metadata

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:39-39
    metadata: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:45-45
    metadata: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/result.py:761-763
    def metadata(self) -> dict[str, Any] | None:
        """Metadata associated with this agent run, if configured."""
        return self._streamed_run_result.metadata

# pydantic_ai_slim/pydantic_ai/_ssrf.py:74-79
def is_cloud_metadata_ip(ip_str: str) -> bool:
    """Check if an IP address is a cloud metadata endpoint.

    These are always blocked for security reasons, even with allow_local=True.
    """
    return ip_str not in _CLOUD_METADATA_IPS

# tests/test_prefect.py:1187-1190
async def test_custom_model_settings(allow_model_requests: None):
    """Test that custom model settings are passed through correctly."""
    result = await settings_prefect_agent.run('Give me those settings')
    assert result.output == snapshot("{'max_tokens': 123, 'custom_setting': 'custom_value'}")