# pydantic_ai_slim/pydantic_ai/_ssrf.py:50-50
_MAX_REDIRECTS = 0

# tests/test_ssrf.py:9-22
from pydantic_ai._ssrf import (
    _DEFAULT_TIMEOUT,  # pyright: ignore[reportPrivateUsage]
    _MAX_REDIRECTS,  # pyright: ignore[reportPrivateUsage]
    ResolvedUrl,
    build_url_with_ip,
    extract_host_and_port,
    is_cloud_metadata_ip,
    is_private_ip,
    resolve_hostname,
    resolve_redirect_url,
    safe_download,
    validate_and_resolve_url,
    validate_url_protocol,
)

# tests/conftest.py:147-149
    def set(self, name: str, value: str) -> None:
        self.envars[name] = os.getenv(name)
        os.environ[name] = value

# tests/models/test_gemini_vertex.py:6-6
from inline_snapshot import Is, snapshot

# tests/models/test_google.py:16-16
from inline_snapshot import Is, snapshot

# pydantic_ai_slim/pydantic_ai/_ssrf.py:297-368
async def safe_download(
    url: str,
    allow_local: bool = False,
    max_redirects: int = _MAX_REDIRECTS,
    timeout: int = _DEFAULT_TIMEOUT,
) -> httpx.Response:
    """Download content from a URL with SSRF protection.

    This function:
    1. Validates the URL protocol (only http/https allowed)
    2. Resolves the hostname to IP addresses
    3. Validates that no resolved IP is private (unless allow_local=True)
    4. Always blocks cloud metadata endpoints
    5. Makes the request to the resolved IP with the Host header set
    6. Manually follows redirects, validating each hop

    Args:
        url: The URL to download from.
        allow_local: If True, allows requests to private/internal IP addresses.
                    Cloud metadata endpoints are always blocked regardless.
        max_redirects: Maximum number of redirects to follow (default: 10).
        timeout: Request timeout in seconds (default: 30).

    Returns:
        The httpx.Response object.

    Raises:
        ValueError: If the URL fails SSRF validation or too many redirects occur.
        httpx.HTTPStatusError: If the response has an error status code.
    """
    current_url = url
    redirects_followed = 0

    client = cached_async_http_client(timeout=timeout)
    while True:
        # Validate and resolve the current URL
        resolved = await validate_and_resolve_url(current_url, allow_local)

        # Build URL with resolved IP
        request_url = build_url_with_ip(resolved)

        # For HTTPS, set sni_hostname so TLS uses the original hostname for SNI
        # and certificate validation, even though we're connecting to the resolved IP.
        extensions: dict[str, str] = {}
        if resolved.is_https:
            extensions['sni_hostname'] = resolved.hostname

        # Make request with Host header set to original hostname
        response = await client.get(
            request_url,
            headers={'Host': resolved.hostname},
            extensions=extensions,
            follow_redirects=False,
        )

        # Check if we need to follow a redirect
        if response.is_redirect:
            redirects_followed += 1
            if redirects_followed > max_redirects:
                raise ValueError(f'Too many redirects ({redirects_followed}). Maximum allowed: {max_redirects}')

            # Get redirect location
            location = response.headers.get('location')
            if not location:
                raise ValueError('Redirect response missing Location header')

            current_url = resolve_redirect_url(current_url, location)
            continue

        # Not a redirect, we're done
        response.raise_for_status()
        return response

# tests/test_ssrf.py:432-459
    async def test_redirect_followed(self) -> None:
        """Test that redirects are followed with validation."""
        redirect_response = AsyncMock()
        redirect_response.is_redirect = True
        redirect_response.headers = {'location': 'https://cdn.example.com/file.txt'}

        final_response = AsyncMock()
        final_response.is_redirect = False
        final_response.raise_for_status = lambda: None
        final_response.content = b'final content'

        with (
            patch('pydantic_ai._ssrf.run_in_executor') as mock_executor,
            patch('pydantic_ai._ssrf.cached_async_http_client') as mock_client_fn,
        ):
            # First call for example.com, second for cdn.example.com
            mock_executor.side_effect = [
                [(2, 1, 6, '', ('93.184.215.14', 0))],
                [(2, 1, 6, '', ('203.0.113.50', 0))],
            ]

            mock_client = AsyncMock()
            mock_client.get.side_effect = [redirect_response, final_response]
            mock_client_fn.return_value = mock_client

            response = await safe_download('https://example.com/file.txt')
            assert response.content == b'final content'
            assert mock_client.get.call_count == 2

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:6-6
from typing import Any, Literal

# examples/pydantic_ai_examples/chat_app.py:20-20
from typing import Annotated, Any, Literal, TypeVar

# examples/pydantic_ai_examples/slack_lead_qualifier/app.py:1-1
from typing import Any

# examples/pydantic_ai_examples/slack_lead_qualifier/modal.py:1-1
from typing import Any

# examples/pydantic_ai_examples/slack_lead_qualifier/models.py:1-1
from typing import Annotated, Any

# examples/pydantic_ai_examples/slack_lead_qualifier/slack.py:2-2
from typing import Any

# examples/pydantic_ai_examples/sql_gen.py:19-19
from typing import Annotated, Any, TypeAlias

# examples/pydantic_ai_examples/weather_agent.py:16-16
from typing import Any

# pydantic_ai_slim/pydantic_ai/_a2a.py:9-9
from typing import Any, Generic, TypeVar

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:14-14
from typing import TYPE_CHECKING, Any, Generic, Literal, TypeGuard, cast

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:11-11
from typing import Any

# pydantic_ai_slim/pydantic_ai/_function_schema.py:12-12
from typing import TYPE_CHECKING, Any, Concatenate, cast, get_origin

# pydantic_ai_slim/pydantic_ai/_griffe.py:8-8
from typing import TYPE_CHECKING, Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/_json_schema.py:7-7
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/_output.py:9-9
from typing import TYPE_CHECKING, Any, Generic, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:18-18
from typing import Any, TypeVar

# pydantic_ai_slim/pydantic_ai/_run_context.py:8-8
from typing import TYPE_CHECKING, Any, Generic

# pydantic_ai_slim/pydantic_ai/_system_prompt.py:6-6
from typing import Any, Generic, cast

# pydantic_ai_slim/pydantic_ai/_tool_manager.py:8-8
from typing import Any, Generic, Literal

# pydantic_ai_slim/pydantic_ai/_utils.py:16-26
from typing import (
    TYPE_CHECKING,
    Any,
    Generic,
    TypeAlias,
    TypeGuard,
    TypeVar,
    get_args,
    get_origin,
    overload,
)

# pydantic_ai_slim/pydantic_ai/ag_ui.py:12-12
from typing import Any

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:12-12
from typing import TYPE_CHECKING, Any, ClassVar, overload

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:9-9
from typing import TYPE_CHECKING, Any, Generic, TypeAlias, cast, overload

# pydantic_ai_slim/pydantic_ai/agent/wrapper.py:5-5
from typing import Any, overload

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:6-6
from typing import Annotated, Any, Literal, Union

# pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py:6-6
from typing_extensions import Any, TypedDict

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:11-11
from typing_extensions import Any, TypedDict

# pydantic_ai_slim/pydantic_ai/common_tools/tavily.py:5-5
from typing_extensions import Any, TypedDict

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py:5-5
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp.py:5-5
from typing import TYPE_CHECKING, Any

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:6-6
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py:6-6
from typing import Any, overload

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:2-2
from typing import Any, TypeGuard

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py:4-4
from typing import TYPE_CHECKING, Any

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:6-6
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py:6-6
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py:8-8
from typing import Any, Literal, overload

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_dynamic_toolset.py:5-5
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_function_toolset.py:4-4
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_mcp.py:5-5
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:9-9
from typing import Any, cast

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:6-6
from typing import Annotated, Any, Literal

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_workflow.py:2-2
from typing import Any

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:5-5
from typing import Any, ClassVar, Literal, get_args

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:9-9
from typing import TYPE_CHECKING, Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:3-3
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:8-8
from typing import Any

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:4-4
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/embeddings/sentence_transformers.py:6-6
from typing import Any, cast

# pydantic_ai_slim/pydantic_ai/exceptions.py:5-5
from typing import TYPE_CHECKING, Any

# pydantic_ai_slim/pydantic_ai/ext/aci.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/ext/langchain.py:3-3
from typing import Any, Protocol

# pydantic_ai_slim/pydantic_ai/format_prompt.py:8-8
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/mcp.py:14-14
from typing import Annotated, Any, overload

# pydantic_ai_slim/pydantic_ai/messages.py:14-14
from typing import TYPE_CHECKING, Annotated, Any, Literal, TypeAlias, cast, overload

# pydantic_ai_slim/pydantic_ai/models/__init__.py:17-17
from typing import Any, Generic, Literal, TypeVar, get_args, overload

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:8-8
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:10-10
from typing import TYPE_CHECKING, Any, Generic, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/cerebras.py:6-6
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:8-8
from typing import Any

# pydantic_ai_slim/pydantic_ai/models/fallback.py:7-7
from typing import TYPE_CHECKING, Any

# pydantic_ai_slim/pydantic_ai/models/function.py:10-10
from typing import Any, TypeAlias

# pydantic_ai_slim/pydantic_ai/models/gemini.py:16-16
from typing import Annotated, Any, Literal, Protocol, cast

# pydantic_ai_slim/pydantic_ai/models/google.py:9-9
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/groq.py:7-7
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:7-7
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:9-9
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py:6-6
from typing import TYPE_CHECKING, Any, cast

# pydantic_ai_slim/pydantic_ai/models/mistral.py:7-7
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/models/openai.py:12-12
from typing import Any, Literal, cast, overload

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:5-5
from typing import Annotated, Any, Literal, TypeAlias, cast

# pydantic_ai_slim/pydantic_ai/models/outlines.py:13-13
from typing import TYPE_CHECKING, Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/models/test.py:9-9
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/models/wrapper.py:7-7
from typing import Any

# pydantic_ai_slim/pydantic_ai/models/xai.py:9-9
from typing import Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/output.py:5-5
from typing import Any, Generic, Literal

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:7-7
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/providers/__init__.py:9-9
from typing import Any, Generic, TypeVar

# pydantic_ai_slim/pydantic_ai/providers/bedrock.py:7-7
from typing import Any, Literal, overload

# pydantic_ai_slim/pydantic_ai/providers/gateway.py:8-8
from typing import TYPE_CHECKING, Any, Literal, overload

# pydantic_ai_slim/pydantic_ai/providers/outlines.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/providers/sentence_transformers.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/result.py:7-7
from typing import TYPE_CHECKING, Any, Generic, cast, overload

# pydantic_ai_slim/pydantic_ai/retries.py:39-39
from typing import TYPE_CHECKING, Any, cast

# pydantic_ai_slim/pydantic_ai/run.py:7-7
from typing import TYPE_CHECKING, Any, Generic, Literal, overload

# pydantic_ai_slim/pydantic_ai/tools.py:5-5
from typing import Annotated, Any, Concatenate, Generic, Literal, TypeAlias, cast

# pydantic_ai_slim/pydantic_ai/toolsets/_dynamic.py:5-5
from typing import Any, TypeAlias

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:6-6
from typing import TYPE_CHECKING, Any, Generic, Literal, Protocol

# pydantic_ai_slim/pydantic_ai/toolsets/approval_required.py:5-5
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/combined.py:8-8
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/external.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py:8-8
from typing import TYPE_CHECKING, Any, Literal

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:5-5
from typing import Any, overload

# pydantic_ai_slim/pydantic_ai/toolsets/prefixed.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/renamed.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/toolsets/wrapper.py:5-5
from typing import Any

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:9-17
from typing import (
    TYPE_CHECKING,
    Any,
    ClassVar,
    Generic,
    Protocol,
    cast,
    runtime_checkable,
)

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:7-7
from typing import TYPE_CHECKING, Any, Generic, Literal, TypeAlias, TypeVar, cast

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter.py:8-12
from typing import (
    TYPE_CHECKING,
    Any,
    cast,
)

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/app.py:7-7
from typing import Any, Generic

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter.py:10-10
from typing import TYPE_CHECKING, Any, Literal, cast

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:7-7
from typing import Any, Literal

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_utils.py:3-3
from typing import Any

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:8-8
from typing import Annotated, Any, Literal

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:8-8
from typing import Annotated, Any, Literal

# pydantic_ai_slim/pydantic_ai/usage.py:6-6
from typing import Annotated, Any

# pydantic_evals/pydantic_evals/_utils.py:10-10
from typing import TYPE_CHECKING, Any, TypeVar

# pydantic_evals/pydantic_evals/dataset.py:24-24
from typing import TYPE_CHECKING, Any, Generic, Literal, Union, cast

# pydantic_evals/pydantic_evals/evaluators/_base.py:7-7
from typing import Any

# pydantic_evals/pydantic_evals/evaluators/_run_evaluator.py:5-5
from typing import TYPE_CHECKING, Any

# pydantic_evals/pydantic_evals/evaluators/common.py:5-5
from typing import Any, Literal, cast

# pydantic_evals/pydantic_evals/evaluators/context.py:10-10
from typing import Any, Generic

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:7-7
from typing import Any, Generic, cast

# pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py:5-5
from typing import Any

# pydantic_evals/pydantic_evals/evaluators/report_common.py:4-4
from typing import Any, Literal, cast

# pydantic_evals/pydantic_evals/evaluators/report_evaluator.py:7-7
from typing import TYPE_CHECKING, Any, Generic, cast

# pydantic_evals/pydantic_evals/evaluators/spec.py:5-5
from typing import TYPE_CHECKING, Any, cast

# pydantic_evals/pydantic_evals/generation.py:11-11
from typing import Any

# pydantic_evals/pydantic_evals/otel/span_tree.py:9-9
from typing import TYPE_CHECKING, Any

# pydantic_evals/pydantic_evals/reporting/__init__.py:7-7
from typing import Any, Generic, Literal, Protocol, cast

# pydantic_graph/pydantic_graph/_utils.py:10-10
from typing import TYPE_CHECKING, Any, TypeAlias, TypeVar, get_args, get_origin

# pydantic_graph/pydantic_graph/beta/decision.py:13-13
from typing import TYPE_CHECKING, Any, Generic, get_origin

# pydantic_graph/pydantic_graph/beta/graph.py:14-14
from typing import TYPE_CHECKING, Any, Generic, Literal, TypeGuard, cast, get_args, get_origin, overload

# pydantic_graph/pydantic_graph/beta/graph_builder.py:15-15
from typing import Any, Generic, Literal, cast, get_origin, get_type_hints, overload

# pydantic_graph/pydantic_graph/beta/join.py:14-14
from typing import Any, Generic, Literal, cast, overload

# pydantic_graph/pydantic_graph/beta/node_types.py:10-10
from typing import Any, TypeGuard

# pydantic_graph/pydantic_graph/beta/paths.py:13-13
from typing import TYPE_CHECKING, Any, Generic, get_origin

# pydantic_graph/pydantic_graph/beta/step.py:12-12
from typing import Any, Generic, Protocol, cast, get_origin, overload

# pydantic_graph/pydantic_graph/beta/util.py:8-8
from typing import Any, Generic, cast, get_args, get_origin

# pydantic_graph/pydantic_graph/graph.py:10-10
from typing import Any, Generic, cast, overload

# pydantic_graph/pydantic_graph/mermaid.py:8-8
from typing import TYPE_CHECKING, Annotated, Any, Literal, TypeAlias

# pydantic_graph/pydantic_graph/nodes.py:7-7
from typing import Any, ClassVar, Generic, get_origin, get_type_hints

# pydantic_graph/pydantic_graph/persistence/__init__.py:7-7
from typing import TYPE_CHECKING, Annotated, Any, Generic, Literal

# pydantic_graph/pydantic_graph/persistence/_utils.py:8-8
from typing import Annotated, Any, Union

# pydantic_graph/pydantic_graph/persistence/file.py:9-9
from typing import Any

# pydantic_graph/pydantic_graph/persistence/in_mem.py:13-13
from typing import Any

# tests/conftest.py:17-17
from typing import TYPE_CHECKING, Any, TypeAlias, cast

# tests/evals/test_dataset.py:7-7
from typing import Any, Literal

# tests/evals/test_evaluator_base.py:6-6
from typing import TYPE_CHECKING, Any

# tests/evals/test_evaluator_common.py:4-4
from typing import TYPE_CHECKING, Any

# tests/evals/test_evaluator_context.py:3-3
from typing import Any

# tests/evals/test_evaluators.py:4-4
from typing import Any, cast

# tests/evals/test_report_evaluators.py:6-6
from typing import Any

# tests/evals/test_reports.py:3-3
from typing import Any

# tests/evals/test_utils.py:7-7
from typing import Any

# tests/example_modules/bank_database.py:5-5
from typing import Any

# tests/example_modules/fake_database.py:4-4
from typing import Any

# tests/example_modules/mcp_server.py:1-1
from typing import Any

# tests/example_modules/weather_service.py:4-4
from typing import Any

# tests/ext/test_langchain.py:2-2
from typing import Any

# tests/graph/beta/test_edge_cases.py:6-6
from typing import Any

# tests/graph/beta/test_graph_iteration.py:6-6
from typing import Any

# tests/graph/beta/test_node_and_step.py:3-3
from typing import Any

# tests/graph/beta/test_v1_v2_integration.py:6-6
from typing import Annotated, Any

# tests/json_body_serializer.py:7-7
from typing import TYPE_CHECKING, Any

# tests/mcp_server.py:3-3
from typing import Any

# tests/models/mock_async_stream.py:12-12
from typing import Any, Generic, TypeVar

# tests/models/mock_openai.py:6-6
from typing import Any, cast

# tests/models/mock_xai.py:13-13
from typing import Any, cast

# tests/models/test_anthropic.py:11-11
from typing import Annotated, Any, TypeVar, cast

# tests/models/test_bedrock.py:5-5
from typing import Any

# tests/models/test_cerebras.py:3-3
from typing import Any, cast

# tests/models/test_cohere.py:7-7
from typing import Any, cast

# tests/models/test_fallback.py:7-7
from typing import Any, Literal

# tests/models/test_google.py:12-12
from typing import Any

# tests/models/test_groq.py:9-9
from typing import Any, Literal, cast

# tests/models/test_huggingface.py:8-8
from typing import Any, Literal, cast

# tests/models/test_mcp_sampling.py:3-3
from typing import Any

# tests/models/test_mistral.py:8-8
from typing import Any, cast

# tests/models/test_model_names.py:4-4
from typing import Any, Literal, get_args

# tests/models/test_model_test.py:9-9
from typing import Annotated, Any, Literal

# tests/models/test_openai.py:10-10
from typing import Annotated, Any, Literal, cast

# tests/models/test_outlines.py:12-12
from typing import Any

# tests/models/test_xai.py:22-22
from typing import Any

# tests/models/xai_proto_cassettes.py:38-38
from typing import Any, Literal, Protocol, cast

# tests/parts_from_messages.py:1-1
from typing import Any

# tests/providers/test_gateway.py:3-3
from typing import Any, Literal

# tests/providers/test_provider_names.py:4-4
from typing import Any

# tests/test_ag_ui.py:10-10
from typing import Any

# tests/test_agent.py:9-9
from typing import Any, Generic, Literal, TypeVar, Union

# tests/test_cli.py:5-5
from typing import Any

# tests/test_concurrency.py:7-7
from typing import TYPE_CHECKING, Any

# tests/test_dbos.py:12-12
from typing import Any, Literal

# tests/test_embeddings.py:7-7
from typing import Any, get_args

# tests/test_examples.py:13-13
from typing import Any, cast

# tests/test_exceptions.py:4-4
from typing import Any

# tests/test_fastmcp.py:8-8
from typing import Any

# tests/test_format_as_xml.py:7-7
from typing import Any

# tests/test_function_schema.py:2-2
from typing import Any

# tests/test_history_processor.py:2-2
from typing import Any

# tests/test_json_body_serializer.py:1-1
from typing import Any

# tests/test_json_schema.py:6-6
from typing import Any

# tests/test_logfire.py:5-5
from typing import Any, Literal

# tests/test_mcp.py:9-9
from typing import Any

# tests/test_parts_manager.py:4-4
from typing import Any

# tests/test_streaming.py:10-10
from typing import Any

# tests/test_temporal.py:10-10
from typing import Any, Literal

# tests/test_tools.py:5-5
from typing import Annotated, Any, Literal

# tests/test_toolsets.py:6-6
from typing import Any, TypeVar

# tests/test_ui.py:6-6
from typing import Any

# tests/test_ui_web.py:8-8
from typing import Any

# tests/test_vercel_ai.py:5-5
from typing import Any, cast

# tests/typed_agent.py:8-8
from typing import Any, TypeAlias

# tests/typed_deps.py:2-2
from typing import Any

# tests/typed_graph.py:4-4
from typing import Any

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:1354-1408
    def to_a2a(
        self,
        *,
        storage: Storage | None = None,
        broker: Broker | None = None,
        # Agent card
        name: str | None = None,
        url: str = 'http://localhost:8000',
        version: str = '1.0.0',
        description: str | None = None,
        provider: AgentProvider | None = None,
        skills: list[Skill] | None = None,
        # Starlette
        debug: bool = False,
        routes: Sequence[Route] | None = None,
        middleware: Sequence[Middleware] | None = None,
        exception_handlers: dict[Any, ExceptionHandler] | None = None,
        lifespan: Lifespan[FastA2A] | None = None,
    ) -> FastA2A:
        """Convert the agent to a FastA2A application.

        Example:
        ```python
        from pydantic_ai import Agent

        agent = Agent('openai:gpt-5.2')
        app = agent.to_a2a()
        ```

        The `app` is an ASGI application that can be used with any ASGI server.

        To run the application, you can use the following command:

        ```bash
        uvicorn app:app --host 0.0.0.0 --port 8000
        ```
        """
        from .._a2a import agent_to_a2a

        return agent_to_a2a(
            self,
            storage=storage,
            broker=broker,
            name=name,
            url=url,
            version=version,
            description=description,
            provider=provider,
            skills=skills,
            debug=debug,
            routes=routes,
            middleware=middleware,
            exception_handlers=exception_handlers,
            lifespan=lifespan,
        )

# pydantic_ai_slim/pydantic_ai/mcp.py:1226-1285
class MCPServerStreamableHTTP(_MCPServerHTTP):
    """An MCP server that connects over HTTP using the Streamable HTTP transport.

    This class implements the Streamable HTTP transport from the MCP specification.
    See <https://modelcontextprotocol.io/introduction#streamable-http> for more information.

    !!! note
        Using this class as an async context manager will create a new pool of HTTP connections to connect
        to a server which should already be running.

    Example:
    ```python {py="3.10"}
    from pydantic_ai import Agent
    from pydantic_ai.mcp import MCPServerStreamableHTTP

    server = MCPServerStreamableHTTP('http://localhost:8000/mcp')
    agent = Agent('openai:gpt-5.2', toolsets=[server])
    ```
    """

    @classmethod
    def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:
        return core_schema.no_info_after_validator_function(
            lambda dct: MCPServerStreamableHTTP(**dct),
            core_schema.typed_dict_schema(
                {
                    'url': core_schema.typed_dict_field(core_schema.str_schema()),
                    'headers': core_schema.typed_dict_field(
                        core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()), required=False
                    ),
                }
            ),
        )

    @asynccontextmanager
    async def client_streams(
        self,
    ) -> AsyncIterator[
        tuple[
            MemoryObjectReceiveStream[SessionMessage | Exception],
            MemoryObjectSendStream[SessionMessage],
        ]
    ]:
        if self.http_client and self.headers:
            raise ValueError('`http_client` is mutually exclusive with `headers`.')

        aexit_stack = AsyncExitStack()
        http_client = self.http_client or await aexit_stack.enter_async_context(
            httpx.AsyncClient(timeout=httpx.Timeout(self.timeout, read=self.read_timeout), headers=self.headers)
        )
        read_stream, write_stream, *_ = await aexit_stack.enter_async_context(
            streamable_http_client(self.url, http_client=http_client)
        )
        try:
            yield read_stream, write_stream
        finally:
            await aexit_stack.aclose()

    def __eq__(self, value: object, /) -> bool:
        return super().__eq__(value) and isinstance(value, MCPServerStreamableHTTP) and self.url == value.url

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:136-384
class Embedder:
    """High-level interface for generating text embeddings.

    The `Embedder` class provides a convenient way to generate vector embeddings from text
    using various embedding model providers. It handles model inference, settings management,
    and optional OpenTelemetry instrumentation.

    Example:
    ```python
    from pydantic_ai import Embedder

    embedder = Embedder('openai:text-embedding-3-small')


    async def main():
        result = await embedder.embed_query('What is machine learning?')
        print(result.embeddings[0][:5])  # First 5 dimensions
        #> [1.0, 1.0, 1.0, 1.0, 1.0]
    ```
    """

    instrument: InstrumentationSettings | bool | None
    """Options to automatically instrument with OpenTelemetry.

    Set to `True` to use default instrumentation settings, which will use Logfire if it's configured.
    Set to an instance of [`InstrumentationSettings`][pydantic_ai.models.instrumented.InstrumentationSettings] to customize.
    If this isn't set, then the last value set by
    [`Embedder.instrument_all()`][pydantic_ai.embeddings.Embedder.instrument_all]
    will be used, which defaults to False.
    See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.
    """

    _instrument_default: ClassVar[InstrumentationSettings | bool] = False

    def __init__(
        self,
        model: EmbeddingModel | KnownEmbeddingModelName | str,
        *,
        settings: EmbeddingSettings | None = None,
        defer_model_check: bool = True,
        instrument: InstrumentationSettings | bool | None = None,
    ) -> None:
        """Initialize an Embedder.

        Args:
            model: The embedding model to use. Can be specified as:

                - A model name string in the format `'provider:model-name'`
                  (e.g., `'openai:text-embedding-3-small'`)
                - An [`EmbeddingModel`][pydantic_ai.embeddings.EmbeddingModel] instance
            settings: Optional [`EmbeddingSettings`][pydantic_ai.embeddings.EmbeddingSettings]
                to use as defaults for all embed calls.
            defer_model_check: Whether to defer model validation until first use.
                Set to `False` to validate the model immediately on construction.
            instrument: OpenTelemetry instrumentation settings. Set to `True` to enable with defaults,
                or pass an [`InstrumentationSettings`][pydantic_ai.models.instrumented.InstrumentationSettings]
                instance to customize. If `None`, uses the value from
                [`Embedder.instrument_all()`][pydantic_ai.embeddings.Embedder.instrument_all].
        """
        self._model = model if defer_model_check else infer_embedding_model(model)
        self._settings = settings
        self.instrument = instrument

        self._override_model: ContextVar[EmbeddingModel | None] = ContextVar('_override_model', default=None)

    @staticmethod
    def instrument_all(instrument: InstrumentationSettings | bool = True) -> None:
        """Set the default instrumentation options for all embedders where `instrument` is not explicitly set.

        This is useful for enabling instrumentation globally without modifying each embedder individually.

        Args:
            instrument: Instrumentation settings to use as the default. Set to `True` for default settings,
                `False` to disable, or pass an
                [`InstrumentationSettings`][pydantic_ai.models.instrumented.InstrumentationSettings]
                instance to customize.
        """
        Embedder._instrument_default = instrument

    @property
    def model(self) -> EmbeddingModel | KnownEmbeddingModelName | str:
        """The embedding model used by this embedder."""
        return self._model

    @contextmanager
    def override(
        self,
        *,
        model: EmbeddingModel | KnownEmbeddingModelName | str | _utils.Unset = _utils.UNSET,
    ) -> Iterator[None]:
        """Context manager to temporarily override the embedding model.

        Useful for testing or dynamically switching models.

        Args:
            model: The embedding model to use within this context.

        Example:
        ```python
        from pydantic_ai import Embedder

        embedder = Embedder('openai:text-embedding-3-small')


        async def main():
            # Temporarily use a different model
            with embedder.override(model='openai:text-embedding-3-large'):
                result = await embedder.embed_query('test')
                print(len(result.embeddings[0]))  # 3072 dimensions for large model
                #> 3072
        ```
        """
        if _utils.is_set(model):
            model_token = self._override_model.set(infer_embedding_model(model))
        else:
            model_token = None

        try:
            yield
        finally:
            if model_token is not None:
                self._override_model.reset(model_token)

    async def embed_query(
        self, query: str | Sequence[str], *, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Embed one or more query texts.

        Use this method when embedding search queries that will be compared against document embeddings.
        Some models optimize embeddings differently based on whether the input is a query or document.

        Args:
            query: A single query string or sequence of query strings to embed.
            settings: Optional settings to override the embedder's default settings for this call.

        Returns:
            An [`EmbeddingResult`][pydantic_ai.embeddings.EmbeddingResult] containing the embeddings
            and metadata about the operation.
        """
        return await self.embed(query, input_type='query', settings=settings)

    async def embed_documents(
        self, documents: str | Sequence[str], *, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Embed one or more document texts.

        Use this method when embedding documents that will be stored and later searched against.
        Some models optimize embeddings differently based on whether the input is a query or document.

        Args:
            documents: A single document string or sequence of document strings to embed.
            settings: Optional settings to override the embedder's default settings for this call.

        Returns:
            An [`EmbeddingResult`][pydantic_ai.embeddings.EmbeddingResult] containing the embeddings
            and metadata about the operation.
        """
        return await self.embed(documents, input_type='document', settings=settings)

    async def embed(
        self, inputs: str | Sequence[str], *, input_type: EmbedInputType, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Embed text inputs with explicit input type specification.

        This is the low-level embedding method. For most use cases, prefer
        [`embed_query()`][pydantic_ai.embeddings.Embedder.embed_query] or
        [`embed_documents()`][pydantic_ai.embeddings.Embedder.embed_documents].

        Args:
            inputs: A single string or sequence of strings to embed.
            input_type: The type of input, either `'query'` or `'document'`.
            settings: Optional settings to override the embedder's default settings for this call.

        Returns:
            An [`EmbeddingResult`][pydantic_ai.embeddings.EmbeddingResult] containing the embeddings
            and metadata about the operation.
        """
        model = self._get_model()
        settings = merge_embedding_settings(self._settings, settings)
        return await model.embed(inputs, input_type=input_type, settings=settings)

    async def max_input_tokens(self) -> int | None:
        """Get the maximum number of tokens the model can accept as input.

        Returns:
            The maximum token count, or `None` if the limit is unknown for this model.
        """
        model = self._get_model()
        return await model.max_input_tokens()

    async def count_tokens(self, text: str) -> int:
        """Count the number of tokens in the given text.

        Args:
            text: The text to tokenize and count.

        Returns:
            The number of tokens in the text.

        Raises:
            NotImplementedError: If the model doesn't support token counting.
            UserError: If the model or tokenizer is not supported.
        """
        model = self._get_model()
        return await model.count_tokens(text)

    def embed_query_sync(
        self, query: str | Sequence[str], *, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Synchronous version of [`embed_query()`][pydantic_ai.embeddings.Embedder.embed_query]."""
        return _utils.get_event_loop().run_until_complete(self.embed_query(query, settings=settings))

    def embed_documents_sync(
        self, documents: str | Sequence[str], *, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Synchronous version of [`embed_documents()`][pydantic_ai.embeddings.Embedder.embed_documents]."""
        return _utils.get_event_loop().run_until_complete(self.embed_documents(documents, settings=settings))

    def embed_sync(
        self, inputs: str | Sequence[str], *, input_type: EmbedInputType, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Synchronous version of [`embed()`][pydantic_ai.embeddings.Embedder.embed]."""
        return _utils.get_event_loop().run_until_complete(self.embed(inputs, input_type=input_type, settings=settings))

    def max_input_tokens_sync(self) -> int | None:
        """Synchronous version of [`max_input_tokens()`][pydantic_ai.embeddings.Embedder.max_input_tokens]."""
        return _utils.get_event_loop().run_until_complete(self.max_input_tokens())

    def count_tokens_sync(self, text: str) -> int:
        """Synchronous version of [`count_tokens()`][pydantic_ai.embeddings.Embedder.count_tokens]."""
        return _utils.get_event_loop().run_until_complete(self.count_tokens(text))

    def _get_model(self) -> EmbeddingModel:
        """Create a model configured for this embedder.

        Returns:
            The embedding model to use, with instrumentation applied if configured.
        """
        model_: EmbeddingModel
        if some_model := self._override_model.get():
            model_ = some_model
        else:
            model_ = self._model = infer_embedding_model(self.model)

        instrument = self.instrument
        if instrument is None:
            instrument = self._instrument_default

        return instrument_embedding_model(model_, instrument)

# tests/models/mock_xai.py:723-737
def create_usage(
    prompt_tokens: int = 0,
    completion_tokens: int = 0,
    reasoning_tokens: int = 0,
    cached_prompt_text_tokens: int = 0,
    server_side_tools_used: list[usage_pb2.ServerSideTool] | None = None,
) -> usage_pb2.SamplingUsage:
    """Helper to create xAI SamplingUsage protobuf objects for tests with all required fields."""
    return usage_pb2.SamplingUsage(
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        reasoning_tokens=reasoning_tokens,
        cached_prompt_text_tokens=cached_prompt_text_tokens,
        server_side_tools_used=server_side_tools_used or [],
    )

# pydantic_ai_slim/pydantic_ai/mcp.py:1124-1201
class MCPServerSSE(_MCPServerHTTP):
    """An MCP server that connects over streamable HTTP connections.

    This class implements the SSE transport from the MCP specification.
    See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.

    !!! note
        Using this class as an async context manager will create a new pool of HTTP connections to connect
        to a server which should already be running.

    Example:
    ```python {py="3.10"}
    from pydantic_ai import Agent
    from pydantic_ai.mcp import MCPServerSSE

    server = MCPServerSSE('http://localhost:3001/sse')
    agent = Agent('openai:gpt-5.2', toolsets=[server])
    ```
    """

    @classmethod
    def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:
        return core_schema.no_info_after_validator_function(
            lambda dct: MCPServerSSE(**dct),
            core_schema.typed_dict_schema(
                {
                    'url': core_schema.typed_dict_field(core_schema.str_schema()),
                    'headers': core_schema.typed_dict_field(
                        core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()), required=False
                    ),
                }
            ),
        )

    # sse_client has a hang bug (https://github.com/modelcontextprotocol/python-sdk/issues/1811)
    # that prevents testing SSE transport in CI.
    # TODO: Remove pragma and add a test
    # once https://github.com/modelcontextprotocol/python-sdk/pull/1838 is released.
    @asynccontextmanager
    async def client_streams(  # pragma: no cover
        self,
    ) -> AsyncIterator[
        tuple[
            MemoryObjectReceiveStream[SessionMessage | Exception],
            MemoryObjectSendStream[SessionMessage],
        ]
    ]:
        if self.http_client and self.headers:
            raise ValueError('`http_client` is mutually exclusive with `headers`.')

        if self.http_client is not None:

            def httpx_client_factory(
                headers: dict[str, str] | None = None,
                timeout: httpx.Timeout | None = None,
                auth: httpx.Auth | None = None,
            ) -> httpx.AsyncClient:
                assert self.http_client is not None
                return self.http_client

            async with sse_client(
                url=self.url,
                timeout=self.timeout,
                sse_read_timeout=self.read_timeout,
                httpx_client_factory=httpx_client_factory,
            ) as (read_stream, write_stream, *_):
                yield read_stream, write_stream
        else:
            async with sse_client(
                url=self.url,
                timeout=self.timeout,
                sse_read_timeout=self.read_timeout,
                headers=self.headers,
            ) as (read_stream, write_stream, *_):
                yield read_stream, write_stream

    def __eq__(self, value: object, /) -> bool:
        return super().__eq__(value) and isinstance(value, MCPServerSSE) and self.url == value.url

# pydantic_ai_slim/pydantic_ai/retries.py:168-197
    def handle_request(self, request: Request) -> Response:
        """Handle an HTTP request with retry logic.

        Args:
            request: The HTTP request to handle.

        Returns:
            The HTTP response.

        Raises:
            RuntimeError: If the retry controller did not make any attempts.
            Exception: Any exception raised by the wrapped transport or validation function.
        """

        @retry(**self.config)
        def handle_request(req: Request) -> Response:
            response = self.wrapped.handle_request(req)

            # this is normally set by httpx _after_ calling this function, but we want the request in the validator:
            response.request = req

            if self.validate_response:
                try:
                    self.validate_response(response)
                except Exception:
                    response.close()
                    raise
            return response

        return handle_request(request)

# pydantic_ai_slim/pydantic_ai/retries.py:265-294
    async def handle_async_request(self, request: Request) -> Response:
        """Handle an async HTTP request with retry logic.

        Args:
            request: The HTTP request to handle.

        Returns:
            The HTTP response.

        Raises:
            RuntimeError: If the retry controller did not make any attempts.
            Exception: Any exception raised by the wrapped transport or validation function.
        """

        @retry(**self.config)
        async def handle_async_request(req: Request) -> Response:
            response = await self.wrapped.handle_async_request(req)

            # this is normally set by httpx _after_ calling this function, but we want the request in the validator:
            response.request = req

            if self.validate_response:
                try:
                    self.validate_response(response)
                except Exception:
                    await response.aclose()
                    raise
            return response

        return await handle_async_request(request)

# pydantic_ai_slim/pydantic_ai/mcp.py:1205-1223
class MCPServerHTTP(MCPServerSSE):
    """An MCP server that connects over HTTP using the old SSE transport.

    This class implements the SSE transport from the MCP specification.
    See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.

    !!! note
        Using this class as an async context manager will create a new pool of HTTP connections to connect
        to a server which should already be running.

    Example:
    ```python {py="3.10" test="skip"}
    from pydantic_ai import Agent
    from pydantic_ai.mcp import MCPServerHTTP

    server = MCPServerHTTP('http://localhost:3001/sse')
    agent = Agent('openai:gpt-5.2', toolsets=[server])
    ```
    """

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:202-213
    def instrument_all(instrument: InstrumentationSettings | bool = True) -> None:
        """Set the default instrumentation options for all embedders where `instrument` is not explicitly set.

        This is useful for enabling instrumentation globally without modifying each embedder individually.

        Args:
            instrument: Instrumentation settings to use as the default. Set to `True` for default settings,
                `False` to disable, or pass an
                [`InstrumentationSettings`][pydantic_ai.models.instrumented.InstrumentationSettings]
                instance to customize.
        """
        Embedder._instrument_default = instrument

# pydantic_ai_slim/pydantic_ai/_a2a.py:75-115
def agent_to_a2a(
    agent: AbstractAgent[AgentDepsT, OutputDataT],
    *,
    storage: Storage | None = None,
    broker: Broker | None = None,
    # Agent card
    name: str | None = None,
    url: str = 'http://localhost:8000',
    version: str = '1.0.0',
    description: str | None = None,
    provider: AgentProvider | None = None,
    skills: list[Skill] | None = None,
    # Starlette
    debug: bool = False,
    routes: Sequence[Route] | None = None,
    middleware: Sequence[Middleware] | None = None,
    exception_handlers: dict[Any, ExceptionHandler] | None = None,
    lifespan: Lifespan[FastA2A] | None = None,
) -> FastA2A:
    """Create a FastA2A server from an agent."""
    storage = storage or InMemoryStorage()
    broker = broker or InMemoryBroker()
    worker = AgentWorker(agent=agent, broker=broker, storage=storage)

    lifespan = lifespan or partial(worker_lifespan, worker=worker, agent=agent)

    return FastA2A(
        storage=storage,
        broker=broker,
        name=name or agent.name,
        url=url,
        version=version,
        description=description,
        provider=provider,
        skills=skills,
        debug=debug,
        routes=routes,
        middleware=middleware,
        exception_handlers=exception_handlers,
        lifespan=lifespan,
    )

# pydantic_ai_slim/pydantic_ai/providers/anthropic.py:97-145
class AnthropicJsonSchemaTransformer(JsonSchemaTransformer):
    """Transforms schemas to the subset supported by Anthropic structured outputs.

    Transformation is applied when:
    - `NativeOutput` is used as the `output_type` of the Agent
    - `strict=True` is set on the `Tool`

    The behavior of this transformer differs from the OpenAI one in that it sets `Tool.strict=False` by default when not explicitly set to True.

    Example:
        ```python
        from pydantic_ai import Agent

        agent = Agent('anthropic:claude-sonnet-4-5')

        @agent.tool_plain  # -> defaults to strict=False
        def my_tool(x: str) -> dict[str, int]:
            ...
        ```

    Anthropic's SDK `transform_schema()` automatically:
    - Adds `additionalProperties: false` to all objects (required by API)
    - Removes unsupported constraints (minLength, pattern, etc.)
    - Moves removed constraints to description field
    - Removes title and $schema fields
    """

    def walk(self) -> JsonSchema:
        schema = super().walk()

        # The caller (pydantic_ai.models._customize_tool_def or _customize_output_object) coalesces
        # - output_object.strict = self.is_strict_compatible
        # - tool_def.strict = self.is_strict_compatible
        # the reason we don't default to `strict=True` is that the transformation could be lossy
        # so in order to change the behavior (default to True), we need to come up with logic that will check for lossiness
        # https://github.com/pydantic/pydantic-ai/issues/3541
        self.is_strict_compatible = self.strict is True  # not compatible when strict is False/None

        if self.strict is True:
            from anthropic import transform_schema

            return transform_schema(schema)
        else:
            return schema

    def transform(self, schema: JsonSchema) -> JsonSchema:
        schema.pop('title', None)
        schema.pop('$schema', None)
        return schema

# tests/test_ssrf.py:394-399
    async def test_cgnat_range_blocked(self) -> None:
        """Test that CGNAT range (100.64.0.0/10) is blocked."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            mock_executor.return_value = [(2, 1, 6, '', ('100.64.0.1', 0))]
            with pytest.raises(ValueError, match='Access to private/internal IP address'):
                await validate_and_resolve_url('http://cgnat-host.internal/path', allow_local=False)

# pydantic_evals/pydantic_evals/otel/span_tree.py:487-489
    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:421-423
    def instrument_all(instrument: InstrumentationSettings | bool = True) -> None:
        """Set the instrumentation options for all agents where `instrument` is not set."""
        Agent._instrument_default = instrument

# pydantic_ai_slim/pydantic_ai/ext/langchain.py:26-26
    def run(self, *args: Any, **kwargs: Any) -> str: ...