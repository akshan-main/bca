## docs/.hooks/algolia.py

from typing_extensions import TypedDict

## pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py

from prefect.cache_policies import INPUTS, RUN_ID, TASK_SOURCE, CachePolicy

def _is_toolset_tool(obj: Any) -> TypeGuard[ToolsetTool]:
    return isinstance(obj, ToolsetTool)

def _replace_toolsets(
    inputs: dict[str, Any],
) -> Any:
    """Replace Toolset objects with a dict containing only hashable fields."""
    inputs = inputs.copy()
    for key, value in inputs.items():
        if _is_toolset_tool(value):
            inputs[key] = {field.name: getattr(value, field.name) for field in fields(value) if field.name != 'toolset'}
    return inputs

    def compute_key(
        self,
        task_ctx: TaskRunContext,
        inputs: dict[str, Any],
        flow_parameters: dict[str, Any],
        **kwargs: Any,
    ) -> str | None:
        """Compute cache key from inputs with timestamps removed and RunContext serialized."""
        if not inputs:
            return None

        inputs_without_toolsets = _replace_toolsets(inputs)
        inputs_with_hashable_context = _replace_run_context(inputs_without_toolsets)
        filtered_inputs = _strip_timestamps(inputs_with_hashable_context)

        return INPUTS.compute_key(task_ctx, filtered_inputs, flow_parameters, **kwargs)

## pydantic_ai_slim/pydantic_ai/models/gemini.py

class _GeminiResponse(TypedDict):
    """Schema for the response from the Gemini API.

    See <https://ai.google.dev/api/generate-content#v1beta.GenerateContentResponse>
    and <https://cloud.google.com/vertex-ai/docs/reference/rest/v1/GenerateContentResponse>
    """

    candidates: list[_GeminiCandidates]
    # usageMetadata appears to be required by both APIs but is omitted when streaming responses until the last response
    usage_metadata: NotRequired[Annotated[_GeminiUsageMetaData, pydantic.Field(alias='usageMetadata')]]
    prompt_feedback: NotRequired[Annotated[_GeminiPromptFeedback, pydantic.Field(alias='promptFeedback')]]
    model_version: NotRequired[Annotated[str, pydantic.Field(alias='modelVersion')]]
    vendor_id: NotRequired[Annotated[str, pydantic.Field(alias='responseId')]]

class _GeminiCandidates(TypedDict):
    """See <https://ai.google.dev/api/generate-content#v1beta.Candidate>."""

    content: NotRequired[_GeminiContent]
    finish_reason: NotRequired[Annotated[Literal['STOP', 'MAX_TOKENS', 'SAFETY'], pydantic.Field(alias='finishReason')]]
    """
    See <https://ai.google.dev/api/generate-content#FinishReason>, lots of other values are possible,
    but let's wait until we see them and know what they mean to add them here.
    """
    avg_log_probs: NotRequired[Annotated[float, pydantic.Field(alias='avgLogProbs')]]
    index: NotRequired[int]
    safety_ratings: NotRequired[Annotated[list[_GeminiSafetyRating], pydantic.Field(alias='safetyRatings')]]

class _GeminiSafetyRating(TypedDict):
    """See <https://ai.google.dev/gemini-api/docs/safety-settings#safety-filters>."""

    category: Literal[
        'HARM_CATEGORY_HARASSMENT',
        'HARM_CATEGORY_HATE_SPEECH',
        'HARM_CATEGORY_SEXUALLY_EXPLICIT',
        'HARM_CATEGORY_DANGEROUS_CONTENT',
        'HARM_CATEGORY_CIVIC_INTEGRITY',
    ]
    probability: Literal['NEGLIGIBLE', 'LOW', 'MEDIUM', 'HIGH']
    blocked: NotRequired[bool]

## pydantic_ai_slim/pydantic_ai/models/xai.py

def _map_model_settings(model_settings: XaiModelSettings) -> dict[str, Any]:
    """Map pydantic_ai ModelSettings to xAI SDK parameters."""
    return {
        _XAI_MODEL_SETTINGS_MAPPING[key]: value
        for key, value in model_settings.items()
        if key in _XAI_MODEL_SETTINGS_MAPPING
    }

## pydantic_evals/pydantic_evals/dataset.py

def _set_experiment_span_attributes(
    eval_span: logfire_api.LogfireSpan,
    report: EvaluationReport[Any, Any, Any],
    metadata: dict[str, Any] | None,
    n_cases: int,
    repeat: int,
) -> None:
    full_experiment_metadata: dict[str, Any] = {'n_cases': n_cases}
    if repeat > 1:
        full_experiment_metadata['repeat'] = repeat
    if metadata is not None:
        full_experiment_metadata['metadata'] = metadata
    if (averages := report.averages()) is not None:
        full_experiment_metadata['averages'] = averages
        if averages.assertions is not None:
            eval_span.set_attribute('assertion_pass_rate', averages.assertions)
    eval_span.set_attribute('logfire.experiment.metadata', full_experiment_metadata)

    if report.analyses:
        eval_span.set_attribute(
            'logfire.experiment.analyses',
            [analysis.model_dump() for analysis in report.analyses],
        )

    if report.report_evaluator_failures:
        eval_span.set_attribute(
            'logfire.experiment.report_evaluator_failures',
            [
                {
                    'name': f.name,
                    'error_message': f.error_message,
                    'error_stacktrace': f.error_stacktrace,
                    'source': f.source.model_dump(),
                }
                for f in report.report_evaluator_failures
            ],
        )

## pydantic_evals/pydantic_evals/otel/span_tree.py

    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

## pydantic_graph/pydantic_graph/persistence/_utils.py

def set_nodes_type_context(nodes: Sequence[type[BaseNode[Any, Any, Any]]]) -> Iterator[None]:
    token = nodes_type_context.set(nodes)
    try:
        yield
    finally:
        nodes_type_context.reset(token)

## tests/conftest.py

    def set(self, name: str, value: str) -> None:
        self.envars[name] = os.getenv(name)
        os.environ[name] = value

    def reset(self) -> None:
        for name, value in self.envars.items():
            if value is None:
                os.environ.pop(name, None)
            else:
                os.environ[name] = value  # pragma: lax no cover

## tests/models/test_gemini.py

def example_usage() -> _GeminiUsageMetaData:
    return _GeminiUsageMetaData(prompt_token_count=1, candidates_token_count=2, total_token_count=3)

def gemini_no_content_response(
    safety_ratings: list[_GeminiSafetyRating], finish_reason: Literal['SAFETY'] | None = 'SAFETY'
) -> _GeminiResponse:
    candidate = _GeminiCandidates(safety_ratings=safety_ratings)
    if finish_reason:  # pragma: no branch
        candidate['finish_reason'] = finish_reason
    return _GeminiResponse(candidates=[candidate], usage_metadata=example_usage())

async def test_safety_settings_safe(
    client_with_handler: ClientWithHandler, env: TestEnv, allow_model_requests: None
) -> None:
    def handler(request: httpx.Request) -> httpx.Response:
        safety_settings = json.loads(request.content)['safetySettings']
        assert safety_settings == [
            {'category': 'HARM_CATEGORY_CIVIC_INTEGRITY', 'threshold': 'BLOCK_LOW_AND_ABOVE'},
            {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_LOW_AND_ABOVE'},
        ]

        return httpx.Response(
            200,
            content=_gemini_response_ta.dump_json(
                gemini_response(_content_model_response(ModelResponse(parts=[TextPart('world')]))),
                by_alias=True,
            ),
            headers={'Content-Type': 'application/json'},
        )

    gemini_client = client_with_handler(handler)
    m = GeminiModel('gemini-1.5-flash', provider=GoogleGLAProvider(http_client=gemini_client, api_key='mock'))
    agent = Agent(m)

    result = await agent.run(
        'hello',
        model_settings=GeminiModelSettings(
            gemini_safety_settings=[
                {'category': 'HARM_CATEGORY_CIVIC_INTEGRITY', 'threshold': 'BLOCK_LOW_AND_ABOVE'},
                {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_LOW_AND_ABOVE'},
            ]
        ),
    )
    assert result.output == 'world'

## tests/models/test_model_settings.py

def test_none_settings_in_hierarchy():
    """Test that None settings at any level don't break the merge hierarchy."""
    captured_settings = None

    def capture_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('captured')])

    # Model with no settings
    model = FunctionModel(capture_settings, settings=None)

    # Agent with settings
    agent_settings = ModelSettings(max_tokens=150, temperature=0.5)
    agent = Agent(model=model, model_settings=agent_settings)

    # Run with no additional settings
    result = agent.run_sync('test', model_settings=None)
    assert result.output == 'captured'

    # Should have agent settings
    assert captured_settings is not None
    assert captured_settings['max_tokens'] == 150
    assert captured_settings['temperature'] == 0.5

## tests/models/xai_proto_cassettes.py

class SampleInteraction:
    """A single `chat.sample()` request/response pair."""

    request_raw: bytes
    response_raw: bytes
    request_json: dict[str, Any] | None = None
    response_json: dict[str, Any] | None = None

class StreamInteraction:
    """A single `chat.stream()` request/response pair."""

    request_raw: bytes
    chunks_raw: list[bytes]
    request_json: dict[str, Any] | None = None
    chunks_json: list[dict[str, Any]] | None = None

class _CassetteChatInstance:
    _client: XaiProtoCassetteClient
    _expected_type: Literal['sample', 'stream']

    async def sample(self) -> chat_types.Response:
        if self._expected_type != 'sample':
            raise RuntimeError(
                f'Cassette expects a stream() call at interaction {self._client.interaction_idx}, '
                f'but sample() was called.'
            )
        interaction = self._client.next_interaction()
        if not isinstance(interaction, SampleInteraction):  # pragma: no cover
            raise RuntimeError(f'Expected SampleInteraction, got {type(interaction).__name__}')

        proto = chat_pb2.GetChatCompletionResponse()
        proto.ParseFromString(interaction.response_raw)
        return chat_types.Response(proto, index=None)

    def stream(self) -> Any:
        if self._expected_type != 'stream':
            raise RuntimeError(
                f'Cassette expects a sample() call at interaction {self._client.interaction_idx}, '
                f'but stream() was called.'
            )
        interaction = self._client.next_interaction()

        async def _aiter():
            if not isinstance(interaction, StreamInteraction):  # pragma: no cover
                raise RuntimeError(f'Expected StreamInteraction, got {type(interaction).__name__}')

            # Reconstruct the aggregated response by applying each chunk, mirroring the SDK behavior.
            aggregated = chat_types.Response(chat_pb2.GetChatCompletionResponse(), index=None)
            for chunk_bytes in interaction.chunks_raw:
                chunk_proto = chat_pb2.GetChatCompletionChunk()
                chunk_proto.ParseFromString(chunk_bytes)
                aggregated.process_chunk(chunk_proto)
                yield aggregated, chat_types.Chunk(chunk_proto, index=None)

        return _aiter()

    def chat(self) -> Any:
        # We don't need to validate kwargs yet, but we keep the signature compatible.
        return type('Chat', (), {'create': self._chat_create})

    def _chat_create(self, *_args: Any, **_kwargs: Any) -> _CassetteChatInstance:
        expected_type = self.peek_interaction_type()
        return _CassetteChatInstance(self, expected_type)

    def _peek_interaction(self) -> Interaction | None:
        """Peek at the next interaction without consuming it."""
        if self.interaction_idx < len(self.cassette.interactions):
            return self.cassette.interactions[self.interaction_idx]
        return None

    def _chat_create(self, *args: Any, **kwargs: Any) -> Any:
        inner_chat = self._inner.chat.create(*args, **kwargs)
        include_debug_json = self.include_debug_json
        client = self

        class _HybridChatInstance:
            async def sample(self) -> chat_types.Response:
                # Replay if we have a recorded SampleInteraction at this index.
                peeked = client._peek_interaction()
                if isinstance(peeked, SampleInteraction):
                    interaction = client._consume_interaction()
                    assert isinstance(interaction, SampleInteraction)
                    proto = chat_pb2.GetChatCompletionResponse()
                    proto.ParseFromString(interaction.response_raw)
                    return chat_types.Response(proto, index=None)

                # Otherwise record a new episode.
                request_raw = inner_chat.proto.SerializeToString()
                request_json: dict[str, Any] | None = None
                if include_debug_json:
                    request_json = MessageToDict(inner_chat.proto, preserving_proto_field_name=True)

                response = await inner_chat.sample()
                response_raw = response.proto.SerializeToString()

                response_json: dict[str, Any] | None = None
                if include_debug_json:
                    response_json = MessageToDict(response.proto, preserving_proto_field_name=True)

                client.cassette.interactions.append(
                    SampleInteraction(
                        request_raw=request_raw,
                        response_raw=response_raw,
                        request_json=request_json,
                        response_json=response_json,
                    )
                )
                client.interaction_idx += 1
                client.dirty = True
                return response

            def stream(self) -> Any:
                async def _aiter():
                    # Replay if we have a recorded StreamInteraction at this index.
                    peeked = client._peek_interaction()
                    if isinstance(peeked, StreamInteraction):
                        interaction = client._consume_interaction()
                        assert isinstance(interaction, StreamInteraction)

                        aggregated = chat_types.Response(chat_pb2.GetChatCompletionResponse(), index=None)
                        for chunk_bytes in interaction.chunks_raw:
                            chunk_proto = chat_pb2.GetChatCompletionChunk()
                            chunk_proto.ParseFromString(chunk_bytes)
                            aggregated.process_chunk(chunk_proto)
                            yield aggregated, chat_types.Chunk(chunk_proto, index=None)
                        return

                    # Otherwise record a new streaming episode.
                    request_raw = inner_chat.proto.SerializeToString()
                    request_json: dict[str, Any] | None = None
                    if include_debug_json:
                        request_json = MessageToDict(inner_chat.proto, preserving_proto_field_name=True)

                    chunks_raw: list[bytes] = []
                    chunks_json: list[dict[str, Any]] = []
                    try:
                        async for response, chunk in inner_chat.stream():
                            chunks_raw.append(chunk.proto.SerializeToString())
                            if include_debug_json:
                                chunks_json.append(
                                    {
                                        'chunk': MessageToDict(
                                            chunk.proto,
                                            preserving_proto_field_name=True,
                                        )
                                    }
                                )
                            yield response, chunk
                    finally:
                        client.cassette.interactions.append(
                            StreamInteraction(
                                request_raw=request_raw,
                                chunks_raw=chunks_raw,
                                request_json=request_json,
                                chunks_json=chunks_json if include_debug_json else None,
                            )
                        )
                        client.interaction_idx += 1
                        client.dirty = True

                return _aiter()

        return _HybridChatInstance()
