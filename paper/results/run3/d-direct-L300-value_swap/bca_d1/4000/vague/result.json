{
  "task_id": "d-direct-L300-value_swap",
  "method": "bca_d1",
  "budget": 4000,
  "query_type": "vague",
  "tokens_used": 3966,
  "symbols_selected": 29,
  "files_included": 17,
  "assembly_time_ms": 196.8,
  "llm_time_ms": 6052.4,
  "llm_input_tokens": 4053,
  "llm_output_tokens": 253,
  "tests_passed": false,
  "test_output": "op scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n/Users/akshankrithick/anaconda3/envs/vvenv/lib/python3.11/asyncio/base_events.py:693: ResourceWarning: unclosed event loop <_UnixSelectorEventLoop running=False closed=False debug=False>",
  "patch": "FILE: examples/pydantic_ai_examples/evals/custom_evaluators.py\nSEARCH:\n    def evaluate(\n        self, ctx: EvaluatorContext[TimeRangeInputs, TimeRangeResponse]\n    ) -> EvaluatorOutput:\n        if isinstance(ctx.output, TimeRangeBuilderSuccess):\n            window_end = ctx.output.max_timestamp_with_offset\n            window_size = window_end - ctx.output.min_timestamp_with_offset\n            return {\n                'window_is_not_too_long': window_size <= timedelta(days=30),\n                'window_is_not_in_the_future': window_end <= ctx.inputs['now'],\n            }\n\n        return {}  # No evaluation needed for errors\nREPLACE:\n    def evaluate(\n        self, ctx: EvaluatorContext[TimeRangeInputs, TimeRangeResponse]\n    ) -> EvaluatorOutput:\n        if isinstance(ctx.output, TimeRangeBuilderSuccess):\n            window_end = ctx.output.min_timestamp_with_offset\n            window_size = ctx.output.max_timestamp_with_offset - window_end\n            return {\n                'window_is_not_too_long': window_size <= timedelta(days=30),\n                'window_is_not_in_the_future': window_end <= ctx.inputs['now'],\n            }\n\n        return {}  # No evaluation needed for errors\n",
  "error": "",
  "test_time_ms": 1420.4,
  "failure_mode": "test_fail",
  "target_file_hit": false,
  "target_symbol_hit": false,
  "context_patch_overlap": 0.059,
  "patch_files_changed": 1,
  "patch_lines_changed": 12,
  "edit_distance_lines": -1,
  "entity_count_extracted": 0,
  "entity_count_mapped": 0,
  "query_identifier_density": 0.0,
  "seed_symbol_keys": [],
  "mutation_symbol_key": "pydantic_ai_slim/pydantic_ai/direct.py::StreamedResponseSync.__enter__",
  "min_hops_seed_to_mutation": -1,
  "median_hops_seed_to_mutation": -1.0,
  "bca_closure_added_symbols": 7,
  "bca_closure_added_tokens": 574,
  "bca_frontier_visited": 28,
  "context_symbol_keys": [
    "examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py::Step",
    "examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py::JSONPatchOp.value",
    "examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py::agent",
    "examples/pydantic_ai_examples/evals/custom_evaluators.py::ValidateTimeRange",
    "examples/pydantic_ai_examples/evals/custom_evaluators.py::ValidateTimeRange.evaluate",
    "examples/pydantic_ai_examples/evals/custom_evaluators.py::UserMessageIsConcise.evaluate",
    "examples/pydantic_ai_examples/evals/custom_evaluators.py::AgentCalledTool.evaluate",
    "examples/pydantic_ai_examples/question_graph.py::ask_agent",
    "examples/pydantic_ai_examples/question_graph.py::QuestionState",
    "examples/pydantic_ai_examples/question_graph.py::Answer",
    "examples/pydantic_ai_examples/question_graph.py::question_graph",
    "examples/pydantic_ai_examples/slack_lead_qualifier/store.py::AnalysisStore",
    "examples/pydantic_ai_examples/slack_lead_qualifier/store.py::AnalysisStore.list",
    "examples/pydantic_ai_examples/slack_lead_qualifier/store.py::AnalysisStore.clear",
    "examples/pydantic_ai_examples/slack_lead_qualifier/store.py::AnalysisStore._get_store",
    "pydantic_ai_slim/pydantic_ai/usage.py::UsageBase.has_values",
    "pydantic_ai_slim/pydantic_ai/usage.py::Usage",
    "pydantic_evals/pydantic_evals/_utils.py::_logfire",
    "pydantic_evals/pydantic_evals/_utils.py::Unset",
    "pydantic_evals/pydantic_evals/otel/span_tree.py::SpanQuery",
    "pydantic_evals/pydantic_evals/otel/span_tree.py::SpanNode",
    "pydantic_evals/pydantic_evals/otel/span_tree.py::SpanPredicate",
    "pydantic_evals/pydantic_evals/otel/span_tree.py::SpanTree",
    "pydantic_graph/pydantic_graph/beta/graph.py::OutputT",
    "pydantic_graph/pydantic_graph/beta/graph.py::EndMarker.value",
    "pydantic_graph/pydantic_graph/beta/graph.py::Graph",
    "pydantic_graph/pydantic_graph/beta/graph.py::GraphRun",
    "tests/conftest.py::allow_model_requests",
    "tests/conftest.py::model",
    "tests/evals/test_dataset.py::TaskInput",
    "tests/evals/test_dataset.py::TaskOutput",
    "tests/evals/test_dataset.py::TaskMetadata",
    "tests/evals/test_evaluators.py::TaskInput",
    "tests/evals/test_evaluators.py::TaskOutput",
    "tests/evals/test_evaluators.py::TaskMetadata",
    "tests/evals/test_evaluators.py::test_evaluator_with_null_values",
    "tests/graph/beta/test_edge_cases.py::EdgeCaseState",
    "tests/graph/beta/test_edge_cases.py::EdgeCaseState.value",
    "tests/graph/beta/test_edge_cases.py::test_step_with_zero_value",
    "tests/models/test_google.py::test_http_video_url_uses_file_uri_on_google_vertex",
    "tests/models/test_openai.py::test_openai_auto_mode_reasoning_field_different_provider_uses_tags",
    "tests/models/test_openai.py::test_openai_auto_mode_no_thinking_field_uses_default_fields",
    "tests/models/test_openai.py::test_openai_auto_mode_mismatched_field_uses_tags",
    "tests/test_logfire.py::test_logfire",
    "tests/test_logfire.py::_test_logfire_metadata_values_callable_dict",
    "tests/test_logfire.py::test_logfire_metadata_values",
    "tests/test_mcp.py::mcp_server",
    "tests/test_mcp.py::model",
    "tests/test_mcp.py::agent",
    "tests/test_mcp.py::test_load_mcp_servers",
    "tests/test_mcp.py::test_load_mcp_servers_with_non_string_values",
    "tests/test_mcp.py::test_load_mcp_servers_with_default_values",
    "tests/test_mcp.py::test_load_mcp_servers_with_default_values_in_url",
    "tests/test_vercel_ai.py::TestDumpProviderMetadata.test_dump_provider_metadata_filters_none_values"
  ],
  "mutation_symbol_lines": 4,
  "mutation_symbol_kind": "method",
  "mutation_file_symbols": 48,
  "graph_node_count": 18010,
  "retrieval_top1_top2_gap": 0.046,
  "retrieval_softmax_entropy": 4.3103,
  "retrieval_softmax_tau": 0.839,
  "retrieval_effective_candidates": 19.84,
  "retrieval_top5_ratio": 0.9528,
  "retrieval_within95_count": 2,
  "retrieval_scored_symbols": 29,
  "retrieval_top1_score": 1.004,
  "retrieval_top5_mean_score": 0.9566,
  "retrieval_budget_utilization": 0.9915,
  "retrieval_file_concentration": 0.0,
  "repo_name": "pydantic-ai",
  "category": "direct_api",
  "mutation_type": "value_swap",
  "source": "discovered"
}