# pydantic_ai_slim/pydantic_ai/usage.py:99-101
    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

# tests/test_temporal.py:2579-2579
    values: list[int] = field(default_factory=list[int])

# tests/graph/beta/test_broadcast_and_spread.py:17-17
    values: list[int] = field(default_factory=list[int])

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:14-14
from typing_inspection.introspection import get_literal_values

# tests/graph/beta/test_edge_cases.py:60-80
async def test_step_with_zero_value():
    """Test handling of zero values (ensure they're not confused with None/falsy)."""
    g = GraphBuilder(state_type=EdgeCaseState, output_type=int)

    @g.step
    async def return_zero(ctx: StepContext[EdgeCaseState, None, None]) -> int:
        return 0

    @g.step
    async def process_zero(ctx: StepContext[EdgeCaseState, None, int]) -> int:
        return ctx.inputs + 1

    g.add(
        g.edge_from(g.start_node).to(return_zero),
        g.edge_from(return_zero).to(process_zero),
        g.edge_from(process_zero).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=EdgeCaseState())
    assert result == 1

# tests/evals/test_multi_run.py:140-148
async def test_repeat_invalid_value():
    """repeat < 1 should raise ValueError."""

    async def task(inputs: str) -> str:
        return inputs  # pragma: no cover

    dataset = Dataset(cases=[Case(inputs='hello')])
    with pytest.raises(ValueError, match='repeat must be >= 1'):
        await dataset.evaluate(task, name='test', progress=False, repeat=0)

# tests/test_logfire.py:594-603
def test_logfire_metadata_values(
    get_logfire_summary: Callable[[], LogfireSummary],
    metadata: dict[str, Any] | Callable[[RunContext[Any]], dict[str, Any]],
    expected: dict[str, Any],
) -> None:
    agent = Agent(model=TestModel(), instrument=InstrumentationSettings(version=2), metadata=metadata)
    agent.run_sync('Hello')

    summary = get_logfire_summary()
    assert summary.attributes[0]['metadata'] == expected

# tests/test_validation_context.py:22-27
class Value(BaseModel):
    x: int

    @field_validator('x')
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

# tests/conftest.py:136-138
def _(value: datetime):  # pragma: no cover
    """Use IsDatetime() for datetime values in snapshots."""
    return 'IsDatetime()'

# pydantic_ai_slim/pydantic_ai/_utils.py:143-143
    value: T

# pydantic_ai_slim/pydantic_ai/_utils.py:143-143
    value: T

# tests/test_validation_context.py:26-27
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

# pydantic_ai_slim/pydantic_ai/messages.py:21-21
from opentelemetry.util.types import AnyValue

# pydantic_evals/pydantic_evals/evaluators/common.py:32-32
    value: Any

# pydantic_evals/pydantic_evals/reporting/__init__.py:707-711
    def render_value(self, name: str | None, v: Any) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:10-10
from pydantic import JsonValue

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:15-15
JSONValue = Any

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:15-15
JSONValue = Any

# tests/graph/beta/test_graph_edge_cases.py:20-20
    value: int = 0

# tests/graph/beta/test_graph_edge_cases.py:20-20
    value: int = 0

# pydantic_evals/pydantic_evals/evaluators/common.py:73-73
    value: Any

# tests/test_ag_ui.py:158-158
    value: int = 0

# pydantic_graph/pydantic_graph/beta/graph.py:86-87
    def value(self) -> OutputT:
        return self._value

# pydantic_evals/pydantic_evals/reporting/__init__.py:736-742
    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# tests/graph/beta/test_edge_labels.py:17-17
    value: int = 0

# tests/test_agent.py:3308-3308
    value: str

# tests/test_prefect.py:1195-1195
    value: str

# tests/test_agent.py:3308-3308
    value: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:693-693
    value_formatter: str | Callable[[Any], str] = '{}'

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:42-45
    value: Any = Field(
        default=None,
        description='The value to apply (for add, replace operations)',
    )

# pydantic_graph/pydantic_graph/beta/graph.py:78-78
    _value: OutputT

# tests/graph/beta/test_joins_and_reducers.py:24-24
    value: int = 0

# pydantic_evals/pydantic_evals/reporting/analyses.py:65-65
    value: float | int

# pydantic_evals/pydantic_evals/reporting/__init__.py:685-685
    value_formatter: str | Callable[[Any], str]

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# tests/graph/beta/test_decisions.py:20-20
    value: int = 0

# tests/graph/beta/test_edge_cases.py:19-19
    value: int = 0

# tests/test_agent.py:114-114
    value: T

# pydantic_evals/pydantic_evals/reporting/__init__.py:682-688
class RenderValueConfig(TypedDict, total=False):
    """A configuration for rendering a values in an Evaluation report."""

    value_formatter: str | Callable[[Any], str]
    diff_checker: Callable[[Any, Any], bool] | None
    diff_formatter: Callable[[Any, Any], str | None] | None
    diff_style: str

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:44-44
    value: EvaluationScalar

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:73-73
    value: EvaluationScalarT

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_evals/pydantic_evals/reporting/__init__.py:692-742
class _ValueRenderer:
    value_formatter: str | Callable[[Any], str] = '{}'
    diff_checker: Callable[[Any, Any], bool] | None = lambda x, y: x != y
    diff_formatter: Callable[[Any, Any], str | None] | None = None
    diff_style: str = 'magenta'

    @staticmethod
    def from_config(config: RenderValueConfig) -> _ValueRenderer:
        return _ValueRenderer(
            value_formatter=config.get('value_formatter', '{}'),
            diff_checker=config.get('diff_checker', lambda x, y: x != y),
            diff_formatter=config.get('diff_formatter'),
            diff_style=config.get('diff_style', 'magenta'),
        )

    def render_value(self, name: str | None, v: Any) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

    def render_diff(self, name: str | None, old: Any | None, new: Any | None) -> str:
        old_str = self._get_value_str(old) or MISSING_VALUE_STR
        new_str = self._get_value_str(new) or MISSING_VALUE_STR
        if old_str == new_str:
            result = old_str
        else:
            result = f'{old_str} â†’ {new_str}'

            has_diff = self.diff_checker and self.diff_checker(old, new)
            if has_diff:  # pragma: no branch
                # If there is a diff, make the name bold and compute the diff_str
                name = name and f'[bold]{name}[/]'
                diff_str = self.diff_formatter and self.diff_formatter(old, new)
                if diff_str:  # pragma: no cover
                    result += f' ({diff_str})'
                result = f'[{self.diff_style}]{result}[/]'

        # Add the name
        if name:
            result = f'{name}: {result}'

        return result

    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:14-14
VALUE_SIG_FIGS = 3  # Significant figures for the default number formatting.

# pydantic_ai_slim/pydantic_ai/models/test.py:45-45
    value: str | None

# pydantic_ai_slim/pydantic_ai/models/test.py:52-52
    value: dict[str, Any] | None

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# tests/test_logfire.py:582-583
def _test_logfire_metadata_values_callable_dict(ctx: RunContext[Any]) -> dict[str, str]:
    return {'model_name': ctx.model.model_name}

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_evals/pydantic_evals/reporting/__init__.py:59-59
MISSING_VALUE_STR = '[i]<missing>[/i]'

# pydantic_ai_slim/pydantic_ai/messages.py:690-690
    return_value: ToolReturnContent

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:9-9
from weakref import WeakValueDictionary