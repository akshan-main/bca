# tests/test_concurrency.py:338-344
    async def test_with_int_limiter(self):
        """Test ConcurrencyLimitedModel with int limiter."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        model = ConcurrencyLimitedModel(TestModel(), limiter=5)
        assert model._limiter.max_running == 5
        assert model._limiter._max_queued is None

# tests/test_concurrency.py:176-180
    async def test_from_int_limit(self):
        """Test creating from simple int."""
        limiter = ConcurrencyLimiter.from_limit(5)
        assert limiter.max_running == 5
        assert limiter._max_queued is None

# tests/models/test_bedrock.py:2690-2729
def test_limit_cache_points_filters_excess_cache_points(bedrock_provider: BedrockProvider):
    """Test that _limit_cache_points filters out excess cache points beyond the limit of 4."""
    model = BedrockConverseModel('us.anthropic.claude-3-5-sonnet-20240620-v1:0', provider=bedrock_provider)

    # Create system prompt (no cache points)
    system_prompt: list[SystemContentBlockTypeDef] = [{'text': 'System prompt'}]

    # Create messages with 5 standalone cachePoint blocks (limit is 4)
    bedrock_messages: list[MessageUnionTypeDef] = [
        {
            'role': 'user',
            'content': [
                {'text': 'Context 1'},
                {'cachePoint': {'type': 'default'}},  # Will be filtered (oldest, over limit)
                {'text': 'Context 2'},
                {'cachePoint': {'type': 'default'}},  # Will be kept (4th newest)
                {'text': 'Context 3'},
                {'cachePoint': {'type': 'default'}},  # Will be kept (3rd newest)
                {'text': 'Context 4'},
                {'cachePoint': {'type': 'default'}},  # Will be kept (2nd newest)
                {'text': 'Question'},
                {'cachePoint': {'type': 'default'}},  # Will be kept (newest)
            ],
        },
    ]

    # Apply limit with no tools (max 4 cache points, we have 5)
    model._limit_cache_points(system_prompt, bedrock_messages, [])  # pyright: ignore[reportPrivateUsage]

    # Verify only 4 cache points remain (the newest ones)
    content = bedrock_messages[0]['content']
    assert isinstance(content, list)

    # Count remaining cache points
    cache_points = [b for b in content if isinstance(b, dict) and 'cachePoint' in b]
    assert len(cache_points) == 4  # Only 4 kept (the limit)

    # Verify no empty blocks exist
    empty_blocks = [b for b in content if isinstance(b, dict) and not b]
    assert len(empty_blocks) == 0

# tests/models/test_anthropic.py:1030-1078
async def test_limit_cache_points_all_settings(allow_model_requests: None):
    """Test cache point limiting with all cache settings enabled."""
    c = completion_message(
        [BetaTextBlock(text='Response', type='text')],
        usage=BetaUsage(input_tokens=10, output_tokens=5),
    )
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))

    agent = Agent(
        m,
        system_prompt='System instructions.',
        model_settings=AnthropicModelSettings(
            anthropic_cache_instructions=True,  # 1 cache point
            anthropic_cache_tool_definitions=True,  # 1 cache point
        ),
    )

    @agent.tool_plain
    def my_tool() -> str:  # pragma: no cover
        return 'result'

    # Add 3 CachePoint markers (total would be 5: 2 from settings + 3 from markers)
    # Only 2 CachePoint markers should be kept
    await agent.run(
        [
            'Context 1',
            CachePoint(),  # Oldest, should be removed
            'Context 2',
            CachePoint(),  # Should be kept
            'Context 3',
            CachePoint(),  # Should be kept
            'Question',
        ]
    )

    completion_kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    messages = completion_kwargs['messages']

    # Count cache_control in messages (excluding system and tools)
    cache_count = 0
    for msg in messages:
        for block in msg['content']:
            if 'cache_control' in block:
                cache_count += 1

    # Should have exactly 2 cache points in messages
    # (4 total - 1 system - 1 tool = 2 available for messages)
    assert cache_count == 2

# tests/models/test_anthropic.py:1030-1078
async def test_limit_cache_points_all_settings(allow_model_requests: None):
    """Test cache point limiting with all cache settings enabled."""
    c = completion_message(
        [BetaTextBlock(text='Response', type='text')],
        usage=BetaUsage(input_tokens=10, output_tokens=5),
    )
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))

    agent = Agent(
        m,
        system_prompt='System instructions.',
        model_settings=AnthropicModelSettings(
            anthropic_cache_instructions=True,  # 1 cache point
            anthropic_cache_tool_definitions=True,  # 1 cache point
        ),
    )

    @agent.tool_plain
    def my_tool() -> str:  # pragma: no cover
        return 'result'

    # Add 3 CachePoint markers (total would be 5: 2 from settings + 3 from markers)
    # Only 2 CachePoint markers should be kept
    await agent.run(
        [
            'Context 1',
            CachePoint(),  # Oldest, should be removed
            'Context 2',
            CachePoint(),  # Should be kept
            'Context 3',
            CachePoint(),  # Should be kept
            'Question',
        ]
    )

    completion_kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    messages = completion_kwargs['messages']

    # Count cache_control in messages (excluding system and tools)
    cache_count = 0
    for msg in messages:
        for block in msg['content']:
            if 'cache_control' in block:
                cache_count += 1

    # Should have exactly 2 cache points in messages
    # (4 total - 1 system - 1 tool = 2 available for messages)
    assert cache_count == 2

# tests/test_concurrency.py:41-204
class TestConcurrencyLimiter:
    """Tests for the ConcurrencyLimiter class."""

    async def test_basic_acquisition(self):
        """Test that limiter limits concurrent access."""
        limiter = ConcurrencyLimiter(max_running=2)
        acquired: list[int] = []

        async def acquire_and_hold(id: int, hold_time: float):
            async with get_concurrency_context(limiter, 'test'):
                acquired.append(id)
                await anyio.sleep(hold_time)

        # Start 3 tasks with limit of 2
        async with anyio.create_task_group() as tg:
            for i in range(3):
                tg.start_soon(acquire_and_hold, i, 0.1)
            await anyio.sleep(0.05)
            assert len(acquired) == 2  # Only 2 can proceed
        assert len(acquired) == 3

    async def test_nowait_acquisition(self):
        """Test that immediate acquisition works."""
        limiter = ConcurrencyLimiter(max_running=10)
        # With high limit, should acquire immediately
        async with get_concurrency_context(limiter, 'test'):
            pass  # No waiting

    async def test_waiting_count_tracking(self):
        """Test that waiting_count is accurately tracked."""
        limiter = ConcurrencyLimiter(max_running=1)
        started = anyio.Event()
        release = anyio.Event()

        async def holder():
            async with get_concurrency_context(limiter, 'test'):
                started.set()
                await release.wait()

        async def waiter():
            async with get_concurrency_context(limiter, 'test'):
                pass

        async with anyio.create_task_group() as tg:
            tg.start_soon(holder)
            await started.wait()

            # Now limiter is held, check waiting count as we add waiters
            assert limiter.waiting_count == 0

            for _ in range(3):
                tg.start_soon(waiter)
            await anyio.sleep(0.01)
            assert limiter.waiting_count == 3

            release.set()
        assert limiter.waiting_count == 0

    async def test_backpressure_raises(self):
        """Test that exceeding max_queued raises ConcurrencyLimitExceeded."""
        limiter = ConcurrencyLimiter(max_running=1, max_queued=2)
        hold = anyio.Event()

        async def holder():
            async with get_concurrency_context(limiter, 'test'):
                await hold.wait()

        async with anyio.create_task_group() as tg:
            # Fill the running slot
            tg.start_soon(holder)
            await anyio.sleep(0.01)

            # Fill the queue (2 allowed)
            tg.start_soon(holder)
            tg.start_soon(holder)
            await anyio.sleep(0.01)

            # This should raise - queue is full
            with pytest.raises(ConcurrencyLimitExceeded):
                async with get_concurrency_context(limiter, 'test'):
                    pass

            hold.set()

    async def test_backpressure_race_condition(self):
        """Test that max_queued is enforced atomically under concurrent load.

        This test verifies the fix for a race condition where multiple tasks could
        simultaneously pass the max_queued check before any of them actually started
        waiting on the limiter.
        """
        limiter = ConcurrencyLimiter(max_running=1, max_queued=1)
        hold = anyio.Event()
        started = anyio.Event()

        async def holder():
            async with get_concurrency_context(limiter, 'holder'):
                started.set()
                await hold.wait()

        # Now launch multiple tasks simultaneously that all try to queue.
        # With max_queued=1, exactly one should succeed in queuing.
        num_concurrent = 5
        results: list[str] = []
        barrier = AsyncBarrier(num_concurrent)

        async def try_acquire(idx: int):
            # Use barrier to ensure all tasks try to acquire at the same time
            await barrier.wait()
            try:
                async with get_concurrency_context(limiter, f'task-{idx}'):
                    results.append(f'acquired-{idx}')
            except ConcurrencyLimitExceeded:
                results.append(f'rejected-{idx}')

        async with anyio.create_task_group() as tg:
            # Fill the running slot and wait for it to be held
            tg.start_soon(holder)
            await started.wait()

            # Launch all tasks simultaneously
            for i in range(num_concurrent):
                tg.start_soon(try_acquire, i)
            await anyio.sleep(0.1)  # Give tasks time to hit the barrier and try to acquire

            # Release the holder
            hold.set()

        # Verify: exactly one task should have been allowed to queue and acquire
        # The rest should have been rejected
        acquired = [r for r in results if r.startswith('acquired-')]
        rejected = [r for r in results if r.startswith('rejected-')]
        assert len(acquired) == 1, f'Expected exactly 1 acquired, got {len(acquired)}: {acquired}'
        assert len(rejected) == num_concurrent - 1, f'Expected {num_concurrent - 1} rejected, got {len(rejected)}'

    async def test_from_int_limit(self):
        """Test creating from simple int."""
        limiter = ConcurrencyLimiter.from_limit(5)
        assert limiter.max_running == 5
        assert limiter._max_queued is None

    async def test_from_limiter_config(self):
        """Test creating from ConcurrencyLimit."""
        config = ConcurrencyLimit(max_running=5, max_queued=10)
        limiter = ConcurrencyLimiter.from_limit(config)
        assert limiter.max_running == 5
        assert limiter._max_queued == 10

    async def test_properties(self):
        """Test the various properties of ConcurrencyLimiter."""
        limiter = ConcurrencyLimiter(max_running=5, name='test-limiter')
        assert limiter.max_running == 5
        assert limiter.running_count == 0
        assert limiter.available_count == 5
        assert limiter.waiting_count == 0
        assert limiter.name == 'test-limiter'

        # After acquiring one slot
        await limiter.acquire('test')
        assert limiter.running_count == 1
        assert limiter.available_count == 4
        limiter.release()
        assert limiter.running_count == 0
        assert limiter.available_count == 5

# tests/models/test_anthropic.py:981-1027
async def test_limit_cache_points_with_cache_messages(allow_model_requests: None):
    """Test that cache points are limited when using cache_messages + CachePoint markers."""
    c = completion_message(
        [BetaTextBlock(text='Response', type='text')],
        usage=BetaUsage(input_tokens=10, output_tokens=5),
    )
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(
        m,
        system_prompt='System instructions.',
        model_settings=AnthropicModelSettings(
            anthropic_cache_messages=True,  # Uses 1 cache point
        ),
    )

    # Add 4 CachePoint markers (total would be 5: 1 from cache_messages + 4 from markers)
    # Only 3 CachePoint markers should be kept (newest ones)
    await agent.run(
        [
            'Context 1',
            CachePoint(),  # Oldest, should be removed
            'Context 2',
            CachePoint(),  # Should be kept
            'Context 3',
            CachePoint(),  # Should be kept
            'Context 4',
            CachePoint(),  # Should be kept
            'Question',
        ]
    )

    completion_kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    messages = completion_kwargs['messages']

    # Count cache_control occurrences in messages
    cache_count = 0
    for msg in messages:
        for block in msg['content']:
            if 'cache_control' in block:
                cache_count += 1

    # anthropic_cache_messages uses 1 cache point (last message only)
    # With 4 CachePoint markers, we'd have 5 total
    # Limit is 4, so 1 oldest CachePoint should be removed
    # Result: 3 cache points from CachePoint markers + 1 from cache_messages = 4 total
    assert cache_count == 4

# tests/models/test_anthropic.py:981-1027
async def test_limit_cache_points_with_cache_messages(allow_model_requests: None):
    """Test that cache points are limited when using cache_messages + CachePoint markers."""
    c = completion_message(
        [BetaTextBlock(text='Response', type='text')],
        usage=BetaUsage(input_tokens=10, output_tokens=5),
    )
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(
        m,
        system_prompt='System instructions.',
        model_settings=AnthropicModelSettings(
            anthropic_cache_messages=True,  # Uses 1 cache point
        ),
    )

    # Add 4 CachePoint markers (total would be 5: 1 from cache_messages + 4 from markers)
    # Only 3 CachePoint markers should be kept (newest ones)
    await agent.run(
        [
            'Context 1',
            CachePoint(),  # Oldest, should be removed
            'Context 2',
            CachePoint(),  # Should be kept
            'Context 3',
            CachePoint(),  # Should be kept
            'Context 4',
            CachePoint(),  # Should be kept
            'Question',
        ]
    )

    completion_kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    messages = completion_kwargs['messages']

    # Count cache_control occurrences in messages
    cache_count = 0
    for msg in messages:
        for block in msg['content']:
            if 'cache_control' in block:
                cache_count += 1

    # anthropic_cache_messages uses 1 cache point (last message only)
    # With 4 CachePoint markers, we'd have 5 total
    # Limit is 4, so 1 oldest CachePoint should be removed
    # Result: 3 cache points from CachePoint markers + 1 from cache_messages = 4 total
    assert cache_count == 4

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# tests/test_concurrency.py:301-436
class TestConcurrencyLimitedModel:
    """Tests for the ConcurrencyLimitedModel wrapper."""

    async def test_basic_concurrency_limit(self):
        """Test that ConcurrencyLimitedModel limits concurrent requests."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        request_count = 0
        max_concurrent = 0
        lock = anyio.Lock()

        base_model = TestModel()
        original_request = TestModel.request.__get__(base_model)

        async def tracking_request(*args: Any, **kwargs: Any):
            nonlocal request_count, max_concurrent
            async with lock:
                request_count += 1
                max_concurrent = max(max_concurrent, request_count)
            try:
                await anyio.sleep(0.1)  # Simulate slow request
                return await original_request(*args, **kwargs)
            finally:
                async with lock:
                    request_count -= 1

        base_model.request = tracking_request

        model = ConcurrencyLimitedModel(base_model, limiter=2)
        agent = Agent(model)

        async with anyio.create_task_group() as tg:
            for i in range(5):
                tg.start_soon(agent.run, f'prompt {i}')

        assert max_concurrent <= 2

    async def test_with_int_limiter(self):
        """Test ConcurrencyLimitedModel with int limiter."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        model = ConcurrencyLimitedModel(TestModel(), limiter=5)
        assert model._limiter.max_running == 5
        assert model._limiter._max_queued is None

    async def test_with_concurrency_limit(self):
        """Test ConcurrencyLimitedModel with ConcurrencyLimit."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        model = ConcurrencyLimitedModel(TestModel(), limiter=ConcurrencyLimit(max_running=5, max_queued=10))
        assert model._limiter.max_running == 5
        assert model._limiter._max_queued == 10

    async def test_with_shared_limiter(self):
        """Test ConcurrencyLimitedModel with shared ConcurrencyLimiter."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        shared_limiter = ConcurrencyLimiter(max_running=3, name='shared-pool')
        model1 = ConcurrencyLimitedModel(TestModel(), limiter=shared_limiter)
        model2 = ConcurrencyLimitedModel(TestModel(), limiter=shared_limiter)

        # Both models should share the same limiter
        assert model1._limiter is model2._limiter
        assert model1._limiter.name == 'shared-pool'

    async def test_shared_limiter_limits_across_models(self):
        """Test that shared limiter limits concurrent requests across multiple models."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        request_count = 0
        max_concurrent = 0
        lock = anyio.Lock()

        shared_limiter = ConcurrencyLimiter(max_running=2)

        def create_tracking_model():
            base_model = TestModel()
            original_request = TestModel.request.__get__(base_model)

            async def tracking_request(*args: Any, **kwargs: Any):
                nonlocal request_count, max_concurrent
                async with lock:
                    request_count += 1
                    max_concurrent = max(max_concurrent, request_count)
                try:
                    await anyio.sleep(0.1)
                    return await original_request(*args, **kwargs)
                finally:
                    async with lock:
                        request_count -= 1

            base_model.request = tracking_request
            return ConcurrencyLimitedModel(base_model, limiter=shared_limiter)

        model1 = create_tracking_model()
        model2 = create_tracking_model()

        agent1 = Agent(model1)
        agent2 = Agent(model2)

        # Run 3 requests on each agent (6 total), but limit is 2
        async with anyio.create_task_group() as tg:
            for i in range(3):
                tg.start_soon(agent1.run, f'prompt {i}')
            for i in range(3):
                tg.start_soon(agent2.run, f'prompt {i}')

        # Should never exceed 2 concurrent requests across both models
        assert max_concurrent <= 2

    async def test_limit_model_concurrency_helper(self):
        """Test the limit_model_concurrency helper function."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel, limit_model_concurrency

        # With limiter
        model = limit_model_concurrency(TestModel(), limiter=5)
        assert isinstance(model, ConcurrencyLimitedModel)

        # Without limiter (returns original)
        base_model = TestModel()
        model = limit_model_concurrency(base_model, limiter=None)
        assert model is base_model

        # With model name string
        model = limit_model_concurrency('test', limiter=5)
        assert isinstance(model, ConcurrencyLimitedModel)

    async def test_model_properties_delegated(self):
        """Test that model properties are properly delegated to wrapped model."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        base_model = TestModel(model_name='custom-test')
        model = ConcurrencyLimitedModel(base_model, limiter=5)

        assert model.model_name == 'custom-test'
        assert model.system == 'test'

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# tests/test_concurrency.py:346-352
    async def test_with_concurrency_limit(self):
        """Test ConcurrencyLimitedModel with ConcurrencyLimit."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        model = ConcurrencyLimitedModel(TestModel(), limiter=ConcurrencyLimit(max_running=5, max_queued=10))
        assert model._limiter.max_running == 5
        assert model._limiter._max_queued == 10

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/concurrency.py:276-295
def normalize_to_limiter(
    limit: AnyConcurrencyLimit,
    *,
    name: str | None = None,
) -> AbstractConcurrencyLimiter | None:
    """Normalize a concurrency limit configuration to an AbstractConcurrencyLimiter.

    Args:
        limit: The concurrency limit configuration.
        name: Optional name for the limiter if one is created.

    Returns:
        An AbstractConcurrencyLimiter if limit is not None, otherwise None.
    """
    if limit is None:
        return None
    elif isinstance(limit, AbstractConcurrencyLimiter):
        return limit
    else:
        return ConcurrencyLimiter.from_limit(limit, name=name)

# tests/test_concurrency.py:411-426
    async def test_limit_model_concurrency_helper(self):
        """Test the limit_model_concurrency helper function."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel, limit_model_concurrency

        # With limiter
        model = limit_model_concurrency(TestModel(), limiter=5)
        assert isinstance(model, ConcurrencyLimitedModel)

        # Without limiter (returns original)
        base_model = TestModel()
        model = limit_model_concurrency(base_model, limiter=None)
        assert model is base_model

        # With model name string
        model = limit_model_concurrency('test', limiter=5)
        assert isinstance(model, ConcurrencyLimitedModel)

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:56-56
    _limiter: AbstractConcurrencyLimiter

# pydantic_ai_slim/pydantic_ai/concurrency.py:113-138
    def from_limit(
        cls,
        limit: int | ConcurrencyLimit,
        *,
        name: str | None = None,
        tracer: Tracer | None = None,
    ) -> Self:
        """Create a ConcurrencyLimiter from a ConcurrencyLimit configuration.

        Args:
            limit: Either an int for simple limiting or a ConcurrencyLimit for full config.
            name: Optional name for this limiter, used for observability.
            tracer: OpenTelemetry tracer for span creation.

        Returns:
            A configured ConcurrencyLimiter.
        """
        if not isinstance(limit, int):
            return cls(max_running=limit, name=name, tracer=tracer)
        else:
            return cls(
                max_running=limit.max_running,
                max_queued=limit.max_queued,
                name=name,
                tracer=tracer,
            )

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:58-76
    def __init__(
        self,
        wrapped: Model | KnownModelName,
        limiter: int | ConcurrencyLimit | AbstractConcurrencyLimiter,
    ):
        """Initialize the ConcurrencyLimitedModel.

        Args:
            wrapped: The model to wrap, either a Model instance or a known model name.
            limiter: The concurrency limit configuration. Can be:
                - An `int`: Simple limit on concurrent operations (unlimited queue).
                - A `ConcurrencyLimit`: Full configuration with optional backpressure.
                - An `AbstractConcurrencyLimiter`: A pre-created limiter for sharing across models.
        """
        super().__init__(wrapped)
        if isinstance(limiter, AbstractConcurrencyLimiter):
            self._limiter = limiter
        else:
            self._limiter = ConcurrencyLimiter.from_limit(limiter)

# pydantic_ai_slim/pydantic_ai/__init__.py:103-103
from .models.concurrency import ConcurrencyLimitedModel, limit_model_concurrency

# pydantic_ai_slim/pydantic_ai/__init__.py:103-103
from .models.concurrency import ConcurrencyLimitedModel, limit_model_concurrency

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:171-171
    _concurrency_limiter: _concurrency.AbstractConcurrencyLimiter | None = dataclasses.field(repr=False)

# pydantic_ai_slim/pydantic_ai/__init__.py:35-48
from .exceptions import (
    AgentRunError,
    ApprovalRequired,
    CallDeferred,
    ConcurrencyLimitExceeded,
    FallbackExceptionGroup,
    IncompleteToolCall,
    ModelAPIError,
    ModelHTTPError,
    ModelRetry,
    UnexpectedModelBehavior,
    UsageLimitExceeded,
    UserError,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:35-48
from .exceptions import (
    AgentRunError,
    ApprovalRequired,
    CallDeferred,
    ConcurrencyLimitExceeded,
    FallbackExceptionGroup,
    IncompleteToolCall,
    ModelAPIError,
    ModelHTTPError,
    ModelRetry,
    UnexpectedModelBehavior,
    UsageLimitExceeded,
    UserError,
)

# tests/test_concurrency.py:304-336
    async def test_basic_concurrency_limit(self):
        """Test that ConcurrencyLimitedModel limits concurrent requests."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        request_count = 0
        max_concurrent = 0
        lock = anyio.Lock()

        base_model = TestModel()
        original_request = TestModel.request.__get__(base_model)

        async def tracking_request(*args: Any, **kwargs: Any):
            nonlocal request_count, max_concurrent
            async with lock:
                request_count += 1
                max_concurrent = max(max_concurrent, request_count)
            try:
                await anyio.sleep(0.1)  # Simulate slow request
                return await original_request(*args, **kwargs)
            finally:
                async with lock:
                    request_count -= 1

        base_model.request = tracking_request

        model = ConcurrencyLimitedModel(base_model, limiter=2)
        agent = Agent(model)

        async with anyio.create_task_group() as tg:
            for i in range(5):
                tg.start_soon(agent.run, f'prompt {i}')

        assert max_concurrent <= 2

# pydantic_ai_slim/pydantic_ai/concurrency.py:141-143
    def name(self) -> str | None:
        """Name of the limiter for observability."""
        return self._name

# tests/test_concurrency.py:354-364
    async def test_with_shared_limiter(self):
        """Test ConcurrencyLimitedModel with shared ConcurrencyLimiter."""
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel

        shared_limiter = ConcurrencyLimiter(max_running=3, name='shared-pool')
        model1 = ConcurrencyLimitedModel(TestModel(), limiter=shared_limiter)
        model2 = ConcurrencyLimitedModel(TestModel(), limiter=shared_limiter)

        # Both models should share the same limiter
        assert model1._limiter is model2._limiter
        assert model1._limiter.name == 'shared-pool'

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:23-28
from .concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:103-103
from .models.concurrency import ConcurrencyLimitedModel, limit_model_concurrency

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:78-86
    async def request(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> ModelResponse:
        """Make a request to the model with concurrency limiting."""
        async with get_concurrency_context(self._limiter, f'model:{self.model_name}'):
            return await self.wrapped.request(messages, model_settings, model_request_parameters)

# tests/test_concurrency.py:182-187
    async def test_from_limiter_config(self):
        """Test creating from ConcurrencyLimit."""
        config = ConcurrencyLimit(max_running=5, max_queued=10)
        limiter = ConcurrencyLimiter.from_limit(config)
        assert limiter.max_running == 5
        assert limiter._max_queued == 10

# pydantic_ai_slim/pydantic_ai/__init__.py:35-48
from .exceptions import (
    AgentRunError,
    ApprovalRequired,
    CallDeferred,
    ConcurrencyLimitExceeded,
    FallbackExceptionGroup,
    IncompleteToolCall,
    ModelAPIError,
    ModelHTTPError,
    ModelRetry,
    UnexpectedModelBehavior,
    UsageLimitExceeded,
    UserError,
)

# tests/models/test_instrumented.py:47-47
from ..conftest import IsDatetime, IsInt, IsStr, try_import

# tests/models/test_instrumented.py:47-47
from ..conftest import IsDatetime, IsInt, IsStr, try_import

# tests/models/test_instrumented.py:47-47
from ..conftest import IsDatetime, IsInt, IsStr, try_import