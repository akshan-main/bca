# pydantic_ai_slim/pydantic_ai/agent/__init__.py:46-46
from ..settings import ModelSettings, merge_model_settings

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:18-18
from .settings import EmbeddingSettings, merge_embedding_settings

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:46-46
from ..settings import ModelSettings, merge_model_settings

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:46-46
from ..settings import ModelSettings, merge_model_settings

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:46-46
from ..settings import ModelSettings, merge_model_settings

# tests/models/test_model_settings.py:18-27
def test_model_settings_property():
    """Test that the Model base class settings property works correctly."""
    # Test with settings
    settings = ModelSettings(max_tokens=100, temperature=0.5)
    test_model = TestModel(settings=settings)
    assert test_model.settings == settings

    # Test without settings
    test_model_no_settings = TestModel()
    assert test_model_no_settings.settings is None

# tests/test_tools.py:2739-2771
async def test_per_tool_timeout_overrides_agent_timeout():
    """Test that per-tool timeout overrides agent-level timeout."""
    import asyncio

    call_count = 0

    async def model_logic(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            return ModelResponse(parts=[ToolCallPart(tool_name='fast_timeout_tool', args={}, tool_call_id='call-1')])
        return ModelResponse(parts=[TextPart(content='done')])

    # Agent has generous 10s timeout, but per-tool timeout is only 0.1s
    agent = Agent(FunctionModel(model_logic), tool_timeout=10.0)

    @agent.tool_plain(timeout=0.1)  # Per-tool timeout overrides agent timeout
    async def fast_timeout_tool() -> str:
        await asyncio.sleep(1.0)  # 1 second, per-tool timeout is 0.1s
        return 'done'  # pragma: no cover

    result = await agent.run('call fast_timeout_tool')

    # Should timeout because per-tool timeout (0.1s) is applied, not agent timeout (10s)
    retry_parts = [
        part
        for msg in result.all_messages()
        if isinstance(msg, ModelRequest)
        for part in msg.parts
        if isinstance(part, RetryPromptPart) and 'Timed out' in str(part.content)
    ]
    assert len(retry_parts) == 1
    assert 'Timed out after 0.1 seconds' in retry_parts[0].content

# pydantic_ai_slim/pydantic_ai/models/wrapper.py:83-85
    def settings(self) -> ModelSettings | None:
        """Get the settings from the wrapped model."""
        return self.wrapped.settings

# examples/pydantic_ai_examples/bank_support.py:17-35
class DatabaseConn:
    """A wrapper over the SQLite connection."""

    sqlite_conn: sqlite3.Connection

    async def customer_name(self, *, id: int) -> str | None:
        res = cur.execute('SELECT name FROM customers WHERE id=?', (id,))
        row = res.fetchone()
        if row:
            return row[0]
        return None

    async def customer_balance(self, *, id: int) -> float:
        res = cur.execute('SELECT balance FROM customers WHERE id=?', (id,))
        row = res.fetchone()
        if row:
            return row[0]
        else:
            raise ValueError('Customer not found')

# pydantic_ai_slim/pydantic_ai/embeddings/wrapper.py:63-65
    def settings(self) -> EmbeddingSettings | None:
        """Get the settings from the wrapped embedding model."""
        return self.wrapped.settings

# tests/test_agent.py:4461-4471
async def test_model_settings_override() -> None:
    def return_settings(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        return ModelResponse(parts=[TextPart(to_json(info.model_settings).decode())])

    my_agent = Agent(FunctionModel(return_settings))
    assert (await my_agent.run('Hello')).output == IsJson(None)
    assert (await my_agent.run('Hello', model_settings={'temperature': 0.5})).output == IsJson({'temperature': 0.5})

    my_agent = Agent(FunctionModel(return_settings), model_settings={'temperature': 0.1})
    assert (await my_agent.run('Hello')).output == IsJson({'temperature': 0.1})
    assert (await my_agent.run('Hello', model_settings={'temperature': 0.5})).output == IsJson({'temperature': 0.5})

# pydantic_ai_slim/pydantic_ai/settings.py:31-31
    temperature: float

# tests/test_embeddings.py:838-856
    async def test_cohere_v4_truncate_priority(self, bedrock_provider: BedrockProvider):
        """Test that bedrock_cohere_truncate takes precedence over base truncate."""

        model = BedrockEmbeddingModel('cohere.embed-v4:0', provider=bedrock_provider)
        # Both settings provided - model-specific should win (START over END from truncate=True)
        embedder = Embedder(model, settings=BedrockEmbeddingSettings(bedrock_cohere_truncate='START', truncate=True))
        result = await embedder.embed_query('Test truncate priority')
        assert result == snapshot(
            EmbeddingResult(
                embeddings=IsList(IsList(IsFloat(), length=1536), length=1),
                inputs=['Test truncate priority'],
                input_type='query',
                model_name='cohere.embed-v4:0',
                provider_name='bedrock',
                timestamp=IsDatetime(),
                usage=RequestUsage(input_tokens=3),
                provider_response_id=IsStr(),
            )
        )

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py:273-280
    def _dbos_overrides(self) -> Iterator[None]:
        # Override with DBOSModel and DBOSMCPServer in the toolsets.
        # Use the configured parallel execution mode for deterministic event ordering during DBOS replay.
        with (
            super().override(model=self._model, toolsets=self._toolsets, tools=[]),
            self.parallel_tool_call_execution_mode(self._parallel_execution_mode),
        ):
            yield

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py:170-173
    def _prefect_overrides(self) -> Iterator[None]:
        # Override with PrefectModel and PrefectMCPServer in the toolsets.
        with super().override(model=self._model, toolsets=self._toolsets, tools=[]):
            yield

# tests/models/test_model_settings.py:46-59
def test_wrapper_model_settings_delegation():
    """Test that WrapperModel correctly delegates settings to wrapped model."""
    # Create a base model with settings
    base_settings = ModelSettings(max_tokens=150, temperature=0.6)
    base_model = TestModel(settings=base_settings)

    # Create wrapper - it should delegate to wrapped model's settings
    wrapper = WrapperModel(base_model)
    assert wrapper.settings == base_settings

    # Test with wrapped model without settings
    base_model_no_settings = TestModel()
    wrapper_no_settings = WrapperModel(base_model_no_settings)
    assert wrapper_no_settings.settings is None

# pydantic_ai_slim/pydantic_ai/providers/gateway.py:242-242
GATEWAY_BASE_URL = 'https://gateway.pydantic.dev/proxy'

# tests/models/test_fallback.py:592-624
async def test_fallback_model_settings_merge():
    """Test that FallbackModel properly merges model settings from wrapped model and runtime settings."""

    def return_settings(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        return ModelResponse(parts=[TextPart(to_json(info.model_settings).decode())])

    base_model = FunctionModel(return_settings, settings=ModelSettings(temperature=0.1, max_tokens=1024))
    fallback_model = FallbackModel(base_model)

    # Test that base model settings are preserved when no additional settings are provided
    agent = Agent(fallback_model)
    result = await agent.run('Hello')
    assert result.output == IsJson({'max_tokens': 1024, 'temperature': 0.1})

    # Test that runtime model_settings are merged with base settings
    agent_with_settings = Agent(fallback_model, model_settings=ModelSettings(temperature=0.5, parallel_tool_calls=True))
    result = await agent_with_settings.run('Hello')
    expected = {'max_tokens': 1024, 'temperature': 0.5, 'parallel_tool_calls': True}
    assert result.output == IsJson(expected)

    # Test that run-time model_settings override both base and agent settings
    result = await agent_with_settings.run(
        'Hello', model_settings=ModelSettings(temperature=0.9, extra_headers={'runtime_setting': 'runtime_value'})
    )
    expected = {
        'max_tokens': 1024,
        'temperature': 0.9,
        'parallel_tool_calls': True,
        'extra_headers': {
            'runtime_setting': 'runtime_value',
        },
    }
    assert result.output == IsJson(expected)

# pydantic_graph/pydantic_graph/graph.py:18-18
from .persistence import BaseStatePersistence

# pydantic_graph/pydantic_graph/graph.py:18-18
from .persistence import BaseStatePersistence