{
  "task_id": "d-retries-L369-condition_inversion",
  "method": "bm25",
  "budget": 8000,
  "query_type": "vague",
  "tokens_used": 8000,
  "symbols_selected": 37,
  "files_included": 14,
  "assembly_time_ms": 239.9,
  "llm_time_ms": 2529.9,
  "llm_input_tokens": 7184,
  "llm_output_tokens": 85,
  "tests_passed": false,
  "test_output": "yncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))",
  "patch": "FILE: pydantic_evals/pydantic_evals/reporting/__init__.py\nSEARCH:\n        if not any(c.source_case_name for c in self.cases) and not any(f.source_case_name for f in self.failures):\nREPLACE:\n        if not any(c.source_case_name for c in self.cases) and all(not f.source_case_name for f in self.failures):\n",
  "error": "",
  "test_time_ms": 1400.7,
  "failure_mode": "test_fail",
  "target_file_hit": false,
  "target_symbol_hit": false,
  "context_patch_overlap": 0.071,
  "patch_files_changed": 1,
  "patch_lines_changed": 1,
  "edit_distance_lines": -1,
  "entity_count_extracted": 0,
  "entity_count_mapped": 0,
  "query_identifier_density": 0.0,
  "seed_symbol_keys": [],
  "mutation_symbol_key": "pydantic_ai_slim/pydantic_ai/retries.py::wait_retry_after",
  "min_hops_seed_to_mutation": -1,
  "median_hops_seed_to_mutation": -1.0,
  "bca_closure_added_symbols": 0,
  "bca_closure_added_tokens": 0,
  "bca_frontier_visited": 0,
  "context_symbol_keys": [
    "pydantic_ai_slim/pydantic_ai/concurrency.py::ConcurrencyLimit.max_running",
    "pydantic_ai_slim/pydantic_ai/concurrency.py::ConcurrencyLimiter.__init__",
    "pydantic_ai_slim/pydantic_ai/concurrency.py::ConcurrencyLimiter.running_count",
    "pydantic_ai_slim/pydantic_ai/concurrency.py::ConcurrencyLimiter.max_running",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRun",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRun.all_messages",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRun.new_messages",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRun.run_id",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRunResult",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRunResult.all_messages",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRunResult.new_messages",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRunResult.run_id",
    "pydantic_ai_slim/pydantic_ai/run.py::AgentRunResultEvent",
    "pydantic_evals/pydantic_evals/dataset.py::InputsT",
    "pydantic_evals/pydantic_evals/dataset.py::OutputT",
    "pydantic_evals/pydantic_evals/dataset.py::MetadataT",
    "pydantic_evals/pydantic_evals/dataset.py::_REPORT_CASES_ADAPTER",
    "pydantic_evals/pydantic_evals/dataset.py::_CaseModel",
    "pydantic_evals/pydantic_evals/dataset.py::Case",
    "pydantic_evals/pydantic_evals/dataset.py::Case.__init__",
    "pydantic_evals/pydantic_evals/dataset.py::Dataset",
    "pydantic_evals/pydantic_evals/dataset.py::Dataset.__init__",
    "pydantic_evals/pydantic_evals/dataset.py::Dataset.evaluate",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::InputsT",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::OutputT",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::MetadataT",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::ReportCase",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::ReportCaseFailure",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::ReportCaseGroup",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::ReportCaseAggregate",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::ReportCaseAggregate.average",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::EvaluationReport",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::EvaluationReport.case_groups",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::EvaluationRenderer",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::EvaluationRenderer._all_cases",
    "pydantic_evals/pydantic_evals/reporting/__init__.py::EvaluationRenderer._baseline_cases_to_include",
    "pydantic_graph/pydantic_graph/beta/graph.py::OutputT",
    "pydantic_graph/pydantic_graph/beta/graph.py::EndMarker.__init__",
    "pydantic_graph/pydantic_graph/beta/graph.py::Graph",
    "pydantic_graph/pydantic_graph/beta/graph.py::GraphTask",
    "pydantic_graph/pydantic_graph/beta/graph.py::GraphRun.__init__",
    "pydantic_graph/pydantic_graph/beta/graph.py::_GraphIterator._get_completed_fork_runs",
    "tests/evals/test_dataset.py::TaskInput",
    "tests/evals/test_dataset.py::TaskOutput",
    "tests/evals/test_dataset.py::TaskMetadata",
    "tests/evals/test_dataset.py::example_cases",
    "tests/evals/test_dataset.py::example_dataset",
    "tests/evals/test_dataset.py::test_dataset_evaluate_with_empty_cases",
    "tests/evals/test_dataset.py::test_unnamed_cases",
    "tests/evals/test_multi_run.py::test_repeat_3_produces_3x_cases",
    "tests/evals/test_multi_run.py::test_repeat_with_unnamed_cases",
    "tests/evals/test_multi_run.py::test_case_groups_returns_none_for_single_run",
    "tests/evals/test_report_evaluators.py::TaskInput",
    "tests/evals/test_report_evaluators.py::TaskOutput",
    "tests/evals/test_report_evaluators.py::test_dataset_with_report_evaluators",
    "tests/evals/test_reporting.py::TaskInput",
    "tests/evals/test_reporting.py::TaskOutput",
    "tests/evals/test_reporting.py::TaskMetadata",
    "tests/evals/test_reporting.py::sample_report",
    "tests/evals/test_reporting.py::test_evaluation_renderer_with_removed_cases",
    "tests/graph/beta/test_edge_cases.py::EdgeCaseState",
    "tests/models/test_model.py::TEST_CASES",
    "tests/profiles/test_openai.py::SamplingParamsCase",
    "tests/profiles/test_openai.py::SAMPLING_PARAMS_CASES",
    "tests/test_ag_ui.py::run_and_collect_events",
    "tests/test_ag_ui.py::StateInt",
    "tests/test_ag_ui.py::create_input",
    "tests/test_ag_ui.py::test_concurrent_runs",
    "tests/test_temporal.py::model",
    "tests/test_temporal.py::simple_temporal_agent",
    "tests/test_temporal.py::SimpleAgentWorkflow",
    "tests/test_temporal.py::Deps",
    "tests/test_temporal.py::Response",
    "tests/test_temporal.py::SimpleAgentWorkflowWithRunSync",
    "tests/test_temporal.py::SimpleAgentWorkflowWithRunStream",
    "tests/test_temporal.py::SimpleAgentWorkflowWithRunStreamEvents",
    "tests/test_temporal.py::HitlAgentWorkflow.__init__",
    "tests/test_temporal.py::source"
  ],
  "mutation_symbol_lines": 71,
  "mutation_symbol_kind": "function",
  "mutation_file_symbols": 43,
  "graph_node_count": 18010,
  "retrieval_top1_top2_gap": 0.265955,
  "retrieval_softmax_entropy": 4.3134,
  "retrieval_softmax_tau": 20.497264,
  "retrieval_effective_candidates": 19.88,
  "retrieval_top5_ratio": 0.9732,
  "retrieval_within95_count": 4,
  "retrieval_scored_symbols": 50,
  "retrieval_top1_score": 23.304706,
  "retrieval_top5_mean_score": 22.680596,
  "retrieval_budget_utilization": 1.0,
  "retrieval_file_concentration": 0.1622,
  "repo_name": "pydantic-ai",
  "category": "retries",
  "mutation_type": "condition_inversion",
  "source": "discovered"
}