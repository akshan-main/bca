# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# tests/models/test_anthropic.py:60-60
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, TestEnv, raise_if_exception, try_import

# tests/models/test_bedrock.py:54-54
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, try_import

# tests/models/test_cohere.py:33-33
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, raise_if_exception, try_import

# tests/models/test_download_item.py:6-6
from ..conftest import IsInstance, IsStr

# tests/models/test_gemini_vertex.py:23-23
from ..conftest import IsDatetime, IsInstance, IsStr, try_import

# tests/models/test_google.py:79-79
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, try_import

# tests/models/test_groq.py:52-52
from ..conftest import IsDatetime, IsInstance, IsStr, raise_if_exception, try_import

# tests/models/test_huggingface.py:41-41
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, raise_if_exception, try_import

# tests/models/test_outlines.py:42-42
from ..conftest import IsDatetime, IsInstance, IsStr, try_import

# tests/models/test_xai.py:73-73
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, try_import

# tests/test_cli.py:8-8
from dirty_equals import IsInstance, IsStr

# tests/test_mcp.py:47-47
from .conftest import IsDatetime, IsInstance, IsNow, IsStr, try_import

# pydantic_evals/pydantic_evals/evaluators/common.py:132-147
class IsInstance(Evaluator[object, object, object]):
    """Check if the output is an instance of a type with the given name."""

    type_name: str
    evaluation_name: str | None = field(default=None)

    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> EvaluationReason:
        output = ctx.output
        for cls in type(output).__mro__:
            if cls.__name__ == self.type_name or cls.__qualname__ == self.type_name:
                return EvaluationReason(value=True)

        reason = f'output is of type {type(output).__name__}'
        if type(output).__qualname__ != type(output).__name__:
            reason += f' (qualname: {type(output).__qualname__})'
        return EvaluationReason(value=False, reason=reason)

# examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py:5-5
from datetime import datetime

# examples/pydantic_ai_examples/chat_app.py:17-17
from datetime import datetime, timezone

# examples/pydantic_ai_examples/evals/agent.py:4-4
from datetime import datetime

# examples/pydantic_ai_examples/flight_booking.py:6-6
import datetime

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:9-9
from datetime import datetime, timezone

# pydantic_ai_slim/pydantic_ai/_utils.py:13-13
from datetime import datetime, timezone

# pydantic_ai_slim/pydantic_ai/direct.py:16-16
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:5-5
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:5-5
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:8-8
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:3-3
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/messages.py:10-10
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/__init__.py:15-15
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:7-7
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:8-8
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/function.py:8-8
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/gemini.py:15-15
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/google.py:8-8
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/groq.py:6-6
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:6-6
from datetime import datetime, timezone

# pydantic_ai_slim/pydantic_ai/models/mistral.py:6-6
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/openai.py:10-10
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/outlines.py:12-12
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/models/test.py:8-8
from datetime import date, datetime, timedelta

# pydantic_ai_slim/pydantic_ai/models/xai.py:8-8
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/result.py:6-6
from datetime import datetime

# pydantic_ai_slim/pydantic_ai/retries.py:37-37
from datetime import datetime, timezone

# pydantic_ai_slim/pydantic_ai/run.py:6-6
from datetime import datetime

# pydantic_evals/pydantic_evals/otel/span_tree.py:6-6
from datetime import datetime, timedelta, timezone

# pydantic_graph/pydantic_graph/persistence/__init__.py:6-6
from datetime import datetime

# pydantic_graph/pydantic_graph/persistence/_utils.py:7-7
from datetime import datetime, timezone

# tests/conftest.py:13-13
from datetime import datetime

# tests/graph/test_persistence.py:7-7
from datetime import datetime, timezone

# tests/models/test_anthropic.py:8-8
from datetime import datetime, timezone

# tests/models/test_bedrock.py:3-3
from datetime import date, datetime, timezone

# tests/models/test_cohere.py:6-6
from datetime import datetime, timezone

# tests/models/test_deepseek.py:3-3
from datetime import datetime, timezone

# tests/models/test_gemini.py:5-5
import datetime

# tests/models/test_google.py:4-4
import datetime

# tests/models/test_groq.py:7-7
from datetime import datetime, timezone

# tests/models/test_huggingface.py:6-6
from datetime import datetime, timezone

# tests/models/test_instrumented.py:5-5
from datetime import datetime

# tests/models/test_mistral.py:6-6
from datetime import datetime, timezone

# tests/models/test_openai.py:8-8
from datetime import datetime, timezone

# tests/models/test_openrouter.py:1-1
import datetime

# tests/test_dbos.py:11-11
from datetime import datetime, timezone

# tests/test_format_as_xml.py:4-4
from datetime import date, datetime, time, timedelta

# tests/test_messages.py:2-2
from datetime import datetime, timezone

# tests/test_prefect.py:8-8
from datetime import datetime, timedelta

# tests/test_streaming.py:3-3
import datetime

# tests/test_tenacity.py:6-6
from datetime import datetime, timezone

# examples/pydantic_ai_examples/evals/models.py:57-57
    now: AwareDatetime

# examples/pydantic_ai_examples/evals/agent.py:20-20
    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

# pydantic_ai_slim/pydantic_ai/_run_context.py:51-51
    retries: dict[str, int] = field(default_factory=dict[str, int])

# pydantic_graph/pydantic_graph/beta/decision.py:57-66
    def branch(self, branch: DecisionBranch[T]) -> Decision[StateT, DepsT, HandledT | T]:
        """Add a new branch to this decision.

        Args:
            branch: The branch to add to this decision.

        Returns:
            A new Decision with the additional branch.
        """
        return Decision(id=self.id, branches=self.branches + [branch], note=self.note)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:91-91
    retries: int = 0

# pydantic_evals/pydantic_evals/__init__.py:9-9
from .dataset import Case, Dataset, increment_eval_metric, set_eval_attribute

# tests/conftest.py:136-138
def _(value: datetime):  # pragma: no cover
    """Use IsDatetime() for datetime values in snapshots."""
    return 'IsDatetime()'

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_types.py:16-16
    retries: int

# pydantic_ai_slim/pydantic_ai/models/__init__.py:867-916
    def _get_instructions(
        messages: Sequence[ModelMessage], model_request_parameters: ModelRequestParameters | None = None
    ) -> str | None:
        """Get instructions from the first ModelRequest found when iterating messages in reverse.

        In the case that a "mock" request was generated to include a tool-return part for a result tool,
        we want to use the instructions from the second-to-most-recent request (which should correspond to the
        original request that generated the response that resulted in the tool-return part).
        """
        instructions = None

        last_two_requests: list[ModelRequest] = []
        for message in reversed(messages):
            if isinstance(message, ModelRequest):
                last_two_requests.append(message)
                if len(last_two_requests) == 2:
                    break
                if message.instructions is not None:
                    instructions = message.instructions
                    break

        # If we don't have two requests, and we didn't already return instructions, there are definitely not any:
        if instructions is None and len(last_two_requests) == 2:
            most_recent_request = last_two_requests[0]
            second_most_recent_request = last_two_requests[1]

            # If we've gotten this far and the most recent request consists of only tool-return parts or retry-prompt parts,
            # we use the instructions from the second-to-most-recent request. This is necessary because when handling
            # result tools, we generate a "mock" ModelRequest with a tool-return part for it, and that ModelRequest will not
            # have the relevant instructions from the agent.

            # While it's possible that you could have a message history where the most recent request has only tool returns,
            # I believe there is no way to achieve that would _change_ the instructions without manually crafting the most
            # recent message. That might make sense in principle for some usage pattern, but it's enough of an edge case
            # that I think it's not worth worrying about, since you can work around this by inserting another ModelRequest
            # with no parts at all immediately before the request that has the tool calls (that works because we only look
            # at the two most recent ModelRequests here).

            # If you have a use case where this causes pain, please open a GitHub issue and we can discuss alternatives.

            if all(p.part_kind == 'tool-return' or p.part_kind == 'retry-prompt' for p in most_recent_request.parts):
                instructions = second_most_recent_request.instructions

        if model_request_parameters and (output_instructions := model_request_parameters.prompted_output_instructions):
            if instructions:
                instructions = '\n\n'.join([instructions, output_instructions])
            else:
                instructions = output_instructions

        return instructions

# pydantic_evals/pydantic_evals/dataset.py:457-487
    def add_case(
        self,
        *,
        name: str | None = None,
        inputs: InputsT,
        metadata: MetadataT | None = None,
        expected_output: OutputT | None = None,
        evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (),
    ) -> None:
        """Adds a case to the dataset.

        This is a convenience method for creating a [`Case`][pydantic_evals.Case] and adding it to the dataset.

        Args:
            name: Optional name for the case. If not provided, a generic name will be assigned.
            inputs: The inputs to the task being evaluated.
            metadata: Optional metadata for the case, which can be used by evaluators.
            expected_output: The expected output of the task, used for comparison in evaluators.
            evaluators: Tuple of evaluators specific to this case, in addition to dataset-level evaluators.
        """
        if name in {case.name for case in self.cases}:
            raise ValueError(f'Duplicate case name: {name!r}')

        case = Case[InputsT, OutputT, MetadataT](
            name=name,
            inputs=inputs,
            metadata=metadata,
            expected_output=expected_output,
            evaluators=evaluators,
        )
        self.cases.append(case)

# pydantic_evals/pydantic_evals/evaluators/common.py:136-136
    evaluation_name: str | None = field(default=None)

# pydantic_evals/pydantic_evals/evaluators/common.py:135-135
    type_name: str

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:76-83
    def id(self) -> str | None:
        """An ID for the toolset that is unique among all toolsets registered with the same agent.

        If you're implementing a concrete implementation that users can instantiate more than once, you should let them optionally pass a custom ID to the constructor and return that here.

        A toolset needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the toolset's activities within the workflow.
        """
        raise NotImplementedError()

# pydantic_evals/pydantic_evals/dataset.py:132-132
    name: str | None