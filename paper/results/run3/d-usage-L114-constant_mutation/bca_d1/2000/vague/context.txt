## pydantic_ai_slim/pydantic_ai/concurrency.py

async def _null_context() -> AsyncIterator[None]:
    """A no-op async context manager."""
    yield

async def _limiter_context(limiter: AbstractConcurrencyLimiter, source: str) -> AsyncIterator[None]:
    """Context manager that acquires and releases a limiter with the given source."""
    await limiter.acquire(source)
    try:
        yield
    finally:
        limiter.release()

def get_concurrency_context(
    limiter: AbstractConcurrencyLimiter | None,
    source: str = 'unnamed',
) -> AbstractAsyncContextManager[None]:
    """Get an async context manager for the concurrency limiter.

    If limiter is None, returns a no-op context manager.

    Args:
        limiter: The AbstractConcurrencyLimiter or None.
        source: Identifier for the source of this acquisition (e.g., 'agent:my-agent' or 'model:gpt-4').

    Returns:
        An async context manager.
    """
    if limiter is None:
        return _null_context()
    return _limiter_context(limiter, source)

## pydantic_ai_slim/pydantic_ai/mcp.py

def load_mcp_servers(config_path: str | Path) -> list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE]:
    """Load MCP servers from a configuration file.

    Environment variables can be referenced in the configuration file using:
    - `${VAR_NAME}` syntax - expands to the value of VAR_NAME, raises error if not defined
    - `${VAR_NAME:-default}` syntax - expands to VAR_NAME if set, otherwise uses the default value

    Args:
        config_path: The path to the configuration file.

    Returns:
        A list of MCP servers.

    Raises:
        FileNotFoundError: If the configuration file does not exist.
        ValidationError: If the configuration file does not match the schema.
        ValueError: If an environment variable referenced in the configuration is not defined and no default value is provided.
    """
    config_path = Path(config_path)

    if not config_path.exists():
        raise FileNotFoundError(f'Config file {config_path} not found')

    config_data = pydantic_core.from_json(config_path.read_bytes())
    expanded_config_data = _expand_env_vars(config_data)
    config = MCPServerConfig.model_validate(expanded_config_data)

    servers: list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE] = []
    for name, server in config.mcp_servers.items():
        server.id = name
        server.tool_prefix = name
        servers.append(server)

    return servers

## pydantic_evals/pydantic_evals/evaluators/report_common.py

    n_thresholds: int = 100

## pydantic_evals/pydantic_evals/reporting/__init__.py

class RenderValueConfig(TypedDict, total=False):
    """A configuration for rendering a values in an Evaluation report."""

    value_formatter: str | Callable[[Any], str]
    diff_checker: Callable[[Any, Any], bool] | None
    diff_formatter: Callable[[Any, Any], str | None] | None
    diff_style: str

## pydantic_evals/pydantic_evals/reporting/render_numbers.py

MULTIPLIER_ONE_DECIMAL_THRESHOLD = 100  # If |multiplier| is below this, use one decimal; otherwise, use none.

BASE_THRESHOLD = 1e-2  # If |old| is below this and delta is > MULTIPLIER_DROP_FACTOR * |old|, drop relative change.

def default_render_duration_diff(old: float, new: float) -> str | None:
    """Format a duration difference (in seconds) with an explicit sign."""
    if old == new:
        return None

    abs_diff_str = _render_duration(new - old, True)
    rel_diff_str = _render_relative(new, old, BASE_THRESHOLD)
    if rel_diff_str is None:
        return abs_diff_str
    else:
        return f'{abs_diff_str} / {rel_diff_str}'

def _render_relative(new: float, base: float, small_base_threshold: float) -> str | None:
    # If we cannot compute a relative change, return just the diff.
    if base == 0:
        return None

    delta = new - base

    # For very small base values with huge changes, drop the relative indicator.
    if abs(base) < small_base_threshold and abs(delta) > MULTIPLIER_DROP_FACTOR * abs(base):
        return None

    # Compute the relative change as a percentage.
    rel_change = (delta / base) * 100
    perc_str = f'{rel_change:+.{PERC_DECIMALS}f}%'
    # If the percentage rounds to 0.0%, return only the absolute difference.
    if perc_str in ('+0.0%', '-0.0%'):
        return None

    # Decide whether to use percentage style or multiplier style.
    if abs(delta) / abs(base) <= 1:
        # Percentage style.
        return perc_str
    else:
        # Multiplier style.
        multiplier = new / base
        if abs(multiplier) < MULTIPLIER_ONE_DECIMAL_THRESHOLD:
            mult_str = f'{multiplier:,.1f}x'
        else:
            mult_str = f'{multiplier:,.0f}x'
        return mult_str

## pydantic_graph/pydantic_graph/beta/graph.py

    def value(self) -> OutputT:
        return self._value

## tests/conftest.py

SNAPSHOT_BYTES_COLLAPSE_THRESHOLD = 50

## tests/graph/test_mermaid.py

def test_mermaid_highlight_wrong():
    with pytest.raises(LookupError):
        graph1.mermaid_code(highlighted_nodes=Spam)

## tests/test_mcp.py

def test_load_mcp_servers_undefined_env_var(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test that undefined environment variables raise an error."""
    config = tmp_path / 'mcp.json'

    # Make sure the environment variable is not set
    monkeypatch.delenv('UNDEFINED_VAR', raising=False)

    config.write_text('{"mcpServers": {"my_server": {"command": "${UNDEFINED_VAR}", "args": []}}}', encoding='utf-8')

    with pytest.raises(ValueError, match='Environment variable \\$\\{UNDEFINED_VAR\\} is not defined'):
        load_mcp_servers(config)

def test_load_mcp_servers_partial_env_vars(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test environment variables in partial strings."""
    config = tmp_path / 'mcp.json'

    monkeypatch.setenv('HOST', 'example.com')
    monkeypatch.setenv('PATH_SUFFIX', 'mcp')
    config.write_text('{"mcpServers": {"server": {"url": "https://${HOST}/api/${PATH_SUFFIX}"}}}', encoding='utf-8')

    servers = load_mcp_servers(config)

    assert len(servers) == 1
    server = servers[0]
    assert isinstance(server, MCPServerStreamableHTTP)
    assert server.url == 'https://example.com/api/mcp'

## tests/test_parts_manager.py

def test_handle_thinking_delta_wrong_part_type():
    manager = ModelResponsePartsManager()

    # Add a text part first
    list(manager.handle_text_delta(vendor_part_id='text', content='hello'))

    # Try to apply thinking delta to the text part - should raise error
    with pytest.raises(UnexpectedModelBehavior, match=r'Cannot apply a thinking delta to existing_part='):
        list(manager.handle_thinking_delta(vendor_part_id='text', content='thinking', signature=None))

## tests/test_streaming.py

async def test_call_tool_wrong_name():
    async def stream_structured_function(_messages: list[ModelMessage], _: AgentInfo) -> AsyncIterator[DeltaToolCalls]:
        yield {0: DeltaToolCall(name='foobar', json_args='{}')}

    agent = Agent(
        FunctionModel(stream_function=stream_structured_function),
        output_type=tuple[str, int],
        retries=0,
    )

    @agent.tool_plain
    async def ret_a(x: str) -> str:  # pragma: no cover
        return x

    with capture_run_messages() as messages:
        with pytest.raises(UnexpectedModelBehavior, match=r'Exceeded maximum retries \(0\) for output validation'):
            async with agent.run_stream('hello'):
                pass

    assert messages == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='hello', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='foobar', args='{}', tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=50, output_tokens=1),
                model_name='function::stream_structured_function',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

## tests/test_validation_context.py

class Value(BaseModel):
    x: int

    @field_validator('x')
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

## tests/typed_agent.py

async def output_validator_wrong(ctx: RunContext[int], result: str) -> str:
    return result
