## examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py

from pydantic import BaseModel, Field

## examples/pydantic_ai_examples/question_graph.py

from pydantic_graph import (
    BaseNode,
    End,
    Graph,
    GraphRunContext,
)

class Ask(BaseNode[QuestionState]):
    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:
        result = await ask_agent.run(
            'Ask a simple question with a single correct answer.',
            message_history=ctx.state.ask_agent_messages,
        )
        ctx.state.ask_agent_messages += result.all_messages()
        ctx.state.question = result.output
        return Answer(result.output)

    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:
        result = await ask_agent.run(
            'Ask a simple question with a single correct answer.',
            message_history=ctx.state.ask_agent_messages,
        )
        ctx.state.ask_agent_messages += result.all_messages()
        ctx.state.question = result.output
        return Answer(result.output)

## pydantic_ai_slim/pydantic_ai/_json_schema.py

from abc import ABC, abstractmethod

## pydantic_ai_slim/pydantic_ai/_ssrf.py

_CLOUD_METADATA_IPS: frozenset[str] = frozenset(
    {
        '169.254.169.254',  # AWS, GCP, Azure metadata endpoint
        'fd00:ec2::254',  # AWS EC2 IPv6 metadata endpoint
        '100.100.100.200',  # Alibaba Cloud metadata endpoint
    }
)

class ResolvedUrl:
    """Result of URL validation and DNS resolution."""

    resolved_ip: str
    """The resolved IP address to connect to."""

    hostname: str
    """The original hostname (used for Host header)."""

    port: int
    """The port number."""

    is_https: bool
    """Whether to use HTTPS."""

    path: str
    """The path including query string and fragment."""

    resolved_ip: str

def extract_host_and_port(url: str) -> tuple[str, str, int, bool]:
    """Extract hostname, path, port, and protocol info from a URL.

    Returns:
        Tuple of (hostname, path_with_query, port, is_https)

    Raises:
        ValueError: If the URL is malformed or uses an unsupported protocol.
    """
    # Validate protocol first, before trying to extract hostname
    _, is_https = validate_url_protocol(url)

    parsed = urlparse(url)
    hostname = parsed.hostname

    if not hostname:
        raise ValueError(f'Invalid URL: no hostname found in "{url}"')

    default_port = 443 if is_https else 80
    port = parsed.port or default_port

    # Reconstruct path with query string
    path = parsed.path or '/'
    if parsed.query:
        path = f'{path}?{parsed.query}'
    if parsed.fragment:
        path = f'{path}#{parsed.fragment}'

    return hostname, path, port, is_https

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py

from pydantic_ai.providers import Provider

## pydantic_ai_slim/pydantic_ai/mcp.py

    prompts_list_changed: bool = False

    resources_list_changed: bool = False

    tools_list_changed: bool = False

## pydantic_ai_slim/pydantic_ai/models/__init__.py

async def download_item(
    item: FileUrl,
    data_format: Literal['bytes', 'base64', 'base64_uri', 'text'] = 'bytes',
    type_format: Literal['mime', 'extension'] = 'mime',
) -> DownloadedItem[str] | DownloadedItem[bytes]:
    """Download an item by URL and return the content as a bytes object or a (base64-encoded) string.

    This function includes SSRF (Server-Side Request Forgery) protection:
    - Only http:// and https:// protocols are allowed
    - Private/internal IP addresses are blocked by default
    - Cloud metadata endpoints (169.254.169.254) are always blocked
    - Hostnames are resolved before requests to prevent DNS rebinding

    Set `item.force_download='allow-local'` to allow private IP addresses.

    Args:
        item: The item to download.
        data_format: The format to return the content in:
            - `bytes`: The raw bytes of the content.
            - `base64`: The base64-encoded content.
            - `base64_uri`: The base64-encoded content as a data URI.
            - `text`: The content as a string.
        type_format: The format to return the media type in:
            - `mime`: The media type as a MIME type.
            - `extension`: The media type as an extension.

    Raises:
        UserError: If the URL points to a YouTube video.
        ValueError: If the URL uses an unsupported protocol or targets a private/internal
            IP address (unless allow-local is set).
    """
    if isinstance(item, VideoUrl) and item.is_youtube:
        raise UserError('Downloading YouTube videos is not supported.')

    from .._ssrf import safe_download

    allow_local = item.force_download == 'allow-local'
    response = await safe_download(item.url, allow_local=allow_local)

    if content_type := response.headers.get('content-type'):
        content_type = content_type.split(';')[0]
        if content_type == 'application/octet-stream':
            content_type = None

    media_type = content_type or item.media_type

    data_type = media_type
    if type_format == 'extension':
        data_type = item.format

    data = response.content
    if data_format in ('base64', 'base64_uri'):
        data = base64.b64encode(data).decode('utf-8')
        if data_format == 'base64_uri':
            data = f'data:{media_type};base64,{data}'
        return DownloadedItem[str](data=data, data_type=data_type)
    elif data_format == 'text':
        return DownloadedItem[str](data=data.decode('utf-8'), data_type=data_type)
    else:
        return DownloadedItem[bytes](data=data, data_type=data_type)

## pydantic_ai_slim/pydantic_ai/providers/google_gla.py

class GoogleGLAProvider(Provider[httpx.AsyncClient]):
    """Provider for Google Generative Language AI API."""

    @property
    def name(self):
        return 'google-gla'

    @property
    def base_url(self) -> str:
        return 'https://generativelanguage.googleapis.com/v1beta/models/'

    @property
    def client(self) -> httpx.AsyncClient:
        return self._client

    def model_profile(self, model_name: str) -> ModelProfile | None:
        return google_model_profile(model_name)

    def __init__(self, api_key: str | None = None, http_client: httpx.AsyncClient | None = None) -> None:
        """Create a new Google GLA provider.

        Args:
            api_key: The API key to use for authentication, if not provided, the `GEMINI_API_KEY` environment variable
                will be used if available.
            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.
        """
        api_key = api_key or os.getenv('GEMINI_API_KEY')
        if not api_key:
            raise UserError(
                'Set the `GEMINI_API_KEY` environment variable or pass it via `GoogleGLAProvider(api_key=...)`'
                'to use the Google GLA provider.'
            )

        self._client = http_client or cached_async_http_client(provider='google-gla')
        self._client.base_url = self.base_url
        # https://cloud.google.com/docs/authentication/api-keys-use#using-with-rest
        self._client.headers['X-Goog-Api-Key'] = api_key

## pydantic_ai_slim/pydantic_ai/providers/openai.py

class OpenAIProvider(Provider[AsyncOpenAI]):
    """Provider for OpenAI API."""

    @property
    def name(self) -> str:
        return 'openai'

    @property
    def base_url(self) -> str:
        return str(self.client.base_url)

    @property
    def client(self) -> AsyncOpenAI:
        return self._client

    def model_profile(self, model_name: str) -> ModelProfile | None:
        return openai_model_profile(model_name)

    @overload
    def __init__(self, *, openai_client: AsyncOpenAI) -> None: ...

    @overload
    def __init__(
        self,
        base_url: str | None = None,
        api_key: str | None = None,
        openai_client: None = None,
        http_client: httpx.AsyncClient | None = None,
    ) -> None: ...

    def __init__(
        self,
        base_url: str | None = None,
        api_key: str | None = None,
        openai_client: AsyncOpenAI | None = None,
        http_client: httpx.AsyncClient | None = None,
    ) -> None:
        """Create a new OpenAI provider.

        Args:
            base_url: The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable
                will be used if available. Otherwise, defaults to OpenAI's base url.
            api_key: The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable
                will be used if available.
            openai_client: An existing
                [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)
                client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.
            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.
        """
        # This is a workaround for the OpenAI client requiring an API key, whilst locally served,
        # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.
        if api_key is None and 'OPENAI_API_KEY' not in os.environ and base_url is not None and openai_client is None:
            api_key = 'api-key-not-set'

        if openai_client is not None:
            assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'
            assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'
            assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'
            self._client = openai_client
        elif http_client is not None:
            self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)
        else:
            http_client = cached_async_http_client(provider='openai')
            self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)

## pydantic_ai_slim/pydantic_ai/result.py

    def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:
        """Return the history of _messages.

        Args:
            output_tool_return_content: The return content of the tool call to set in the last message.
                This provides a convenient way to modify the content of the output tool call if you want to continue
                the conversation and want to set the response to the output tool call. If `None`, the last message will
                not be modified.

        Returns:
            List of messages.
        """
        # this is a method to be consistent with the other methods
        if output_tool_return_content is not None:
            raise NotImplementedError('Setting output tool return content is not supported for this result type.')
        return self._all_messages

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_models.py

class CamelBaseModel(BaseModel, ABC):
    """Base model with camelCase aliases."""

    model_config = ConfigDict(alias_generator=to_camel, populate_by_name=True, extra='forbid')

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py

class BaseUIPart(CamelBaseModel, ABC):
    """Abstract base class for all UI parts."""

class StepStartUIPart(BaseUIPart):
    """A step boundary part of a message."""

    type: Literal['step-start'] = 'step-start'

## pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py

_default_model: models.Model | models.KnownModelName = 'openai:gpt-5.2'

## tests/conftest.py

def disable_ssrf_protection_for_vcr():
    """Disable SSRF protection for VCR compatibility.

    VCR cassettes record requests with the original hostname. Since SSRF protection
    resolves hostnames to IPs before making requests, we need to disable the validation
    for VCR tests to match the pre-recorded cassettes.

    This fixture patches validate_and_resolve_url to return the hostname in place
    of the resolved IP, allowing the request URL to use the original hostname.
    """
    from unittest.mock import patch

    from pydantic_ai._ssrf import ResolvedUrl, extract_host_and_port

    async def mock_validate_and_resolve(url: str, allow_local: bool) -> ResolvedUrl:
        hostname, path, port, is_https = extract_host_and_port(url)
        # Return hostname in place of resolved IP - this allows VCR matching
        return ResolvedUrl(resolved_ip=hostname, hostname=hostname, port=port, is_https=is_https, path=path)

    with patch('pydantic_ai._ssrf.validate_and_resolve_url', mock_validate_and_resolve):
        yield

## tests/evals/test_report_evaluators.py

def test_confusion_matrix_evaluator_skips_none():
    cases = [
        _make_report_case('c1', output='cat', expected_output='cat'),
        _make_report_case('c2', output='dog', expected_output=None),  # should be skipped
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(predicted_from='output', expected_from='expected_output')
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    assert result.class_labels == ['cat']
    assert result.matrix == [[1]]

def test_precision_recall_evaluator_skips_missing_scores():
    """PrecisionRecallEvaluator skips cases missing score or positive data."""
    cases = [
        _make_report_case('c1', scores={'confidence': 0.9}, assertions={'is_correct': True}),
        _make_report_case('c2', scores={}, assertions={'is_correct': False}),  # missing score
        _make_report_case('c3', scores={'confidence': 0.3}, assertions={}),  # missing assertion
    ]
    report = _make_report(cases)

    evaluator = PrecisionRecallEvaluator(
        score_key='confidence',
        positive_from='assertions',
        positive_key='is_correct',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, PrecisionRecall)
    assert len(result.curves) == 1

## tests/evals/test_reporting.py

async def test_evaluation_renderer_diff_with_changed_metadata(sample_report_case: ReportCase):
    """Test EvaluationRenderer diff table where both reports have the same metadata."""

    baseline_report = EvaluationReport(
        cases=[sample_report_case],
        name='baseline_report',
        experiment_metadata={
            'updated-key': 'original value',
            'preserved-key': 'preserved value',
            'old-key': 'old value',
        },
    )

    new_report = EvaluationReport(
        cases=[sample_report_case],
        name='new_report',
        experiment_metadata={
            'updated-key': 'updated value',
            'preserved-key': 'preserved value',
            'new-key': 'new value',
        },
    )

    output = new_report.render(
        include_input=False,
        include_metadata=False,
        include_expected_output=False,
        include_output=False,
        include_durations=True,
        include_total_duration=False,
        include_removed_cases=False,
        include_averages=False,
        include_error_stacktrace=False,
        include_evaluator_failures=True,
        input_config={},
        metadata_config={},
        output_config={},
        score_configs={},
        label_configs={},
        metric_configs={},
        duration_config={},
        include_reasons=False,
        baseline=baseline_report,
        include_errors=False,  # Prevent failures table from being added
    )
    assert output == snapshot("""\
╭─ Evaluation Diff: baseline_report → new_report ─╮
│ + new-key: new value                            │
│ - old-key: old value                            │
│ preserved-key: preserved value                  │
│ updated-key: original value → updated value     │
╰─────────────────────────────────────────────────╯
┏━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓
┃ Case ID   ┃ Scores       ┃ Labels        ┃ Metrics         ┃ Assertions ┃ Duration ┃
┡━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩
│ test_case │ score1: 2.50 │ label1: hello │ accuracy: 0.950 │ ✔          │  100.0ms │
└───────────┴──────────────┴───────────────┴─────────────────┴────────────┴──────────┘
""")

## tests/models/test_bedrock.py

async def test_image_url_input(
    allow_model_requests: None, bedrock_provider: BedrockProvider, disable_ssrf_protection_for_vcr: None
):
    m = BedrockConverseModel('us.amazon.nova-pro-v1:0', provider=bedrock_provider)
    agent = Agent(m, instructions='You are a helpful chatbot.')

    result = await agent.run(
        [
            'What is this vegetable?',
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ]
    )
    assert result.output == snapshot(
        'The image shows a potato. It is oval in shape and has a yellow skin with numerous dark brown patches. These patches are known as lenticels, which are pores that allow the potato to breathe. The potato is a root vegetable that is widely cultivated and consumed around the world. It is a versatile ingredient that can be used in a variety of dishes, including mashed potatoes, fries, and potato salad.'
    )

async def test_video_url_input(
    allow_model_requests: None, bedrock_provider: BedrockProvider, disable_ssrf_protection_for_vcr: None
):
    m = BedrockConverseModel('us.amazon.nova-pro-v1:0', provider=bedrock_provider)
    agent = Agent(m, instructions='You are a helpful chatbot.')

    result = await agent.run(
        [
            'Explain me this video',
            VideoUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/small_video.mp4'),
        ]
    )
    assert result.output == snapshot(
        'The video shows a camera set up on a tripod, pointed at a scenic view of a rocky landscape under a clear sky. The camera remains stationary throughout the video, capturing the same view without any changes.'
    )

## tests/models/test_download_item.py

async def test_download_item_audio_mpeg(disable_ssrf_protection_for_vcr: None) -> None:
    downloaded_item = await download_item(
        AudioUrl(url='https://smokeshow.helpmanual.io/4l1l1s0s6q4741012x1w/common_voice_en_537507.mp3'),
        data_format='bytes',
    )
    assert downloaded_item['data_type'] == 'audio/mpeg'
    assert downloaded_item['data'] == IsInstance(bytes)

async def test_download_item_no_content_type(disable_ssrf_protection_for_vcr: None) -> None:
    downloaded_item = await download_item(
        DocumentUrl(url='https://raw.githubusercontent.com/pydantic/pydantic-ai/refs/heads/main/docs/help.md'),
        data_format='text',
    )
    assert downloaded_item['data_type'] == 'text/markdown'
    assert downloaded_item['data'] == IsStr()

## tests/models/test_gemini.py

async def test_image_url_input(
    allow_model_requests: None, gemini_api_key: str, disable_ssrf_protection_for_vcr: None
) -> None:
    m = GeminiModel('gemini-2.0-flash-exp', provider=GoogleGLAProvider(api_key=gemini_api_key))
    agent = Agent(m)

    image_url = ImageUrl(url='https://goo.gle/instrument-img')

    result = await agent.run(['What is the name of this fruit?', image_url])
    assert result.output == snapshot("This is not a fruit; it's a pipe organ console.")

async def test_video_url_input(
    allow_model_requests: None, gemini_api_key: str, disable_ssrf_protection_for_vcr: None
) -> None:
    m = GeminiModel('gemini-2.5-flash', provider=GoogleGLAProvider(api_key=gemini_api_key))
    agent = Agent(m, instructions='You are a helpful chatbot.')

    video_url = VideoUrl(url='https://data.grepit.app/assets/tiny_video.mp4')

    result = await agent.run(['Explain me this video', video_url])
    assert result.output.strip() == snapshot(
        """\
This video is a beautiful, static shot (or appears to be a still image) of a picturesque Mediterranean scene.

Here's a breakdown:

*   **Setting:** It depicts a narrow, charming alleyway or street, likely in a coastal town or island.
*   **Architecture:** The buildings on both sides are traditional, whitewashed stucco, characteristic of Cycladic architecture often found in the Greek islands. There are minimal details on the walls, with some simple light fixtures on the left and a blue-painted window frame on the right.
*   **Cafe/Restaurant:** Along the left side of the alley, several rustic wooden tables and chairs are set up, suggesting an outdoor cafe or restaurant. On the far left wall, there are also some woven baskets hanging or placed.
*   **Pathway:** The ground is paved with a distinctive pattern of light-colored stones, adding to the quaint aesthetic.
*   **The Sea View:** The alley opens up directly to a stunning view of the sea. The water is a vibrant blue, with noticeable waves gently breaking.
*   **Horizon:** In the distance, across the sparkling water, another landmass or island can be seen under a clear, bright blue sky.
*   **Atmosphere:** The overall impression is one of serenity, beauty, and a quintessential Mediterranean vacation spot, perfect for enjoying a meal or drink with a breathtaking ocean view.

It strongly evokes places like Mykonos or Santorini in Greece, known for their iconic white buildings and narrow pathways leading to the sea.\
"""
    )

## tests/models/test_openai.py

async def test_document_url_input(
    allow_model_requests: None, openai_api_key: str, disable_ssrf_protection_for_vcr: None
):
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key=openai_api_key))
    agent = Agent(m)

    document_url = DocumentUrl(url='https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf')

    result = await agent.run(['What is the main content on this document?', document_url])
    assert result.output == snapshot('The document contains the text "Dummy PDF file" on its single page.')

class MyDefaultDc:
    x: int = 1

class MyDefaultRecursiveDc:
    field: MyDefaultRecursiveDc | None = None

## tests/models/test_outlines.py

def test_request_image_url(transformers_multimodal_model: OutlinesModel, disable_ssrf_protection_for_vcr: None) -> None:
    agent = Agent(transformers_multimodal_model)
    result = agent.run_sync(
        [
            "What's on the image?",
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ],
        model_settings=ModelSettings(extra_body={'max_new_tokens': 100}),
    )
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content=[
                            "What's on the image?",
                            ImageUrl(
                                url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'
                            ),
                        ],
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(parts=[TextPart(content=IsStr())], timestamp=IsDatetime(), run_id=IsStr()),
        ]
    )

## tests/models/test_xai.py

async def test_xai_stream_empty_tool_call_name(allow_model_requests: None):
    """Test streaming skips tool calls with empty function name."""
    # Create a tool call with empty name
    empty_name_tool_call = chat_pb2.ToolCall(
        id='empty_name_001',
        type=chat_pb2.ToolCallType.TOOL_CALL_TYPE_CLIENT_SIDE_TOOL,
        function=chat_pb2.FunctionCall(name='', arguments='{}'),  # Empty name
    )

    # Create a streaming response with a tool call that has an empty name
    chunk = create_stream_chunk(content='Hello', finish_reason='stop')
    response = create_response_with_tool_calls(
        content='Hello',
        tool_calls=[empty_name_tool_call],
        finish_reason='stop',
        usage=create_usage(prompt_tokens=5, completion_tokens=2),
    )

    stream = [(response, chunk)]
    mock_client = MockXai.create_mock_stream([stream])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    async with agent.run_stream('Hello') as result:
        text_chunks = [c async for c in result.stream_text(debounce_by=None)]
        # Should get text, but skip the empty-name tool call
        assert 'Hello' in text_chunks[-1]

## tests/test_mcp.py

async def test_server_capabilities_list_changed_fields() -> None:
    """Test that ServerCapabilities correctly parses listChanged fields."""
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'])
    async with server:
        caps = server.capabilities
        assert isinstance(caps.prompts_list_changed, bool)
        assert isinstance(caps.tools_list_changed, bool)
        assert isinstance(caps.resources_list_changed, bool)

## tests/test_ssrf.py

    def test_private_ips_detected(self, ip: str) -> None:
        assert is_private_ip(ip) is True

    def test_public_ips_allowed(self, ip: str) -> None:
        assert is_private_ip(ip) is False

    def test_cloud_metadata_ips_detected(self, ip: str) -> None:
        assert is_cloud_metadata_ip(ip) is True

    def test_non_metadata_ips(self, ip: str) -> None:
        assert is_cloud_metadata_ip(ip) is False

    async def test_public_ip_allowed(self) -> None:
        """Test that public IPs are allowed."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            mock_executor.return_value = [(2, 1, 6, '', ('93.184.215.14', 0))]
            resolved = await validate_and_resolve_url('https://example.com/path', allow_local=False)
            assert resolved.resolved_ip == '93.184.215.14'
            assert resolved.hostname == 'example.com'
            assert resolved.port == 443
            assert resolved.is_https is True
            assert resolved.path == '/path'

    async def test_private_ip_blocked_by_default(self) -> None:
        """Test that private IPs are blocked by default."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            mock_executor.return_value = [(2, 1, 6, '', ('192.168.1.1', 0))]
            with pytest.raises(ValueError, match='Access to private/internal IP address'):
                await validate_and_resolve_url('http://internal.local/path', allow_local=False)

    async def test_private_ip_allowed_with_allow_local(self) -> None:
        """Test that private IPs are allowed with allow_local=True."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            mock_executor.return_value = [(2, 1, 6, '', ('192.168.1.1', 0))]
            resolved = await validate_and_resolve_url('http://internal.local/path', allow_local=True)
            assert resolved.resolved_ip == '192.168.1.1'

    async def test_cloud_metadata_always_blocked(self) -> None:
        """Test that cloud metadata IPs are always blocked, even with allow_local=True."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            mock_executor.return_value = [(2, 1, 6, '', ('169.254.169.254', 0))]
            with pytest.raises(ValueError, match='Access to cloud metadata service'):
                await validate_and_resolve_url('http://metadata.google.internal/path', allow_local=True)

    async def test_literal_private_ip_blocked(self) -> None:
        """Test that literal private IPs in URLs are blocked."""
        with pytest.raises(ValueError, match='Access to private/internal IP address'):
            await validate_and_resolve_url('http://192.168.1.1/path', allow_local=False)

    async def test_redirect_to_private_ip_blocked(self) -> None:
        """Test that redirects to private IPs are blocked."""
        redirect_response = AsyncMock()
        redirect_response.is_redirect = True
        redirect_response.headers = {'location': 'http://internal.local/file.txt'}

        with (
            patch('pydantic_ai._ssrf.run_in_executor') as mock_executor,
            patch('pydantic_ai._ssrf.cached_async_http_client') as mock_client_fn,
        ):
            # First call for example.com (public), second for internal.local (private)
            mock_executor.side_effect = [
                [(2, 1, 6, '', ('93.184.215.14', 0))],
                [(2, 1, 6, '', ('192.168.1.1', 0))],
            ]

            mock_client = AsyncMock()
            mock_client.get.return_value = redirect_response
            mock_client_fn.return_value = mock_client

            with pytest.raises(ValueError, match='Access to private/internal IP address'):
                await safe_download('https://example.com/file.txt')

    async def test_multiple_ips_with_any_private_blocked(self) -> None:
        """Test that if any IP in the resolution is private, request is blocked."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            # DNS returns multiple IPs, one of which is private
            mock_executor.return_value = [
                (2, 1, 6, '', ('8.8.8.8', 0)),  # Public
                (10, 1, 6, '', ('::1', 0)),  # Private IPv6 loopback
            ]
            with pytest.raises(ValueError, match='Access to private/internal IP address'):
                await validate_and_resolve_url('http://attacker.com/path', allow_local=False)

## tests/test_temporal.py

def test_pydantic_ai_plugin_with_pydantic_payload_converter_unchanged() -> None:
    """When converter already uses PydanticPayloadConverter, return it unchanged."""
    plugin = PydanticAIPlugin()
    converter = DataConverter(payload_converter_class=PydanticPayloadConverter)
    config: dict[str, Any] = {'data_converter': converter}
    result = plugin.configure_client(config)  # type: ignore[arg-type]
    assert result['data_converter'] is converter

## tests/test_vercel_ai.py

async def test_adapter_text_part_with_provider_metadata():
    """Test TextPart with provider_name and provider_details preserves metadata and roundtrips."""
    messages: list[ModelMessage] = [
        ModelResponse(
            parts=[
                TextPart(
                    content='Hello with metadata',
                    id='text_123',
                    provider_name='openai',
                    provider_details={'model': 'gpt-4', 'finish_reason': 'stop'},
                ),
            ]
        ),
    ]

    ui_messages = VercelAIAdapter.dump_messages(messages)
    ui_message_dicts = [msg.model_dump() for msg in ui_messages]

    assert ui_message_dicts == snapshot(
        [
            {
                'id': IsStr(),
                'role': 'assistant',
                'metadata': None,
                'parts': [
                    {
                        'type': 'text',
                        'text': 'Hello with metadata',
                        'state': 'done',
                        'provider_metadata': {
                            'pydantic_ai': {
                                'id': 'text_123',
                                'provider_name': 'openai',
                                'provider_details': {'model': 'gpt-4', 'finish_reason': 'stop'},
                            }
                        },
                    }
                ],
            }
        ]
    )

    # Verify roundtrip
    reloaded_messages = VercelAIAdapter.load_messages(ui_messages)
    _sync_timestamps(messages, reloaded_messages)
    assert reloaded_messages == messages
