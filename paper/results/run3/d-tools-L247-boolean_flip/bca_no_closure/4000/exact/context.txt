## examples/pydantic_ai_examples/chat_app.py

async def index() -> FileResponse:
    return FileResponse((THIS_DIR / 'chat_app.html'), media_type='text/html')

## examples/pydantic_ai_examples/data_analyst.py

    def get(self, ref: str) -> pd.DataFrame:
        if ref not in self.output:
            raise ModelRetry(
                f'Error: {ref} is not a valid variable reference. Check the previous messages and try again.'
            )
        return self.output[ref]

## examples/pydantic_ai_examples/slack_lead_qualifier/store.py

    async def add(cls, analysis: Analysis):
        await cls._get_store().put.aio(analysis.profile.email, analysis.model_dump())

    async def list(cls) -> list[Analysis]:
        return [
            Analysis.model_validate(analysis)
            async for analysis in cls._get_store().values.aio()
        ]

    def _get_store(cls) -> modal.Dict:
        return modal.Dict.from_name('analyses', create_if_missing=True)  # type: ignore ### [/analysis_store]

## pydantic_ai_slim/pydantic_ai/_cli/__init__.py

    def __init__(self, special_suggestions: list[str] | None = None):
        super().__init__()
        self.special_suggestions = special_suggestions or []

## pydantic_ai_slim/pydantic_ai/_function_schema.py

    json_schema: ObjectJsonSchema

## pydantic_ai_slim/pydantic_ai/_output.py

    def id(self) -> str | None:
        return '<output>'  # pragma: no cover

    def label(self) -> str:
        return "the agent's output tools"

    async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:
        return {
            tool_def.name: ToolsetTool(
                toolset=self,
                tool_def=tool_def,
                max_retries=self.max_retries,
                args_validator=self.processors[tool_def.name].validator,
            )
            for tool_def in self._tool_defs
        }

    async def call_tool(
        self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]
    ) -> Any:
        output = await self.processors[name].call(tool_args, ctx, wrap_validation_errors=False)
        for validator in self.output_validators:
            output = await validator.validate(output, ctx, wrap_validation_errors=False)
        return output

## pydantic_ai_slim/pydantic_ai/_utils.py

class Some(Generic[T]):
    """Analogous to Rust's `Option::Some` type."""

    value: T

def is_set(t_or_unset: T | Unset) -> TypeGuard[T]:
    return t_or_unset is not UNSET

## pydantic_ai_slim/pydantic_ai/agent/wrapper.py

    def __init__(self, wrapped: AbstractAgent[AgentDepsT, OutputDataT]):
        self.wrapped = wrapped

    def model(self) -> models.Model | models.KnownModelName | str | None:
        return self.wrapped.model

    def name(self, value: str | None) -> None:
        self.wrapped.name = value

    def deps_type(self) -> type:
        return self.wrapped.deps_type

    def output_type(self) -> OutputSpec[OutputDataT]:
        return self.wrapped.output_type

    def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:
        return self.wrapped.event_stream_handler

    def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:
        return self.wrapped.toolsets

    async def __aenter__(self) -> AbstractAgent[AgentDepsT, OutputDataT]:
        return await self.wrapped.__aenter__()

    async def __aexit__(self, *args: Any) -> bool | None:
        return await self.wrapped.__aexit__(*args)

    def output_json_schema(self, output_type: OutputSpec[OutputDataT | RunOutputDataT] | None = None) -> JsonSchema:
        return self.wrapped.output_json_schema(output_type=output_type)

## pydantic_ai_slim/pydantic_ai/direct.py

    def get(self) -> messages.ModelResponse:
        """Build a ModelResponse from the data received from the stream so far."""
        return self._ensure_stream_ready().get()

## pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py

    def model(self) -> Model:
        return self._model

    def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:
        with self._dbos_overrides():
            return super().toolsets

    def _dbos_overrides(self) -> Iterator[None]:
        # Override with DBOSModel and DBOSMCPServer in the toolsets.
        # Use the configured parallel execution mode for deterministic event ordering during DBOS replay.
        with (
            super().override(model=self._model, toolsets=self._toolsets, tools=[]),
            self.parallel_tool_call_execution_mode(self._parallel_execution_mode),
        ):
            yield

## pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp.py

    def tool_for_tool_def(self, tool_def: ToolDefinition) -> ToolsetTool[AgentDepsT]:
        raise NotImplementedError

    def id(self) -> str | None:
        return self.wrapped.id

    async def __aexit__(self, *args: Any) -> bool | None:
        return None

    def visit_and_replace(
        self, visitor: Callable[[AbstractToolset[AgentDepsT]], AbstractToolset[AgentDepsT]]
    ) -> AbstractToolset[AgentDepsT]:
        # DBOS-ified toolsets cannot be swapped out after the fact.
        return self

    async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:
        tool_defs = await self._dbos_wrapped_get_tools_step(ctx)
        return {name: self.tool_for_tool_def(tool_def) for name, tool_def in tool_defs.items()}

## pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py

    def model(self) -> Model:
        return self._model

    def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:
        with self._prefect_overrides():
            return super().toolsets

    def _prefect_overrides(self) -> Iterator[None]:
        # Override with PrefectModel and PrefectMCPServer in the toolsets.
        with super().override(model=self._model, toolsets=self._toolsets, tools=[]):
            yield

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py

    def model(self) -> Model:
        return self._temporal_model

    def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:
        with self._temporal_overrides(force=True):
            return super().toolsets

    def temporal_activities(self) -> list[Callable[..., Any]]:
        return self._temporal_activities

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_mcp.py

    def tool_for_tool_def(self, tool_def: ToolDefinition) -> ToolsetTool[AgentDepsT]:
        raise NotImplementedError

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py

    def deserialize_run_context(cls, ctx: dict[str, Any], deps: Any) -> TemporalRunContext[Any]:
        """Deserialize the run context from a `dict[str, Any]`."""
        return cls(**ctx, deps=deps)

## pydantic_ai_slim/pydantic_ai/messages.py

    def user_text_prompt(cls, user_prompt: str, *, instructions: str | None = None) -> ModelRequest:
        """Create a `ModelRequest` with a single user prompt as text."""
        return cls(parts=[UserPromptPart(user_prompt)], instructions=instructions)

## pydantic_ai_slim/pydantic_ai/models/test.py

    def gen_tool_args(self, tool_def: ToolDefinition) -> Any:
        return _JsonSchemaTestData(tool_def.parameters_json_schema, self.seed).generate()

## pydantic_ai_slim/pydantic_ai/output.py

    json_schema: ObjectJsonSchema

## pydantic_ai_slim/pydantic_ai/retries.py

    async def aclose(self) -> None:
        await self.wrapped.aclose()

## pydantic_ai_slim/pydantic_ai/run.py

    def _node_to_task(self, node: _agent_graph.AgentNode[AgentDepsT, OutputDataT]) -> GraphTaskRequest:
        return GraphTaskRequest(NodeStep(type(node)).id, inputs=node, fork_stack=())

## pydantic_ai_slim/pydantic_ai/toolsets/_dynamic.py

    def id(self) -> str | None:
        return self._id

    def copy(self) -> DynamicToolset[AgentDepsT]:
        """Create a copy of this toolset for use in a new agent run."""
        return DynamicToolset(
            self.toolset_func,
            per_run_step=self.per_run_step,
            id=self._id,
        )

    async def __aenter__(self) -> Self:
        return self

    async def __aexit__(self, *args: Any) -> bool | None:
        try:
            result = None
            if self._toolset is not None:
                result = await self._toolset.__aexit__(*args)
        finally:
            self._toolset = None
            self._run_step = None
        return result

    async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:
        if self._toolset is None or (self.per_run_step and ctx.run_step != self._run_step):
            if self._toolset is not None:
                await self._toolset.__aexit__()

            toolset = self.toolset_func(ctx)
            if inspect.isawaitable(toolset):
                toolset = await toolset

            if toolset is not None:
                await toolset.__aenter__()

            self._toolset = toolset
            self._run_step = ctx.run_step

        if self._toolset is None:
            return {}

        return await self._toolset.get_tools(ctx)

## pydantic_ai_slim/pydantic_ai/toolsets/abstract.py

    def validate_python(
        self, input: Any, *, allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False, **kwargs: Any
    ) -> Any: ...

## pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py

    def id(self) -> str | None:
        return self._id

    async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:
        async with self:
            return {
                mcp_tool.name: self.tool_for_tool_def(
                    ToolDefinition(
                        name=mcp_tool.name,
                        description=mcp_tool.description,
                        parameters_json_schema=mcp_tool.inputSchema,
                        metadata={
                            'meta': mcp_tool.meta,
                            'annotations': mcp_tool.annotations.model_dump() if mcp_tool.annotations else None,
                            'output_schema': mcp_tool.outputSchema or None,
                        },
                    )
                )
                for mcp_tool in await self.client.list_tools()
            }

    def tool_for_tool_def(self, tool_def: ToolDefinition) -> ToolsetTool[AgentDepsT]:
        return ToolsetTool[AgentDepsT](
            tool_def=tool_def,
            toolset=self,
            max_retries=self.max_retries,
            args_validator=TOOL_SCHEMA_VALIDATOR,
        )

## pydantic_ai_slim/pydantic_ai/toolsets/filtered.py

    async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:
        return {
            name: tool for name, tool in (await super().get_tools(ctx)).items() if self.filter_func(ctx, tool.tool_def)
        }

## pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter.py

    def label(self) -> str:
        """Return the label for this toolset."""
        return 'the AG-UI frontend tools'  # pragma: no cover

    def build_run_input(cls, body: bytes) -> RunAgentInput:
        """Build an AG-UI run input object from the request body."""
        return RunAgentInput.model_validate_json(body)

    def build_event_stream(self) -> UIEventStream[RunAgentInput, BaseEvent, AgentDepsT, OutputDataT]:
        """Build an AG-UI event stream transformer."""
        return AGUIEventStream(self.run_input, accept=self.accept)

    def messages(self) -> list[ModelMessage]:
        """Pydantic AI messages from the AG-UI run input."""
        return self.load_messages(self.run_input.messages)

    def toolset(self) -> AbstractToolset[AgentDepsT] | None:
        """Toolset representing frontend tools from the AG-UI run input."""
        if self.run_input.tools:
            return _AGUIFrontendToolset[AgentDepsT](self.run_input.tools)
        return None

## pydantic_evals/pydantic_evals/otel/span_tree.py

    def first_child(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:
        """Return the first immediate child that satisfies the given predicate, or None if none match."""
        return next(self._filter_children(predicate), None)

    def find(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:
        """Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order."""
        return list(self._filter(predicate))

    def first(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:
        """Find the first node that matches a predicate, scanning from each root in DFS order. Returns `None` if not found."""
        return next(self._filter(predicate), None)

    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## tests/conftest.py

    def set(self, name: str, value: str) -> None:
        self.envars[name] = os.getenv(name)
        os.environ[name] = value

    def reset(self) -> None:
        for name, value in self.envars.items():
            if value is None:
                os.environ.pop(name, None)
            else:
                os.environ[name] = value  # pragma: lax no cover

## tests/evals/test_otel.py

async def test_span_query_logical_combinations():
    """Test logical combinations (AND/OR) in SpanQuery."""

    with context_subtree() as tree:
        with logfire.span('root1', level='0'):
            with logfire.span('child1', level='1', category='important'):
                pass
            with logfire.span('child2', level='1', category='normal'):
                pass
            with logfire.span('special', level='1', category='important', priority='high'):
                pass
    assert isinstance(tree, SpanTree)

    # Test AND logic
    and_query: SpanQuery = {'and_': [{'name_contains': '1'}, {'has_attributes': {'level': '1'}}]}
    matched_nodes = list(tree.find(and_query))
    assert len(matched_nodes) == 1, matched_nodes
    assert all(node.name in ['child1'] for node in matched_nodes)

    # Test OR logic
    or_query: SpanQuery = {'or_': [{'name_contains': '2'}, {'has_attributes': {'level': '0'}}]}
    matched_nodes = list(tree.find(or_query))
    assert len(matched_nodes) == 2
    assert any(node.name == 'child2' for node in matched_nodes)
    assert any(node.attributes.get('level') == '0' for node in matched_nodes)

    # Test complex combination (AND + OR)
    complex_query: SpanQuery = {
        'and_': [
            {'has_attributes': {'level': '1'}},
            {'or_': [{'has_attributes': {'category': 'important'}}, {'name_equals': 'child2'}]},
        ]
    }
    matched_nodes = list(tree.find(complex_query))
    assert len(matched_nodes) == 3  # child1, child2, special
    matched_names = [node.name for node in matched_nodes]
    assert set(matched_names) == {'child1', 'child2', 'special'}

## tests/graph/beta/test_broadcast_and_spread.py

    values: list[int] = field(default_factory=list[int])

## tests/models/test_model_test.py

def test_different_content_input(content: AudioUrl | VideoUrl | ImageUrl | BinaryContent):
    agent = Agent()
    result = agent.run_sync(['x', content], model=TestModel(custom_output_text='custom'))
    assert result.output == snapshot('custom')
    assert result.usage() == snapshot(RunUsage(requests=1, input_tokens=51, output_tokens=1))

## tests/test_dbos.py

def now_func() -> datetime:
    return datetime.now()

## tests/test_mcp.py

async def test_stdio_server_with_cwd(run_context: RunContext[int]):
    test_dir = Path(__file__).parent
    server = MCPServerStdio('python', ['mcp_server.py'], cwd=test_dir)
    async with server:
        tools = await server.get_tools(run_context)
        assert len(tools) == snapshot(20)

## tests/test_prefect.py

def conditions(city: str) -> str:
    # Simplified version without RunContext
    return "It's raining"

## tests/test_temporal.py

    async def run(self, prompt: str) -> None:
        with simple_temporal_agent.override(toolsets=[FunctionToolset()]):
            pass

    async def decode(
        self, payloads: Sequence[temporalio.api.common.v1.Payload]
    ) -> list[temporalio.api.common.v1.Payload]:  # pragma: no cover
        return list(payloads)

## tests/test_tools.py

def test_output_type_deferred_tool_requests_by_itself():
    with pytest.raises(UserError, match='At least one output type must be provided other than `DeferredToolRequests`.'):
        Agent(TestModel(), output_type=DeferredToolRequests)

def test_output_type_empty():
    with pytest.raises(UserError, match='At least one output type must be provided.'):
        Agent(TestModel(), output_type=[])

## tests/test_ui.py

class DummyUIRunInput(BaseModel):
    messages: list[ModelMessage] = field(default_factory=list[ModelMessage])
    tool_defs: list[ToolDefinition] = field(default_factory=list[ToolDefinition])
    state: dict[str, Any] = field(default_factory=dict[str, Any])

    def build_run_input(cls, body: bytes) -> DummyUIRunInput:
        return DummyUIRunInput.model_validate_json(body)

    def dump_messages(cls, messages: Sequence[ModelMessage]) -> list[ModelMessage]:
        return list(messages)

    def load_messages(cls, messages: Sequence[ModelMessage]) -> list[ModelMessage]:
        return list(messages)

    def build_event_stream(self) -> UIEventStream[DummyUIRunInput, str, AgentDepsT, OutputDataT]:
        return DummyUIEventStream[AgentDepsT, OutputDataT](self.run_input, accept=self.accept)

    def messages(self) -> list[ModelMessage]:
        return self.load_messages(self.run_input.messages)

    def state(self) -> dict[str, Any] | None:
        return self.run_input.state

    def toolset(self) -> AbstractToolset[AgentDepsT] | None:
        return ExternalToolset(self.run_input.tool_defs) if self.run_input.tool_defs else None
