## docs/.hooks/main.py

bundle_path: Path | None = None

## docs/.hooks/test_snippets.py

def test_extract_fragment_content_specific_section():
    """Test extracting specific section."""
    content = """line 1
### [section1]
content 1
content 2
### [/section1]
line 6"""

    with temp_text_file(content) as temp_path:
        parsed = parse_file_sections(temp_path)

    assert parsed.render([], []) == snapshot(
        RenderedSnippet(
            content="""\
line 1
content 1
content 2
line 6\
""",
            highlights=[],
            original_range=LineRange(start_line=0, end_line=6),
        )
    )

    assert parsed.render(['section1'], []) == snapshot(
        RenderedSnippet(
            content="""\
content 1
content 2

...\
""",
            highlights=[],
            original_range=LineRange(start_line=2, end_line=4),
        )
    )

    assert parsed.render([], ['section1']) == snapshot(
        RenderedSnippet(
            content="""\
line 1
content 1
content 2
line 6\
""",
            highlights=[LineRange(start_line=1, end_line=3)],
            original_range=LineRange(start_line=0, end_line=6),
        )
    )

## examples/pydantic_ai_examples/chat_app.py

async def lifespan(_app: fastapi.FastAPI):
    async with Database.connect() as db:
        yield {'db': db}

## examples/pydantic_ai_examples/rag.py

    path: str

## pydantic_ai_slim/pydantic_ai/_a2a.py

async def worker_lifespan(
    app: FastA2A, worker: Worker, agent: AbstractAgent[AgentDepsT, OutputDataT]
) -> AsyncIterator[None]:
    """Custom lifespan that runs the worker during application startup.

    This ensures the worker is started and ready to process tasks as soon as the application starts.
    """
    async with app.task_manager, agent:
        async with worker.run():
            yield

## pydantic_ai_slim/pydantic_ai/_function_schema.py

    takes_ctx: bool

## pydantic_ai_slim/pydantic_ai/_griffe.py

def _disable_griffe_logging():
    # Hacky, but suggested here: https://github.com/mkdocstrings/griffe/issues/293#issuecomment-2167668117
    old_level = logging.root.getEffectiveLevel()
    logging.root.setLevel(logging.ERROR)
    yield
    logging.root.setLevel(old_level)

## pydantic_ai_slim/pydantic_ai/_output.py

    _takes_ctx: bool = field(init=False)

## pydantic_ai_slim/pydantic_ai/_ssrf.py

    path: str

## pydantic_ai_slim/pydantic_ai/_system_prompt.py

    _takes_ctx: bool = field(init=False)

## pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_toolset.py

def prefectify_toolset(
    toolset: AbstractToolset[AgentDepsT],
    mcp_task_config: TaskConfig,
    tool_task_config: TaskConfig,
    tool_task_config_by_name: dict[str, TaskConfig | None],
) -> AbstractToolset[AgentDepsT]:
    """Wrap a toolset to integrate it with Prefect.

    Args:
        toolset: The toolset to wrap.
        mcp_task_config: The Prefect task config to use for MCP server tasks.
        tool_task_config: The default Prefect task config to use for tool calls.
        tool_task_config_by_name: Per-tool task configuration. Keys are tool names, values are TaskConfig or None.
    """
    if isinstance(toolset, FunctionToolset):
        from ._function_toolset import PrefectFunctionToolset

        return PrefectFunctionToolset(
            wrapped=toolset,
            task_config=tool_task_config,
            tool_task_config=tool_task_config_by_name,
        )

    try:
        from pydantic_ai.mcp import MCPServer

        from ._mcp_server import PrefectMCPServer
    except ImportError:
        pass
    else:
        if isinstance(toolset, MCPServer):
            return PrefectMCPServer(
                wrapped=toolset,
                task_config=mcp_task_config,
            )

    return toolset

## pydantic_ai_slim/pydantic_ai/messages.py

def _multi_modal_content_identifier(identifier: str | bytes) -> str:
    """Generate stable identifier for multi-modal content to help LLM in finding a specific file in tool call responses."""
    if isinstance(identifier, str):
        identifier = identifier.encode('utf-8')
    return hashlib.sha1(identifier).hexdigest()[:6]

## pydantic_ai_slim/pydantic_ai/models/bedrock.py

    bedrock_additional_model_response_fields_paths: list[str]

    bedrock_service_tier: ServiceTierTypeDef

## pydantic_ai_slim/pydantic_ai/models/gemini.py

class _GeminiFileDataPart(_BasePart):
    file_data: Annotated[_GeminiFileData, pydantic.Field(alias='fileData')]

class _GeminiFunctionCall(TypedDict):
    """See <https://ai.google.dev/api/caching#FunctionCall>."""

    name: str
    args: dict[str, Any]

class _GeminiFunction(TypedDict):
    name: str
    description: str
    parameters_json_schema: NotRequired[dict[str, Any]]

class _GeminiFunctionCallingConfig(TypedDict):
    mode: Literal['ANY', 'AUTO']
    allowed_function_names: list[str]

## pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py

class MCPSamplingModelSettings(ModelSettings, total=False):
    """Settings used for an MCP Sampling model request."""

    # ALL FIELDS MUST BE `mcp_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    mcp_model_preferences: ModelPreferences
    """Model preferences to use for MCP Sampling."""

## pydantic_ai_slim/pydantic_ai/models/openai.py

    openai_service_tier: Literal['auto', 'default', 'flex', 'priority']

    openai_include_code_execution_outputs: bool

## pydantic_ai_slim/pydantic_ai/models/xai.py

    xai_include_code_execution_output: bool

## pydantic_ai_slim/pydantic_ai/tools.py

    takes_ctx: bool

## pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py

def _stringify(value: Any) -> str:
    if isinstance(value, str):
        return value
    try:
        # If the value can be serialized to JSON, use that.
        # If that behavior is undesirable, the user could manually call repr on the arguments to the judge_* functions
        return to_json(value).decode()
    except Exception:
        return repr(value)

## pydantic_evals/pydantic_evals/reporting/render_numbers.py

def default_render_duration_diff(old: float, new: float) -> str | None:
    """Format a duration difference (in seconds) with an explicit sign."""
    if old == new:
        return None

    abs_diff_str = _render_duration(new - old, True)
    rel_diff_str = _render_relative(new, old, BASE_THRESHOLD)
    if rel_diff_str is None:
        return abs_diff_str
    else:
        return f'{abs_diff_str} / {rel_diff_str}'

## pydantic_graph/pydantic_graph/beta/decision.py

    path: Path

    _path_builder: PathBuilder[StateT, DepsT, OutputT]

## pydantic_graph/pydantic_graph/beta/paths.py

    path: Path

## tests/conftest.py

def raise_if_exception(e: Any) -> None:
    if isinstance(e, Exception):
        raise e

def assets_path() -> Path:
    return Path(__file__).parent / 'assets'

## tests/evals/test_render_numbers.py

def test_default_render_number_diff(old: int | float, new: int | float, expected: str | None):
    assert default_render_number_diff(old, new) == expected

def test_default_render_duration_diff(old: float, new: float, expected: str | None):
    assert default_render_duration_diff(old, new) == expected

## tests/graph/beta/test_broadcast_and_spread.py

async def test_broadcast_with_different_outputs():
    """Test that broadcasts can produce different types of outputs."""
    g = GraphBuilder(state_type=CounterState, output_type=list[int | str])

    @g.step
    async def source(ctx: StepContext[CounterState, None, None]) -> int:
        return 42

    @g.step
    async def return_int(ctx: StepContext[CounterState, None, int]) -> int:
        return ctx.inputs

    @g.step
    async def return_str(ctx: StepContext[CounterState, None, int]) -> str:
        return str(ctx.inputs)

    collect = g.join(reduce_list_append, initial_factory=list[int | str])

    g.add(
        g.edge_from(g.start_node).to(source),
        g.edge_from(source).to(return_int, return_str),
        g.edge_from(return_int, return_str).to(collect),
        g.edge_from(collect).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=CounterState())
    # Order may vary
    assert set(result) == {42, '42'}

## tests/graph/beta/test_decisions.py

    path_taken: str | None = None

async def test_decision_with_state_modification():
    """Test that decision branches can modify state."""
    g = GraphBuilder(state_type=DecisionState, output_type=int)

    @g.step
    async def get_value(ctx: StepContext[DecisionState, None, None]) -> int:
        return 5

    @g.step
    async def small_value(ctx: StepContext[DecisionState, None, int]) -> int:
        ctx.state.path_taken = 'small'
        return ctx.inputs * 2

    @g.step
    async def large_value(ctx: StepContext[DecisionState, None, int]) -> int:  # pragma: no cover
        ctx.state.path_taken = 'large'
        return ctx.inputs * 10

    g.add(
        g.edge_from(g.start_node).to(get_value),
        g.edge_from(get_value).to(
            g.decision()
            .branch(g.match(TypeExpression[int], matches=lambda x: x < 10).to(small_value))
            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 10).to(large_value))
        ),
        g.edge_from(small_value, large_value).to(g.end_node),
    )

    graph = g.build()
    state = DecisionState()
    result = await graph.run(state=state)
    assert result == 10
    assert state.path_taken == 'small'

## tests/graph/beta/test_edge_cases.py

async def test_step_that_modifies_deps():
    """Test that deps modifications don't persist (deps should be immutable)."""

    @dataclass
    class MutableDeps:
        value: int

    g = GraphBuilder(state_type=EdgeCaseState, deps_type=MutableDeps, output_type=int)

    @g.step
    async def try_modify_deps(ctx: StepContext[EdgeCaseState, MutableDeps, None]) -> int:
        original = ctx.deps.value
        # Attempt to modify (this DOES mutate the object, but that's user error)
        ctx.deps.value = 999
        return original

    @g.step
    async def check_deps(ctx: StepContext[EdgeCaseState, MutableDeps, int]) -> int:
        # Deps will show the mutation since it's the same object
        return ctx.deps.value

    g.add(
        g.edge_from(g.start_node).to(try_modify_deps),
        g.edge_from(try_modify_deps).to(check_deps),
        g.edge_from(check_deps).to(g.end_node),
    )

    graph = g.build()
    deps = MutableDeps(value=42)
    result = await graph.run(state=EdgeCaseState(), deps=deps)
    # The deps object was mutated (user responsibility to avoid this)
    assert result == 999
    assert deps.value == 999

## tests/models/test_mistral.py

async def test_stream_result_type_primitif_dict(allow_model_requests: None):
    """This test tests the primitif result with the pydantic ai format model response"""

    class MyTypedDict(TypedDict, total=False):
        first: str
        second: str

    stream = [
        text_chunk('{'),
        text_chunk('"'),
        text_chunk('f'),
        text_chunk('i'),
        text_chunk('r'),
        text_chunk('s'),
        text_chunk('t'),
        text_chunk('"'),
        text_chunk(':'),
        text_chunk(' '),
        text_chunk('"'),
        text_chunk('O'),
        text_chunk('n'),
        text_chunk('e'),
        text_chunk('"'),
        text_chunk(','),
        text_chunk(' '),
        text_chunk('"'),
        text_chunk('s'),
        text_chunk('e'),
        text_chunk('c'),
        text_chunk('o'),
        text_chunk('n'),
        text_chunk('d'),
        text_chunk('"'),
        text_chunk(':'),
        text_chunk(' '),
        text_chunk('"'),
        text_chunk('T'),
        text_chunk('w'),
        text_chunk('o'),
        text_chunk('"'),
        text_chunk('}'),
        chunk([]),
    ]

    mock_client = MockMistralAI.create_stream_mock(stream)
    model = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(model=model, output_type=MyTypedDict)

    async with agent.run_stream('User prompt value') as result:
        assert not result.is_complete
        v = [c async for c in result.stream_output(debounce_by=None)]
        assert v == snapshot(
            [
                {'first': 'O'},
                {'first': 'On'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One', 'second': ''},
                {'first': 'One', 'second': 'T'},
                {'first': 'One', 'second': 'Tw'},
                {'first': 'One', 'second': 'Two'},
                {'first': 'One', 'second': 'Two'},
                {'first': 'One', 'second': 'Two'},
                {'first': 'One', 'second': 'Two'},
            ]
        )
        assert result.is_complete
        assert result.usage().input_tokens == 34
        assert result.usage().output_tokens == 34

        # double check usage matches stream count
        assert result.usage().output_tokens == len(stream)

## tests/models/test_model_names.py

def modify_response(response: dict[str, Any], filter_headers: list[str]) -> dict[str, Any]:  # pragma: lax no cover
    for header in response['headers'].copy():
        assert isinstance(header, str)
        if header.lower() in filter_headers:
            del response['headers'][header]
    return response

## tests/models/test_model_test.py

def test_different_content_input(content: AudioUrl | VideoUrl | ImageUrl | BinaryContent):
    agent = Agent()
    result = agent.run_sync(['x', content], model=TestModel(custom_output_text='custom'))
    assert result.output == snapshot('custom')
    assert result.usage() == snapshot(RunUsage(requests=1, input_tokens=51, output_tokens=1))

## tests/providers/test_google_vertex.py

def save_service_account(service_account_path: Path, project_id: str) -> None:
    service_account = prepare_service_account_contents(project_id)

    service_account_path.write_text(json.dumps(service_account, indent=2), encoding='utf-8')

## tests/test_a2a.py

async def test_a2a_runtime_error_without_lifespan():
    agent = Agent(model=model, output_type=tuple[str, str])
    app = agent.to_a2a()

    transport = httpx.ASGITransport(app)
    async with httpx.AsyncClient(transport=transport) as http_client:
        a2a_client = A2AClient(http_client=http_client)

        message = Message(
            role='user',
            parts=[TextPart(text='Hello, world!', kind='text')],
            kind='message',
            message_id=str(uuid.uuid4()),
        )

        with pytest.raises(RuntimeError, match='TaskManager was not properly initialized.'):
            await a2a_client.send_message(message=message)

## tests/test_dbos.py

class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

## tests/test_prefect.py

class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

## tests/test_settings.py

def test_specific_prefix_settings(settings: tuple[type[ModelSettings], str]):
    settings_cls, prefix = settings
    global_settings = set(ModelSettings.__annotations__.keys())
    specific_settings = set(settings_cls.__annotations__.keys()) - global_settings
    assert all(setting.startswith(prefix) for setting in specific_settings), (
        f'{prefix} is not a prefix for {specific_settings}'
    )

## tests/test_temporal.py

class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

## tests/test_tools.py

def test_suppress_griffe_logging(caplog: LogCaptureFixture):
    # This would cause griffe to emit a warning log if we didn't suppress the griffe logging.

    agent = Agent(FunctionModel(get_json_schema))
    agent.tool_plain(tool_without_return_annotation_in_docstring)

    result = agent.run_sync('')
    json_schema = json.loads(result.output)
    assert json_schema == snapshot(
        {
            'description': "A tool that documents what it returns but doesn't have a return annotation in the docstring.",
            'name': 'tool_without_return_annotation_in_docstring',
            'outer_typed_dict_key': None,
            'parameters_json_schema': {'additionalProperties': False, 'properties': {}, 'type': 'object'},
            'strict': None,
            'kind': 'function',
            'sequential': False,
            'metadata': None,
            'timeout': None,
        }
    )

    # Without suppressing griffe logging, we get:
    # assert caplog.messages == snapshot(['<module>:4: No type or annotation for returned value 1'])
    assert caplog.messages == snapshot([])
