## docs/.hooks/snippets.py

class RenderedSnippet:
    content: str
    highlights: list[LineRange]
    original_range: LineRange

## examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py

from pydantic import BaseModel, Field

## examples/pydantic_ai_examples/chat_app.py

async def get_chat(database: Database = Depends(get_db)) -> Response:
    msgs = await database.get_messages()
    return Response(
        b'\n'.join(json.dumps(to_chat_message(m)).encode('utf-8') for m in msgs),
        media_type='text/plain',
    )

## examples/pydantic_ai_examples/evals/agent.py

async def infer_time_range(inputs: TimeRangeInputs) -> TimeRangeResponse:
    """Infer a time range from a user prompt."""
    deps = TimeRangeDeps(now=inputs['now'])
    return (await time_range_agent.run(inputs['prompt'], deps=deps)).output

## examples/pydantic_ai_examples/flight_booking.py

class Deps:
    web_page_text: str
    req_origin: str
    req_destination: str
    req_date: datetime.date

async def buy_tickets(flight_details: FlightDetails, seat: SeatPreference):
    print(f'Purchasing flight {flight_details=!r} {seat=!r}...')

## examples/pydantic_ai_examples/question_graph.py

async def run_as_continuous():
    state = QuestionState()
    node = Ask()
    end = await question_graph.run(node, state=state)
    print('END:', end.output)

## examples/pydantic_ai_examples/rag.py

def slugify(value: str, separator: str, unicode: bool = False) -> str:
    """Slugify a string, to make it URL friendly."""
    # Taken unchanged from https://github.com/Python-Markdown/markdown/blob/3.7/markdown/extensions/toc.py#L38
    if not unicode:
        # Replace Extended Latin characters with ASCII, i.e. `žlutý` => `zluty`
        value = unicodedata.normalize('NFKD', value)
        value = value.encode('ascii', 'ignore').decode('ascii')
    value = re.sub(r'[^\w\s-]', '', value).strip().lower()
    return re.sub(rf'[{separator}\s]+', separator, value)

## examples/pydantic_ai_examples/slack_lead_qualifier/agent.py

async def analyze_profile(profile: Profile) -> Analysis | None:
    result = await agent.run(profile.as_prompt())
    return result.output  ### [/analyze_profile]

## pydantic_ai_slim/pydantic_ai/_json_schema.py

from abc import ABC, abstractmethod

## pydantic_ai_slim/pydantic_ai/_otel_messages.py

    type: Literal['text']

## pydantic_ai_slim/pydantic_ai/_utils.py

def generate_tool_call_id() -> str:
    """Generate a tool call id.

    Ensure that the tool call id is unique.
    """
    return f'pyd_ai_{uuid.uuid4().hex}'

## pydantic_ai_slim/pydantic_ai/agent/__init__.py

    instrument: InstrumentationSettings | bool | None

## pydantic_ai_slim/pydantic_ai/format_prompt.py

    def _set_scalar_text(self, element: ElementTree.Element, value: Any) -> bool:
        """Set element.text for scalar types. Return True if handled, False otherwise."""
        if value is None:
            element.text = self.none_str
        elif isinstance(value, str):
            element.text = value
        elif isinstance(value, bytes | bytearray):
            element.text = value.decode(errors='ignore')
        elif isinstance(value, bool | int | float | Enum):
            element.text = str(value)
        elif isinstance(value, date | time):
            element.text = value.isoformat()
        elif isinstance(value, timedelta):
            element.text = str(value)
        elif isinstance(value, Decimal):
            element.text = str(value)
        elif isinstance(value, UUID):
            element.text = str(value)
        else:
            return False
        return True

## pydantic_ai_slim/pydantic_ai/messages.py

def _multi_modal_content_identifier(identifier: str | bytes) -> str:
    """Generate stable identifier for multi-modal content to help LLM in finding a specific file in tool call responses."""
    if isinstance(identifier, str):
        identifier = identifier.encode('utf-8')
    return hashlib.sha1(identifier).hexdigest()[:6]

## pydantic_ai_slim/pydantic_ai/models/google.py

def _map_executable_code(executable_code: ExecutableCode, provider_name: str, tool_call_id: str) -> BuiltinToolCallPart:
    return BuiltinToolCallPart(
        provider_name=provider_name,
        tool_name=CodeExecutionTool.kind,
        args=executable_code.model_dump(mode='json'),
        tool_call_id=tool_call_id,
    )

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py

def _json_dumps(obj: Any) -> str:
    """Dump an object to JSON string."""
    return to_json(obj).decode('utf-8')

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_models.py

class CamelBaseModel(BaseModel, ABC):
    """Base model with camelCase aliases."""

    model_config = ConfigDict(alias_generator=to_camel, populate_by_name=True, extra='forbid')

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py

class BaseChunk(CamelBaseModel, ABC):
    """Abstract base class for response SSE events."""

    def encode(self, sdk_version: int) -> str:
        return self.model_dump_json(by_alias=True, exclude_none=True)

    def encode(self, sdk_version: int) -> str:
        return self.model_dump_json(by_alias=True, exclude_none=True)

## pydantic_evals/pydantic_evals/_utils.py

class Unset:
    """A singleton to represent an unset value.

    Used to distinguish between explicitly set `None` values and values that were never set.

    Copied from pydantic_ai/_utils.py.
    """

    pass

def is_set(t_or_unset: T | Unset) -> TypeIs[T]:
    """Check if a value is set (not the UNSET singleton).

    Args:
        t_or_unset: The value to check, which may be the UNSET singleton or a regular value.

    Returns:
        True if the value is not UNSET, narrowing the type to T in a type-aware way.
    """
    return t_or_unset is not UNSET

## pydantic_evals/pydantic_evals/reporting/__init__.py

    def failures_table(
        self,
        *,
        include_input: bool = False,
        include_metadata: bool = False,
        include_expected_output: bool = False,
        include_error_message: bool = True,
        include_error_stacktrace: bool = True,
        input_config: RenderValueConfig | None = None,
        metadata_config: RenderValueConfig | None = None,
    ) -> Table:
        """Return a table containing the failures in this report."""
        renderer = EvaluationRenderer(
            include_input=include_input,
            include_metadata=include_metadata,
            include_expected_output=include_expected_output,
            include_output=False,
            include_durations=False,
            include_total_duration=False,
            include_removed_cases=False,
            include_averages=False,
            input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},
            metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},
            output_config=_DEFAULT_VALUE_CONFIG,
            score_configs={},
            label_configs={},
            metric_configs={},
            duration_config=_DEFAULT_DURATION_CONFIG,
            include_reasons=False,
            include_error_message=include_error_message,
            include_error_stacktrace=include_error_stacktrace,
            include_evaluator_failures=False,  # Not applicable for failures table
        )
        return renderer.build_failures_table(self)

    value_formatter: str | Callable[[Any], str]

    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

    include_scores: bool

    include_labels: bool

    include_metrics: bool

    include_assertions: bool

## pydantic_graph/pydantic_graph/_utils.py

class Unset:
    """A singleton to represent an unset value.

    Copied from pydantic_ai/_utils.py.
    """

    pass

def is_set(t_or_unset: T | Unset) -> TypeIs[T]:
    return t_or_unset is not UNSET

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## tests/conftest.py

def text_document_content(assets_path: Path) -> BinaryContent:
    content = assets_path.joinpath('dummy.txt').read_text(encoding='utf-8')
    bin_content = BinaryContent(data=content.encode(), media_type='text/plain')
    return bin_content

## tests/evals/test_report_evaluators.py

def _make_report(cases: list[ReportCase]) -> EvaluationReport:
    return EvaluationReport(name='test', cases=cases)

## tests/example_modules/mcp_server.py

async def get_weather_forecast(location: str) -> str:
    """Get the weather forecast for a location."""
    return f'The weather in {location} is sunny and 26 degrees Celsius.'

## tests/graph/beta/test_edge_labels.py

class LabelState:
    value: int = 0

## tests/mcp_server.py

async def get_image_resource_link() -> ResourceLink:
    return ResourceLink(
        type='resource_link',
        uri=AnyUrl('resource://kiwi.jpg'),
        name='kiwi.jpeg',
    )

async def kiwi_resource() -> bytes:
    return Path(__file__).parent.joinpath('assets/kiwi.jpg').read_bytes()

async def get_audio_resource_link() -> ResourceLink:
    return ResourceLink(
        type='resource_link',
        uri=AnyUrl('resource://marcelo.mp3'),
        name='marcelo.mp3',
    )

async def marcelo_resource() -> bytes:
    return Path(__file__).parent.joinpath('assets/marcelo.mp3').read_bytes()

async def get_product_name() -> EmbeddedResource:
    return EmbeddedResource(
        type='resource',
        resource=TextResourceContents(
            uri=AnyUrl('resource://product_name.txt'),
            text='Pydantic AI',
        ),
    )

async def get_product_name_link() -> ResourceLink:
    return ResourceLink(
        type='resource_link',
        uri=AnyUrl('resource://product_name.txt'),
        name='product_name.txt',
    )

async def product_name_resource() -> str:
    return Path(__file__).parent.joinpath('assets/product_name.txt').read_text(encoding='utf-8')

async def greeting_resource_template(name: str) -> str:
    """Dynamic greeting resource template."""
    return f'Hello, {name}!'

async def get_image() -> Image:
    data = Path(__file__).parent.joinpath('assets/kiwi.jpg').read_bytes()
    return Image(data=data, format='jpg')

async def get_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

async def get_unstructured_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

async def get_error(value: bool = False):
    if value:
        return 'This is not an error'

    raise ValueError('This is an error. Call the tool with True instead')

async def get_none():
    return None

async def get_multiple_items():
    return [
        'This is a string',
        'Another string',
        {'foo': 'bar', 'baz': 123},
        await get_image(),
    ]

async def get_log_level(ctx: Context) -> str:  # type: ignore
    """Get the current log level.

    Returns:
        The current log level.
    """
    await ctx.info('this is a log message')
    return log_level

class UserResponse(BaseModel):
    response: str

async def use_elicitation(ctx: Context[ServerSession, None], question: str) -> str:
    """Use elicitation callback to ask the user a question."""

    result = await ctx.elicit(message=question, schema=UserResponse)

    if result.action == 'accept' and result.data:
        return f'User responded: {result.data.response}'
    else:
        return f'User {result.action}ed the elicitation'

async def hidden_tool() -> str:
    """A tool that is hidden by default."""
    return 'I was hidden!'

async def enable_hidden_tool(ctx: Context[ServerSession, None]) -> str:
    """Enable the hidden tool, triggering a ToolListChangedNotification."""
    mcp._tool_manager.add_tool(hidden_tool)  # pyright: ignore[reportPrivateUsage]
    await ctx.session.send_tool_list_changed()
    return 'Hidden tool enabled'

async def set_logging_level(level: str) -> None:
    global log_level
    log_level = level

## tests/models/test_download_item.py

async def test_download_item_raises_user_error_with_youtube_url() -> None:
    with pytest.raises(UserError, match='Downloading YouTube videos is not supported.'):
        _ = await download_item(VideoUrl(url='https://youtu.be/lCdaVNyHtjU'), data_format='bytes')

## tests/models/test_gemini.py

class AsyncByteStreamList(httpx.AsyncByteStream):
    data: list[bytes]

    async def __aiter__(self) -> AsyncIterator[bytes]:
        for chunk in self.data:
            yield chunk

def gemini_response(content: _GeminiContent, finish_reason: Literal['STOP'] | None = 'STOP') -> _GeminiResponse:
    candidate = _GeminiCandidates(content=content, index=0, safety_ratings=[])
    if finish_reason:  # pragma: no branch
        candidate['finish_reason'] = finish_reason
    return _GeminiResponse(candidates=[candidate], usage_metadata=example_usage(), model_version='gemini-1.5-flash-123')

## tests/models/test_model_function.py

def bar(ctx, x: int) -> str:  # pyright: ignore[reportUnknownParameterType,reportMissingParameterType]
    return str(x + 2)

async def test_pass_neither():
    with pytest.raises(TypeError, match='Either `function` or `stream_function` must be provided'):
        FunctionModel()  # pyright: ignore[reportCallIssue]

async def test_pass_both():
    Agent(FunctionModel(return_last, stream_function=stream_text_function))

async def test_return_empty():
    agent = Agent(FunctionModel(stream_function=stream_text_function_empty))
    with pytest.raises(ValueError, match='Stream function must return at least one item'):
        async with agent.run_stream(''):
            pass

## tests/test_dbos.py

async def test_dbos_agent_run(allow_model_requests: None, dbos: DBOS):
    # Note: this runs as a DBOS workflow because we automatically wrap the run function.
    result = await simple_dbos_agent.run('What is the capital of Mexico?')
    assert result.output == snapshot('The capital of Mexico is Mexico City.')

async def test_dbos_agent_run_in_workflow_with_toolsets(allow_model_requests: None, dbos: DBOS):
    # Since DBOS does not automatically wrap the tools in a workflow, and allows dynamic steps, we can pass in toolsets directly.
    result = await simple_dbos_agent.run('What is the capital of Mexico?', toolsets=[FunctionToolset()])
    assert result.output == snapshot('The capital of Mexico is Mexico City.')

async def test_custom_model_settings(allow_model_requests: None, dbos: DBOS):
    result = await settings_dbos_agent.run('Give me those settings')
    assert result.output == snapshot("{'max_tokens': 123, 'custom_setting': 'custom_value'}")

## tests/test_mcp.py

async def test_agent_with_server_not_running(agent: Agent, allow_model_requests: None):
    result = await agent.run('What is 0 degrees Celsius in Fahrenheit?')
    assert result.output == snapshot('0 degrees Celsius is 32.0 degrees Fahrenheit.')

## tests/test_parts_manager.py

def test_handle_thinking_delta_no_content():
    manager = ModelResponsePartsManager()

    with pytest.raises(UnexpectedModelBehavior, match='Cannot create a ThinkingPart with no content'):
        list(manager.handle_thinking_delta(vendor_part_id=None, content=None, signature=None))

## tests/test_prefect.py

async def test_prefect_agent_run(allow_model_requests: None) -> None:
    """Test that agent.run() works (auto-wrapped as flow)."""
    result = await simple_prefect_agent.run('What is the capital of Mexico?')
    assert result.output == snapshot('The capital of Mexico is Mexico City.')

async def test_prefect_agent_with_model_retry(allow_model_requests: None) -> None:
    """Test that ModelRetry works correctly."""
    result = await model_retry_prefect_agent.run('What is the weather in CDMX?')
    assert 'sunny' in result.output.lower() or 'mexico city' in result.output.lower()

## tests/test_tools.py

async def get_json_schema(_messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
    if len(info.function_tools) == 1:
        r = info.function_tools[0]
        return ModelResponse(parts=[TextPart(pydantic_core.to_json(r).decode())])
    else:
        return ModelResponse(parts=[TextPart(pydantic_core.to_json(info.function_tools).decode())])

def test_tool_name():
    agent = Agent(FunctionModel(get_json_schema))

    def my_tool(ctx: RunContext, arg: str) -> str: ...  # pragma: no branch

    agent.tool(name='foo_tool')(my_tool)
    result = agent.run_sync('Hello')
    json_schema = json.loads(result.output)
    assert json_schema['name'] == 'foo_tool'

## tests/test_toolsets.py

async def test_function_toolset_with_defaults_overridden():
    defaults_toolset = FunctionToolset[None](require_parameter_descriptions=True)

    @defaults_toolset.tool(require_parameter_descriptions=False)
    def subtract(a: int, b: int) -> int:
        """Subtract two numbers"""
        return a - b  # pragma: no cover

## tests/test_ui.py

class DummyUIRunInput(BaseModel):
    messages: list[ModelMessage] = field(default_factory=list[ModelMessage])
    tool_defs: list[ToolDefinition] = field(default_factory=list[ToolDefinition])
    state: dict[str, Any] = field(default_factory=dict[str, Any])

## tests/test_validation_context.py

class Value(BaseModel):
    x: int

    @field_validator('x')
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

## tests/typed_agent.py

def ok_tool_prepare(ctx: RunContext[MyDeps], x: int, y: str) -> str:
    return f'{ctx.deps.foo} {x} {y}'

def wrong_tool_prepare(ctx: RunContext[MyDeps], x: int, y: str) -> str:
    return f'{ctx.deps.foo} {x} {y}'

## tests/typed_graph.py

def use_double(node: BaseNode[None, None, X]) -> None:
    """Shoe that `Double` is valid as a `BaseNode[None, int, X]`."""
    print(node)
