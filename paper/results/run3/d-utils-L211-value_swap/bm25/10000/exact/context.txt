# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_graph/pydantic_graph/persistence/file.py:14-14
from .. import _utils as _graph_utils, exceptions

# pydantic_ai_slim/pydantic_ai/__init__.py:35-48
from .exceptions import (
    AgentRunError,
    ApprovalRequired,
    CallDeferred,
    ConcurrencyLimitExceeded,
    FallbackExceptionGroup,
    IncompleteToolCall,
    ModelAPIError,
    ModelHTTPError,
    ModelRetry,
    UnexpectedModelBehavior,
    UsageLimitExceeded,
    UserError,
)

# tests/test_validation_context.py:22-27
class Value(BaseModel):
    x: int

    @field_validator('x')
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# pydantic_ai_slim/pydantic_ai/_utils.py:143-143
    value: T

# pydantic_ai_slim/pydantic_ai/_utils.py:143-143
    value: T

# tests/test_validation_context.py:26-27
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

# pydantic_ai_slim/pydantic_ai/messages.py:21-21
from opentelemetry.util.types import AnyValue

# pydantic_evals/pydantic_evals/evaluators/common.py:32-32
    value: Any

# pydantic_evals/pydantic_evals/reporting/__init__.py:707-711
    def render_value(self, name: str | None, v: Any) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:10-10
from pydantic import JsonValue

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:15-15
JSONValue = Any

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:15-15
JSONValue = Any

# tests/graph/beta/test_graph_edge_cases.py:20-20
    value: int = 0

# tests/graph/beta/test_graph_edge_cases.py:20-20
    value: int = 0

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# pydantic_evals/pydantic_evals/evaluators/common.py:73-73
    value: Any

# tests/test_ag_ui.py:158-158
    value: int = 0

# pydantic_graph/pydantic_graph/beta/graph.py:86-87
    def value(self) -> OutputT:
        return self._value

# pydantic_evals/pydantic_evals/reporting/__init__.py:736-742
    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# tests/graph/beta/test_edge_labels.py:17-17
    value: int = 0

# tests/test_agent.py:3308-3308
    value: str

# tests/test_prefect.py:1195-1195
    value: str

# tests/test_agent.py:3308-3308
    value: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:693-693
    value_formatter: str | Callable[[Any], str] = '{}'

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:42-45
    value: Any = Field(
        default=None,
        description='The value to apply (for add, replace operations)',
    )

# pydantic_graph/pydantic_graph/beta/graph.py:78-78
    _value: OutputT

# tests/graph/beta/test_joins_and_reducers.py:24-24
    value: int = 0

# pydantic_evals/pydantic_evals/reporting/analyses.py:65-65
    value: float | int

# tests/test_temporal.py:2579-2579
    values: list[int] = field(default_factory=list[int])

# pydantic_evals/pydantic_evals/reporting/__init__.py:685-685
    value_formatter: str | Callable[[Any], str]

# tests/graph/beta/test_decisions.py:20-20
    value: int = 0

# tests/graph/beta/test_edge_cases.py:19-19
    value: int = 0

# tests/test_agent.py:114-114
    value: T

# tests/graph/beta/test_broadcast_and_spread.py:17-17
    values: list[int] = field(default_factory=list[int])

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:44-44
    value: EvaluationScalar

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:73-73
    value: EvaluationScalarT

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_evals/pydantic_evals/reporting/__init__.py:692-742
class _ValueRenderer:
    value_formatter: str | Callable[[Any], str] = '{}'
    diff_checker: Callable[[Any, Any], bool] | None = lambda x, y: x != y
    diff_formatter: Callable[[Any, Any], str | None] | None = None
    diff_style: str = 'magenta'

    @staticmethod
    def from_config(config: RenderValueConfig) -> _ValueRenderer:
        return _ValueRenderer(
            value_formatter=config.get('value_formatter', '{}'),
            diff_checker=config.get('diff_checker', lambda x, y: x != y),
            diff_formatter=config.get('diff_formatter'),
            diff_style=config.get('diff_style', 'magenta'),
        )

    def render_value(self, name: str | None, v: Any) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

    def render_diff(self, name: str | None, old: Any | None, new: Any | None) -> str:
        old_str = self._get_value_str(old) or MISSING_VALUE_STR
        new_str = self._get_value_str(new) or MISSING_VALUE_STR
        if old_str == new_str:
            result = old_str
        else:
            result = f'{old_str} → {new_str}'

            has_diff = self.diff_checker and self.diff_checker(old, new)
            if has_diff:  # pragma: no branch
                # If there is a diff, make the name bold and compute the diff_str
                name = name and f'[bold]{name}[/]'
                diff_str = self.diff_formatter and self.diff_formatter(old, new)
                if diff_str:  # pragma: no cover
                    result += f' ({diff_str})'
                result = f'[{self.diff_style}]{result}[/]'

        # Add the name
        if name:
            result = f'{name}: {result}'

        return result

    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:14-14
VALUE_SIG_FIGS = 3  # Significant figures for the default number formatting.

# examples/pydantic_ai_examples/evals/agent.py:12-20
class TimeRangeDeps:
    """Dependencies for the time range inference agent.

    While we could just get the current time using datetime.now() directly in the tools or system prompt, passing it
    via deps makes it easier to use a repeatable value during testing. While there are packages like `time-machine`
    that can do this for you, that kind of monkey-patching approach can become unwieldy as things get more complex.
    """

    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

# pydantic_ai_slim/pydantic_ai/models/test.py:45-45
    value: str | None

# pydantic_ai_slim/pydantic_ai/models/test.py:52-52
    value: dict[str, Any] | None

# pydantic_ai_slim/pydantic_ai/_utils.py:342-344
    async def is_exhausted(self) -> bool:
        """Returns True if the stream is exhausted, False otherwise."""
        return isinstance(await self.peek(), Unset)

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_evals/pydantic_evals/reporting/__init__.py:59-59
MISSING_VALUE_STR = '[i]<missing>[/i]'

# pydantic_ai_slim/pydantic_ai/messages.py:690-690
    return_value: ToolReturnContent

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:14-14
from typing_inspection.introspection import get_literal_values

# tests/test_format_as_xml.py:601-603
def test_invalid_value():
    with pytest.raises(TypeError, match='Unsupported type'):
        format_as_xml(object())

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:9-9
from weakref import WeakValueDictionary

# pydantic_ai_slim/pydantic_ai/usage.py:99-101
    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

# pydantic_evals/pydantic_evals/reporting/__init__.py:817-821
    def render_value(self, name: str | None, v: float | int) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

# tests/test_validation_context.py:23-23
    x: int

# pydantic_ai_slim/pydantic_ai/format_prompt.py:135-155
    def _set_scalar_text(self, element: ElementTree.Element, value: Any) -> bool:
        """Set element.text for scalar types. Return True if handled, False otherwise."""
        if value is None:
            element.text = self.none_str
        elif isinstance(value, str):
            element.text = value
        elif isinstance(value, bytes | bytearray):
            element.text = value.decode(errors='ignore')
        elif isinstance(value, bool | int | float | Enum):
            element.text = str(value)
        elif isinstance(value, date | time):
            element.text = value.isoformat()
        elif isinstance(value, timedelta):
            element.text = str(value)
        elif isinstance(value, Decimal):
            element.text = str(value)
        elif isinstance(value, UUID):
            element.text = str(value)
        else:
            return False
        return True

# pydantic_evals/pydantic_evals/reporting/__init__.py:917-917
    def render_value(self, name: str | None, v: T_contra) -> str: ...  # pragma: no branch

# pydantic_evals/pydantic_evals/reporting/__init__.py:923-923
_DEFAULT_VALUE_CONFIG = RenderValueConfig()

# pydantic_evals/pydantic_evals/reporting/__init__.py:884-890
    def _get_value_str(self, value: float | int | None) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# pydantic_ai_slim/pydantic_ai/_utils.py:159-160
def is_set(t_or_unset: T | Unset) -> TypeGuard[T]:
    return t_or_unset is not UNSET

# pydantic_evals/pydantic_evals/reporting/__init__.py:810-810
    value_formatter: str | Callable[[float | int], str]

# tests/models/test_openai.py:1624-1636
async def test_openai_store_false(allow_model_requests: None):
    """Test that openai_store=False is correctly passed to the OpenAI API."""
    c = completion_message(ChatCompletionMessage(content='hello', role='assistant'))
    mock_client = MockOpenAI.create_mock(c)
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(openai_client=mock_client))
    agent = Agent(m, model_settings=OpenAIChatModelSettings(openai_store=False))

    result = await agent.run('test')
    assert result.output == 'hello'

    # Verify the store parameter was passed to the mock
    kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    assert kwargs.get('store') is False

# tests/test_agent.py:4709-4797
def test_dynamic_true_reevaluate_system_prompt():
    """When dynamic is true, the system prompt is reevaluated
    i.e: SystemPromptPart(
            content="B",       <--- Updated value
    )
    """
    agent = Agent('test', system_prompt='Foobar')

    dynamic_value = 'A'

    @agent.system_prompt(dynamic=True)
    async def func():
        return dynamic_value

    res = agent.run_sync('Hello')

    assert res.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    SystemPromptPart(content='Foobar', timestamp=IsNow(tz=timezone.utc)),
                    SystemPromptPart(
                        content=dynamic_value,
                        dynamic_ref=func.__qualname__,
                        timestamp=IsNow(tz=timezone.utc),
                    ),
                    UserPromptPart(content='Hello', timestamp=IsNow(tz=timezone.utc)),
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
                kind='request',
            ),
            ModelResponse(
                parts=[TextPart(content='success (no tool calls)')],
                usage=RequestUsage(input_tokens=53, output_tokens=4),
                model_name='test',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
                kind='response',
            ),
        ]
    )

    dynamic_value = 'B'

    res_two = agent.run_sync('World', message_history=res.all_messages())

    assert res_two.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    SystemPromptPart(content='Foobar', timestamp=IsNow(tz=timezone.utc)),
                    SystemPromptPart(
                        content='B',
                        dynamic_ref=func.__qualname__,
                        timestamp=IsNow(tz=timezone.utc),
                    ),
                    UserPromptPart(content='Hello', timestamp=IsNow(tz=timezone.utc)),
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
                kind='request',
            ),
            ModelResponse(
                parts=[TextPart(content='success (no tool calls)')],
                usage=RequestUsage(input_tokens=53, output_tokens=4),
                model_name='test',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
                kind='response',
            ),
            ModelRequest(
                parts=[UserPromptPart(content='World', timestamp=IsNow(tz=timezone.utc), part_kind='user-prompt')],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
                kind='request',
            ),
            ModelResponse(
                parts=[TextPart(content='success (no tool calls)')],
                usage=RequestUsage(input_tokens=54, output_tokens=8),
                model_name='test',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
                kind='response',
            ),
        ]
    )

    assert res_two.new_messages() == res_two.all_messages()[-2:]

# tests/test_format_as_xml.py:611-617
def test_parse_invalid_value():
    class Invalid(BaseModel):
        name: str = Field(default='Alice', title='Name')
        bad: Any = object()

    with pytest.raises(TypeError, match='Unsupported type'):
        format_as_xml(Invalid(), include_field_info='once')

# tests/evals/test_multi_run.py:140-148
async def test_repeat_invalid_value():
    """repeat < 1 should raise ValueError."""

    async def task(inputs: str) -> str:
        return inputs  # pragma: no cover

    dataset = Dataset(cases=[Case(inputs='hello')])
    with pytest.raises(ValueError, match='repeat must be >= 1'):
        await dataset.evaluate(task, name='test', progress=False, repeat=0)

# pydantic_evals/pydantic_evals/reporting/__init__.py:751-751
    value_formatter: str | Callable[[float | int], str]

# tests/models/anthropic/test_output.py:465-483
def test_strict_false_tool_no_output(
    allow_model_requests: None,
    anthropic_model: ANTHROPIC_MODEL_FIXTURE,
) -> None:
    """Tool with strict=False, no output_type → no beta header."""
    model = anthropic_model('claude-sonnet-4-5')
    hook = create_header_verification_hook(expect_beta=False, test_name='test_strict_false_tool_no_output')
    model.client._client.event_hooks['request'].append(hook)  # pyright: ignore[reportPrivateUsage]

    agent = Agent(model)

    @agent.tool_plain(strict=False)
    def calculate_distance(city_a: str, city_b: str) -> str:
        return f'Distance from {city_a} to {city_b}: 504 km'

    agent.run_sync('How far is Madrid from Lisbon?')

    if errors := hook.errors:  # type: ignore[attr-defined]
        assert False, '\n'.join(sorted(errors))

# tests/graph/beta/test_joins_and_reducers.py:331-372
async def test_reduce_first_value():
    """Test ReduceFirstValue cancels sibling tasks"""

    @dataclass
    class StateWithResults:
        results: list[str] = field(default_factory=list[str])

    g = GraphBuilder(state_type=StateWithResults, output_type=str)

    @g.step
    async def generate(ctx: StepContext[StateWithResults, None, None]) -> list[int]:
        return [1, 2, 3, 4, 5]

    @g.step
    async def slow_process(ctx: StepContext[StateWithResults, None, int]) -> str:
        # First task finishes quickly
        if ctx.inputs == 1:
            await asyncio.sleep(0.001)
        else:
            # Others take longer (should be cancelled)
            await asyncio.sleep(10)
        ctx.state.results.append(f'completed-{ctx.inputs}')
        return f'result-{ctx.inputs}'

    first_join = g.join(ReduceFirstValue[str](), initial='')

    g.add(
        g.edge_from(g.start_node).to(generate),
        g.edge_from(generate).map().to(slow_process),
        g.edge_from(slow_process).to(first_join),
        g.edge_from(first_join).to(g.end_node),
    )

    graph = g.build()
    state = StateWithResults()
    result = await graph.run(state=state)

    # Only the first value should be returned
    assert result.startswith('result-')
    # Due to cancellation, not all 5 tasks should complete
    # (though timing can be tricky, so we just verify we got a result)
    assert 'completed-1' in state.results

# pydantic_ai_slim/pydantic_ai/run.py:316-316
    _traceparent_value: str | None = dataclasses.field(repr=False, compare=False, default=None)

# tests/test_logfire.py:594-603
def test_logfire_metadata_values(
    get_logfire_summary: Callable[[], LogfireSummary],
    metadata: dict[str, Any] | Callable[[RunContext[Any]], dict[str, Any]],
    expected: dict[str, Any],
) -> None:
    agent = Agent(model=TestModel(), instrument=InstrumentationSettings(version=2), metadata=metadata)
    agent.run_sync('Hello')

    summary = get_logfire_summary()
    assert summary.attributes[0]['metadata'] == expected

# pydantic_evals/pydantic_evals/reporting/__init__.py:682-688
class RenderValueConfig(TypedDict, total=False):
    """A configuration for rendering a values in an Evaluation report."""

    value_formatter: str | Callable[[Any], str]
    diff_checker: Callable[[Any, Any], bool] | None
    diff_formatter: Callable[[Any, Any], str | None] | None
    diff_style: str

# tests/test_embeddings.py:596-612
    async def test_titan_v2_with_normalize_false(self, bedrock_provider: BedrockProvider):
        """Test Titan V2 with normalize=False (override default)."""

        model = BedrockEmbeddingModel('amazon.titan-embed-text-v2:0', provider=bedrock_provider)
        embedder = Embedder(model, settings=BedrockEmbeddingSettings(bedrock_titan_normalize=False))
        result = await embedder.embed_query('Test normalization disabled')
        assert result == snapshot(
            EmbeddingResult(
                embeddings=IsList(IsList(IsFloat(), length=1024), length=1),
                inputs=['Test normalization disabled'],
                input_type='query',
                model_name='amazon.titan-embed-text-v2:0',
                provider_name='bedrock',
                timestamp=IsDatetime(),
                usage=RequestUsage(input_tokens=4),
            )
        )

# tests/test_agent.py:3929-4041
    def test_exhaustive_strategy_executes_all_tools(self):
        """Test that 'exhaustive' strategy executes all tools while using first final result."""
        tool_called: list[str] = []

        def return_model(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
            assert info.output_tools is not None
            return ModelResponse(
                parts=[
                    ToolCallPart('regular_tool', {'x': 42}),
                    ToolCallPart('final_result', {'value': 'first'}),
                    ToolCallPart('another_tool', {'y': 2}),
                    ToolCallPart('final_result', {'value': 'second'}),
                    ToolCallPart('unknown_tool', {'value': '???'}),
                    ToolCallPart('deferred_tool', {'x': 4}),
                ],
            )

        agent = Agent(FunctionModel(return_model), output_type=OutputType, end_strategy='exhaustive')

        @agent.tool_plain
        def regular_tool(x: int) -> int:
            """A regular tool that should be called."""
            tool_called.append('regular_tool')
            return x

        @agent.tool_plain
        def another_tool(y: int) -> int:
            """Another tool that should be called."""
            tool_called.append('another_tool')
            return y

        async def defer(ctx: RunContext[None], tool_def: ToolDefinition) -> ToolDefinition | None:
            return replace(tool_def, kind='external')

        @agent.tool_plain(prepare=defer)
        def deferred_tool(x: int) -> int:  # pragma: no cover
            return x + 1

        result = agent.run_sync('test exhaustive strategy')

        # Verify the result came from the first final tool
        assert result.output.value == 'first'

        # Verify all regular tools were called
        assert sorted(tool_called) == sorted(['regular_tool', 'another_tool'])

        # Verify we got tool returns in the correct order
        assert result.all_messages() == snapshot(
            [
                ModelRequest(
                    parts=[UserPromptPart(content='test exhaustive strategy', timestamp=IsNow(tz=timezone.utc))],
                    timestamp=IsNow(tz=timezone.utc),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[
                        ToolCallPart(tool_name='regular_tool', args={'x': 42}, tool_call_id=IsStr()),
                        ToolCallPart(tool_name='final_result', args={'value': 'first'}, tool_call_id=IsStr()),
                        ToolCallPart(tool_name='another_tool', args={'y': 2}, tool_call_id=IsStr()),
                        ToolCallPart(tool_name='final_result', args={'value': 'second'}, tool_call_id=IsStr()),
                        ToolCallPart(tool_name='unknown_tool', args={'value': '???'}, tool_call_id=IsStr()),
                        ToolCallPart(
                            tool_name='deferred_tool',
                            args={'x': 4},
                            tool_call_id=IsStr(),
                        ),
                    ],
                    usage=RequestUsage(input_tokens=53, output_tokens=27),
                    model_name='function:return_model:',
                    timestamp=IsNow(tz=timezone.utc),
                    run_id=IsStr(),
                ),
                ModelRequest(
                    parts=[
                        ToolReturnPart(
                            tool_name='final_result',
                            content='Final result processed.',
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                        ToolReturnPart(
                            tool_name='final_result',
                            content='Final result processed.',
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                        ToolReturnPart(
                            tool_name='regular_tool',
                            content=42,
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                        ToolReturnPart(
                            tool_name='another_tool', content=2, tool_call_id=IsStr(), timestamp=IsNow(tz=timezone.utc)
                        ),
                        RetryPromptPart(
                            content="Unknown tool name: 'unknown_tool'. Available tools: 'final_result', 'regular_tool', 'another_tool', 'deferred_tool'",
                            tool_name='unknown_tool',
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                        ToolReturnPart(
                            tool_name='deferred_tool',
                            content='Tool not executed - a final result was already processed.',
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                    ],
                    timestamp=IsNow(tz=timezone.utc),
                    run_id=IsStr(),
                ),
            ]
        )

# tests/test_agent.py:3929-4041
    def test_exhaustive_strategy_executes_all_tools(self):
        """Test that 'exhaustive' strategy executes all tools while using first final result."""
        tool_called: list[str] = []

        def return_model(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
            assert info.output_tools is not None
            return ModelResponse(
                parts=[
                    ToolCallPart('regular_tool', {'x': 42}),
                    ToolCallPart('final_result', {'value': 'first'}),
                    ToolCallPart('another_tool', {'y': 2}),
                    ToolCallPart('final_result', {'value': 'second'}),
                    ToolCallPart('unknown_tool', {'value': '???'}),
                    ToolCallPart('deferred_tool', {'x': 4}),
                ],
            )

        agent = Agent(FunctionModel(return_model), output_type=OutputType, end_strategy='exhaustive')

        @agent.tool_plain
        def regular_tool(x: int) -> int:
            """A regular tool that should be called."""
            tool_called.append('regular_tool')
            return x

        @agent.tool_plain
        def another_tool(y: int) -> int:
            """Another tool that should be called."""
            tool_called.append('another_tool')
            return y

        async def defer(ctx: RunContext[None], tool_def: ToolDefinition) -> ToolDefinition | None:
            return replace(tool_def, kind='external')

        @agent.tool_plain(prepare=defer)
        def deferred_tool(x: int) -> int:  # pragma: no cover
            return x + 1

        result = agent.run_sync('test exhaustive strategy')

        # Verify the result came from the first final tool
        assert result.output.value == 'first'

        # Verify all regular tools were called
        assert sorted(tool_called) == sorted(['regular_tool', 'another_tool'])

        # Verify we got tool returns in the correct order
        assert result.all_messages() == snapshot(
            [
                ModelRequest(
                    parts=[UserPromptPart(content='test exhaustive strategy', timestamp=IsNow(tz=timezone.utc))],
                    timestamp=IsNow(tz=timezone.utc),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[
                        ToolCallPart(tool_name='regular_tool', args={'x': 42}, tool_call_id=IsStr()),
                        ToolCallPart(tool_name='final_result', args={'value': 'first'}, tool_call_id=IsStr()),
                        ToolCallPart(tool_name='another_tool', args={'y': 2}, tool_call_id=IsStr()),
                        ToolCallPart(tool_name='final_result', args={'value': 'second'}, tool_call_id=IsStr()),
                        ToolCallPart(tool_name='unknown_tool', args={'value': '???'}, tool_call_id=IsStr()),
                        ToolCallPart(
                            tool_name='deferred_tool',
                            args={'x': 4},
                            tool_call_id=IsStr(),
                        ),
                    ],
                    usage=RequestUsage(input_tokens=53, output_tokens=27),
                    model_name='function:return_model:',
                    timestamp=IsNow(tz=timezone.utc),
                    run_id=IsStr(),
                ),
                ModelRequest(
                    parts=[
                        ToolReturnPart(
                            tool_name='final_result',
                            content='Final result processed.',
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                        ToolReturnPart(
                            tool_name='final_result',
                            content='Final result processed.',
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                        ToolReturnPart(
                            tool_name='regular_tool',
                            content=42,
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                        ToolReturnPart(
                            tool_name='another_tool', content=2, tool_call_id=IsStr(), timestamp=IsNow(tz=timezone.utc)
                        ),
                        RetryPromptPart(
                            content="Unknown tool name: 'unknown_tool'. Available tools: 'final_result', 'regular_tool', 'another_tool', 'deferred_tool'",
                            tool_name='unknown_tool',
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                        ToolReturnPart(
                            tool_name='deferred_tool',
                            content='Tool not executed - a final result was already processed.',
                            tool_call_id=IsStr(),
                            timestamp=IsNow(tz=timezone.utc),
                        ),
                    ],
                    timestamp=IsNow(tz=timezone.utc),
                    run_id=IsStr(),
                ),
            ]
        )

# tests/models/anthropic/test_output.py:314-327
def test_no_tools_native_output_strict_false(
    allow_model_requests: None,
    anthropic_model: ANTHROPIC_MODEL_FIXTURE,
) -> None:
    """Agent with NativeOutput(strict=False) → raises UserError."""
    model = anthropic_model('claude-sonnet-4-5')

    agent = Agent(model, output_type=NativeOutput(CityInfo, strict=False))

    with pytest.raises(
        UserError,
        match='Setting `strict=False` on `output_type=NativeOutput\\(\\.\\.\\.\\)` is not allowed for Anthropic models.',
    ):
        agent.run_sync('Tell me about Rome')

# tests/models/test_gemini.py:1487-1498
async def test_gemini_additional_properties_is_false(allow_model_requests: None, gemini_api_key: str):
    m = GeminiModel('gemini-2.0-flash', provider=GoogleGLAProvider(api_key=gemini_api_key))
    agent = Agent(m)

    @agent.tool_plain
    async def get_temperature(location: CurrentLocation) -> float:  # pragma: no cover
        return 20.0

    result = await agent.run('What is the temperature in Tokyo?')
    assert result.output == snapshot(
        'I need the country to find the temperature in Tokyo. Could you please tell me which country Tokyo is in?\n'
    )

# tests/profiles/test_anthropic.py:99-129
def test_strict_true_nested_model():
    """With strict=True, nested models are transformed."""

    class Address(BaseModel):
        street: str
        city: str

    class Person(BaseModel):
        name: str
        address: Address

    transformer = AnthropicJsonSchemaTransformer(Person.model_json_schema(), strict=True)
    transformed = transformer.walk()

    assert transformer.is_strict_compatible is True
    assert transformed == snapshot(
        {
            '$defs': {
                'Address': {
                    'type': 'object',
                    'properties': {'street': {'type': 'string'}, 'city': {'type': 'string'}},
                    'additionalProperties': False,
                    'required': ['street', 'city'],
                }
            },
            'type': 'object',
            'properties': {'name': {'type': 'string'}, 'address': {'$ref': '#/$defs/Address'}},
            'additionalProperties': False,
            'required': ['name', 'address'],
        }
    )

# tests/profiles/test_anthropic.py:137-159
def test_strict_false_preserves_schema():
    """With strict=False, schemas are not transformed (only title/$schema removed)."""

    class User(BaseModel):
        username: Annotated[str, Field(min_length=3)]
        age: int

    original_schema = User.model_json_schema()
    transformer = AnthropicJsonSchemaTransformer(original_schema, strict=False)
    transformed = transformer.walk()

    assert transformer.is_strict_compatible is False
    # Constraints preserved, title removed
    assert transformed == snapshot(
        {
            'type': 'object',
            'properties': {
                'username': {'minLength': 3, 'type': 'string'},
                'age': {'type': 'integer'},
            },
            'required': ['username', 'age'],
        }
    )

# tests/evals/test_report_evaluators.py:472-482
def test_report_rendering_include_analyses_false():
    cases = [
        _make_report_case('c1', output='cat', expected_output='cat'),
    ]
    report = _make_report(cases)
    report.analyses = [
        ScalarResult(title='Accuracy', value=100.0, unit='%'),
    ]

    rendered = report.render(width=120, include_analyses=False)
    assert 'Accuracy: 100.0 %' not in rendered

# tests/profiles/test_google.py:59-65
def test_const_false_boolean_infers_type():
    """When converting const to enum, type should be inferred for False boolean."""
    schema = {'const': False}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': [False], 'type': 'boolean'})

# tests/models/test_openai.py:1639-1651
async def test_openai_store_true(allow_model_requests: None):
    """Test that openai_store=True is correctly passed to the OpenAI API."""
    c = completion_message(ChatCompletionMessage(content='hello', role='assistant'))
    mock_client = MockOpenAI.create_mock(c)
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(openai_client=mock_client))
    agent = Agent(m, model_settings=OpenAIChatModelSettings(openai_store=True))

    result = await agent.run('test')
    assert result.output == 'hello'

    # Verify the store parameter was passed to the mock
    kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    assert kwargs.get('store') is True

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# tests/models/anthropic/test_output.py:487-506
def test_strict_false_tool_native_output(
    allow_model_requests: None,
    anthropic_model: ANTHROPIC_MODEL_FIXTURE,
) -> None:
    """Tool with strict=False, NativeOutput → beta from native only + output_format."""
    model = anthropic_model('claude-sonnet-4-5')
    hook = create_header_verification_hook(expect_beta=True, test_name='test_strict_false_tool_native_output')
    model.client._client.event_hooks['request'].append(hook)  # pyright: ignore[reportPrivateUsage]

    agent = Agent(model, output_type=NativeOutput(CityInfo))

    @agent.tool_plain(strict=False)
    def get_currency(country: str) -> str:
        return 'Mexican Peso (MXN)' if country == 'Mexico' else 'Unknown'

    result = agent.run_sync('Give me details about Mexico City')

    assert isinstance(result.output, CityInfo)
    if errors := hook.errors:  # type: ignore[attr-defined]
        assert False, '\n'.join(sorted(errors))

# tests/test_temporal.py:3115-3127
def test_pydantic_ai_plugin_preserves_custom_payload_codec() -> None:
    """When converter has a custom payload_codec, preserve it while replacing payload_converter_class."""
    plugin = PydanticAIPlugin()
    codec = MockPayloadCodec()
    converter = DataConverter(
        payload_converter_class=DefaultPayloadConverter,
        payload_codec=codec,
    )
    config: dict[str, Any] = {'data_converter': converter}
    result = plugin.configure_client(config)  # type: ignore[arg-type]
    assert result['data_converter'] is not converter
    assert result['data_converter'].payload_converter_class is PydanticPayloadConverter
    assert result['data_converter'].payload_codec is codec

# tests/models/anthropic/test_output.py:92-112
def test_strict_tools_supported_model_explicit_false(
    allow_model_requests: None, weather_tool_responses: list[BetaMessage]
):
    """sonnet-4-5: strict=False → no strict field, no beta header."""
    mock_client = MockAnthropic.create_mock(weather_tool_responses)
    model = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(model)

    @agent.tool_plain(strict=False)
    def get_weather(location: str) -> str:
        return f'Weather in {location}'

    agent.run_sync('What is the weather in Paris?')

    completion_kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    tools = completion_kwargs['tools']
    betas = completion_kwargs.get('betas')

    assert 'strict' not in tools[0]
    assert tools[0]['input_schema']['additionalProperties'] is False
    assert betas is OMIT

# tests/graph/beta/test_edge_cases.py:60-80
async def test_step_with_zero_value():
    """Test handling of zero values (ensure they're not confused with None/falsy)."""
    g = GraphBuilder(state_type=EdgeCaseState, output_type=int)

    @g.step
    async def return_zero(ctx: StepContext[EdgeCaseState, None, None]) -> int:
        return 0

    @g.step
    async def process_zero(ctx: StepContext[EdgeCaseState, None, int]) -> int:
        return ctx.inputs + 1

    g.add(
        g.edge_from(g.start_node).to(return_zero),
        g.edge_from(return_zero).to(process_zero),
        g.edge_from(process_zero).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=EdgeCaseState())
    assert result == 1

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:1227-1234
    def is_end_node(
        node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],
    ) -> TypeIs[End[result.FinalResult[S]]]:
        """Check if the node is a `End`, narrowing the type if it is.

        This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
        """
        return isinstance(node, End)

# tests/test_logfire.py:582-583
def _test_logfire_metadata_values_callable_dict(ctx: RunContext[Any]) -> dict[str, str]:
    return {'model_name': ctx.model.model_name}

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:1207-1214
    def is_call_tools_node(
        node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],
    ) -> TypeIs[_agent_graph.CallToolsNode[T, S]]:
        """Check if the node is a `CallToolsNode`, narrowing the type if it is.

        This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.
        """
        return isinstance(node, _agent_graph.CallToolsNode)

# pydantic_evals/pydantic_evals/reporting/__init__.py:696-696
    diff_style: str = 'magenta'

# pydantic_evals/pydantic_evals/reporting/__init__.py:688-688
    diff_style: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:694-694
    diff_checker: Callable[[Any, Any], bool] | None = lambda x, y: x != y