# pydantic_ai_slim/pydantic_ai/usage.py:200-200
    details: dict[str, int] = dataclasses.field(default_factory=dict[str, int])

# pydantic_ai_slim/pydantic_ai/usage.py:46-50
    details: Annotated[
        dict[str, int],
        # `details` can not be `None` any longer, but we still want to support deserializing model responses stored in a DB before this was changed
        BeforeValidator(lambda d: d or {}),
    ] = dataclasses.field(default_factory=dict[str, int])

# tests/test_usage_limits.py:3-3
import operator

# pydantic_ai_slim/pydantic_ai/models/openai.py:2314-2319
    def _map_provider_details(self, chunk: ChatCompletionChunk) -> dict[str, Any] | None:
        """Hook that generates the provider details from chunk content.

        This method may be overridden by subclasses of `OpenAIStreamResponse` to customize the provider details.
        """
        return _map_provider_details(chunk.choices[0])

# pydantic_ai_slim/pydantic_ai/usage.py:136-166
    def extract(
        cls,
        data: Any,
        *,
        provider: str,
        provider_url: str,
        provider_fallback: str,
        api_flavor: str = 'default',
        details: dict[str, Any] | None = None,
    ) -> RequestUsage:
        """Extract usage information from the response data using genai-prices.

        Args:
            data: The response data from the model API.
            provider: The actual provider ID
            provider_url: The provider base_url
            provider_fallback: The fallback provider ID to use if the actual provider is not found in genai-prices.
                For example, an OpenAI model should set this to "openai" in case it has an obscure provider ID.
            api_flavor: The API flavor to use when extracting usage information,
                e.g. 'chat' or 'responses' for OpenAI.
            details: Becomes the `details` field on the returned `RequestUsage` for convenience.
        """
        details = details and {}
        for provider_id, provider_api_url in [(None, provider_url), (provider, None), (provider_fallback, None)]:
            try:
                provider_obj = get_snapshot().find_provider(None, provider_id, provider_api_url)
                _model_ref, extracted_usage = provider_obj.extract_usage(data, api_flavor=api_flavor)
                return cls(**{k: v for k, v in extracted_usage.__dict__.items() if v is not None}, details=details)
            except Exception:
                pass
        return cls(details=details)

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:175-256
    def handle_thinking_delta(
        self,
        *,
        vendor_part_id: Hashable | None,
        content: str | None = None,
        id: str | None = None,
        signature: str | None = None,
        provider_name: str | None = None,
        provider_details: ProviderDetailsDelta = None,
    ) -> Iterator[ModelResponseStreamEvent]:
        """Handle incoming thinking content, creating or updating a ThinkingPart in the manager as appropriate.

        When `vendor_part_id` is None, the latest part is updated if it exists and is a ThinkingPart;
        otherwise, a new ThinkingPart is created. When a non-None ID is specified, the ThinkingPart corresponding
        to that vendor ID is either created or updated.

        Args:
            vendor_part_id: The ID the vendor uses to identify this piece
                of thinking. If None, a new part will be created unless the latest part is already
                a ThinkingPart.
            content: The thinking content to append to the appropriate ThinkingPart.
            id: An optional id for the thinking part.
            signature: An optional signature for the thinking content.
            provider_name: An optional provider name for the thinking part.
            provider_details: Either a dict of provider-specific details, or a callable that takes
                the existing part's `provider_details` and returns the updated details. Callables
                allow provider-specific update logic without the parts manager knowing the details.

        Yields:
            A `PartStartEvent` if a new part was created, or a `PartDeltaEvent` if an existing part was updated.

        Raises:
            UnexpectedModelBehavior: If attempting to apply a thinking delta to a part that is not a ThinkingPart.
        """
        existing_thinking_part_and_index: tuple[ThinkingPart, int] | None = None

        if vendor_part_id is None:
            # If the vendor_part_id is None, check if the latest part is a ThinkingPart to update
            existing_thinking_part_and_index = self._latest_part_if_of_type(ThinkingPart)
        else:
            # Otherwise, attempt to look up an existing ThinkingPart by vendor_part_id
            part_index = self._vendor_id_to_part_index.get(vendor_part_id)
            if part_index is not None:
                existing_part = self._parts[part_index]
                if not isinstance(existing_part, ThinkingPart):
                    raise UnexpectedModelBehavior(f'Cannot apply a thinking delta to {existing_part=}')
                existing_thinking_part_and_index = existing_part, part_index

        if existing_thinking_part_and_index is None:
            if content is not None or signature is not None or provider_details is not None:
                # There is no existing thinking part that should be updated, so create a new one
                # Resolve provider_details if it's a callback (with None since there's no existing part)
                resolved_details: dict[str, Any] | None
                resolved_details = provider_details(None) if callable(provider_details) else provider_details
                part = ThinkingPart(
                    content=content or '',
                    id=id,
                    signature=signature,
                    provider_name=provider_name,
                    provider_details=resolved_details,
                )
                new_part_index = self._append_part(part, vendor_part_id)
                yield PartStartEvent(index=new_part_index, part=part)
            else:
                raise UnexpectedModelBehavior(
                    'Cannot create a ThinkingPart with no content, signature, or provider_details'
                )
        else:
            existing_thinking_part, part_index = existing_thinking_part_and_index

            # Skip if nothing to update
            if content is None and signature is None and provider_name is None and provider_details is None:
                return

            part_delta = ThinkingPartDelta(
                content_delta=content,
                signature_delta=signature,
                provider_name=self._resolve_provider_name(existing_thinking_part, provider_name),
                provider_details=provider_details,
            )
            self._parts[part_index] = part_delta.apply(existing_thinking_part)
            yield PartDeltaEvent(index=part_index, delta=part_delta)

# pydantic_ai_slim/pydantic_ai/messages.py:1988-1990
    def tool_call_id(self) -> str:
        """An ID used for matching details about the call to its result."""
        return self.part.tool_call_id

# tests/test_usage_limits.py:258-284
def test_add_usages_with_none_detail_value():
    """Test that None values in details are skipped when incrementing usage."""
    usage = RunUsage(
        requests=1,
        input_tokens=10,
        output_tokens=20,
        details={'reasoning_tokens': 5},
    )

    # Create a usage with None in details (simulating model response with missing detail)
    incr_usage = RunUsage(
        requests=1,
        input_tokens=5,
        output_tokens=10,
    )
    # Manually set a None value in details to simulate edge case from model responses
    incr_usage.details = {'reasoning_tokens': None, 'other_tokens': 10}  # type: ignore[dict-item]

    result = usage + incr_usage
    assert result == snapshot(
        RunUsage(
            requests=2,
            input_tokens=15,
            output_tokens=30,
            details={'reasoning_tokens': 5, 'other_tokens': 10},
        )
    )

# examples/pydantic_ai_examples/flight_booking.py:28-35
class FlightDetails(BaseModel):
    """Details of the most suitable flight."""

    flight_number: str
    price: int
    origin: str = Field(description='Three-letter airport code')
    destination: str = Field(description='Three-letter airport code')
    date: datetime.date

# pydantic_evals/pydantic_evals/reporting/__init__.py:807-910
class _NumberRenderer:
    """See documentation of `RenderNumberConfig` for more details about the parameters here."""

    value_formatter: str | Callable[[float | int], str]
    diff_formatter: str | Callable[[float | int, float | int], str | None] | None
    diff_atol: float
    diff_rtol: float
    diff_increase_style: str
    diff_decrease_style: str

    def render_value(self, name: str | None, v: float | int) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

    def render_diff(self, name: str | None, old: float | int | None, new: float | int | None) -> str:
        old_str = self._get_value_str(old)
        new_str = self._get_value_str(new)
        if old_str == new_str:
            result = old_str
        else:
            result = f'{old_str} â†’ {new_str}'

            diff_style = self._get_diff_style(old, new)
            if diff_style:
                # If there is a diff, make the name bold and compute the diff_str
                name = name and f'[bold]{name}[/]'
                diff_str = self._get_diff_str(old, new)
                if diff_str:  # pragma: no branch
                    result += f' ({diff_str})'
                result = f'[{diff_style}]{result}[/]'

        # Add the name
        if name:  # pragma: no branch
            result = f'{name}: {result}'

        return result

    @staticmethod
    def infer_from_config(
        config: RenderNumberConfig, kind: Literal['score', 'metric', 'duration'], values: list[float | int]
    ) -> _NumberRenderer:
        value_formatter = config.get('value_formatter', UNSET)
        if isinstance(value_formatter, Unset):
            value_formatter = default_render_number

        diff_formatter = config.get('diff_formatter', UNSET)
        if isinstance(diff_formatter, Unset):
            diff_formatter = default_render_number_diff

        diff_atol = config.get('diff_atol', UNSET)
        if isinstance(diff_atol, Unset):
            diff_atol = 1e-6

        diff_rtol = config.get('diff_rtol', UNSET)
        if isinstance(diff_rtol, Unset):
            values_are_ints = all(isinstance(v, int) for v in values)
            diff_rtol = 0.001 if values_are_ints else 0.05

        diff_increase_style = config.get('diff_increase_style', UNSET)
        if isinstance(diff_increase_style, Unset):
            diff_increase_style = 'green' if kind == 'score' else 'red'

        diff_decrease_style = config.get('diff_decrease_style', UNSET)
        if isinstance(diff_decrease_style, Unset):
            diff_decrease_style = 'red' if kind == 'score' else 'green'

        return _NumberRenderer(
            value_formatter=value_formatter,
            diff_formatter=diff_formatter,
            diff_rtol=diff_rtol,
            diff_atol=diff_atol,
            diff_increase_style=diff_increase_style,
            diff_decrease_style=diff_decrease_style,
        )

    def _get_value_str(self, value: float | int | None) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

    def _get_diff_str(self, old: float | int | None, new: float | int | None) -> str | None:
        if old is None or new is None:  # pragma: no cover
            return None
        if isinstance(self.diff_formatter, str):  # pragma: no cover
            return self.diff_formatter.format(new - old)
        elif self.diff_formatter is None:  # pragma: no cover
            return None
        else:
            return self.diff_formatter(old, new)

    def _get_diff_style(self, old: float | int | None, new: float | int | None) -> str | None:
        # 1 means new is higher, -1 means new is lower, 0 means no change
        if old is None or new is None:
            return None

        diff = new - old
        if abs(diff) < self.diff_atol + self.diff_rtol * abs(old):
            return None
        return self.diff_increase_style if diff > 0 else self.diff_decrease_style

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:417-420
class _OpenRouterCostDetails:
    """OpenRouter specific cost details."""

    upstream_inference_cost: float | None = None

# pydantic_ai_slim/pydantic_ai/models/__init__.py:1029-1031
    def usage(self) -> RequestUsage:
        """Get the usage of the response so far. This will not be the final usage until the stream is exhausted."""
        return self._usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:17-42
from pydantic_ai import (
    AudioUrl,
    BinaryContent,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentUrl,
    FinishReason,
    ImageUrl,
    ModelMessage,
    ModelProfileSpec,
    ModelRequest,
    ModelResponse,
    ModelResponsePart,
    ModelResponseStreamEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
    _utils,
    usage,
)

# pydantic_ai_slim/pydantic_ai/models/cohere.py:11-11
from .. import ModelHTTPError, usage

# pydantic_ai_slim/pydantic_ai/models/function.py:14-14
from .. import _utils, usage

# pydantic_ai_slim/pydantic_ai/models/gemini.py:24-24
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/google.py:14-14
from .. import UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/groq.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:11-11
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/openai.py:18-18
from .. import ModelAPIError, ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/messages.py:1994-1996
    def call_id(self) -> str:
        """An ID used for matching details about the call to its result."""
        return self.part.tool_call_id  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:426-429
class _OpenRouterPromptTokenDetails(completion_usage.PromptTokensDetails):
    """Wraps OpenAI completion token details with OpenRouter specific attributes."""

    video_tokens: int | None = None

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:432-435
class _OpenRouterCompletionTokenDetails(completion_usage.CompletionTokensDetails):
    """Wraps OpenAI completion token details with OpenRouter specific attributes."""

    image_tokens: int | None = None

# pydantic_ai_slim/pydantic_ai/result.py:560-571
    def usage(self) -> RunUsage:
        """Return the usage of the whole run.

        !!! note
            This won't return the full usage until the stream is finished.
        """
        if self._run_result is not None:
            return self._run_result.usage()
        elif self._stream_response is not None:
            return self._stream_response.usage()
        else:
            raise ValueError('No stream response or run result provided')  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/result.py:162-168
    def usage(self) -> RunUsage:
        """Return the usage of the whole run.

        !!! note
            This won't return the full usage until the stream is finished.
        """
        return self._initial_run_ctx_usage + self._raw_stream_response.usage()

# pydantic_ai_slim/pydantic_ai/result.py:743-749
    def usage(self) -> RunUsage:
        """Return the usage of the whole run.

        !!! note
            This won't return the full usage until the stream is finished.
        """
        return self._streamed_run_result.usage()

# pydantic_ai_slim/pydantic_ai/direct.py:390-392
    def usage(self) -> RequestUsage:
        """Get the usage of the response so far."""
        return self._ensure_stream_ready().usage()

# examples/pydantic_ai_examples/flight_booking.py:70-75
async def extract_flights(ctx: RunContext[Deps]) -> list[FlightDetails]:
    """Get details of all flights."""
    # we pass the usage to the search agent so requests within this agent are counted
    result = await extraction_agent.run(ctx.deps.web_page_text, usage=ctx.usage)
    logfire.info('found {flight_count} flights', flight_count=len(result.output))
    return result.output

# pydantic_ai_slim/pydantic_ai/_run_context.py:37-37
    usage: RunUsage

# pydantic_ai_slim/pydantic_ai/run.py:423-425
    def usage(self) -> _usage.RunUsage:
        """Return the usage of the whole run."""
        return self._state.usage

# pydantic_ai_slim/pydantic_ai/messages.py:1302-1302
    usage: RequestUsage = field(default_factory=RequestUsage)

# pydantic_ai_slim/pydantic_ai/run.py:284-286
    def usage(self) -> _usage.RunUsage:
        """Get usage statistics for the run so far, including token usage, model requests, and so on."""
        return self._graph_run.state.usage

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:76-76
    usage: RequestUsage = field(default_factory=RequestUsage)

# pydantic_ai_slim/pydantic_ai/models/openai.py:753-758
    def _process_provider_details(self, response: chat.ChatCompletion) -> dict[str, Any] | None:
        """Hook that response content to provider details.

        This method may be overridden by subclasses of `OpenAIChatModel` to apply custom mappings.
        """
        return _map_provider_details(response.choices[0])

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:90-90
    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:38-39
    def usage(self) -> RequestUsage:
        return self.response.usage  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:47-48
    def usage(self) -> RequestUsage:
        return self.response.usage  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:56-57
    def usage(self) -> RequestUsage:
        return self.response.usage  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/profiles/harmony.py:7-15
def harmony_model_profile(model_name: str) -> ModelProfile | None:
    """The model profile for the OpenAI Harmony Response format.

    See <https://cookbook.openai.com/articles/openai-harmony> for more details.
    """
    profile = openai_model_profile(model_name)
    return OpenAIModelProfile(
        openai_supports_tool_choice_required=False, ignore_streamed_leading_whitespace=True
    ).update(profile)

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:464-464
    usage: _OpenRouterUsage | None = None  # type: ignore[reportIncompatibleVariableOverride]

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:645-645
    usage: _OpenRouterUsage | None = None  # type: ignore[reportIncompatibleVariableOverride]

# pydantic_ai_slim/pydantic_ai/models/test.py:480-482
    def _bool_gen(self) -> bool:
        """Generate a boolean from a JSON Schema boolean."""
        return bool(self.seed % 2)

# pydantic_ai_slim/pydantic_ai/usage.py:246-247
class Usage(RunUsage):
    """Deprecated alias for `RunUsage`."""

# pydantic_ai_slim/pydantic_ai/tools.py:235-235
A = TypeVar('A')

# tests/typed_graph.py:90-94
class A(BaseNode[MyState, MyDeps]):
    async def run(self, ctx: GraphRunContext[MyState, MyDeps]) -> B:
        assert ctx.state.x == 1
        assert ctx.deps.y == 'y'
        return B()

# tests/test_agent.py:150-150
    a: int