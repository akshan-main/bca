## docs/.hooks/algolia.py

from typing_extensions import TypedDict

## docs/.hooks/snippets.py

class RenderedSnippet:
    content: str
    highlights: list[LineRange]
    original_range: LineRange

## examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py

from pydantic import BaseModel, Field

## examples/pydantic_ai_examples/bank_support.py

class SupportDependencies:
    customer_id: int
    db: DatabaseConn

class SupportOutput(BaseModel):
    support_advice: str
    """Advice returned to the customer"""
    block_card: bool
    """Whether to block their card or not"""
    risk: int
    """Risk level of query"""

    support_advice: str

async def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:
    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)
    return f"The customer's name is {customer_name!r}"

async def customer_balance(ctx: RunContext[SupportDependencies]) -> str:
    """Returns the customer's current account balance."""
    balance = await ctx.deps.db.customer_balance(
        id=ctx.deps.customer_id,
    )
    return f'${balance:.2f}'

## examples/pydantic_ai_examples/question_graph.py

from pydantic_graph import (
    BaseNode,
    End,
    Graph,
    GraphRunContext,
)

## examples/pydantic_ai_examples/slack_lead_qualifier/agent.py

async def analyze_profile(profile: Profile) -> Analysis | None:
    result = await agent.run(profile.as_prompt())
    return result.output  ### [/analyze_profile]

## examples/pydantic_ai_examples/slack_lead_qualifier/modal.py

def setup_logfire():
    import logfire

    logfire.configure(service_name=app.name)
    logfire.instrument_pydantic_ai()
    logfire.instrument_httpx(capture_all=True)  ### [/setup_logfire]

## pydantic_ai_slim/pydantic_ai/_a2a.py

from typing import Any, Generic, TypeVar

## pydantic_ai_slim/pydantic_ai/_function_schema.py

def _is_call_ctx(annotation: Any) -> bool:
    """Return whether the annotation is the `RunContext` class, parameterized or not."""
    return annotation is RunContext or get_origin(annotation) is RunContext

## pydantic_ai_slim/pydantic_ai/_otel_messages.py

    type: Literal['text']

## pydantic_ai_slim/pydantic_ai/_ssrf.py

class ResolvedUrl:
    """Result of URL validation and DNS resolution."""

    resolved_ip: str
    """The resolved IP address to connect to."""

    hostname: str
    """The original hostname (used for Host header)."""

    port: int
    """The port number."""

    is_https: bool
    """Whether to use HTTPS."""

    path: str
    """The path including query string and fragment."""

def is_cloud_metadata_ip(ip_str: str) -> bool:
    """Check if an IP address is a cloud metadata endpoint.

    These are always blocked for security reasons, even with allow_local=True.
    """
    return ip_str in _CLOUD_METADATA_IPS

def build_url_with_ip(resolved: ResolvedUrl) -> str:
    """Build a URL using a resolved IP address instead of the hostname.

    For IPv6 addresses, wraps them in brackets as required by URL syntax.
    """
    scheme = 'https' if resolved.is_https else 'http'
    default_port = 443 if resolved.is_https else 80

    # IPv6 addresses need brackets in URLs
    try:
        ip_obj = ipaddress.ip_address(resolved.resolved_ip)
        if isinstance(ip_obj, ipaddress.IPv6Address):
            host_part = f'[{resolved.resolved_ip}]'
        else:
            host_part = resolved.resolved_ip
    except ValueError:
        host_part = resolved.resolved_ip

    # Only include port if non-default
    if resolved.port == default_port:
        host_part = f'{host_part}:{resolved.port}'

    return urlunparse((scheme, host_part, resolved.path, '', '', ''))

## pydantic_ai_slim/pydantic_ai/_utils.py

def sync_anext(iterator: Iterator[T]) -> T:
    """Get the next item from a sync iterator, raising `StopAsyncIteration` if it's exhausted.

    Useful when iterating over a sync iterator in an async context.
    """
    try:
        return next(iterator)
    except StopIteration as e:
        raise StopAsyncIteration() from e

def get_event_loop():
    try:
        event_loop = asyncio.get_event_loop()
    except RuntimeError:  # pragma: lax no cover
        event_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(event_loop)
    return event_loop

## pydantic_ai_slim/pydantic_ai/concurrency.py

def get_concurrency_context(
    limiter: AbstractConcurrencyLimiter | None,
    source: str = 'unnamed',
) -> AbstractAsyncContextManager[None]:
    """Get an async context manager for the concurrency limiter.

    If limiter is None, returns a no-op context manager.

    Args:
        limiter: The AbstractConcurrencyLimiter or None.
        source: Identifier for the source of this acquisition (e.g., 'agent:my-agent' or 'model:gpt-4').

    Returns:
        An async context manager.
    """
    if limiter is None:
        return _null_context()
    return _limiter_context(limiter, source)

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py

from temporalio.plugin import SimplePlugin

class AgentPlugin(SimplePlugin):
    """Temporal worker plugin for a specific Pydantic AI agent."""

    def __init__(self, agent: TemporalAgent[Any, Any]):
        super().__init__(  # type: ignore[reportUnknownMemberType]
            name='AgentPlugin',
            activities=agent.temporal_activities,
        )

## pydantic_ai_slim/pydantic_ai/ext/langchain.py

from typing import Any, Protocol

## pydantic_ai_slim/pydantic_ai/models/fallback.py

def _default_fallback_condition_factory(exceptions: tuple[type[Exception], ...]) -> Callable[[Exception], bool]:
    """Create a default fallback condition for the given exceptions."""

    def fallback_condition(exception: Exception) -> bool:
        return isinstance(exception, exceptions)

    return fallback_condition

## pydantic_ai_slim/pydantic_ai/models/test.py

def _get_string_usage(text: str) -> RequestUsage:
    response_tokens = _estimate_string_tokens(text)
    return RequestUsage(output_tokens=response_tokens)

## pydantic_ai_slim/pydantic_ai/providers/bedrock.py

def bedrock_deepseek_model_profile(model_name: str) -> ModelProfile | None:
    """Get the model profile for a DeepSeek model used via Bedrock."""
    profile = deepseek_model_profile(model_name)
    if 'r1' in model_name:
        return BedrockModelProfile(bedrock_send_back_thinking_parts=True).update(profile)
    return profile  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/providers/groq.py

def groq_moonshotai_model_profile(model_name: str) -> ModelProfile | None:
    """Get the model profile for an MoonshotAI model used with the Groq provider."""
    return ModelProfile(supports_json_object_output=True, supports_json_schema_output=True).update(
        moonshotai_model_profile(model_name)
    )

def meta_groq_model_profile(model_name: str) -> ModelProfile | None:
    """Get the model profile for a Meta model used with the Groq provider."""
    if model_name in {'llama-4-maverick-17b-128e-instruct', 'llama-4-scout-17b-16e-instruct'}:
        return ModelProfile(supports_json_object_output=True, supports_json_schema_output=True).update(
            meta_model_profile(model_name)
        )
    else:
        return meta_model_profile(model_name)

## pydantic_ai_slim/pydantic_ai/toolsets/abstract.py

class SchemaValidatorProt(Protocol):
    """Protocol for a Pydantic Core `SchemaValidator` or `PluggableSchemaValidator` (which is private but API-compatible)."""

    def validate_json(
        self,
        input: str | bytes | bytearray,
        *,
        allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False,
        **kwargs: Any,
    ) -> Any: ...

    def validate_python(
        self, input: Any, *, allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False, **kwargs: Any
    ) -> Any: ...

    def validate_python(
        self, input: Any, *, allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False, **kwargs: Any
    ) -> Any: ...

## pydantic_evals/pydantic_evals/dataset.py

async def _run_report_evaluators(
    report_evaluators: list[ReportEvaluator],
    report_ctx: ReportEvaluatorContext[Any, Any, Any],
) -> None:
    """Run report evaluators and append their analyses to the report."""
    report = report_ctx.report
    for report_eval in report_evaluators:
        evaluator_name = report_eval.get_serialization_name()
        with logfire_span(
            'report_evaluator: {evaluator_name}',
            evaluator_name=evaluator_name,
        ):
            try:
                result = await report_eval.evaluate_async(report_ctx)
            except Exception as e:
                report.report_evaluator_failures.append(
                    EvaluatorFailure(
                        name=evaluator_name,
                        error_message=f'{type(e).__name__}: {e}',
                        error_stacktrace=traceback.format_exc(),
                        source=report_eval.as_spec(),
                    )
                )
            else:
                if isinstance(result, list):
                    report.analyses.extend(result)
                else:
                    report.analyses.append(result)

## pydantic_evals/pydantic_evals/evaluators/report_evaluator.py

    report: EvaluationReport[InputsT, OutputT, MetadataT]

## pydantic_evals/pydantic_evals/reporting/__init__.py

class ReportCase(Generic[InputsT, OutputT, MetadataT]):
    """A single case in an evaluation report."""

    name: str
    """The name of the [case][pydantic_evals.Case]."""
    inputs: InputsT
    """The inputs to the task, from [`Case.inputs`][pydantic_evals.dataset.Case.inputs]."""
    metadata: MetadataT | None
    """Any metadata associated with the case, from [`Case.metadata`][pydantic_evals.dataset.Case.metadata]."""
    expected_output: OutputT | None
    """The expected output of the task, from [`Case.expected_output`][pydantic_evals.dataset.Case.expected_output]."""
    output: OutputT
    """The output of the task execution."""

    metrics: dict[str, float | int]
    attributes: dict[str, Any]

    scores: dict[str, EvaluationResult[int | float]]
    labels: dict[str, EvaluationResult[str]]
    assertions: dict[str, EvaluationResult[bool]]

    task_duration: float
    total_duration: float  # includes evaluator execution time

    source_case_name: str | None = None
    """The original case name before run-indexing. Serves as the aggregation key
    for multi-run experiments. None when repeat == 1."""

    trace_id: str | None = None
    """The trace ID of the case span."""
    span_id: str | None = None
    """The span ID of the case span."""

    evaluator_failures: list[EvaluatorFailure] = field(default_factory=list[EvaluatorFailure])

class RenderValueConfig(TypedDict, total=False):
    """A configuration for rendering a values in an Evaluation report."""

    value_formatter: str | Callable[[Any], str]
    diff_checker: Callable[[Any, Any], bool] | None
    diff_formatter: Callable[[Any, Any], str | None] | None
    diff_style: str

class _AbstractRenderer(Protocol[T_contra]):
    def render_value(self, name: str | None, v: T_contra) -> str: ...  # pragma: no branch

    def render_diff(self, name: str | None, old: T_contra | None, new: T_contra | None) -> str: ...  # pragma: no branch

## pydantic_graph/pydantic_graph/beta/join.py

def reduce_dict_update(current: dict[K, V], inputs: Mapping[K, V]) -> dict[K, V]:
    """A reducer that updates a dict."""
    current.update(inputs)
    return current

class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## tests/evals/test_dataset.py

class TaskInput(BaseModel):
    query: str

class TaskOutput(BaseModel):
    answer: str
    confidence: float = 1.0

class TaskMetadata(BaseModel):
    difficulty: str = 'easy'
    category: str = 'general'

def simple_evaluator() -> type[Evaluator[TaskInput, TaskOutput, TaskMetadata]]:
    @dataclass
    class SimpleEvaluator(Evaluator[TaskInput, TaskOutput, TaskMetadata]):
        def evaluate(self, ctx: EvaluatorContext[TaskInput, TaskOutput, TaskMetadata]):
            if ctx.expected_output is None:  # pragma: no cover
                return {'result': 'no_expected_output'}

            return {
                'correct': ctx.output.answer == ctx.expected_output.answer,
                'confidence': ctx.output.confidence,
            }

    return SimpleEvaluator

async def test_dataset_evaluate_with_custom_name(example_dataset: Dataset[TaskInput, TaskOutput, TaskMetadata]):
    """Test evaluating a dataset with a custom task name."""

    async def task(inputs: TaskInput) -> TaskOutput:
        return TaskOutput(answer=inputs.query.upper())

    report = await example_dataset.evaluate(task, task_name='custom_task')
    assert report.name == 'custom_task'

async def test_dataset_evaluate_with_empty_cases(example_dataset: Dataset[TaskInput, TaskOutput, TaskMetadata]):
    """Test evaluating a dataset with no cases."""
    dataset = Dataset(cases=[])

    async def task(inputs: TaskInput) -> TaskOutput:  # pragma: no cover
        return TaskOutput(answer=inputs.query.upper())

    report = await dataset.evaluate(task)
    assert len(report.cases) == 0

## tests/evals/test_evaluator_base.py

async def test_evaluation_name():
    """Test evaluator name method."""
    evaluator = SimpleEvaluator()
    assert evaluator.get_serialization_name() == 'SimpleEvaluator'
    assert evaluator.get_default_evaluation_name() == 'SimpleEvaluator'

## tests/evals/test_multi_run.py

async def test_repeat_invalid_value():
    """repeat < 1 should raise ValueError."""

    async def task(inputs: str) -> str:
        return inputs  # pragma: no cover

    dataset = Dataset(cases=[Case(inputs='hello')])
    with pytest.raises(ValueError, match='repeat must be >= 1'):
        await dataset.evaluate(task, name='test', progress=False, repeat=0)

## tests/evals/test_otel.py

async def test_or_cannot_be_mixed(span_tree: SpanTree):
    with pytest.raises(ValueError) as exc_info:
        span_tree.first({'name_equals': 'child1', 'or_': [SpanQuery(name_equals='child2')]})
    assert str(exc_info.value) == snapshot("Cannot combine 'or_' conditions with other conditions at the same level")

## tests/evals/test_report_evaluators.py

def _make_report(cases: list[ReportCase]) -> EvaluationReport:
    return EvaluationReport(name='test', cases=cases)

def test_confusion_matrix_evaluator_from_expected_output_and_output():
    cases = [
        _make_report_case('c1', output='cat', expected_output='cat'),
        _make_report_case('c2', output='dog', expected_output='cat'),
        _make_report_case('c3', output='dog', expected_output='dog'),
        _make_report_case('c4', output='cat', expected_output='dog'),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='output',
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    assert result.class_labels == ['cat', 'dog']
    # matrix[expected_idx][predicted_idx]
    # cat->cat=1, cat->dog=1, dog->cat=1, dog->dog=1
    assert result.matrix == [[1, 1], [1, 1]]

def test_confusion_matrix_evaluator_from_labels():
    cases = [
        _make_report_case('c1', expected_output='positive', labels={'predicted': 'positive'}),
        _make_report_case('c2', expected_output='negative', labels={'predicted': 'positive'}),
        _make_report_case('c3', expected_output='negative', labels={'predicted': 'negative'}),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='labels',
        predicted_key='predicted',
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    assert result.class_labels == ['negative', 'positive']
    # expected=negative, predicted=negative: 1
    # expected=negative, predicted=positive: 1
    # expected=positive, predicted=positive: 1
    assert result.matrix == [[1, 1], [0, 1]]

def test_confusion_matrix_evaluator_from_metadata():
    cases = [
        _make_report_case('c1', expected_output='A', metadata={'pred': 'A'}),
        _make_report_case('c2', expected_output='B', metadata={'pred': 'A'}),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='metadata',
        predicted_key='pred',
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    assert result.class_labels == ['A', 'B']
    assert result.matrix == [[1, 0], [1, 0]]

def test_confusion_matrix_evaluator_skips_none():
    cases = [
        _make_report_case('c1', output='cat', expected_output='cat'),
        _make_report_case('c2', output='dog', expected_output=None),  # should be skipped
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(predicted_from='output', expected_from='expected_output')
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    assert result.class_labels == ['cat']
    assert result.matrix == [[1]]

def test_confusion_matrix_labels_requires_key():
    evaluator = ConfusionMatrixEvaluator(predicted_from='labels', predicted_key=None)
    cases = [_make_report_case('c1', expected_output='a', labels={})]
    report = _make_report(cases)
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)

    with pytest.raises(ValueError, match="'key' is required"):
        evaluator.evaluate(ctx)

def test_precision_recall_evaluator_from_metrics():
    cases = [
        _make_report_case('c1', metrics={'score': 0.9}, assertions={'positive': True}),
        _make_report_case('c2', metrics={'score': 0.1}, assertions={'positive': False}),
    ]
    report = _make_report(cases)

    evaluator = PrecisionRecallEvaluator(
        score_from='metrics',
        score_key='score',
        positive_from='assertions',
        positive_key='positive',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, PrecisionRecall)
    assert len(result.curves) == 1

def test_precision_recall_evaluator_empty():
    report = _make_report([])
    evaluator = PrecisionRecallEvaluator(
        score_from='scores',
        score_key='s',
        positive_from='assertions',
        positive_key='p',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, PrecisionRecall)
    assert len(result.curves) == 0

def test_precision_recall_assertions_requires_key():
    evaluator = PrecisionRecallEvaluator(
        score_from='scores',
        score_key='s',
        positive_from='assertions',
        positive_key=None,
    )
    cases = [_make_report_case('c1', scores={'s': 0.5})]
    report = _make_report(cases)
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)

    with pytest.raises(ValueError, match="'positive_key' is required"):
        evaluator.evaluate(ctx)

def test_precision_recall_labels_requires_key():
    evaluator = PrecisionRecallEvaluator(
        score_from='scores',
        score_key='s',
        positive_from='labels',
        positive_key=None,
    )
    cases = [_make_report_case('c1', scores={'s': 0.5})]
    report = _make_report(cases)
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)

    with pytest.raises(ValueError, match="'positive_key' is required"):
        evaluator.evaluate(ctx)

def test_report_rendering_includes_analyses():
    cases = [
        _make_report_case('c1', output='cat', expected_output='cat'),
    ]
    report = _make_report(cases)
    report.analyses = [
        ScalarResult(title='Accuracy', value=100.0, unit='%'),
        ConfusionMatrix(
            title='CM',
            class_labels=['cat'],
            matrix=[[1]],
        ),
    ]

    rendered = report.render(width=120)
    assert 'Accuracy: 100.0 %' in rendered
    assert 'CM' in rendered

def test_report_rendering_include_analyses_false():
    cases = [
        _make_report_case('c1', output='cat', expected_output='cat'),
    ]
    report = _make_report(cases)
    report.analyses = [
        ScalarResult(title='Accuracy', value=100.0, unit='%'),
    ]

    rendered = report.render(width=120, include_analyses=False)
    assert 'Accuracy: 100.0 %' not in rendered

def test_report_rendering_include_evaluator_failures_false():
    from pydantic_evals.evaluators.evaluator import EvaluatorFailure
    from pydantic_evals.evaluators.spec import EvaluatorSpec

    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.report_evaluator_failures = [
        EvaluatorFailure(
            name='BrokenEvaluator',
            error_message='ValueError: oops',
            error_stacktrace='Traceback ...',
            source=EvaluatorSpec(name='BrokenEvaluator', arguments=None),
        ),
    ]

    rendered = report.render(width=120, include_evaluator_failures=False)
    assert 'Report Evaluator Failures' not in rendered
    assert 'BrokenEvaluator' not in rendered

def test_evaluation_report_analyses_default():
    report = EvaluationReport(name='test', cases=[])
    assert report.analyses == []

def test_report_evaluator_get_serialization_name():
    """get_serialization_name works as classmethod and on instance."""
    assert ConfusionMatrixEvaluator.get_serialization_name() == 'ConfusionMatrixEvaluator'
    assert PrecisionRecallEvaluator.get_serialization_name() == 'PrecisionRecallEvaluator'
    # Also works on instance
    assert ConfusionMatrixEvaluator().get_serialization_name() == 'ConfusionMatrixEvaluator'

def test_report_evaluator_as_spec_no_args():
    """Report evaluator with all defaults produces spec with no arguments."""
    from pydantic_evals.evaluators.spec import EvaluatorSpec

    evaluator = ConfusionMatrixEvaluator()
    spec = evaluator.as_spec()
    assert isinstance(spec, EvaluatorSpec)
    assert spec.name == 'ConfusionMatrixEvaluator'
    assert spec.arguments is None

def test_report_evaluator_as_spec_with_args():
    """Report evaluator with non-default args produces spec with arguments."""
    evaluator = ConfusionMatrixEvaluator(predicted_from='labels', predicted_key='pred', title='Custom CM')
    spec = evaluator.as_spec()
    assert spec.name == 'ConfusionMatrixEvaluator'
    assert isinstance(spec.arguments, dict)
    assert spec.arguments['predicted_from'] == 'labels'
    assert spec.arguments['predicted_key'] == 'pred'
    assert spec.arguments['title'] == 'Custom CM'

def test_report_evaluator_as_spec_single_arg_non_first_field():
    """Report evaluator with one non-default arg that isn't the first field uses dict form."""
    evaluator = ConfusionMatrixEvaluator(title='My Matrix')
    spec = evaluator.as_spec()
    assert spec.name == 'ConfusionMatrixEvaluator'
    # title is not the first field, so dict form is used to preserve the field name
    assert isinstance(spec.arguments, dict)
    assert spec.arguments == {'title': 'My Matrix'}

def test_report_evaluator_as_spec_single_arg_first_field():
    """Report evaluator with one non-default arg that is the first field uses tuple form."""
    evaluator = ConfusionMatrixEvaluator(predicted_from='labels')
    spec = evaluator.as_spec()
    assert spec.name == 'ConfusionMatrixEvaluator'
    assert isinstance(spec.arguments, tuple)
    assert spec.arguments == ('labels',)

def test_report_evaluator_build_serialization_arguments_excludes_defaults():
    """ConfusionMatrixEvaluator with all defaults returns empty dict."""
    evaluator = ConfusionMatrixEvaluator()
    args = evaluator.build_serialization_arguments()
    assert args == {}

def test_confusion_matrix_evaluator_metadata_non_dict():
    """ConfusionMatrixEvaluator with metadata_from but non-dict metadata returns str(metadata)."""
    cases = [
        _make_report_case('c1', expected_output='A', metadata='some_string'),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='metadata',
        predicted_key=None,
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    assert result.class_labels == ['A', 'some_string']
    assert result.matrix == [[0, 1], [0, 0]]

def test_confusion_matrix_evaluator_metadata_key_with_non_dict():
    """ConfusionMatrixEvaluator with metadata key but non-dict metadata skips the case."""
    cases = [
        _make_report_case('c1', expected_output='A', metadata='some_string'),
        _make_report_case('c2', expected_output='B', metadata={'pred': 'B'}),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='metadata',
        predicted_key='pred',
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    # c1 should be skipped (non-dict metadata with key), only c2 used
    assert result.class_labels == ['B']
    assert result.matrix == [[1]]

def test_precision_recall_evaluator_skips_missing_scores():
    """PrecisionRecallEvaluator skips cases missing score or positive data."""
    cases = [
        _make_report_case('c1', scores={'confidence': 0.9}, assertions={'is_correct': True}),
        _make_report_case('c2', scores={}, assertions={'is_correct': False}),  # missing score
        _make_report_case('c3', scores={'confidence': 0.3}, assertions={}),  # missing assertion
    ]
    report = _make_report(cases)

    evaluator = PrecisionRecallEvaluator(
        score_key='confidence',
        positive_from='assertions',
        positive_key='is_correct',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, PrecisionRecall)
    assert len(result.curves) == 1

def test_precision_recall_evaluator_positive_from_expected_output():
    """PrecisionRecallEvaluator with positive_from='expected_output'."""
    cases = [
        _make_report_case('c1', scores={'conf': 0.9}, expected_output='yes'),
        _make_report_case('c2', scores={'conf': 0.1}, expected_output=''),
        _make_report_case('c3', scores={'conf': 0.5}, expected_output=None),  # skipped
    ]
    report = _make_report(cases)

    evaluator = PrecisionRecallEvaluator(
        score_key='conf',
        positive_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, PrecisionRecall)
    assert len(result.curves) == 1

def test_precision_recall_evaluator_positive_from_labels():
    """PrecisionRecallEvaluator with positive_from='labels'."""
    cases = [
        _make_report_case('c1', scores={'conf': 0.9}, labels={'is_pos': 'yes'}),
        _make_report_case('c2', scores={'conf': 0.1}, labels={'is_pos': ''}),
    ]
    report = _make_report(cases)

    evaluator = PrecisionRecallEvaluator(
        score_key='conf',
        positive_from='labels',
        positive_key='is_pos',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, PrecisionRecall)
    assert len(result.curves) == 1

async def test_async_report_evaluator():
    """Async report evaluator is awaited through evaluate_async."""

    @dataclass
    class AsyncEvaluator(ReportEvaluator):
        async def evaluate(self, ctx: ReportEvaluatorContext) -> ScalarResult:
            return ScalarResult(title='Async Result', value=42)

    evaluator = AsyncEvaluator()
    report = _make_report([_make_report_case('c1', output='x')])
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = await evaluator.evaluate_async(ctx)

    assert isinstance(result, ScalarResult)
    assert result.value == 42

def test_report_rendering_with_failures():
    """Report rendering includes report_evaluator_failures."""
    from pydantic_evals.evaluators.evaluator import EvaluatorFailure
    from pydantic_evals.evaluators.spec import EvaluatorSpec

    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.report_evaluator_failures = [
        EvaluatorFailure(
            name='BrokenEvaluator',
            error_message='ValueError: something went wrong',
            error_stacktrace='Traceback ...',
            source=EvaluatorSpec(name='BrokenEvaluator', arguments=None),
        ),
    ]

    rendered = report.render(width=120)
    assert 'Report Evaluator Failures:' in rendered
    assert 'BrokenEvaluator' in rendered
    assert 'something went wrong' in rendered

def test_report_rendering_scalar_without_unit():
    """ScalarResult rendering without a unit."""
    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.analyses = [
        ScalarResult(title='Count', value=10),
    ]

    rendered = report.render(width=120)
    assert 'Count: 10' in rendered

def test_report_rendering_precision_recall():
    """PrecisionRecall rendering."""
    from pydantic_evals.reporting.analyses import PrecisionRecallCurve, PrecisionRecallPoint

    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.analyses = [
        PrecisionRecall(
            title='PR Curve',
            curves=[
                PrecisionRecallCurve(
                    name='test_curve',
                    points=[PrecisionRecallPoint(threshold=0.5, precision=0.8, recall=0.7)],
                    auc=0.75,
                ),
            ],
        ),
    ]

    rendered = report.render(width=120)
    assert 'PR Curve' in rendered
    assert 'test_curve' in rendered
    assert 'AUC=0.7500' in rendered

def test_report_rendering_table_result():
    """TableResult rendering."""
    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.analyses = [
        TableResult(
            title='Summary Table',
            columns=['Name', 'Value'],
            rows=[['accuracy', 0.95], ['f1', 0.9]],
        ),
    ]

    rendered = report.render(width=120)
    assert 'Summary Table' in rendered
    assert 'accuracy' in rendered

## tests/evals/test_reporting.py

def sample_assertion(mock_evaluator: Evaluator[TaskInput, TaskOutput, TaskMetadata]) -> EvaluationResult[bool]:
    return EvaluationResult(
        name='MockEvaluator',
        value=True,
        reason=None,
        source=mock_evaluator.as_spec(),
    )

def sample_score(mock_evaluator: Evaluator[TaskInput, TaskOutput, TaskMetadata]) -> EvaluationResult[float]:
    return EvaluationResult(
        name='MockEvaluator',
        value=2.5,
        reason='my reason',
        source=mock_evaluator.as_spec(),
    )

def sample_label(mock_evaluator: Evaluator[TaskInput, TaskOutput, TaskMetadata]) -> EvaluationResult[str]:
    return EvaluationResult(
        name='MockEvaluator',
        value='hello',
        reason=None,
        source=mock_evaluator.as_spec(),
    )

def sample_report(sample_report_case: ReportCase) -> EvaluationReport:
    return EvaluationReport(
        cases=[sample_report_case],
        name='test_report',
    )

## tests/evals/test_reports.py

def sample_evaluator_output() -> dict[str, Any]:
    return {'correct': True, 'confidence': 0.95}

def sample_evaluation_result(
    sample_evaluator_output: dict[str, Any], mock_evaluator: Evaluator[TaskInput, TaskOutput, TaskMetadata]
) -> EvaluationResult[bool]:
    return EvaluationResult(
        name='MockEvaluator',
        value=True,
        reason=None,
        source=mock_evaluator.as_spec(),
    )

def sample_report(sample_report_case: ReportCase) -> EvaluationReport:
    return EvaluationReport(
        cases=[sample_report_case],
        name='test_report',
    )

async def test_report_init(sample_report_case: ReportCase):
    """Test EvaluationReport initialization."""
    report = EvaluationReport(
        cases=[sample_report_case],
        name='test_report',
    )

    assert report.name == 'test_report'
    assert len(report.cases) == 1

async def test_report_serialization(sample_report: EvaluationReport):
    """Test serializing a report to dict."""
    # Serialize the report
    serialized = EvaluationReportAdapter.dump_python(sample_report)

    # Check the serialized structure
    assert 'cases' in serialized
    assert 'name' in serialized

    # Check the values
    assert serialized['name'] == 'test_report'
    assert len(serialized['cases']) == 1

## tests/evals/test_utils.py

def test_get_unwrapped_function_name_basic():
    """Test get_unwrapped_function_name with basic function."""

    def test_func():
        pass

    assert get_unwrapped_function_name(test_func) == 'test_func'

def test_get_unwrapped_function_name_partial():
    """Test get_unwrapped_function_name with partial function."""

    def test_func(x: int, y: int):
        raise NotImplementedError

    partial_func = partial(test_func, y=42)
    assert get_unwrapped_function_name(partial_func) == 'test_func'

## tests/graph/beta/test_broadcast_and_spread.py

class CounterState:
    values: list[int] = field(default_factory=list[int])

## tests/graph/beta/test_decisions.py

class DecisionState:
    path_taken: str | None = None
    value: int = 0

## tests/graph/beta/test_edge_cases.py

class EdgeCaseState:
    value: int = 0
    error_raised: bool = False

## tests/graph/beta/test_edge_labels.py

class LabelState:
    value: int = 0

## tests/graph/beta/test_graph_builder.py

class SimpleState:
    counter: int = 0
    result: str | None = None

async def test_explicit_graph_name():
    """Test setting an explicit graph name."""
    g = GraphBuilder(name='ExplicitName', input_type=int, output_type=int)

    g.add(g.edge_from(g.start_node).to(g.end_node))

    graph = g.build()
    assert graph.name == 'ExplicitName'

## tests/graph/beta/test_graph_edge_cases.py

class MyState:
    value: int = 0

## tests/graph/beta/test_graph_execution.py

class ExecutionState:
    log: list[str] = field(default_factory=list[str])
    counter: int = 0

## tests/graph/beta/test_v1_v2_integration.py

class IntegrationState:
    log: list[str] = field(default_factory=list[str])

async def test_v1_node_conditional_return():
    """Test v1 nodes with conditional returns creating implicit decisions."""

    @dataclass
    class RouterNode(BaseNode[IntegrationState, None, str]):
        value: int

        async def run(self, ctx: GraphRunContext[IntegrationState, None]) -> PathA | PathB:
            if self.value < 10:
                return PathA()
            else:
                return PathB()

    @dataclass
    class PathA(BaseNode[IntegrationState, None, str]):
        async def run(self, ctx: GraphRunContext[IntegrationState, None]) -> End[str]:
            return End('Path A')

    @dataclass
    class PathB(BaseNode[IntegrationState, None, str]):
        async def run(self, ctx: GraphRunContext[IntegrationState, None]) -> End[str]:
            return End('Path B')

    g = GraphBuilder(state_type=IntegrationState, input_type=int, output_type=str)

    @g.step
    async def create_router(ctx: StepContext[IntegrationState, None, int]) -> RouterNode:
        return RouterNode(ctx.inputs)

    g.add(
        g.node(RouterNode),
        g.node(PathA),
        g.node(PathB),
        g.edge_from(g.start_node).to(create_router),
    )

    graph = g.build()

    assert str(graph) == snapshot("""\
stateDiagram-v2
  create_router
  RouterNode
  state decision <<choice>>
  PathA
  PathB

  [*] --> create_router
  create_router --> RouterNode
  RouterNode --> decision
  decision --> PathA
  decision --> PathB
  PathA --> [*]
  PathB --> [*]\
""")

    # Test path A
    result_a = await graph.run(state=IntegrationState(), inputs=5)
    assert result_a == 'Path A'

    # Test path B
    result_b = await graph.run(state=IntegrationState(), inputs=15)
    assert result_b == 'Path B'

## tests/graph/test_file_persistence.py

class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

## tests/graph/test_mermaid.py

class Foo(BaseNode):
    async def run(self, ctx: GraphRunContext) -> Bar:
        return Bar()

class Bar(BaseNode[None, None, None]):
    async def run(self, ctx: GraphRunContext) -> End[None]:
        return End(None)

## tests/models/test_model_test.py

class AgentRunDeps:
    run_id: int

## tests/models/test_openai.py

def tool_with_discriminated_union(
    x: Annotated[
        Annotated[int, Tag('int')] | Annotated[MyDefaultDc, Tag('MyDefaultDc')],
        Discriminator(lambda x: type(x).__name__),
    ],
) -> str:
    return f'{x}'  # pragma: no cover

## tests/test_ag_ui.py

async def run_and_collect_events(
    agent: Agent[AgentDepsT, OutputDataT],
    *run_inputs: RunAgentInput,
    deps: AgentDepsT = None,
    on_complete: OnCompleteFunc[BaseEvent] | None = None,
) -> list[dict[str, Any]]:
    events = list[dict[str, Any]]()
    for run_input in run_inputs:
        async for event in run_ag_ui(agent, run_input, deps=deps, on_complete=on_complete):
            events.append(json.loads(event.removeprefix('data: ')))
    return events

## tests/test_agent.py

async def test_agent_iter_metadata_surfaces_on_result() -> None:
    agent = Agent(TestModel(custom_output_text='iter metadata output'), metadata={'env': 'tests'})

    async with agent.iter('iter metadata prompt') as agent_run:
        async for _ in agent_run:
            pass

    assert agent_run.metadata == {'env': 'tests'}
    assert agent_run.result is not None
    assert agent_run.result.metadata == {'env': 'tests'}

## tests/test_deps.py

class MyDeps:
    foo: int
    bar: int

## tests/test_mcp.py

async def test_tool_metadata_extraction():
    """Test that MCP tool metadata is properly extracted into ToolDefinition."""

    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'])
    async with server:
        ctx = RunContext(deps=None, model=TestModel(), usage=RunUsage())
        tools = [tool.tool_def for tool in (await server.get_tools(ctx)).values()]
        # find `celsius_to_fahrenheit`
        celsius_to_fahrenheit = next(tool for tool in tools if tool.name == 'celsius_to_fahrenheit')
        assert celsius_to_fahrenheit.metadata is not None
        assert celsius_to_fahrenheit.metadata.get('annotations') is not None
        assert celsius_to_fahrenheit.metadata.get('annotations', {}).get('title', None) == 'Celsius to Fahrenheit'
        assert celsius_to_fahrenheit.metadata.get('output_schema') is not None
        assert celsius_to_fahrenheit.metadata.get('output_schema', {}).get('type', None) == 'object'

## tests/test_parts_manager.py

def test_handle_thinking_delta_new_part_with_vendor_id():
    manager = ModelResponsePartsManager()

    event = next(manager.handle_thinking_delta(vendor_part_id='thinking', content='new thought', signature=None))
    assert isinstance(event, PartStartEvent)
    assert event.index == 0

    parts = manager.get_parts()
    assert parts == snapshot([ThinkingPart(content='new thought')])

def test_get_part_by_vendor_id():
    manager = ModelResponsePartsManager()

    event = next(manager.handle_text_delta(vendor_part_id='content', content='hello'))
    assert isinstance(event, PartStartEvent)

    part = manager.get_part_by_vendor_id('content')
    assert part == snapshot(TextPart(content='hello', part_kind='text'))

    assert manager.get_part_by_vendor_id('missing') is None

## tests/test_prefect.py

def conditions(city: str) -> str:
    # Simplified version without RunContext
    return "It's raining"

## tests/test_ssrf.py

    def test_http_default_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='203.0.113.50', hostname='example.com', port=80, is_https=False, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'http://203.0.113.50/path'

    def test_https_default_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='203.0.113.50', hostname='example.com', port=443, is_https=True, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'https://203.0.113.50/path'

## tests/test_ui.py

async def test_run_stream_native_metadata_forwarded():
    agent = Agent(model=TestModel(custom_output_text='native meta'))
    adapter = DummyUIAdapter(agent, DummyUIRunInput(messages=[ModelRequest.user_text_prompt('Hello')]))

    events = [event async for event in adapter.run_stream_native(metadata={'ui': 'native'})]
    run_result_event = next(event for event in events if isinstance(event, AgentRunResultEvent))

    assert run_result_event.result.metadata == {'ui': 'native'}
