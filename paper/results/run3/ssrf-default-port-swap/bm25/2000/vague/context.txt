# pydantic_evals/pydantic_evals/dataset.py:51-51
from .reporting import EvaluationReport, ReportCase, ReportCaseAggregate, ReportCaseFailure

# pydantic_evals/pydantic_evals/reporting/__init__.py:134-134
ReportCaseFailureAdapter = TypeAdapter(ReportCaseFailure[Any, Any, Any])

# pydantic_evals/pydantic_evals/reporting/__init__.py:315-315
    report_evaluator_failures: list[EvaluatorFailure] = field(default_factory=list[EvaluatorFailure])

# tests/test_ssrf.py:242-247
    def test_https_default_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='203.0.113.50', hostname='example.com', port=443, is_https=True, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'https://203.0.113.50/path'

# pydantic_evals/pydantic_evals/reporting/__init__.py:639-672
    def failures_table(
        self,
        *,
        include_input: bool = False,
        include_metadata: bool = False,
        include_expected_output: bool = False,
        include_error_message: bool = True,
        include_error_stacktrace: bool = True,
        input_config: RenderValueConfig | None = None,
        metadata_config: RenderValueConfig | None = None,
    ) -> Table:
        """Return a table containing the failures in this report."""
        renderer = EvaluationRenderer(
            include_input=include_input,
            include_metadata=include_metadata,
            include_expected_output=include_expected_output,
            include_output=False,
            include_durations=False,
            include_total_duration=False,
            include_removed_cases=False,
            include_averages=False,
            input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},
            metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},
            output_config=_DEFAULT_VALUE_CONFIG,
            score_configs={},
            label_configs={},
            metric_configs={},
            duration_config=_DEFAULT_DURATION_CONFIG,
            include_reasons=False,
            include_error_message=include_error_message,
            include_error_stacktrace=include_error_stacktrace,
            include_evaluator_failures=False,  # Not applicable for failures table
        )
        return renderer.build_failures_table(self)

# pydantic_evals/pydantic_evals/reporting/__init__.py:156-156
    failures: Sequence[ReportCaseFailure[InputsT, OutputT, MetadataT]]

# pydantic_evals/pydantic_evals/dataset.py:85-85
_REPORT_CASE_FAILURES_ADAPTER = TypeAdapter(list[ReportCaseFailure])

# pydantic_evals/pydantic_evals/reporting/__init__.py:307-309
    failures: list[ReportCaseFailure[InputsT, OutputT, MetadataT]] = field(
        default_factory=list[ReportCaseFailure[InputsT, OutputT, MetadataT]]
    )

# tests/evals/test_report_evaluators.py:755-773
def test_report_rendering_with_failures():
    """Report rendering includes report_evaluator_failures."""
    from pydantic_evals.evaluators.evaluator import EvaluatorFailure
    from pydantic_evals.evaluators.spec import EvaluatorSpec

    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.report_evaluator_failures = [
        EvaluatorFailure(
            name='BrokenEvaluator',
            error_message='ValueError: something went wrong',
            error_stacktrace='Traceback ...',
            source=EvaluatorSpec(name='BrokenEvaluator', arguments=None),
        ),
    ]

    rendered = report.render(width=120)
    assert 'Report Evaluator Failures:' in rendered
    assert 'BrokenEvaluator' in rendered
    assert 'something went wrong' in rendered

# tests/test_ssrf.py:192-197
    def test_basic_https_url(self) -> None:
        hostname, path, port, is_https = extract_host_and_port('https://example.com/path')
        assert hostname == 'example.com'
        assert path == '/path'
        assert port == 443
        assert is_https is True

# pydantic_ai_slim/pydantic_ai/_ssrf.py:64-64
    port: int

# pydantic_evals/pydantic_evals/dataset.py:51-51
from .reporting import EvaluationReport, ReportCase, ReportCaseAggregate, ReportCaseFailure

# tests/evals/test_report_evaluators.py:848-876
async def test_report_evaluator_failure_does_not_block_others():
    """When one report evaluator fails, subsequent evaluators still run."""

    @dataclass
    class BrokenEvaluator(ReportEvaluator):
        def evaluate(self, ctx: ReportEvaluatorContext) -> ReportAnalysis:
            raise RuntimeError('first evaluator broke')

    @dataclass
    class WorkingEvaluator(ReportEvaluator):
        def evaluate(self, ctx: ReportEvaluatorContext) -> ScalarResult:
            return ScalarResult(title='Count', value=len(ctx.report.cases))

    dataset = Dataset[str, str, None](
        cases=[Case(inputs='hello', expected_output='world')],
        report_evaluators=[BrokenEvaluator(), WorkingEvaluator()],
    )

    async def task(inputs: str) -> str:
        return inputs

    report = await dataset.evaluate(task, progress=False)
    # The broken evaluator's failure is captured
    assert len(report.report_evaluator_failures) == 1
    assert 'first evaluator broke' in report.report_evaluator_failures[0].error_message
    # The working evaluator still ran and produced its analysis
    assert len(report.analyses) == 1
    assert isinstance(report.analyses[0], ScalarResult)
    assert report.analyses[0].value == 1

# pydantic_evals/pydantic_evals/reporting/__init__.py:109-109
    name: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:102-102
    evaluator_failures: list[EvaluatorFailure] = field(default_factory=list[EvaluatorFailure])

# tests/evals/test_report_evaluators.py:879-908
async def test_report_evaluator_failures_set_on_span(capfire: CaptureLogfire):
    """Report evaluator failures are set as a span attribute on the experiment span."""

    @dataclass
    class BrokenEvaluator(ReportEvaluator):
        def evaluate(self, ctx: ReportEvaluatorContext) -> ReportAnalysis:
            raise RuntimeError('evaluator broke')

    dataset = Dataset[str, str, None](
        cases=[Case(inputs='hello', expected_output='world')],
        report_evaluators=[BrokenEvaluator()],
    )

    async def task(inputs: str) -> str:
        return inputs

    report = await dataset.evaluate(task, progress=False)
    assert len(report.report_evaluator_failures) == 1

    spans = capfire.exporter.exported_spans_as_dict(parse_json_attributes=True)
    experiment_spans = [s for s in spans if s['name'] == 'evaluate {name}']
    assert len(experiment_spans) == 1
    attrs = experiment_spans[0]['attributes']
    failures = attrs.get('logfire.experiment.report_evaluator_failures')
    assert failures is not None
    assert len(failures) == 1
    assert failures[0]['name'] == 'BrokenEvaluator'
    assert 'evaluator broke' in failures[0]['error_message']
    assert 'error_stacktrace' in failures[0]
    assert failures[0]['source']['name'] == 'BrokenEvaluator'

# pydantic_evals/pydantic_evals/reporting/__init__.py:111-111
    inputs: InputsT

# pydantic_evals/pydantic_evals/reporting/__init__.py:129-129
    span_id: str | None = None

# pydantic_evals/pydantic_evals/reporting/__init__.py:113-113
    metadata: MetadataT | None

# pydantic_evals/pydantic_evals/reporting/__init__.py:127-127
    trace_id: str | None = None

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:120-125
    def shutdown(self) -> None:
        """Shut downs the exporter.

        Calls to export after the exporter has been shut down will fail.
        """
        self._stopped = True

# pydantic_evals/pydantic_evals/reporting/__init__.py:990-1004
    def build_failures_table(self, title: str) -> Table:
        """Build and return a Rich Table for the failures output."""
        table = Table(title=title, show_lines=True)
        table.add_column('Case ID', style='bold')
        if self.include_input:
            table.add_column('Inputs', overflow='fold')
        if self.include_metadata:
            table.add_column('Metadata', overflow='fold')
        if self.include_expected_output:
            table.add_column('Expected Output', overflow='fold')
        if self.include_error_message:
            table.add_column('Error Message', overflow='fold')
        if self.include_error_stacktrace:
            table.add_column('Error Stacktrace', overflow='fold')
        return table

# pydantic_evals/pydantic_evals/reporting/__init__.py:952-952
    include_evaluator_failures: bool

# pydantic_evals/pydantic_evals/reporting/__init__.py:1326-1338
    def _render_evaluator_failures(
        self,
        failures: list[EvaluatorFailure],
    ) -> str:
        if not failures:
            return EMPTY_CELL_STR  # pragma: no cover
        lines: list[str] = []
        for failure in failures:
            line = f'[red]{failure.name}[/]'
            if failure.error_message:
                line += f': {failure.error_message}'
            lines.append(line)
        return '\n'.join(lines)

# pydantic_evals/pydantic_evals/reporting/__init__.py:118-118
    error_message: str