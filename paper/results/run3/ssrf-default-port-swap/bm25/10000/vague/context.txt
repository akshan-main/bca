# pydantic_evals/pydantic_evals/dataset.py:51-51
from .reporting import EvaluationReport, ReportCase, ReportCaseAggregate, ReportCaseFailure

# pydantic_evals/pydantic_evals/reporting/__init__.py:134-134
ReportCaseFailureAdapter = TypeAdapter(ReportCaseFailure[Any, Any, Any])

# pydantic_evals/pydantic_evals/reporting/__init__.py:315-315
    report_evaluator_failures: list[EvaluatorFailure] = field(default_factory=list[EvaluatorFailure])

# tests/test_ssrf.py:242-247
    def test_https_default_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='203.0.113.50', hostname='example.com', port=443, is_https=True, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'https://203.0.113.50/path'

# pydantic_evals/pydantic_evals/reporting/__init__.py:639-672
    def failures_table(
        self,
        *,
        include_input: bool = False,
        include_metadata: bool = False,
        include_expected_output: bool = False,
        include_error_message: bool = True,
        include_error_stacktrace: bool = True,
        input_config: RenderValueConfig | None = None,
        metadata_config: RenderValueConfig | None = None,
    ) -> Table:
        """Return a table containing the failures in this report."""
        renderer = EvaluationRenderer(
            include_input=include_input,
            include_metadata=include_metadata,
            include_expected_output=include_expected_output,
            include_output=False,
            include_durations=False,
            include_total_duration=False,
            include_removed_cases=False,
            include_averages=False,
            input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},
            metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},
            output_config=_DEFAULT_VALUE_CONFIG,
            score_configs={},
            label_configs={},
            metric_configs={},
            duration_config=_DEFAULT_DURATION_CONFIG,
            include_reasons=False,
            include_error_message=include_error_message,
            include_error_stacktrace=include_error_stacktrace,
            include_evaluator_failures=False,  # Not applicable for failures table
        )
        return renderer.build_failures_table(self)

# pydantic_evals/pydantic_evals/reporting/__init__.py:156-156
    failures: Sequence[ReportCaseFailure[InputsT, OutputT, MetadataT]]

# pydantic_evals/pydantic_evals/dataset.py:85-85
_REPORT_CASE_FAILURES_ADAPTER = TypeAdapter(list[ReportCaseFailure])

# pydantic_evals/pydantic_evals/reporting/__init__.py:307-309
    failures: list[ReportCaseFailure[InputsT, OutputT, MetadataT]] = field(
        default_factory=list[ReportCaseFailure[InputsT, OutputT, MetadataT]]
    )

# tests/evals/test_report_evaluators.py:755-773
def test_report_rendering_with_failures():
    """Report rendering includes report_evaluator_failures."""
    from pydantic_evals.evaluators.evaluator import EvaluatorFailure
    from pydantic_evals.evaluators.spec import EvaluatorSpec

    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.report_evaluator_failures = [
        EvaluatorFailure(
            name='BrokenEvaluator',
            error_message='ValueError: something went wrong',
            error_stacktrace='Traceback ...',
            source=EvaluatorSpec(name='BrokenEvaluator', arguments=None),
        ),
    ]

    rendered = report.render(width=120)
    assert 'Report Evaluator Failures:' in rendered
    assert 'BrokenEvaluator' in rendered
    assert 'something went wrong' in rendered

# tests/test_ssrf.py:192-197
    def test_basic_https_url(self) -> None:
        hostname, path, port, is_https = extract_host_and_port('https://example.com/path')
        assert hostname == 'example.com'
        assert path == '/path'
        assert port == 443
        assert is_https is True

# pydantic_ai_slim/pydantic_ai/_ssrf.py:64-64
    port: int

# pydantic_evals/pydantic_evals/dataset.py:51-51
from .reporting import EvaluationReport, ReportCase, ReportCaseAggregate, ReportCaseFailure

# tests/evals/test_report_evaluators.py:848-876
async def test_report_evaluator_failure_does_not_block_others():
    """When one report evaluator fails, subsequent evaluators still run."""

    @dataclass
    class BrokenEvaluator(ReportEvaluator):
        def evaluate(self, ctx: ReportEvaluatorContext) -> ReportAnalysis:
            raise RuntimeError('first evaluator broke')

    @dataclass
    class WorkingEvaluator(ReportEvaluator):
        def evaluate(self, ctx: ReportEvaluatorContext) -> ScalarResult:
            return ScalarResult(title='Count', value=len(ctx.report.cases))

    dataset = Dataset[str, str, None](
        cases=[Case(inputs='hello', expected_output='world')],
        report_evaluators=[BrokenEvaluator(), WorkingEvaluator()],
    )

    async def task(inputs: str) -> str:
        return inputs

    report = await dataset.evaluate(task, progress=False)
    # The broken evaluator's failure is captured
    assert len(report.report_evaluator_failures) == 1
    assert 'first evaluator broke' in report.report_evaluator_failures[0].error_message
    # The working evaluator still ran and produced its analysis
    assert len(report.analyses) == 1
    assert isinstance(report.analyses[0], ScalarResult)
    assert report.analyses[0].value == 1

# pydantic_evals/pydantic_evals/reporting/__init__.py:109-109
    name: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:102-102
    evaluator_failures: list[EvaluatorFailure] = field(default_factory=list[EvaluatorFailure])

# tests/evals/test_report_evaluators.py:879-908
async def test_report_evaluator_failures_set_on_span(capfire: CaptureLogfire):
    """Report evaluator failures are set as a span attribute on the experiment span."""

    @dataclass
    class BrokenEvaluator(ReportEvaluator):
        def evaluate(self, ctx: ReportEvaluatorContext) -> ReportAnalysis:
            raise RuntimeError('evaluator broke')

    dataset = Dataset[str, str, None](
        cases=[Case(inputs='hello', expected_output='world')],
        report_evaluators=[BrokenEvaluator()],
    )

    async def task(inputs: str) -> str:
        return inputs

    report = await dataset.evaluate(task, progress=False)
    assert len(report.report_evaluator_failures) == 1

    spans = capfire.exporter.exported_spans_as_dict(parse_json_attributes=True)
    experiment_spans = [s for s in spans if s['name'] == 'evaluate {name}']
    assert len(experiment_spans) == 1
    attrs = experiment_spans[0]['attributes']
    failures = attrs.get('logfire.experiment.report_evaluator_failures')
    assert failures is not None
    assert len(failures) == 1
    assert failures[0]['name'] == 'BrokenEvaluator'
    assert 'evaluator broke' in failures[0]['error_message']
    assert 'error_stacktrace' in failures[0]
    assert failures[0]['source']['name'] == 'BrokenEvaluator'

# pydantic_evals/pydantic_evals/reporting/__init__.py:111-111
    inputs: InputsT

# pydantic_evals/pydantic_evals/reporting/__init__.py:129-129
    span_id: str | None = None

# pydantic_evals/pydantic_evals/reporting/__init__.py:113-113
    metadata: MetadataT | None

# pydantic_evals/pydantic_evals/reporting/__init__.py:127-127
    trace_id: str | None = None

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:120-125
    def shutdown(self) -> None:
        """Shut downs the exporter.

        Calls to export after the exporter has been shut down will fail.
        """
        self._stopped = True

# pydantic_evals/pydantic_evals/reporting/__init__.py:990-1004
    def build_failures_table(self, title: str) -> Table:
        """Build and return a Rich Table for the failures output."""
        table = Table(title=title, show_lines=True)
        table.add_column('Case ID', style='bold')
        if self.include_input:
            table.add_column('Inputs', overflow='fold')
        if self.include_metadata:
            table.add_column('Metadata', overflow='fold')
        if self.include_expected_output:
            table.add_column('Expected Output', overflow='fold')
        if self.include_error_message:
            table.add_column('Error Message', overflow='fold')
        if self.include_error_stacktrace:
            table.add_column('Error Stacktrace', overflow='fold')
        return table

# tests/evals/test_report_evaluators.py:485-501
def test_report_rendering_include_evaluator_failures_false():
    from pydantic_evals.evaluators.evaluator import EvaluatorFailure
    from pydantic_evals.evaluators.spec import EvaluatorSpec

    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.report_evaluator_failures = [
        EvaluatorFailure(
            name='BrokenEvaluator',
            error_message='ValueError: oops',
            error_stacktrace='Traceback ...',
            source=EvaluatorSpec(name='BrokenEvaluator', arguments=None),
        ),
    ]

    rendered = report.render(width=120, include_evaluator_failures=False)
    assert 'Report Evaluator Failures' not in rendered
    assert 'BrokenEvaluator' not in rendered

# pydantic_evals/pydantic_evals/reporting/__init__.py:952-952
    include_evaluator_failures: bool

# pydantic_evals/pydantic_evals/reporting/__init__.py:1326-1338
    def _render_evaluator_failures(
        self,
        failures: list[EvaluatorFailure],
    ) -> str:
        if not failures:
            return EMPTY_CELL_STR  # pragma: no cover
        lines: list[str] = []
        for failure in failures:
            line = f'[red]{failure.name}[/]'
            if failure.error_message:
                line += f': {failure.error_message}'
            lines.append(line)
        return '\n'.join(lines)

# pydantic_evals/pydantic_evals/reporting/__init__.py:118-118
    error_message: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:1192-1211
    def build_failure_row(self, case: ReportCaseFailure) -> list[str]:
        """Build a table row for a single case failure."""
        row = [case.name]

        if self.include_input:
            row.append(self.input_renderer.render_value(None, case.inputs) or EMPTY_CELL_STR)

        if self.include_metadata:
            row.append(self.metadata_renderer.render_value(None, case.metadata) or EMPTY_CELL_STR)

        if self.include_expected_output:
            row.append(self.output_renderer.render_value(None, case.expected_output) or EMPTY_CELL_STR)

        if self.include_error_message:
            row.append(case.error_message or EMPTY_CELL_STR)

        if self.include_error_stacktrace:
            row.append(case.error_stacktrace or EMPTY_CELL_STR)

        return row

# tests/test_ssrf.py:249-254
    def test_custom_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='203.0.113.50', hostname='example.com', port=8080, is_https=False, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'http://203.0.113.50:8080/path'

# pydantic_evals/pydantic_evals/reporting/__init__.py:115-115
    expected_output: OutputT | None

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:93-93
    openai_chat_supports_file_urls: bool = False

# pydantic_evals/pydantic_evals/reporting/__init__.py:1340-1349
    def _render_evaluator_failures_diff(
        self,
        baseline_failures: list[EvaluatorFailure],
        new_failures: list[EvaluatorFailure],
    ) -> str:
        baseline_str = self._render_evaluator_failures(baseline_failures)
        new_str = self._render_evaluator_failures(new_failures)
        if baseline_str == new_str:
            return baseline_str  # pragma: no cover
        return f'{baseline_str}\nâ†’\n{new_str}'

# pydantic_evals/pydantic_evals/reporting/__init__.py:120-120
    error_stacktrace: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:123-123
    source_case_name: str | None = None

# pydantic_ai_slim/pydantic_ai/_ssrf.py:151-179
def extract_host_and_port(url: str) -> tuple[str, str, int, bool]:
    """Extract hostname, path, port, and protocol info from a URL.

    Returns:
        Tuple of (hostname, path_with_query, port, is_https)

    Raises:
        ValueError: If the URL is malformed or uses an unsupported protocol.
    """
    # Validate protocol first, before trying to extract hostname
    _, is_https = validate_url_protocol(url)

    parsed = urlparse(url)
    hostname = parsed.hostname

    if not hostname:
        raise ValueError(f'Invalid URL: no hostname found in "{url}"')

    default_port = 80 if is_https else 443
    port = parsed.port or default_port

    # Reconstruct path with query string
    path = parsed.path or '/'
    if parsed.query:
        path = f'{path}?{parsed.query}'
    if parsed.fragment:
        path = f'{path}#{parsed.fragment}'

    return hostname, path, port, is_https

# tests/models/test_xai.py:1547-1557
async def test_xai_audio_url_not_supported(allow_model_requests: None):
    """Test that AudioUrl raises NotImplementedError."""
    response = create_response(content='This should not be reached')
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    audio_url = AudioUrl(url='https://example.com/audio.mp3')

    with pytest.raises(NotImplementedError, match='AudioUrl is not supported by xAI SDK'):
        await agent.run(['What is in this audio?', audio_url])

# tests/models/test_xai.py:1560-1570
async def test_xai_video_url_not_supported(allow_model_requests: None):
    """Test that VideoUrl raises NotImplementedError."""
    response = create_response(content='This should not be reached')
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    video_url = VideoUrl(url='https://example.com/video.mp4')

    with pytest.raises(NotImplementedError, match='VideoUrl is not supported by xAI SDK'):
        await agent.run(['What is in this video?', video_url])

# tests/test_ssrf.py:235-240
    def test_http_default_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='203.0.113.50', hostname='example.com', port=80, is_https=False, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'http://203.0.113.50/path'

# pydantic_evals/pydantic_evals/dataset.py:1051-1078
async def _run_report_evaluators(
    report_evaluators: list[ReportEvaluator],
    report_ctx: ReportEvaluatorContext[Any, Any, Any],
) -> None:
    """Run report evaluators and append their analyses to the report."""
    report = report_ctx.report
    for report_eval in report_evaluators:
        evaluator_name = report_eval.get_serialization_name()
        with logfire_span(
            'report_evaluator: {evaluator_name}',
            evaluator_name=evaluator_name,
        ):
            try:
                result = await report_eval.evaluate_async(report_ctx)
            except Exception as e:
                report.report_evaluator_failures.append(
                    EvaluatorFailure(
                        name=evaluator_name,
                        error_message=f'{type(e).__name__}: {e}',
                        error_stacktrace=traceback.format_exc(),
                        source=report_eval.as_spec(),
                    )
                )
            else:
                if isinstance(result, list):
                    report.analyses.extend(result)
                else:
                    report.analyses.append(result)

# tests/test_ssrf.py:185-190
    def test_basic_http_url(self) -> None:
        hostname, path, port, is_https = extract_host_and_port('http://example.com/path')
        assert hostname == 'example.com'
        assert path == '/path'
        assert port == 80
        assert is_https is False

# tests/test_ssrf.py:261-266
    def test_ipv6_address_custom_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='2001:db8::1', hostname='example.com', port=8443, is_https=True, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'https://[2001:db8::1]:8443/path'

# pydantic_ai_slim/pydantic_ai/_ssrf.py:67-67
    is_https: bool

# tests/models/test_google.py:4837-4857
async def test_http_video_url_downloads_on_google_gla(mocker: MockerFixture):
    """HTTP VideoUrls are downloaded on google-gla with video_metadata preserved."""
    model = GoogleModel('gemini-1.5-flash', provider=GoogleProvider(api_key='test-key'))

    mock_download = mocker.patch(
        'pydantic_ai.models.google.download_item',
        return_value={'data': b'fake video data', 'data_type': 'video/mp4'},
    )

    video = VideoUrl(
        url='https://example.com/video.mp4',
        vendor_metadata={'start_offset': '10s', 'end_offset': '20s'},
    )
    content = await model._map_user_prompt(UserPromptPart(content=[video]))  # pyright: ignore[reportPrivateUsage]

    mock_download.assert_called_once()
    assert len(content) == 1
    assert 'inline_data' in content[0]
    assert 'file_data' not in content[0]
    # video_metadata is preserved even when video is downloaded
    assert content[0].get('video_metadata') == {'start_offset': '10s', 'end_offset': '20s'}

# tests/evals/test_report_evaluators.py:827-845
async def test_report_evaluator_exception_during_evaluate():
    """Report evaluator that raises an exception records a failure."""

    @dataclass
    class BrokenEvaluator(ReportEvaluator):
        def evaluate(self, ctx: ReportEvaluatorContext) -> ReportAnalysis:
            raise RuntimeError('evaluator broke')

    dataset = Dataset[str, str, None](
        cases=[Case(inputs='hello', expected_output='world')],
        report_evaluators=[BrokenEvaluator()],
    )

    async def task(inputs: str) -> str:
        return inputs

    report = await dataset.evaluate(task, progress=False)
    assert len(report.report_evaluator_failures) == 1
    assert 'evaluator broke' in report.report_evaluator_failures[0].error_message

# pydantic_evals/pydantic_evals/evaluators/report_evaluator.py:33-33
    report: EvaluationReport[InputsT, OutputT, MetadataT]

# tests/typed_agent.py:71-75
async def prep_wrong_type(ctx: RunContext[int], tool_def: ToolDefinition) -> ToolDefinition | None:
    if ctx.deps == 42:
        return None
    else:
        return tool_def

# tests/test_settings.py:1-1
import importlib

# tests/test_tenacity.py:29-186
class TestTenacityTransport:
    """Tests for the synchronous TenacityTransport."""

    def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.__enter__ = Mock(return_value=mock_transport)
        mock_transport.__exit__ = Mock(return_value=None)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with transport:
            result = transport.handle_request(request)

        assert result is mock_response
        mock_transport.handle_request.assert_called_once_with(request)

    def test_retry_on_exception(self):
        """Test that exceptions trigger retries."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response = Mock(spec=httpx.Response)

        # Fail twice, succeed on third attempt
        mock_transport.handle_request.side_effect = [
            httpx.ConnectError('Connection failed'),
            httpx.ConnectError('Connection failed again'),
            mock_response,
        ]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),  # Very short wait for tests
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response
        assert mock_transport.handle_request.call_count == 3

    def test_retry_exhausted(self):
        """Test that retry exhaustion re-raises the last exception."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.handle_request.side_effect = httpx.ConnectError('Connection failed')

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(2),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with pytest.raises(httpx.ConnectError, match='Connection failed'):
            transport.handle_request(request)

        assert mock_transport.handle_request.call_count == 2

    def test_validate_response_success(self):
        """Test that validate_response is called and doesn't raise."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response = Mock(spec=httpx.Response)
        mock_response.status_code = 200
        mock_transport.handle_request.return_value = mock_response

        validate_response = Mock()
        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport, validate_response)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response
        validate_response.assert_called_once_with(mock_response)

    def test_validate_response_triggers_retry(self):
        """Test that validate_response can trigger retries."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response_fail = Mock(spec=httpx.Response)
        mock_response_fail.status_code = 429
        mock_response_success = Mock(spec=httpx.Response)
        mock_response_success.status_code = 200

        mock_transport.handle_request.side_effect = [mock_response_fail, mock_response_success]

        def validate_response(response: httpx.Response):
            if response.status_code == 429:
                raise httpx.HTTPStatusError('Rate limited', request=request, response=response)

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.HTTPStatusError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport, validate_response)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response_success
        assert mock_transport.handle_request.call_count == 2

    def test_raise_for_status_in_validate_response(self):
        """Test that response.raise_for_status() works in validate_response callback."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response_fail = Mock(spec=httpx.Response)
        mock_response_fail.status_code = 429
        mock_response_fail.is_success = False
        mock_response_fail.is_error = True
        mock_response_fail.request = None  # Initially None, will be set by transport

        # Mock raise_for_status to check if request is set
        def mock_raise_for_status():
            if mock_response_fail.request is None:
                raise RuntimeError(  # pragma: no cover
                    'Cannot call `raise_for_status` as the request instance has not been set on this response.'
                )
            raise httpx.HTTPStatusError(
                'Too Many Requests', request=mock_response_fail.request, response=mock_response_fail
            )

        mock_response_fail.raise_for_status = mock_raise_for_status

        mock_response_success = Mock(spec=httpx.Response)
        mock_response_success.status_code = 200
        mock_response_success.is_success = True
        mock_response_success.is_error = False
        mock_response_success.raise_for_status = Mock()  # Should not raise

        mock_transport.handle_request.side_effect = [mock_response_fail, mock_response_success]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.HTTPStatusError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(
            config, mock_transport, validate_response=lambda response: response.raise_for_status()
        )

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response_success
        assert mock_transport.handle_request.call_count == 2
        # Verify that the request was set on the failed response before raise_for_status was called
        assert mock_response_fail.request is request
        mock_response_success.raise_for_status.assert_called_once()

# examples/pydantic_ai_examples/bank_support.py:45-45
    support_advice: str

# pydantic_evals/pydantic_evals/dataset.py:51-51
from .reporting import EvaluationReport, ReportCase, ReportCaseAggregate, ReportCaseFailure

# pydantic_evals/pydantic_evals/dataset.py:51-51
from .reporting import EvaluationReport, ReportCase, ReportCaseAggregate, ReportCaseFailure

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/conftest.py:249-260
def try_import() -> Iterator[Callable[[], bool]]:
    import_success = False

    def check_import() -> bool:
        return import_success

    try:
        yield check_import
    except ImportError:
        pass
    else:
        import_success = True

# tests/evals/test_reports.py:173-233
async def test_report_with_error(mock_evaluator: Evaluator[TaskInput, TaskOutput, TaskMetadata]):
    """Test a report with error in one of the cases."""
    # Create an evaluator output
    error_output = EvaluationResult[bool](
        name='error_evaluator',
        value=False,  # No result
        reason='Test error message',
        source=mock_evaluator.as_spec(),
    )

    # Create a case
    error_case = ReportCase(
        name='error_case',
        inputs={'query': 'What is 1/0?'},
        output=None,
        expected_output={'answer': 'Error'},
        metadata={'difficulty': 'hard'},
        metrics={},
        attributes={'error': 'Division by zero'},
        scores={},
        labels={},
        assertions={error_output.name: error_output},
        task_duration=0.05,
        total_duration=0.1,
        trace_id='test-error-trace-id',
        span_id='test-error-span-id',
    )

    # Create a report with the error case
    report = EvaluationReport(
        cases=[error_case],
        name='error_report',
    )

    assert ReportCaseAdapter.dump_python(report.cases[0]) == snapshot(
        {
            'assertions': {
                'error_evaluator': {
                    'name': 'error_evaluator',
                    'reason': 'Test error message',
                    'source': {'arguments': None, 'name': 'MockEvaluator'},
                    'value': False,
                }
            },
            'attributes': {'error': 'Division by zero'},
            'evaluator_failures': [],
            'expected_output': {'answer': 'Error'},
            'inputs': {'query': 'What is 1/0?'},
            'labels': {},
            'metadata': {'difficulty': 'hard'},
            'metrics': {},
            'name': 'error_case',
            'output': None,
            'scores': {},
            'span_id': 'test-error-span-id',
            'source_case_name': None,
            'task_duration': 0.05,
            'total_duration': 0.1,
            'trace_id': 'test-error-trace-id',
        }
    )

# tests/test_ssrf.py:227-229
    def test_invalid_url_no_hostname(self) -> None:
        with pytest.raises(ValueError, match='Invalid URL: no hostname found'):
            extract_host_and_port('http://')

# pydantic_ai_slim/pydantic_ai/messages.py:344-355
    def _infer_media_type(self) -> str:
        """Return the media type of the audio file, based on the url.

        References:
        - Gemini: https://ai.google.dev/gemini-api/docs/audio#supported-formats
        """
        mime_type, _ = _mime_types.guess_type(self.url)
        if mime_type is None:
            raise ValueError(
                f'Could not infer media type from audio URL: {self.url}. Explicitly provide a `media_type` instead.'
            )
        return mime_type

# tests/typed_agent.py:79-80
def wrong_tool_prepare(ctx: RunContext[MyDeps], x: int, y: str) -> str:
    return f'{ctx.deps.foo} {x} {y}'

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:12-12
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, SpanExporter, SpanExportResult

# tests/evals/test_report_evaluators.py:149-150
def _make_report(cases: list[ReportCase]) -> EvaluationReport:
    return EvaluationReport(name='test', cases=cases)

# examples/pydantic_ai_examples/bank_support.py:44-50
class SupportOutput(BaseModel):
    support_advice: str
    """Advice returned to the customer"""
    block_card: bool
    """Whether to block their card or not"""
    risk: int
    """Risk level of query"""

# examples/pydantic_ai_examples/bank_support.py:53-62
support_agent = Agent(
    'openai:gpt-5.2',
    deps_type=SupportDependencies,
    output_type=SupportOutput,
    instructions=(
        'You are a support agent in our bank, give the '
        'customer support and judge the risk level of their query. '
        "Reply using the customer's name."
    ),
)

# pydantic_ai_slim/pydantic_ai/retries.py:18-26
from httpx import (
    AsyncBaseTransport,
    AsyncHTTPTransport,
    BaseTransport,
    HTTPStatusError,
    HTTPTransport,
    Request,
    Response,
)

# pydantic_ai_slim/pydantic_ai/retries.py:18-26
from httpx import (
    AsyncBaseTransport,
    AsyncHTTPTransport,
    BaseTransport,
    HTTPStatusError,
    HTTPTransport,
    Request,
    Response,
)

# tests/evals/test_reporting.py:98-102
def sample_report(sample_report_case: ReportCase) -> EvaluationReport:
    return EvaluationReport(
        cases=[sample_report_case],
        name='test_report',
    )

# tests/evals/test_reporting.py:98-102
def sample_report(sample_report_case: ReportCase) -> EvaluationReport:
    return EvaluationReport(
        cases=[sample_report_case],
        name='test_report',
    )

# tests/models/test_model.py:3-3
from importlib import import_module

# tests/test_temporal.py:166-166
TEMPORAL_PORT = 7243

# pydantic_ai_slim/pydantic_ai/_ssrf.py:130-148
def validate_url_protocol(url: str) -> tuple[str, bool]:
    """Validate that the URL uses an allowed protocol (http or https).

    Args:
        url: The URL to validate.

    Returns:
        Tuple of (scheme, is_https).

    Raises:
        ValueError: If the protocol is not http or https.
    """
    parsed = urlparse(url)
    scheme = parsed.scheme.lower()

    if scheme not in ('http', 'https'):
        raise ValueError(f'URL protocol "{scheme}" is not allowed. Only http:// and https:// are supported.')

    return scheme, scheme == 'https'

# tests/typed_graph.py:142-144
def run_persistence_wrong() -> None:
    p = FullStatePersistence[str, int]()
    g5.run_sync(A(), persistence=p, state=MyState(x=1), deps=MyDeps(y='y'))  # type: ignore[arg-type]

# pydantic_evals/pydantic_evals/evaluators/report_evaluator.py:11-11
from ..reporting.analyses import ReportAnalysis

# pydantic_evals/pydantic_evals/evaluators/report_evaluator.py:11-11
from ..reporting.analyses import ReportAnalysis

# pydantic_evals/pydantic_evals/evaluators/report_evaluator.py:11-11
from ..reporting.analyses import ReportAnalysis

# tests/conftest.py:4-4
import importlib.util

# tests/conftest.py:4-4
import importlib.util

# pydantic_evals/pydantic_evals/dataset.py:45-45
from .evaluators.evaluator import EvaluatorFailure

# tests/test_ssrf.py:199-204
    def test_custom_port(self) -> None:
        hostname, path, port, is_https = extract_host_and_port('http://example.com:8080/path')
        assert hostname == 'example.com'
        assert path == '/path'
        assert port == 8080
        assert is_https is False

# tests/test_tenacity.py:189-344
class TestAsyncTenacityTransport:
    """Tests for the asynchronous AsyncTenacityTransport."""

    async def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_async_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = AsyncTenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        async with transport:
            result = await transport.handle_async_request(request)

        assert result is mock_response
        mock_transport.handle_async_request.assert_called_once_with(request)

    async def test_retry_on_exception(self):
        """Test that exceptions trigger retries."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_response = Mock(spec=httpx.Response)

        # Fail twice, succeed on third attempt
        mock_transport.handle_async_request.side_effect = [
            httpx.ConnectError('Connection failed'),
            httpx.ConnectError('Connection failed again'),
            mock_response,
        ]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = AsyncTenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        result = await transport.handle_async_request(request)

        assert result is mock_response
        assert mock_transport.handle_async_request.call_count == 3

    async def test_retry_exhausted(self):
        """Test that retry exhaustion re-raises the last exception."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_transport.handle_async_request.side_effect = httpx.ConnectError('Connection failed')

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(2),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = AsyncTenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with pytest.raises(httpx.ConnectError, match='Connection failed'):
            await transport.handle_async_request(request)

        assert mock_transport.handle_async_request.call_count == 2

    async def test_validate_response_success(self):
        """Test that validate_response is called and doesn't raise."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_response = Mock(spec=httpx.Response)
        mock_response.status_code = 200
        mock_transport.handle_async_request.return_value = mock_response

        validate_response = Mock()
        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = AsyncTenacityTransport(config, mock_transport, validate_response)

        request = httpx.Request('GET', 'https://example.com')
        result = await transport.handle_async_request(request)

        assert result is mock_response
        validate_response.assert_called_once_with(mock_response)

    async def test_validate_response_triggers_retry(self):
        """Test that validate_response can trigger retries."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_response_fail = Mock(spec=httpx.Response)
        mock_response_fail.status_code = 429
        mock_response_success = Mock(spec=httpx.Response)
        mock_response_success.status_code = 200

        mock_transport.handle_async_request.side_effect = [mock_response_fail, mock_response_success]

        def validate_response(response: httpx.Response):
            if response.status_code == 429:
                raise httpx.HTTPStatusError('Rate limited', request=request, response=response)

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.HTTPStatusError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = AsyncTenacityTransport(config, mock_transport, validate_response)

        request = httpx.Request('GET', 'https://example.com')
        result = await transport.handle_async_request(request)

        assert result is mock_response_success
        assert mock_transport.handle_async_request.call_count == 2

    async def test_raise_for_status_in_validate_response(self):
        """Test that response.raise_for_status() works in validate_response callback."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_response_fail = Mock(spec=httpx.Response)
        mock_response_fail.status_code = 429
        mock_response_fail.is_success = False
        mock_response_fail.is_error = True
        mock_response_fail.request = None  # Initially None, will be set by transport

        # Mock raise_for_status to check if request is set
        def mock_raise_for_status():
            if mock_response_fail.request is None:
                raise RuntimeError(  # pragma: no cover
                    'Cannot call `raise_for_status` as the request instance has not been set on this response.'
                )
            raise httpx.HTTPStatusError(
                'Too Many Requests', request=mock_response_fail.request, response=mock_response_fail
            )

        mock_response_fail.raise_for_status = mock_raise_for_status

        mock_response_success = Mock(spec=httpx.Response)
        mock_response_success.status_code = 200
        mock_response_success.is_success = True
        mock_response_success.is_error = False
        mock_response_success.raise_for_status = Mock()  # Should not raise

        mock_transport.handle_async_request.side_effect = [mock_response_fail, mock_response_success]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.HTTPStatusError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = AsyncTenacityTransport(
            config, mock_transport, validate_response=lambda response: response.raise_for_status()
        )

        request = httpx.Request('GET', 'https://example.com')
        result = await transport.handle_async_request(request)

        assert result is mock_response_success
        assert mock_transport.handle_async_request.call_count == 2
        # Verify that the request was set on the failed response before raise_for_status was called
        assert mock_response_fail.request is request
        mock_response_success.raise_for_status.assert_called_once()

# tests/models/test_fallback.py:58-58
failure_model = FunctionModel(failure_response)

# tests/graph/test_mermaid.py:421-428
def test_wrong_return_type():
    @dataclass
    class NoReturnType(BaseNode):
        async def run(self, ctx: GraphRunContext) -> int:  # type: ignore
            raise NotImplementedError()

    with pytest.raises(GraphSetupError, match="Invalid return type: <class 'int'>"):
        NoReturnType.get_node_def({})