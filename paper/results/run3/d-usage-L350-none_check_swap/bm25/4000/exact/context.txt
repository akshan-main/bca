# pydantic_ai_slim/pydantic_ai/usage.py:266-266
    output_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:292-293
    def response_tokens_limit(self) -> int | None:
        return self.output_tokens_limit

# tests/test_usage_limits.py:258-284
def test_add_usages_with_none_detail_value():
    """Test that None values in details are skipped when incrementing usage."""
    usage = RunUsage(
        requests=1,
        input_tokens=10,
        output_tokens=20,
        details={'reasoning_tokens': 5},
    )

    # Create a usage with None in details (simulating model response with missing detail)
    incr_usage = RunUsage(
        requests=1,
        input_tokens=5,
        output_tokens=10,
    )
    # Manually set a None value in details to simulate edge case from model responses
    incr_usage.details = {'reasoning_tokens': None, 'other_tokens': 10}  # type: ignore[dict-item]

    result = usage + incr_usage
    assert result == snapshot(
        RunUsage(
            requests=2,
            input_tokens=15,
            output_tokens=30,
            details={'reasoning_tokens': 5, 'other_tokens': 10},
        )
    )

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/models/gemini.py:77-98
class GeminiModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    gemini_safety_settings: list[GeminiSafetySettings]
    """Safety settings options for Gemini model request."""

    gemini_thinking_config: ThinkingConfig
    """Thinking is "on" by default in both the API and AI Studio.

    Being on by default doesn't mean the model will send back thoughts. For that, you would need to set `include_thoughts`
    to `True`. If you want to avoid the model spending any tokens on thinking, you can set `thinking_budget` to `0`.

    See more about it on <https://ai.google.dev/gemini-api/docs/thinking>.
    """

    gemini_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI provider.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """

# pydantic_ai_slim/pydantic_ai/models/gemini.py:77-98
class GeminiModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    gemini_safety_settings: list[GeminiSafetySettings]
    """Safety settings options for Gemini model request."""

    gemini_thinking_config: ThinkingConfig
    """Thinking is "on" by default in both the API and AI Studio.

    Being on by default doesn't mean the model will send back thoughts. For that, you would need to set `include_thoughts`
    to `True`. If you want to avoid the model spending any tokens on thinking, you can set `thinking_budget` to `0`.

    See more about it on <https://ai.google.dev/gemini-api/docs/thinking>.
    """

    gemini_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI provider.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """

# tests/test_dbos.py:1492-1493
class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

# tests/test_dbos.py:1492-1493
class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

# tests/test_dbos.py:1492-1493
class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

# pydantic_ai_slim/pydantic_ai/models/openai.py:372-373
class OpenAIModelSettings(OpenAIChatModelSettings, total=False):
    """Deprecated alias for `OpenAIChatModelSettings`."""

# pydantic_ai_slim/pydantic_ai/_utils.py:140-143
class Some(Generic[T]):
    """Analogous to Rust's `Option::Some` type."""

    value: T

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:234-234
    openrouter_usage: OpenRouterUsageConfig

# pydantic_ai_slim/pydantic_ai/settings.py:96-96
    seed: int

# pydantic_ai_slim/pydantic_ai/models/groq.py:107-116
class GroqModelSettings(ModelSettings, total=False):
    """Settings used for a Groq model request."""

    # ALL FIELDS MUST BE `groq_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    groq_reasoning_format: Literal['hidden', 'raw', 'parsed']
    """The format of the reasoning output.

    See [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details.
    """

# pydantic_ai_slim/pydantic_ai/settings.py:52-52
    top_p: float

# tests/test_tools.py:2665-2675
def test_tool_timeout_default_none():
    """Test that timeout defaults to None when not specified."""
    agent = Agent(TestModel())

    @agent.tool_plain
    def tool_without_timeout() -> str:
        return 'done'  # pragma: no cover

    tool = agent._function_toolset.tools['tool_without_timeout']
    assert tool.timeout is None
    assert tool.tool_def.timeout is None

# pydantic_ai_slim/pydantic_ai/models/cohere.py:89-90
class CohereModelSettings(ModelSettings, total=False):
    """Settings used for a Cohere model request."""

# pydantic_ai_slim/pydantic_ai/models/gemini.py:77-98
class GeminiModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    gemini_safety_settings: list[GeminiSafetySettings]
    """Safety settings options for Gemini model request."""

    gemini_thinking_config: ThinkingConfig
    """Thinking is "on" by default in both the API and AI Studio.

    Being on by default doesn't mean the model will send back thoughts. For that, you would need to set `include_thoughts`
    to `True`. If you want to avoid the model spending any tokens on thinking, you can set `thinking_budget` to `0`.

    See more about it on <https://ai.google.dev/gemini-api/docs/thinking>.
    """

    gemini_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI provider.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """

# pydantic_ai_slim/pydantic_ai/models/google.py:161-194
class GoogleModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    google_safety_settings: list[SafetySettingDict]
    """The safety settings to use for the model.

    See <https://ai.google.dev/gemini-api/docs/safety-settings> for more information.
    """

    google_thinking_config: ThinkingConfigDict
    """The thinking configuration to use for the model.

    See <https://ai.google.dev/gemini-api/docs/thinking> for more information.
    """

    google_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI API.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """

    google_video_resolution: MediaResolution
    """The video resolution to use for the model.

    See <https://ai.google.dev/api/generate-content#MediaResolution> for more information.
    """

    google_cached_content: str
    """The name of the cached content to use for the model.

    See <https://ai.google.dev/gemini-api/docs/caching> for more information.
    """

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# tests/mcp_server.py:164-165
async def get_none():
    return None

# pydantic_ai_slim/pydantic_ai/settings.py:72-72
    timeout: float | Timeout

# pydantic_ai_slim/pydantic_ai/models/mistral.py:114-115
class MistralModelSettings(ModelSettings, total=False):
    """Settings used for a Mistral model request."""

# pydantic_ai_slim/pydantic_ai/models/openai.py:301-368
class OpenAIChatModelSettings(ModelSettings, total=False):
    """Settings used for an OpenAI model request."""

    # ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    openai_reasoning_effort: ReasoningEffort
    """Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).

    Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
    result in faster responses and fewer tokens used on reasoning in a response.
    """

    openai_logprobs: bool
    """Include log probabilities in the response.

    For Chat models, these will be included in `ModelResponse.provider_details['logprobs']`.
    For Responses models, these will be included in the response output parts `TextPart.provider_details['logprobs']`.
    """

    openai_top_logprobs: int
    """Include log probabilities of the top n tokens in the response."""

    openai_store: bool | None
    """Whether or not to store the output of this request in OpenAI's systems.

    If `False`, OpenAI will not store the request for its own internal review or training.
    See [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat/create#chat-create-store)."""

    openai_user: str
    """A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse.

    See [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details.
    """

    openai_service_tier: Literal['auto', 'default', 'flex', 'priority']
    """The service tier to use for the model request.

    Currently supported values are `auto`, `default`, `flex`, and `priority`.
    For more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier).
    """

    openai_prediction: ChatCompletionPredictionContentParam
    """Enables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs).

    This feature is currently only supported for some OpenAI models.
    """

    openai_prompt_cache_key: str
    """Used by OpenAI to cache responses for similar requests to optimize your cache hit rates.

    See the [OpenAI Prompt Caching documentation](https://platform.openai.com/docs/guides/prompt-caching#how-it-works) for more information.
    """

    openai_prompt_cache_retention: Literal['in_memory', '24h']
    """The retention policy for the prompt cache. Set to 24h to enable extended prompt caching, which keeps cached prefixes active for longer, up to a maximum of 24 hours.

    See the [OpenAI Prompt Caching documentation](https://platform.openai.com/docs/guides/prompt-caching#how-it-works) for more information.
    """

    openai_continuous_usage_stats: bool
    """When True, enables continuous usage statistics in streaming responses.

    When enabled, the API returns cumulative usage data with each chunk rather than only at the end.
    This setting correctly handles the cumulative nature of these stats by using only the final
    usage values rather than summing all intermediate values.

    See [OpenAI's streaming documentation](https://platform.openai.com/docs/api-reference/chat/create#stream_options) for more information.
    """

# pydantic_ai_slim/pydantic_ai/format_prompt.py:84-84
    none_str: str

# pydantic_ai_slim/pydantic_ai/models/xai.py:102-102
    xai_user: str

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:201-238
class OpenRouterModelSettings(ModelSettings, total=False):
    """Settings used for an OpenRouter model request."""

    # ALL FIELDS MUST BE `openrouter_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    openrouter_models: list[str]
    """A list of fallback models.

    These models will be tried, in order, if the main model returns an error. [See details](https://openrouter.ai/docs/features/model-routing#the-models-parameter)
    """

    openrouter_provider: OpenRouterProviderConfig
    """OpenRouter routes requests to the best available providers for your model. By default, requests are load balanced across the top providers to maximize uptime.

    You can customize how your requests are routed using the provider object. [See more](https://openrouter.ai/docs/features/provider-routing)"""

    openrouter_preset: str
    """Presets allow you to separate your LLM configuration from your code.

    Create and manage presets through the OpenRouter web application to control provider routing, model selection, system prompts, and other parameters, then reference them in OpenRouter API requests. [See more](https://openrouter.ai/docs/features/presets)"""

    openrouter_transforms: list[OpenRouterTransforms]
    """To help with prompts that exceed the maximum context size of a model.

    Transforms work by removing or truncating messages from the middle of the prompt, until the prompt fits within the model's context window. [See more](https://openrouter.ai/docs/features/message-transforms)
    """

    openrouter_reasoning: OpenRouterReasoning
    """To control the reasoning tokens in the request.

    The reasoning config object consolidates settings for controlling reasoning strength across different models. [See more](https://openrouter.ai/docs/use-cases/reasoning-tokens)
    """

    openrouter_usage: OpenRouterUsageConfig
    """To control the usage of the model.

    The usage config object consolidates settings for enabling detailed usage information. [See more](https://openrouter.ai/docs/use-cases/usage-accounting)
    """

# pydantic_ai_slim/pydantic_ai/settings.py:14-14
    max_tokens: int

# pydantic_ai_slim/pydantic_ai/settings.py:137-137
    logit_bias: dict[str, int]

# pydantic_ai_slim/pydantic_ai/settings.py:173-173
    extra_body: object

# pydantic_ai_slim/pydantic_ai/models/openai.py:360-360
    openai_continuous_usage_stats: bool

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:106-107
class HuggingFaceModelSettings(ModelSettings, total=False):
    """Settings used for a Hugging Face model request."""

# pydantic_ai_slim/pydantic_ai/settings.py:31-31
    temperature: float

# tests/typed_deps.py:53-54
async def my_prepare_none(ctx: RunContext, tool_defn: ToolDefinition) -> None:
    pass

# pydantic_ai_slim/pydantic_ai/settings.py:162-162
    extra_headers: dict[str, str]

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar

# pydantic_ai_slim/pydantic_ai/_output.py:14-14
from typing_extensions import Self, TypedDict, TypeVar