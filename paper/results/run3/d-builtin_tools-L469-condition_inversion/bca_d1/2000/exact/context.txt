## examples/pydantic_ai_examples/evals/models.py

    now: AwareDatetime

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    builtin_tools: list[AbstractBuiltinTool | BuiltinToolFunc[DepsT]] = dataclasses.field(repr=False)

## pydantic_ai_slim/pydantic_ai/builtin_tools.py

def _tool_discriminator(tool_data: dict[str, Any] | AbstractBuiltinTool) -> str:
    if not isinstance(tool_data, dict):
        return tool_data.get('kind', AbstractBuiltinTool.kind)
    else:
        return tool_data.kind

DEPRECATED_BUILTIN_TOOLS: frozenset[type[AbstractBuiltinTool]] = frozenset({UrlContextTool})  # pyright: ignore[reportDeprecated]

SUPPORTED_BUILTIN_TOOLS = frozenset(cls for cls in BUILTIN_TOOL_TYPES.values() if cls not in DEPRECATED_BUILTIN_TOOLS)

BUILTIN_TOOLS_REQUIRING_CONFIG: frozenset[type[AbstractBuiltinTool]] = frozenset({MCPServerTool, MemoryTool})

## pydantic_ai_slim/pydantic_ai/models/openai.py

    openai_builtin_tools: Sequence[FileSearchToolParam | WebSearchToolParam | ComputerToolParam]

## pydantic_ai_slim/pydantic_ai/providers/bedrock.py

def _without_builtin_tools(profile: ModelProfile | None) -> ModelProfile:
    return replace(profile or BedrockModelProfile(), supported_builtin_tools=frozenset())

## pydantic_ai_slim/pydantic_ai/ui/_web/api.py

    builtin_tools: list[str]

## pydantic_evals/pydantic_evals/evaluators/common.py

class IsInstance(Evaluator[object, object, object]):
    """Check if the output is an instance of a type with the given name."""

    type_name: str
    evaluation_name: str | None = field(default=None)

    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> EvaluationReason:
        output = ctx.output
        for cls in type(output).__mro__:
            if cls.__name__ == self.type_name or cls.__qualname__ == self.type_name:
                return EvaluationReason(value=True)

        reason = f'output is of type {type(output).__name__}'
        if type(output).__qualname__ != type(output).__name__:
            reason += f' (qualname: {type(output).__qualname__})'
        return EvaluationReason(value=False, reason=reason)

    type_name: str

    evaluation_name: str | None = field(default=None)

## pydantic_graph/pydantic_graph/beta/decision.py

class Decision(Generic[StateT, DepsT, HandledT]):
    """Decision node for conditional branching in graph execution.

    A Decision node evaluates conditions and routes execution to different
    branches based on the input data type or custom matching logic.
    """

    id: NodeID
    """Unique identifier for this decision node."""

    branches: list[DecisionBranch[Any]]
    """List of branches that can be taken from this decision."""

    note: str | None
    """Optional documentation note for this decision."""

    def branch(self, branch: DecisionBranch[T]) -> Decision[StateT, DepsT, HandledT | T]:
        """Add a new branch to this decision.

        Args:
            branch: The branch to add to this decision.

        Returns:
            A new Decision with the additional branch.
        """
        return Decision(id=self.id, branches=self.branches + [branch], note=self.note)

    def _force_handled_contravariant(self, inputs: HandledT) -> Never:  # pragma: no cover
        """Forces this type to be contravariant in the HandledT type variable.

        This is an implementation detail of how we can type-check that all possible input types have
        been exhaustively covered.

        Args:
            inputs: Input data of handled types.

        Raises:
            RuntimeError: Always, as this method should never be executed.
        """
        raise RuntimeError('This method should never be called, it is just defined for typing purposes.')

class DecisionBranch(Generic[SourceT]):
    """Represents a single branch within a decision node.

    Each branch defines the conditions under which it should be taken
    and the path to follow when those conditions are met.

    Note: with the current design, it is actually _critical_ that this class is invariant in SourceT for the sake
    of type-checking that inputs to a Decision are actually handled. See the `# type: ignore` comment in
    `tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch` for an example of how this works.
    """

    source: TypeOrTypeExpression[SourceT]
    """The expected type of data for this branch.

    This is necessary for exhaustiveness-checking when handling the inputs to a decision node."""

    matches: Callable[[Any], bool] | None
    """An optional predicate function used to determine whether input data matches this branch.

    If `None`, default logic is used which attempts to check the value for type-compatibility with the `source` type:
    * If `source` is `Any` or `object`, the branch will always match
    * If `source` is a `Literal` type, this branch will match if the value is one of the parametrizing literal values
    * If `source` is any other type, the value will be checked for matching using `isinstance`

    Inputs are tested against each branch of a decision node in order, and the path of the first matching branch is
    used to handle the input value.
    """

    path: Path
    """The execution path to follow when an input value matches this branch of a decision node.

    This can include transforming, mapping, and broadcasting the output before sending to the next node or nodes.

    The path can also include position-aware labels which are used when generating mermaid diagrams."""

    destinations: list[AnyDestinationNode]
    """The destination nodes that can be referenced by DestinationMarker in the path."""

## tests/mcp_server.py

async def get_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

## tests/test_agent.py

def test_agent_builtin_tools_runtime_vs_agent_level():
    """Test that runtime builtin_tools parameter is merged with agent-level builtin_tools."""
    model = TestModel()

    agent = Agent(
        model=model,
        builtin_tools=[
            WebSearchTool(),
            CodeExecutionTool(),
            MCPServerTool(id='deepwiki', url='https://mcp.deepwiki.com/mcp'),
            MCPServerTool(id='github', url='https://api.githubcopilot.com/mcp'),
        ],
    )

    # Runtime tool with same unique ID should override agent-level tool
    with pytest.raises(Exception, match='TestModel does not support built-in tools'):
        agent.run_sync(
            'Hello',
            builtin_tools=[
                WebSearchTool(search_context_size='high'),
                MCPServerTool(id='example', url='https://mcp.example.com/mcp'),
                MCPServerTool(id='github', url='https://mcp.githubcopilot.com/mcp', authorization_token='token'),
            ],
        )

    assert model.last_model_request_parameters is not None
    assert model.last_model_request_parameters.builtin_tools == snapshot(
        [
            WebSearchTool(search_context_size='high'),
            CodeExecutionTool(),
            MCPServerTool(id='deepwiki', url='https://mcp.deepwiki.com/mcp'),
            MCPServerTool(id='github', url='https://mcp.githubcopilot.com/mcp', authorization_token='token'),
            MCPServerTool(id='example', url='https://mcp.example.com/mcp'),
        ]
    )

## tests/test_dbos.py

def conditions(ctx: RunContext, city: str) -> str:
    if ctx.run_step % 2 == 0:
        return "It's sunny"  # pragma: lax no cover
    else:
        return "It's raining"

## tests/test_prefect.py

def conditions(city: str) -> str:
    # Simplified version without RunContext
    return "It's raining"

## tests/test_ui_web.py

def test_supported_builtin_tools(profile_name: str):
    """Test profile.supported_builtin_tools returns proper tool types."""
    if profile_name == 'base':
        profile: ModelProfile = ModelProfile()
    elif profile_name == 'openai':
        profile = OpenAIModelProfile()
    elif profile_name == 'google':
        profile = GoogleModelProfile()
    else:
        profile = GroqModelProfile()

    result = profile.supported_builtin_tools
    assert isinstance(result, frozenset)
    assert all(issubclass(t, AbstractBuiltinTool) for t in result)
