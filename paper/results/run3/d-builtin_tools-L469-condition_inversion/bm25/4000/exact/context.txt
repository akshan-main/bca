# pydantic_evals/pydantic_evals/reporting/__init__.py:1261-1275
    def _render_dict(
        self,
        case_dict: Mapping[str, EvaluationResult[T] | T],
        renderers: Mapping[str, _AbstractRenderer[T]],
        *,
        include_names: bool = True,
    ) -> str:
        diff_lines: list[str] = []
        for key, val in case_dict.items():
            value = cast(EvaluationResult[T], val).value if isinstance(val, EvaluationResult) else val
            rendered = renderers[key].render_value(key if include_names else None, value)
            if self.include_reasons and isinstance(val, EvaluationResult) and (reason := val.reason):
                rendered += f'\n  Reason: {reason}\n'
            diff_lines.append(rendered)
        return '\n'.join(diff_lines) if diff_lines else EMPTY_CELL_STR

# pydantic_evals/pydantic_evals/reporting/__init__.py:1243-1259
    def _render_dicts_diff(
        baseline_dict: dict[str, T],
        new_dict: dict[str, T],
        renderers: Mapping[str, _AbstractRenderer[T]],
        *,
        include_names: bool = True,
    ) -> str:
        keys: set[str] = set()
        keys.update(baseline_dict.keys())
        keys.update(new_dict.keys())
        diff_lines: list[str] = []
        for key in sorted(keys):
            old_val = baseline_dict.get(key)
            new_val = new_dict.get(key)
            rendered = renderers[key].render_diff(key if include_names else None, old_val, new_val)
            diff_lines.append(rendered)
        return '\n'.join(diff_lines) if diff_lines else EMPTY_CELL_STR

# tests/models/test_openai.py:4153-4168
def test_azure_400_non_dict_body(allow_model_requests: None) -> None:
    """Test a 400 error from Azure where the body is not a dictionary."""
    mock_client = MockOpenAI.create_mock(
        APIStatusError(
            'Bad Request',
            response=httpx.Response(status_code=400, request=httpx.Request('POST', 'https://example.com/v1')),
            body='Raw string body',
        )
    )
    m = OpenAIChatModel('gpt-5-mini', provider=AzureProvider(openai_client=cast(AsyncAzureOpenAI, mock_client)))
    agent = Agent(m)

    with pytest.raises(ModelHTTPError) as exc_info:
        agent.run_sync('hello')

    assert exc_info.value.status_code == 400

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:11-12
def _is_dict(obj: Any) -> TypeGuard[dict[str, Any]]:
    return isinstance(obj, dict)

# tests/mcp_server.py:146-147
async def get_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_utils.py:31-35
from typing_extensions import (
    ParamSpec,
    TypeIs,
    is_typeddict,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# tests/typed_agent.py:176-182
structured_dict = StructuredDict(
    {
        'type': 'object',
        'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}},
        'required': ['name', 'age'],
    }
)

# tests/models/test_openai.py:1700-1701
class MyNormalTypedDict(TypedDict):
    foo: str

# examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py:27-45
async def document_predict_state() -> list[CustomEvent]:
    """Enable document state prediction.

    Returns:
        CustomEvent containing the event to enable state prediction.
    """
    return [
        CustomEvent(
            type=EventType.CUSTOM,
            name='PredictState',
            value=[
                {
                    'state_key': 'document',
                    'tool': 'write_document',
                    'tool_argument': 'document',
                },
            ],
        ),
    ]

# tests/graph/beta/test_joins_and_reducers.py:84-107
async def test_reduce_dict_update():
    """Test reduce_dict_update that merges dictionaries."""
    g = GraphBuilder(state_type=SimpleState, output_type=dict[str, int])

    @g.step
    async def generate_keys(ctx: StepContext[SimpleState, None, None]) -> list[str]:
        return ['a', 'b', 'c']

    @g.step
    async def create_dict(ctx: StepContext[SimpleState, None, str]) -> dict[str, int]:
        return {ctx.inputs: len(ctx.inputs)}

    dict_join = g.join(reduce_dict_update, initial_factory=dict[str, int])

    g.add(
        g.edge_from(g.start_node).to(generate_keys),
        g.edge_from(generate_keys).map().to(create_dict),
        g.edge_from(create_dict).to(dict_join),
        g.edge_from(dict_join).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=SimpleState())
    assert result == {'a': 1, 'b': 1, 'c': 1}

# pydantic_graph/pydantic_graph/beta/join.py:118-121
def reduce_dict_update(current: dict[K, V], inputs: Mapping[K, V]) -> dict[K, V]:
    """A reducer that updates a dict."""
    current.update(inputs)
    return current

# pydantic_ai_slim/pydantic_ai/messages.py:1230-1241
    def args_as_dict(self) -> dict[str, Any]:
        """Return the arguments as a Python dictionary.

        This is just for convenience with models that require dicts as input.
        """
        if not self.args:
            return {}
        if isinstance(self.args, dict):
            return self.args
        args = pydantic_core.from_json(self.args)
        assert isinstance(args, dict), 'args should be a dict'
        return cast(dict[str, Any], args)

# pydantic_graph/pydantic_graph/beta/join.py:118-121
def reduce_dict_update(current: dict[K, V], inputs: Mapping[K, V]) -> dict[K, V]:
    """A reducer that updates a dict."""
    current.update(inputs)
    return current

# tests/models/test_openai.py:1709-1710
class MyPartialTypedDict(TypedDict, total=False):
    foo: str

# pydantic_evals/pydantic_evals/evaluators/spec.py:56-64
    def kwargs(self) -> dict[str, Any]:
        """Get the keyword arguments for the evaluator.

        Returns:
            A dictionary of keyword arguments if arguments is a dict, otherwise an empty dict.
        """
        if isinstance(self.arguments, dict):
            return self.arguments
        return {}

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:9-9
from weakref import WeakValueDictionary

# tests/models/test_openai.py:1704-1706
class MyOptionalTypedDict(TypedDict):
    foo: NotRequired[str]
    bar: str

# tests/models/test_openai.py:1753-1754
def tool_with_typed_dict(x: MyNormalTypedDict) -> str:
    return f'{x}'  # pragma: no cover

# tests/evals/test_report_evaluators.py:632-651
def test_confusion_matrix_evaluator_metadata_key_with_non_dict():
    """ConfusionMatrixEvaluator with metadata key but non-dict metadata skips the case."""
    cases = [
        _make_report_case('c1', expected_output='A', metadata='some_string'),
        _make_report_case('c2', expected_output='B', metadata={'pred': 'B'}),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='metadata',
        predicted_key='pred',
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    # c1 should be skipped (non-dict metadata with key), only c2 used
    assert result.class_labels == ['B']
    assert result.matrix == [[1]]

# tests/mcp_server.py:151-152
async def get_unstructured_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

# tests/typed_agent.py:183-183
structured_dict_agent = Agent(output_type=structured_dict)

# tests/evals/test_evaluator_common.py:93-117
async def test_contains_dict():
    """Test Contains evaluator with dictionaries."""
    evaluator = Contains(value={'key': 'value'})

    # Test dictionary containment
    assert evaluator.evaluate(MockContext(output={'key': 'value', 'extra': 'data'})) == snapshot(
        EvaluationReason(value=True)
    )

    # Test dictionary key missing
    assert evaluator.evaluate(MockContext(output={'different': 'value'})) == snapshot(
        EvaluationReason(value=False, reason="Output dictionary does not contain expected key 'key'")
    )

    # Test dictionary value mismatch
    assert evaluator.evaluate(MockContext(output={'key': 'different'})) == snapshot(
        EvaluationReason(
            value=False,
            reason="Output dictionary has different value for key 'key': 'different' != 'value'",
        )
    )

    # Test non-dict value in dict
    evaluator_single = Contains(value='key')
    assert evaluator_single.evaluate(MockContext(output={'key': 'value'})) == snapshot(EvaluationReason(value=True))

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:549-556
    def event_to_dict(event: LogRecord) -> dict[str, Any]:
        if not event.body:
            body = {}  # pragma: no cover
        elif isinstance(event.body, Mapping):
            body = event.body
        else:
            body = {'body': event.body}
        return {**body, **(event.attributes or {})}

# tests/test_mcp.py:1086-1163
async def test_tool_returning_dict(allow_model_requests: None, agent: Agent):
    async with agent:
        result = await agent.run('Get me a dict, respond on one line')
        assert result.output == snapshot('{"foo":"bar","baz":123}')
        assert result.all_messages() == snapshot(
            [
                ModelRequest(
                    parts=[
                        UserPromptPart(
                            content='Get me a dict, respond on one line',
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[ToolCallPart(tool_name='get_dict', args='{}', tool_call_id='call_oqKviITBj8PwpQjGyUu4Zu5x')],
                    usage=RequestUsage(
                        input_tokens=195,
                        output_tokens=11,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={
                        'finish_reason': 'tool_calls',
                        'timestamp': IsDatetime(),
                    },
                    provider_response_id='chatcmpl-BRloOs7Bb2tq8wJyy9Rv7SQ7L65a7',
                    finish_reason='tool_call',
                    run_id=IsStr(),
                ),
                ModelRequest(
                    parts=[
                        ToolReturnPart(
                            tool_name='get_dict',
                            content={'foo': 'bar', 'baz': 123},
                            tool_call_id='call_oqKviITBj8PwpQjGyUu4Zu5x',
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[TextPart(content='{"foo":"bar","baz":123}')],
                    usage=RequestUsage(
                        input_tokens=222,
                        output_tokens=11,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={
                        'finish_reason': 'stop',
                        'timestamp': IsDatetime(),
                    },
                    provider_response_id='chatcmpl-BRloPczU1HSCWnreyo21DdNtdOM7L',
                    finish_reason='stop',
                    run_id=IsStr(),
                ),
            ]
        )

# pydantic_evals/pydantic_evals/evaluators/report_common.py:29-29
    predicted_key: str | None = None