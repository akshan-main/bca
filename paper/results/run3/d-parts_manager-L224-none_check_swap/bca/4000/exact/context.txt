## docs/.hooks/snippets.py

class RenderedSnippet:
    content: str
    highlights: list[LineRange]
    original_range: LineRange

## examples/pydantic_ai_examples/chat_app.py

    timestamp: str

## examples/pydantic_ai_examples/evals/agent.py

    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

async def infer_time_range(inputs: TimeRangeInputs) -> TimeRangeResponse:
    """Infer a time range from a user prompt."""
    deps = TimeRangeDeps(now=inputs['now'])
    return (await time_range_agent.run(inputs['prompt'], deps=deps)).output

## examples/pydantic_ai_examples/flight_booking.py

    date: datetime.date

class Deps:
    web_page_text: str
    req_origin: str
    req_destination: str
    req_date: datetime.date

## examples/pydantic_ai_examples/slack_lead_qualifier/agent.py

async def analyze_profile(profile: Profile) -> Analysis | None:
    result = await agent.run(profile.as_prompt())
    return result.output  ### [/analyze_profile]

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

    request: _messages.ModelRequest

    model_response: _messages.ModelResponse

## pydantic_ai_slim/pydantic_ai/_otel_messages.py

    type: Literal['text']

## pydantic_ai_slim/pydantic_ai/_utils.py

class Unset:
    """A singleton to represent an unset value."""

    pass

def number_to_datetime(x: int | float) -> datetime:
    return _datetime_ta.validate_python(x)

## pydantic_ai_slim/pydantic_ai/embeddings/result.py

    provider_details: dict[str, Any] | None = None

## pydantic_ai_slim/pydantic_ai/messages.py

    provider_details: dict[str, Any] | None = None

    provider_details: dict[str, Any] | None = None

## pydantic_ai_slim/pydantic_ai/models/__init__.py

    provider_details: dict[str, Any] | None = field(default=None, init=False)

    _parts_manager: ModelResponsePartsManager = field(default_factory=ModelResponsePartsManager, init=False)

def get_user_agent() -> str:
    """Get the user agent string for the HTTP client."""
    from .. import __version__

    return f'pydantic-ai/{__version__}'

## pydantic_ai_slim/pydantic_ai/models/gemini.py

class _GeminiContent(TypedDict):
    role: Literal['user', 'model']
    parts: list[_GeminiPartUnion]

class _GeminiThoughtPart(TypedDict):
    thought: bool
    thought_signature: Annotated[str, pydantic.Field(alias='thoughtSignature')]

## pydantic_ai_slim/pydantic_ai/retries.py

    sleep: Callable[[int | float], None | Awaitable[None]]

## pydantic_evals/pydantic_evals/_utils.py

class Unset:
    """A singleton to represent an unset value.

    Used to distinguish between explicitly set `None` values and values that were never set.

    Copied from pydantic_ai/_utils.py.
    """

    pass

## pydantic_graph/pydantic_graph/beta/graph.py

    def value(self) -> OutputT:
        return self._value

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## pydantic_graph/pydantic_graph/beta/util.py

class Some(Generic[T]):
    """Container for explicitly present values in Maybe type pattern.

    This class represents a value that is definitely present, as opposed to None.
    It's part of the Maybe pattern, similar to Option/Maybe in functional programming,
    allowing distinction between "no value" (None) and "value is None" (Some(None)).
    """

    value: T
    """The wrapped value."""

## tests/evals/test_utils.py

def test_unset():
    """Test Unset singleton."""
    assert isinstance(UNSET, Unset)
    assert UNSET is not Unset()  # note: we might want to change this and make it a true singleton..

## tests/graph/beta/test_broadcast_and_spread.py

class CounterState:
    values: list[int] = field(default_factory=list[int])

    values: list[int] = field(default_factory=list[int])

## tests/graph/beta/test_decisions.py

class DecisionState:
    path_taken: str | None = None
    value: int = 0

## tests/graph/beta/test_edge_cases.py

class EdgeCaseState:
    value: int = 0
    error_raised: bool = False

async def test_step_with_zero_value():
    """Test handling of zero values (ensure they're not confused with None/falsy)."""
    g = GraphBuilder(state_type=EdgeCaseState, output_type=int)

    @g.step
    async def return_zero(ctx: StepContext[EdgeCaseState, None, None]) -> int:
        return 0

    @g.step
    async def process_zero(ctx: StepContext[EdgeCaseState, None, int]) -> int:
        return ctx.inputs + 1

    g.add(
        g.edge_from(g.start_node).to(return_zero),
        g.edge_from(return_zero).to(process_zero),
        g.edge_from(process_zero).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=EdgeCaseState())
    assert result == 1

## tests/graph/beta/test_edge_labels.py

class LabelState:
    value: int = 0

## tests/graph/beta/test_graph_builder.py

class SimpleState:
    counter: int = 0
    result: str | None = None

## tests/graph/beta/test_graph_edge_cases.py

class MyState:
    value: int = 0

## tests/graph/beta/test_graph_execution.py

class ExecutionState:
    log: list[str] = field(default_factory=list[str])
    counter: int = 0

## tests/graph/beta/test_graph_iteration.py

class IterState:
    counter: int = 0

## tests/graph/beta/test_v1_v2_integration.py

class IntegrationState:
    log: list[str] = field(default_factory=list[str])

## tests/models/test_cohere.py

async def test_request_simple_success_with_vcr(allow_model_requests: None, co_api_key: str):
    m = CohereModel('command-r7b-12-2024', provider=CohereProvider(api_key=co_api_key))
    agent = Agent(m)
    result = await agent.run('hello')
    assert result.output == snapshot('Hello! How can I assist you today?')

async def test_cohere_model_builtin_tools(allow_model_requests: None, co_api_key: str):
    m = CohereModel('command-r7b-12-2024', provider=CohereProvider(api_key=co_api_key))
    agent = Agent(m, builtin_tools=[WebSearchTool()])
    with pytest.raises(UserError, match=r"Builtin tool\(s\) \['WebSearchTool'\] not supported by this model"):
        await agent.run('Hello')

## tests/models/test_gemini.py

def gemini_response(content: _GeminiContent, finish_reason: Literal['STOP'] | None = 'STOP') -> _GeminiResponse:
    candidate = _GeminiCandidates(content=content, index=0, safety_ratings=[])
    if finish_reason:  # pragma: no branch
        candidate['finish_reason'] = finish_reason
    return _GeminiResponse(candidates=[candidate], usage_metadata=example_usage(), model_version='gemini-1.5-flash-123')

## tests/models/test_groq.py

async def test_extra_headers(allow_model_requests: None, groq_api_key: str):
    # This test doesn't do anything, it's just here to ensure that calls with `extra_headers` don't cause errors, including type.
    m = GroqModel('llama-3.3-70b-versatile', provider=GroqProvider(api_key=groq_api_key))
    agent = Agent(m, model_settings=GroqModelSettings(extra_headers={'Extra-Header-Key': 'Extra-Header-Value'}))
    await agent.run('hello')

## tests/models/test_mcp_sampling.py

def fake_session(create_message: Any) -> Any:
    return FakeSession(create_message)

## tests/models/test_mistral.py

def func_chunk(
    tool_calls: list[MistralToolCall], finish_reason: MistralCompletionResponseStreamChoiceFinishReason | None = None
) -> MistralCompletionEvent:
    return chunk([MistralDeltaMessage(tool_calls=tool_calls, role='assistant')], finish_reason=finish_reason)

## tests/models/test_model_names.py

    object: Literal['model']

## tests/models/test_openai.py

async def test_extra_headers(allow_model_requests: None, openai_api_key: str):
    # This test doesn't do anything, it's just here to ensure that calls with `extra_headers` don't cause errors, including type.
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key=openai_api_key))
    agent = Agent(m, model_settings=OpenAIChatModelSettings(extra_headers={'Extra-Header-Key': 'Extra-Header-Value'}))
    await agent.run('hello')

## tests/models/test_xai.py

async def test_xai_encrypted_content_only(allow_model_requests: None):
    """Test encrypted content (signature) appears when enabled"""
    response = create_response(content='4', encrypted_content='sig-abc')
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m, model_settings=XaiModelSettings(xai_include_encrypted_content=True, max_tokens=20))

    result = await agent.run('What is 2+2? Return just "4".')
    assert result.output == '4'
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='What is 2+2? Return just "4".', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ThinkingPart(content='', signature='sig-abc', provider_name='xai'), TextPart(content='4')],
                model_name=XAI_REASONING_MODEL,
                timestamp=IsDatetime(),
                provider_name='xai',
                provider_url='https://api.x.ai/v1',
                provider_response_id=IsStr(),
                finish_reason='stop',
                run_id=IsStr(),
            ),
        ]
    )

async def test_xai_thinking_part_with_content_and_signature_in_history(allow_model_requests: None):
    """Test that ThinkingPart with BOTH content AND signature in history is properly mapped."""
    # First response with BOTH reasoning content AND encrypted signature
    # This is needed because provider_name is only set to 'xai' when there's a signature
    # And content is only mapped when provider_name matches
    response1 = create_response(
        content='first response',
        reasoning_content='First reasoning',
        encrypted_content='encrypted_signature_123',
        usage=create_usage(prompt_tokens=10, completion_tokens=5),
    )
    # Second response
    response2 = create_response(
        content='second response',
        usage=create_usage(prompt_tokens=20, completion_tokens=5),
    )

    mock_client = MockXai.create_mock([response1, response2])
    m = XaiModel(XAI_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    # Run twice to test message history containing ThinkingPart with content AND signature
    result1 = await agent.run('First question')
    result2 = await agent.run('Second question', message_history=result1.new_messages())

    # Verify kwargs - second call should have ThinkingPart mapped with both reasoning_content AND encrypted_content
    assert get_mock_chat_create_kwargs(mock_client) == snapshot(
        [
            {
                'model': XAI_REASONING_MODEL,
                'messages': [{'content': [{'text': 'First question'}], 'role': 'ROLE_USER'}],
                'tools': None,
                'tool_choice': None,
                'response_format': None,
                'use_encrypted_content': False,
                'include': [],
            },
            {
                'model': XAI_REASONING_MODEL,
                'messages': [
                    {'content': [{'text': 'First question'}], 'role': 'ROLE_USER'},
                    # ThinkingPart with BOTH content and signature
                    {
                        'content': [{'text': ''}],
                        'reasoning_content': 'First reasoning',
                        'encrypted_content': 'encrypted_signature_123',
                        'role': 'ROLE_ASSISTANT',
                    },
                    {'content': [{'text': 'first response'}], 'role': 'ROLE_ASSISTANT'},
                    {'content': [{'text': 'Second question'}], 'role': 'ROLE_USER'},
                ],
                'tools': None,
                'tool_choice': None,
                'response_format': None,
                'use_encrypted_content': False,
                'include': [],
            },
        ]
    )

    assert result2.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='First question', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ThinkingPart(content='First reasoning', signature=IsStr(), provider_name='xai'),
                    TextPart(content='first response'),
                ],
                usage=RequestUsage(input_tokens=10, output_tokens=5),
                model_name=XAI_REASONING_MODEL,
                timestamp=IsDatetime(),
                provider_name='xai',
                provider_url='https://api.x.ai/v1',
                provider_response_id=IsStr(),
                finish_reason='stop',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[UserPromptPart(content='Second question', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='second response')],
                usage=RequestUsage(input_tokens=20, output_tokens=5),
                model_name=XAI_REASONING_MODEL,
                timestamp=IsDatetime(),
                provider_name='xai',
                provider_url='https://api.x.ai/v1',
                provider_response_id=IsStr(),
                finish_reason='stop',
                run_id=IsStr(),
            ),
        ]
    )

## tests/providers/test_xai.py

def test_xai_pass_xai_client() -> None:
    xai_client = AsyncClient(api_key='api-key')
    provider = XaiProvider(xai_client=xai_client)
    assert provider.client == xai_client

## tests/test_ag_ui.py

def uuid_str() -> str:
    """Generate a random UUID string."""
    return uuid.uuid4().hex

## tests/test_agent.py

class UserContext:
    location: str | None

## tests/test_dbos.py

def workflow_raises(exc_type: type[Exception], exc_message: str) -> Iterator[None]:
    """Helper for asserting that a DBOS workflow fails with the expected error."""
    with pytest.raises(Exception) as exc_info:
        yield
    assert isinstance(exc_info.value, Exception)
    assert str(exc_info.value) == exc_message

async def test_dbos_agent_run(allow_model_requests: None, dbos: DBOS):
    # Note: this runs as a DBOS workflow because we automatically wrap the run function.
    result = await simple_dbos_agent.run('What is the capital of Mexico?')
    assert result.output == snapshot('The capital of Mexico is Mexico City.')

async def test_dbos_agent_run_in_workflow_with_toolsets(allow_model_requests: None, dbos: DBOS):
    # Since DBOS does not automatically wrap the tools in a workflow, and allows dynamic steps, we can pass in toolsets directly.
    result = await simple_dbos_agent.run('What is the capital of Mexico?', toolsets=[FunctionToolset()])
    assert result.output == snapshot('The capital of Mexico is Mexico City.')

async def test_custom_model_settings(allow_model_requests: None, dbos: DBOS):
    result = await settings_dbos_agent.run('Give me those settings')
    assert result.output == snapshot("{'max_tokens': 123, 'custom_setting': 'custom_value'}")

## tests/test_deps.py

class MyDeps:
    foo: int
    bar: int

## tests/test_mcp.py

async def test_agent_with_server_not_running(agent: Agent, allow_model_requests: None):
    result = await agent.run('What is 0 degrees Celsius in Fahrenheit?')
    assert result.output == snapshot('0 degrees Celsius is 32.0 degrees Fahrenheit.')

## tests/test_parts_manager.py

def test_handle_thinking_delta_no_content():
    manager = ModelResponsePartsManager()

    with pytest.raises(UnexpectedModelBehavior, match='Cannot create a ThinkingPart with no content'):
        list(manager.handle_thinking_delta(vendor_part_id=None, content=None, signature=None))

def test_handle_thinking_delta_no_content_or_signature():
    manager = ModelResponsePartsManager()

    # Add a thinking part first
    list(manager.handle_thinking_delta(vendor_part_id='thinking', content='initial', signature=None))

    # Updating with no content, signature, or provider_details emits no event
    events = list(manager.handle_thinking_delta(vendor_part_id='thinking', content=None, signature=None))
    assert events == []

## tests/test_prefect.py

async def test_prefect_agent_run(allow_model_requests: None) -> None:
    """Test that agent.run() works (auto-wrapped as flow)."""
    result = await simple_prefect_agent.run('What is the capital of Mexico?')
    assert result.output == snapshot('The capital of Mexico is Mexico City.')

async def test_custom_model_settings(allow_model_requests: None):
    """Test that custom model settings are passed through correctly."""
    result = await settings_prefect_agent.run('Give me those settings')
    assert result.output == snapshot("{'max_tokens': 123, 'custom_setting': 'custom_value'}")

## tests/test_thinking_part.py

def test_thinking_part_delta_applies_both_content_and_signature():
    thinking_part = ThinkingPart(content='Initial content', signature='initial_sig')
    delta = ThinkingPartDelta(content_delta=' added', signature_delta='new_sig')

    result = delta.apply(thinking_part)

    # The content is appended, and the signature is updated.
    assert result == snapshot(ThinkingPart(content='Initial content added', signature='new_sig'))

def test_thinking_part_delta_applies_content_only_preserves_signature():
    thinking_part = ThinkingPart(content='Initial content', signature='initial_sig')
    delta_content_only = ThinkingPartDelta(content_delta=' more', signature_delta=None)

    result_content_only = delta_content_only.apply(thinking_part)

    # The content is appended, and the signature is preserved.
    assert result_content_only == snapshot(ThinkingPart(content='Initial content more', signature='initial_sig'))
