# pydantic_ai_slim/pydantic_ai/usage.py:99-101
    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

# tests/test_temporal.py:2579-2579
    values: list[int] = field(default_factory=list[int])

# tests/graph/beta/test_broadcast_and_spread.py:17-17
    values: list[int] = field(default_factory=list[int])

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:14-14
from typing_inspection.introspection import get_literal_values

# tests/graph/beta/test_edge_cases.py:60-80
async def test_step_with_zero_value():
    """Test handling of zero values (ensure they're not confused with None/falsy)."""
    g = GraphBuilder(state_type=EdgeCaseState, output_type=int)

    @g.step
    async def return_zero(ctx: StepContext[EdgeCaseState, None, None]) -> int:
        return 0

    @g.step
    async def process_zero(ctx: StepContext[EdgeCaseState, None, int]) -> int:
        return ctx.inputs + 1

    g.add(
        g.edge_from(g.start_node).to(return_zero),
        g.edge_from(return_zero).to(process_zero),
        g.edge_from(process_zero).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=EdgeCaseState())
    assert result == 1

# tests/evals/test_multi_run.py:140-148
async def test_repeat_invalid_value():
    """repeat < 1 should raise ValueError."""

    async def task(inputs: str) -> str:
        return inputs  # pragma: no cover

    dataset = Dataset(cases=[Case(inputs='hello')])
    with pytest.raises(ValueError, match='repeat must be >= 1'):
        await dataset.evaluate(task, name='test', progress=False, repeat=0)

# tests/test_logfire.py:594-603
def test_logfire_metadata_values(
    get_logfire_summary: Callable[[], LogfireSummary],
    metadata: dict[str, Any] | Callable[[RunContext[Any]], dict[str, Any]],
    expected: dict[str, Any],
) -> None:
    agent = Agent(model=TestModel(), instrument=InstrumentationSettings(version=2), metadata=metadata)
    agent.run_sync('Hello')

    summary = get_logfire_summary()
    assert summary.attributes[0]['metadata'] == expected

# tests/test_validation_context.py:22-27
class Value(BaseModel):
    x: int

    @field_validator('x')
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

# tests/conftest.py:136-138
def _(value: datetime):  # pragma: no cover
    """Use IsDatetime() for datetime values in snapshots."""
    return 'IsDatetime()'

# pydantic_ai_slim/pydantic_ai/_utils.py:143-143
    value: T

# pydantic_ai_slim/pydantic_ai/_utils.py:143-143
    value: T

# tests/test_validation_context.py:26-27
    def increment_value(cls, value: int, info: ValidationInfo):
        return value + (info.context or 0)

# pydantic_ai_slim/pydantic_ai/messages.py:21-21
from opentelemetry.util.types import AnyValue

# pydantic_evals/pydantic_evals/evaluators/common.py:32-32
    value: Any

# pydantic_evals/pydantic_evals/reporting/__init__.py:707-711
    def render_value(self, name: str | None, v: Any) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:10-10
from pydantic import JsonValue

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:15-15
JSONValue = Any

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:15-15
JSONValue = Any

# tests/graph/beta/test_graph_edge_cases.py:20-20
    value: int = 0

# tests/graph/beta/test_graph_edge_cases.py:20-20
    value: int = 0

# pydantic_evals/pydantic_evals/evaluators/common.py:73-73
    value: Any

# tests/test_ag_ui.py:158-158
    value: int = 0

# pydantic_graph/pydantic_graph/beta/graph.py:86-87
    def value(self) -> OutputT:
        return self._value

# pydantic_evals/pydantic_evals/reporting/__init__.py:736-742
    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# tests/graph/beta/test_edge_labels.py:17-17
    value: int = 0

# tests/test_agent.py:3308-3308
    value: str

# tests/test_prefect.py:1195-1195
    value: str

# tests/test_agent.py:3308-3308
    value: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:693-693
    value_formatter: str | Callable[[Any], str] = '{}'

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:42-45
    value: Any = Field(
        default=None,
        description='The value to apply (for add, replace operations)',
    )

# pydantic_graph/pydantic_graph/beta/graph.py:78-78
    _value: OutputT

# tests/graph/beta/test_joins_and_reducers.py:24-24
    value: int = 0

# pydantic_evals/pydantic_evals/reporting/analyses.py:65-65
    value: float | int

# pydantic_evals/pydantic_evals/reporting/__init__.py:685-685
    value_formatter: str | Callable[[Any], str]

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# tests/graph/beta/test_decisions.py:20-20
    value: int = 0

# tests/graph/beta/test_edge_cases.py:19-19
    value: int = 0

# tests/test_agent.py:114-114
    value: T

# pydantic_evals/pydantic_evals/reporting/__init__.py:682-688
class RenderValueConfig(TypedDict, total=False):
    """A configuration for rendering a values in an Evaluation report."""

    value_formatter: str | Callable[[Any], str]
    diff_checker: Callable[[Any, Any], bool] | None
    diff_formatter: Callable[[Any, Any], str | None] | None
    diff_style: str

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:44-44
    value: EvaluationScalar

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:73-73
    value: EvaluationScalarT

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_evals/pydantic_evals/reporting/__init__.py:692-742
class _ValueRenderer:
    value_formatter: str | Callable[[Any], str] = '{}'
    diff_checker: Callable[[Any, Any], bool] | None = lambda x, y: x != y
    diff_formatter: Callable[[Any, Any], str | None] | None = None
    diff_style: str = 'magenta'

    @staticmethod
    def from_config(config: RenderValueConfig) -> _ValueRenderer:
        return _ValueRenderer(
            value_formatter=config.get('value_formatter', '{}'),
            diff_checker=config.get('diff_checker', lambda x, y: x != y),
            diff_formatter=config.get('diff_formatter'),
            diff_style=config.get('diff_style', 'magenta'),
        )

    def render_value(self, name: str | None, v: Any) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

    def render_diff(self, name: str | None, old: Any | None, new: Any | None) -> str:
        old_str = self._get_value_str(old) or MISSING_VALUE_STR
        new_str = self._get_value_str(new) or MISSING_VALUE_STR
        if old_str == new_str:
            result = old_str
        else:
            result = f'{old_str} â†’ {new_str}'

            has_diff = self.diff_checker and self.diff_checker(old, new)
            if has_diff:  # pragma: no branch
                # If there is a diff, make the name bold and compute the diff_str
                name = name and f'[bold]{name}[/]'
                diff_str = self.diff_formatter and self.diff_formatter(old, new)
                if diff_str:  # pragma: no cover
                    result += f' ({diff_str})'
                result = f'[{self.diff_style}]{result}[/]'

        # Add the name
        if name:
            result = f'{name}: {result}'

        return result

    def _get_value_str(self, value: Any) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:14-14
VALUE_SIG_FIGS = 3  # Significant figures for the default number formatting.

# pydantic_ai_slim/pydantic_ai/models/test.py:45-45
    value: str | None

# pydantic_ai_slim/pydantic_ai/models/test.py:52-52
    value: dict[str, Any] | None

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_ai_slim/pydantic_ai/_utils.py:30-30
from pydantic.json_schema import JsonSchemaValue

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# tests/test_logfire.py:582-583
def _test_logfire_metadata_values_callable_dict(ctx: RunContext[Any]) -> dict[str, str]:
    return {'model_name': ctx.model.model_name}

# pydantic_graph/pydantic_graph/beta/join.py:141-147
class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

# pydantic_evals/pydantic_evals/reporting/__init__.py:59-59
MISSING_VALUE_STR = '[i]<missing>[/i]'

# pydantic_ai_slim/pydantic_ai/messages.py:690-690
    return_value: ToolReturnContent

# tests/test_format_as_xml.py:601-603
def test_invalid_value():
    with pytest.raises(TypeError, match='Unsupported type'):
        format_as_xml(object())

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:9-9
from weakref import WeakValueDictionary

# tests/evals/test_evaluators.py:305-332
async def test_evaluator_with_null_values():
    """Test evaluator with null expected_output and metadata."""

    @dataclass
    class NullValueEvaluator(Evaluator[TaskInput, TaskOutput, TaskMetadata]):
        def evaluate(self, ctx: EvaluatorContext[TaskInput, TaskOutput, TaskMetadata]) -> EvaluatorOutput:
            return {
                'has_expected_output': ctx.expected_output is not None,
                'has_metadata': ctx.metadata is not None,
            }

    evaluator = NullValueEvaluator()
    context = EvaluatorContext[TaskInput, TaskOutput, TaskMetadata](
        name=None,
        inputs=TaskInput(query='What is 2+2?'),
        output=TaskOutput(answer='4'),
        expected_output=None,
        metadata=None,
        duration=0.1,
        _span_tree=SpanTree(),
        attributes={},
        metrics={},
    )

    result = evaluator.evaluate(context)
    assert isinstance(result, dict)
    assert result['has_expected_output'] is False
    assert result['has_metadata'] is False

# pydantic_evals/pydantic_evals/reporting/__init__.py:817-821
    def render_value(self, name: str | None, v: float | int) -> str:
        result = self._get_value_str(v)
        if name:
            result = f'{name}: {result}'
        return result

# tests/test_validation_context.py:23-23
    x: int

# pydantic_evals/pydantic_evals/reporting/__init__.py:917-917
    def render_value(self, name: str | None, v: T_contra) -> str: ...  # pragma: no branch

# pydantic_evals/pydantic_evals/reporting/__init__.py:923-923
_DEFAULT_VALUE_CONFIG = RenderValueConfig()

# pydantic_evals/pydantic_evals/reporting/__init__.py:884-890
    def _get_value_str(self, value: float | int | None) -> str:
        if value is None:
            return MISSING_VALUE_STR
        if isinstance(self.value_formatter, str):
            return self.value_formatter.format(value)
        else:
            return self.value_formatter(value)

# pydantic_evals/pydantic_evals/reporting/__init__.py:810-810
    value_formatter: str | Callable[[float | int], str]

# tests/test_format_as_xml.py:611-617
def test_parse_invalid_value():
    class Invalid(BaseModel):
        name: str = Field(default='Alice', title='Name')
        bad: Any = object()

    with pytest.raises(TypeError, match='Unsupported type'):
        format_as_xml(Invalid(), include_field_info='once')

# pydantic_evals/pydantic_evals/reporting/__init__.py:751-751
    value_formatter: str | Callable[[float | int], str]

# tests/graph/beta/test_joins_and_reducers.py:331-372
async def test_reduce_first_value():
    """Test ReduceFirstValue cancels sibling tasks"""

    @dataclass
    class StateWithResults:
        results: list[str] = field(default_factory=list[str])

    g = GraphBuilder(state_type=StateWithResults, output_type=str)

    @g.step
    async def generate(ctx: StepContext[StateWithResults, None, None]) -> list[int]:
        return [1, 2, 3, 4, 5]

    @g.step
    async def slow_process(ctx: StepContext[StateWithResults, None, int]) -> str:
        # First task finishes quickly
        if ctx.inputs == 1:
            await asyncio.sleep(0.001)
        else:
            # Others take longer (should be cancelled)
            await asyncio.sleep(10)
        ctx.state.results.append(f'completed-{ctx.inputs}')
        return f'result-{ctx.inputs}'

    first_join = g.join(ReduceFirstValue[str](), initial='')

    g.add(
        g.edge_from(g.start_node).to(generate),
        g.edge_from(generate).map().to(slow_process),
        g.edge_from(slow_process).to(first_join),
        g.edge_from(first_join).to(g.end_node),
    )

    graph = g.build()
    state = StateWithResults()
    result = await graph.run(state=state)

    # Only the first value should be returned
    assert result.startswith('result-')
    # Due to cancellation, not all 5 tasks should complete
    # (though timing can be tricky, so we just verify we got a result)
    assert 'completed-1' in state.results

# pydantic_ai_slim/pydantic_ai/run.py:316-316
    _traceparent_value: str | None = dataclasses.field(repr=False, compare=False, default=None)

# tests/test_vercel_ai.py:4409-4434
    async def test_dump_provider_metadata_filters_none_values(self):
        """Test that dump_provider_metadata only includes non-None values."""

        # All None - should return None
        result = dump_provider_metadata(id=None, provider_name=None, provider_details=None)
        assert result is None

        # Some values
        result = dump_provider_metadata(id='test_id', provider_name=None, provider_details={'key': 'val'})
        assert result == {'pydantic_ai': {'id': 'test_id', 'provider_details': {'key': 'val'}}}

        # All values
        result = dump_provider_metadata(
            id='full_id',
            signature='sig',
            provider_name='provider',
            provider_details={'detail': 1},
        )
        assert result == {
            'pydantic_ai': {
                'id': 'full_id',
                'signature': 'sig',
                'provider_name': 'provider',
                'provider_details': {'detail': 1},
            }
        }

# tests/test_usage_limits.py:258-284
def test_add_usages_with_none_detail_value():
    """Test that None values in details are skipped when incrementing usage."""
    usage = RunUsage(
        requests=1,
        input_tokens=10,
        output_tokens=20,
        details={'reasoning_tokens': 5},
    )

    # Create a usage with None in details (simulating model response with missing detail)
    incr_usage = RunUsage(
        requests=1,
        input_tokens=5,
        output_tokens=10,
    )
    # Manually set a None value in details to simulate edge case from model responses
    incr_usage.details = {'reasoning_tokens': None, 'other_tokens': 10}  # type: ignore[dict-item]

    result = usage + incr_usage
    assert result == snapshot(
        RunUsage(
            requests=2,
            input_tokens=15,
            output_tokens=30,
            details={'reasoning_tokens': 5, 'other_tokens': 10},
        )
    )

# tests/test_mcp.py:2013-2032
def test_load_mcp_servers_with_non_string_values(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test that non-string primitive values (int, bool, null) in nested structures are passed through unchanged."""
    config = tmp_path / 'mcp.json'

    # Create a config with environment variables and extra fields containing primitives
    # The extra fields will be ignored during validation but go through _expand_env_vars
    monkeypatch.setenv('PYTHON_CMD', 'python')
    config.write_text(
        '{"mcpServers": {"my_server": {"command": "${PYTHON_CMD}", "args": ["-m", "tests.mcp_server"], '
        '"metadata": {"count": 42, "enabled": true, "value": null}}}}',
        encoding='utf-8',
    )

    # This should successfully expand env vars and ignore the metadata field
    servers = load_mcp_servers(config)

    assert len(servers) == 1
    server = servers[0]
    assert isinstance(server, MCPServerStdio)
    assert server.command == 'python'

# pydantic_evals/pydantic_evals/dataset.py:934-941
    def record_metric(self, name: str, value: int | float) -> None:
        """Record a metric value.

        Args:
            name: The name of the metric.
            value: The value of the metric.
        """
        self.metrics[name] = value

# tests/test_mcp.py:2074-2090
def test_load_mcp_servers_with_default_values_in_url(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test ${VAR:-default} syntax in URLs."""
    config = tmp_path / 'mcp.json'

    # Test with default values in URL
    monkeypatch.delenv('HOST', raising=False)
    monkeypatch.setenv('PROTOCOL', 'https')
    config.write_text(
        '{"mcpServers": {"server": {"url": "${PROTOCOL:-http}://${HOST:-localhost}:${PORT:-8080}/mcp"}}}',
        encoding='utf-8',
    )

    servers = load_mcp_servers(config)
    assert len(servers) == 1
    server = servers[0]
    assert isinstance(server, MCPServerStreamableHTTP)
    assert server.url == 'https://localhost:8080/mcp'

# pydantic_evals/pydantic_evals/reporting/__init__.py:696-696
    diff_style: str = 'magenta'

# pydantic_evals/pydantic_evals/reporting/__init__.py:688-688
    diff_style: str

# pydantic_evals/pydantic_evals/reporting/__init__.py:694-694
    diff_checker: Callable[[Any, Any], bool] | None = lambda x, y: x != y

# pydantic_evals/pydantic_evals/reporting/__init__.py:686-686
    diff_checker: Callable[[Any, Any], bool] | None