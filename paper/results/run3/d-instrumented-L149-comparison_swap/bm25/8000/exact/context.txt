# pydantic_ai_slim/pydantic_ai/models/instrumented.py:92-92
    event_mode: Literal['attributes', 'logs'] = 'attributes'

# .github/set_docs_main_preview_url.py:21-21
version_id = m.group(1)

# .github/set_docs_main_preview_url.py:21-21
version_id = m.group(1)

# tests/models/test_instrumented.py:1487-1495
def test_deprecated_event_mode_warning():
    with pytest.warns(
        UserWarning,
        match='event_mode is only relevant for version=1 which is deprecated and will be removed in a future release',
    ):
        settings = InstrumentationSettings(event_mode='logs')
    assert settings.event_mode == 'logs'
    assert settings.version == 1
    assert InstrumentationSettings().version == 2

# clai/clai/__init__.py:6-6
__version__ = _metadata_version('clai')

# clai/clai/__init__.py:6-6
__version__ = _metadata_version('clai')

# clai/clai/__init__.py:6-6
__version__ = _metadata_version('clai')

# tests/test_vercel_ai.py:4471-4529
class TestSdkVersion:
    async def test_tool_input_start_chunk_excludes_provider_metadata_for_v5(self):
        chunk = ToolInputStartChunk(
            tool_call_id='tc_1',
            tool_name='my_tool',
            provider_metadata={'pydantic_ai': {'id': 'test_id', 'provider_name': 'openai'}},
        )
        encoded_v5 = json.loads(chunk.encode(sdk_version=5))
        encoded_v6 = json.loads(chunk.encode(sdk_version=6))

        assert 'providerMetadata' not in encoded_v5
        assert encoded_v5 == snapshot({'type': 'tool-input-start', 'toolCallId': 'tc_1', 'toolName': 'my_tool'})

        assert 'providerMetadata' in encoded_v6
        assert encoded_v6 == snapshot(
            {
                'type': 'tool-input-start',
                'toolCallId': 'tc_1',
                'toolName': 'my_tool',
                'providerMetadata': {'pydantic_ai': {'id': 'test_id', 'provider_name': 'openai'}},
            }
        )

    async def test_event_stream_uses_sdk_version(self):
        async def event_generator():
            part = ToolCallPart(
                tool_name='my_tool',
                tool_call_id='tc_ver',
                args={'key': 'value'},
                id='tool_call_id_ver',
                provider_name='anthropic',
            )
            yield PartStartEvent(index=0, part=part)
            yield PartEndEvent(index=0, part=part)

        request = SubmitMessage(
            id='foo',
            messages=[UIMessage(id='bar', role='user', parts=[TextUIPart(text='Test')])],
        )

        event_stream_v5 = VercelAIEventStream(run_input=request, sdk_version=5)
        events_v5: list[str | dict[str, Any]] = [
            '[DONE]' if '[DONE]' in event else json.loads(event.removeprefix('data: '))
            async for event in event_stream_v5.encode_stream(event_stream_v5.transform_stream(event_generator()))
        ]
        tool_input_start_v5: dict[str, Any] = next(
            e for e in events_v5 if isinstance(e, dict) and e.get('type') == 'tool-input-start'
        )
        assert 'providerMetadata' not in tool_input_start_v5

        event_stream_v6 = VercelAIEventStream(run_input=request, sdk_version=6)
        events_v6: list[str | dict[str, Any]] = [
            '[DONE]' if '[DONE]' in event else json.loads(event.removeprefix('data: '))
            async for event in event_stream_v6.encode_stream(event_stream_v6.transform_stream(event_generator()))
        ]
        tool_input_start_v6: dict[str, Any] = next(
            e for e in events_v6 if isinstance(e, dict) and e.get('type') == 'tool-input-start'
        )
        assert 'providerMetadata' in tool_input_start_v6

# pydantic_ai_slim/pydantic_ai/ui/_web/app.py:28-28
CHAT_UI_VERSION = '1.0.0'

# tests/models/xai_proto_cassettes.py:157-157
    version: int = 1

# tests/test_cli.py:34-36
def test_cli_version(capfd: CaptureFixture[str]):
    assert cli(['--version']) == 0
    assert capfd.readouterr().out.startswith('clai - Pydantic AI CLI')

# clai/clai/__init__.py:1-1
from importlib.metadata import version as _metadata_version

# clai/clai/__init__.py:1-1
from importlib.metadata import version as _metadata_version

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:95-95
    version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter.py:79-79
    sdk_version: Literal[5, 6] = 5

# pydantic_ai_slim/pydantic_ai/models/gemini.py:860-860
    model_version: NotRequired[Annotated[str, pydantic.Field(alias='modelVersion')]]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:80-80
    sdk_version: Literal[5, 6] = 5

# tests/test_utils.py:144-150
def test_package_versions(capsys: pytest.CaptureFixture[str]):
    if os.getenv('CI'):
        with capsys.disabled():  # pragma: lax no cover
            print('\npackage versions:')
            packages = sorted((package.metadata['Name'], package.version) for package in distributions())
            for name, version in packages:
                print(f'{name:30} {version}')

# tests/test_vercel_ai.py:4494-4529
    async def test_event_stream_uses_sdk_version(self):
        async def event_generator():
            part = ToolCallPart(
                tool_name='my_tool',
                tool_call_id='tc_ver',
                args={'key': 'value'},
                id='tool_call_id_ver',
                provider_name='anthropic',
            )
            yield PartStartEvent(index=0, part=part)
            yield PartEndEvent(index=0, part=part)

        request = SubmitMessage(
            id='foo',
            messages=[UIMessage(id='bar', role='user', parts=[TextUIPart(text='Test')])],
        )

        event_stream_v5 = VercelAIEventStream(run_input=request, sdk_version=5)
        events_v5: list[str | dict[str, Any]] = [
            '[DONE]' if '[DONE]' in event else json.loads(event.removeprefix('data: '))
            async for event in event_stream_v5.encode_stream(event_stream_v5.transform_stream(event_generator()))
        ]
        tool_input_start_v5: dict[str, Any] = next(
            e for e in events_v5 if isinstance(e, dict) and e.get('type') == 'tool-input-start'
        )
        assert 'providerMetadata' not in tool_input_start_v5

        event_stream_v6 = VercelAIEventStream(run_input=request, sdk_version=6)
        events_v6: list[str | dict[str, Any]] = [
            '[DONE]' if '[DONE]' in event else json.loads(event.removeprefix('data: '))
            async for event in event_stream_v6.encode_stream(event_stream_v6.transform_stream(event_generator()))
        ]
        tool_input_start_v6: dict[str, Any] = next(
            e for e in events_v6 if isinstance(e, dict) and e.get('type') == 'tool-input-start'
        )
        assert 'providerMetadata' in tool_input_start_v6

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:97-179
    def __init__(
        self,
        *,
        tracer_provider: TracerProvider | None = None,
        meter_provider: MeterProvider | None = None,
        include_binary_content: bool = True,
        include_content: bool = True,
        version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION,
        event_mode: Literal['attributes', 'logs'] = 'attributes',
        logger_provider: LoggerProvider | None = None,
    ):
        """Create instrumentation options.

        Args:
            tracer_provider: The OpenTelemetry tracer provider to use.
                If not provided, the global tracer provider is used.
                Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.
            meter_provider: The OpenTelemetry meter provider to use.
                If not provided, the global meter provider is used.
                Calling `logfire.configure()` sets the global meter provider, so most users don't need this.
            include_binary_content: Whether to include binary content in the instrumentation events.
            include_content: Whether to include prompts, completions, and tool call arguments and responses
                in the instrumentation events.
            version: Version of the data format. This is unrelated to the Pydantic AI package version.
                Version 1 is based on the legacy event-based OpenTelemetry GenAI spec
                    and will be removed in a future release.
                    The parameters `event_mode` and `logger_provider` are only relevant for version 1.
                Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes:
                    - `gen_ai.system_instructions` for instructions passed to the agent.
                    - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans.
                    - `pydantic_ai.all_messages` on agent run spans.
            event_mode: The mode for emitting events in version 1.
                If `'attributes'`, events are attached to the span as attributes.
                If `'logs'`, events are emitted as OpenTelemetry log-based events.
            logger_provider: The OpenTelemetry logger provider to use.
                If not provided, the global logger provider is used.
                Calling `logfire.configure()` sets the global logger provider, so most users don't need this.
                This is only used if `event_mode='logs'` and `version=1`.
        """
        from pydantic_ai import __version__

        tracer_provider = tracer_provider or get_tracer_provider()
        meter_provider = meter_provider or get_meter_provider()
        logger_provider = logger_provider or get_logger_provider()
        scope_name = 'pydantic-ai'
        self.tracer = tracer_provider.get_tracer(scope_name, __version__)
        self.meter = meter_provider.get_meter(scope_name, __version__)
        self.logger = logger_provider.get_logger(scope_name, __version__)
        self.event_mode = event_mode
        self.include_binary_content = include_binary_content
        self.include_content = include_content

        if event_mode != 'logs' and version != 1:
            warnings.warn(
                'event_mode is only relevant for version=1 which is deprecated and will be removed in a future release.',
                stacklevel=2,
            )
            version = 1

        self.version = version

        # As specified in the OpenTelemetry GenAI metrics spec:
        # https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage
        tokens_histogram_kwargs = dict(
            name='gen_ai.client.token.usage',
            unit='{token}',
            description='Measures number of input and output tokens used',
        )
        try:
            self.tokens_histogram = self.meter.create_histogram(
                **tokens_histogram_kwargs,
                explicit_bucket_boundaries_advisory=TOKEN_HISTOGRAM_BOUNDARIES,
            )
        except TypeError:  # pragma: lax no cover
            # Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory
            self.tokens_histogram = self.meter.create_histogram(
                **tokens_histogram_kwargs,  # pyright: ignore
            )
        self.cost_histogram = self.meter.create_histogram(
            'operation.cost',
            unit='{USD}',
            description='Monetary cost',
        )

# pydantic_ai_slim/pydantic_ai/_instrumentation.py:30-56
    def for_version(cls, version: int) -> Self:
        """Create instrumentation configuration for a specific version.

        Args:
            version: The instrumentation version (1, 2, or 3+)

        Returns:
            InstrumentationConfig instance with version-appropriate settings
        """
        if version <= 2:
            return cls(
                agent_run_span_name='agent run',
                agent_name_attr='agent_name',
                tool_span_name='running tool',
                tool_arguments_attr='tool_arguments',
                tool_result_attr='tool_response',
                output_tool_span_name='running output function',
            )
        else:
            return cls(
                agent_run_span_name='invoke_agent',
                agent_name_attr='gen_ai.agent.name',
                tool_span_name='execute_tool',  # Will be formatted with tool name
                tool_arguments_attr='gen_ai.tool.call.arguments',
                tool_result_attr='gen_ai.tool.call.result',
                output_tool_span_name='execute_tool',
            )

# pydantic_ai_slim/pydantic_ai/_run_context.py:49-49
    instrumentation_version: int = DEFAULT_INSTRUMENTATION_VERSION

# tests/ext/test_langchain.py:71-76
def test_langchain_tool_conversion():
    pydantic_tool = tool_from_langchain(langchain_tool)

    agent = Agent('test', tools=[pydantic_tool], retries=7)
    result = agent.run_sync('foobar')
    assert result.output == snapshot("{\"file_search\":\"I was called with {'dir_path': '.', 'pattern': 'a'}\"}")

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:20-20
from pydantic_ai._instrumentation import DEFAULT_INSTRUMENTATION_VERSION

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:20-20
from pydantic_ai._instrumentation import DEFAULT_INSTRUMENTATION_VERSION

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:20-20
from pydantic_ai._instrumentation import DEFAULT_INSTRUMENTATION_VERSION

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:20-20
from pydantic_ai._instrumentation import DEFAULT_INSTRUMENTATION_VERSION

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:20-20
from pydantic_ai._instrumentation import DEFAULT_INSTRUMENTATION_VERSION

# tests/test_cli.py:270-273
def test_handle_slash_command_other():
    io = StringIO()
    assert handle_slash_command('/foobar', [], False, Console(file=io), 'default') == (None, False)
    assert io.getvalue() == snapshot('Unknown command `/foobar`\n')

# tests/ext/test_langchain.py:112-133
def test_langchain_tool_conversion_no_defaults():
    langchain_tool = SimulatedLangChainTool(
        name='file_search',
        description='Recursively search for files in a subdirectory that match the regex pattern',
        args={
            'dir_path': {
                'description': 'Subdirectory to search in.',
                'title': 'Dir Path',
                'type': 'string',
            },
            'pattern': {
                'description': 'Unix shell regex, where * matches everything.',
                'title': 'Pattern',
                'type': 'string',
            },
        },
    )
    pydantic_tool = tool_from_langchain(langchain_tool)

    agent = Agent('test', tools=[pydantic_tool], retries=7)
    result = agent.run_sync('foobar')
    assert result.output == snapshot("{\"file_search\":\"I was called with {'dir_path': 'a', 'pattern': 'a'}\"}")

# tests/ext/test_langchain.py:136-159
def test_langchain_tool_conversion_no_required():
    langchain_tool = SimulatedLangChainTool(
        name='file_search',
        description='Recursively search for files in a subdirectory that match the regex pattern',
        args={
            'dir_path': {
                'default': '.',
                'description': 'Subdirectory to search in.',
                'title': 'Dir Path',
                'type': 'string',
            },
            'pattern': {
                'default': '*',
                'description': 'Unix shell regex, where * matches everything.',
                'title': 'Pattern',
                'type': 'string',
            },
        },
    )
    pydantic_tool = tool_from_langchain(langchain_tool)

    agent = Agent('test', tools=[pydantic_tool], retries=7)
    result = agent.run_sync('foobar')
    assert result.output == snapshot("{\"file_search\":\"I was called with {'dir_path': '.', 'pattern': '*'}\"}")

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:270-319
def _run_chat_command(
    args: argparse.Namespace, console: Console, name_version: str, default_model: str, prog_name: str
) -> int:
    """Handle the chat command."""
    agent: Agent[None, str] = cli_agent
    if args.agent:
        loaded = load_agent(args.agent)
        if loaded is None:
            console.print(f'[red]Error: Could not load agent from {args.agent}[/red]')
            return 1
        agent = loaded

    model_arg_set = args.model is not None
    if agent.model is None or model_arg_set:
        try:
            agent.model = infer_model(args.model or default_model)
        except UserError as e:
            console.print(f'Error initializing [magenta]{args.model}[/magenta]:\n[red]{e}[/red]')
            return 1

    model_name = agent.model if isinstance(agent.model, str) else f'{agent.model.system}:{agent.model.model_name}'
    if args.agent and model_arg_set:
        console.print(
            f'{name_version} using custom agent [magenta]{args.agent}[/magenta] with [magenta]{model_name}[/magenta]',
            highlight=False,
        )
    elif args.agent:
        console.print(f'{name_version} using custom agent [magenta]{args.agent}[/magenta]', highlight=False)
    else:
        console.print(f'{name_version} with [magenta]{model_name}[/magenta]', highlight=False)

    stream = not args.no_stream
    if args.code_theme == 'light':
        code_theme = 'default'
    elif args.code_theme == 'dark':
        code_theme = 'monokai'
    else:
        code_theme = args.code_theme  # pragma: no cover

    if args.prompt:
        try:
            asyncio.run(ask_agent(agent, args.prompt, stream, console, code_theme))
        except KeyboardInterrupt:
            pass
        return 0

    try:
        return asyncio.run(run_chat(stream, agent, console, code_theme, prog_name))
    except KeyboardInterrupt:  # pragma: no cover
        return 0

# pydantic_graph/pydantic_graph/_utils.py:89-95
def comma_and(items: list[str]) -> str:
    """Join with a comma and 'and' for the last item."""
    if len(items) == 1:
        return items[0]
    else:
        # oxford comma ¯\_(ツ)_/¯
        return ', '.join(items[:-1]) + ', and ' + items[-1]

# tests/test_usage_limits.py:3-3
import operator

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:146-202
def _cli_web(args_list: list[str], prog_name: str, default_model: str, qualified_model_names: list[str]) -> int:
    """Handle the web subcommand."""
    parser = argparse.ArgumentParser(
        prog=f'{prog_name} web',
        description='Start a web-based chat interface for a generic or specified agent',
    )
    parser.add_argument(
        '--agent',
        '-a',
        help='Agent to serve, in format "module:variable" (e.g., "mymodule:agent"). '
        'If omitted, creates a generic agent with the first specified model as default.',
    )
    model_arg = parser.add_argument(
        '-m',
        '--model',
        action='append',
        dest='models',
        help='Model to make available (can be repeated, e.g., -m openai:gpt-5 -m anthropic:claude-sonnet-4-5). '
        'Format: "provider:model_name". First model is preselected in UI; additional models appear as options.',
    )
    model_arg.completer = argcomplete.ChoicesCompleter(qualified_model_names)  # type: ignore[reportPrivateUsage]
    parser.add_argument(
        '-t',
        '--tool',
        choices=SUPPORTED_CLI_TOOL_IDS,
        action='append',
        dest='tools',
        help=f'Builtin tool to make available in the UI (can be repeated, e.g., -t web_search -t code_execution). '
        f'Available: {", ".join(SUPPORTED_CLI_TOOL_IDS)}.',
    )
    parser.add_argument(
        '-i',
        '--instructions',
        help="System instructions. When `--agent` is specified, these are additional to the agent's existing instructions "
        'and will be passed as extra instructions to each run.',
    )
    parser.add_argument(
        '--html-source',
        help='URL or file path for the chat UI HTML. If not specified, the UI is downloaded from a CDN.',
    )
    parser.add_argument('--host', default='127.0.0.1', help='Host to bind server (default: 127.0.0.1)')
    parser.add_argument('--port', type=int, default=7932, help='Port to bind server (default: 7932)')
    argcomplete.autocomplete(parser)
    args = parser.parse_args(args_list)

    from .web import run_web_command

    return run_web_command(
        agent_path=args.agent,
        host=args.host,
        port=args.port,
        models=args.models or [],
        tools=args.tools or [],
        instructions=args.instructions,
        default_model=default_model,
        html_source=args.html_source,
    )

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:43-43
from ..models.instrumented import InstrumentationSettings, InstrumentedModel, instrument_model

# tests/test_dbos.py:976-980
def conditions(ctx: RunContext, city: str) -> str:
    if ctx.run_step % 2 == 0:
        return "It's sunny"  # pragma: lax no cover
    else:
        return "It's raining"

# tests/test_dbos.py:976-980
def conditions(ctx: RunContext, city: str) -> str:
    if ctx.run_step % 2 == 0:
        return "It's sunny"  # pragma: lax no cover
    else:
        return "It's raining"

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:205-217
def _extract_version(model_name: str) -> int | None:
    """Extract the version number from a model name.

    Examples:
        - 'amazon.titan-embed-text-v1' -> 1
        - 'amazon.titan-embed-text-v2:0' -> 2
        - 'cohere.embed-english-v3' -> 3
        - 'cohere.embed-v4:0' -> 4
    """
    if match := re.search(r'v(\d+)', model_name):
        return int(match.group(1))
    else:  # pragma: no cover
        return None

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:205-267
def _cli_chat(args_list: list[str], prog_name: str, default_model: str, qualified_model_names: list[str]) -> int:
    """Handle the chat command (default)."""
    parser = argparse.ArgumentParser(
        prog=prog_name,
        description=f"""\
Pydantic AI CLI v{__version__}

subcommands:
  web           Start a web-based chat interface for an agent
                Run "clai web --help" for more information
""",
        formatter_class=argparse.RawTextHelpFormatter,
    )

    parser.add_argument(
        '-l',
        '--list-models',
        action='store_true',
        help='List all available models and exit',
    )
    parser.add_argument('--version', action='store_true', help='Show version and exit')

    # Chat arguments
    parser.add_argument(
        'prompt',
        nargs='?',
        help='AI prompt for one-shot mode. If omitted, starts interactive mode.',
    )
    model_arg = parser.add_argument(
        '-m',
        '--model',
        help=f'Model to use, in format "<provider>:<model>" e.g. "openai:gpt-5" or "anthropic:claude-sonnet-4-5". Defaults to "{default_model}".',
    )
    model_arg.completer = argcomplete.ChoicesCompleter(qualified_model_names)  # type: ignore[reportPrivateUsage]
    parser.add_argument(
        '-a',
        '--agent',
        help='Custom Agent to use, in format "module:variable", e.g. "mymodule.submodule:my_agent"',
    )
    parser.add_argument(
        '-t',
        '--code-theme',
        help='Which colors to use for code, can be "dark", "light" or any theme from pygments.org/styles/. Defaults to "dark" which works well on dark terminals.',
        default='dark',
    )
    parser.add_argument('--no-stream', action='store_true', help='Disable streaming from the model')
    argcomplete.autocomplete(parser)
    args = parser.parse_args(args_list)

    console = Console()
    name_version = f'[green]{prog_name} - Pydantic AI CLI v{__version__}[/green]'

    if args.version:
        console.print(name_version, highlight=False)
        return 0
    if args.list_models:
        console.print(f'{name_version}\n\n[green]Available models:[/green]')
        for model in qualified_model_names:
            console.print(f'  {model}', highlight=False)
        return 0

    # Default to chat command
    return _run_chat_command(args, console, name_version, default_model, prog_name)

# pydantic_ai_slim/pydantic_ai/_instrumentation.py:14-95
class InstrumentationNames:
    """Configuration for instrumentation span names and attributes based on version."""

    # Agent run span configuration
    agent_run_span_name: str
    agent_name_attr: str

    # Tool execution span configuration
    tool_span_name: str
    tool_arguments_attr: str
    tool_result_attr: str

    # Output Tool execution span configuration
    output_tool_span_name: str

    @classmethod
    def for_version(cls, version: int) -> Self:
        """Create instrumentation configuration for a specific version.

        Args:
            version: The instrumentation version (1, 2, or 3+)

        Returns:
            InstrumentationConfig instance with version-appropriate settings
        """
        if version <= 2:
            return cls(
                agent_run_span_name='agent run',
                agent_name_attr='agent_name',
                tool_span_name='running tool',
                tool_arguments_attr='tool_arguments',
                tool_result_attr='tool_response',
                output_tool_span_name='running output function',
            )
        else:
            return cls(
                agent_run_span_name='invoke_agent',
                agent_name_attr='gen_ai.agent.name',
                tool_span_name='execute_tool',  # Will be formatted with tool name
                tool_arguments_attr='gen_ai.tool.call.arguments',
                tool_result_attr='gen_ai.tool.call.result',
                output_tool_span_name='execute_tool',
            )

    def get_agent_run_span_name(self, agent_name: str) -> str:
        """Get the formatted agent span name.

        Args:
            agent_name: Name of the agent being executed

        Returns:
            Formatted span name
        """
        if self.agent_run_span_name == 'invoke_agent':
            return f'invoke_agent {agent_name}'
        return self.agent_run_span_name

    def get_tool_span_name(self, tool_name: str) -> str:
        """Get the formatted tool span name.

        Args:
            tool_name: Name of the tool being executed

        Returns:
            Formatted span name
        """
        if self.tool_span_name == 'execute_tool':
            return f'execute_tool {tool_name}'
        return self.tool_span_name

    def get_output_tool_span_name(self, tool_name: str) -> str:
        """Get the formatted output tool span name.

        Args:
            tool_name: Name of the tool being executed

        Returns:
            Formatted span name
        """
        if self.output_tool_span_name == 'execute_tool':
            return f'execute_tool {tool_name}'
        return self.output_tool_span_name

# tests/test_temporal.py:1579-1583
class SimpleAgentWorkflowWithEventStreamHandler:
    @workflow.run
    async def run(self, prompt: str) -> str:
        result = await simple_temporal_agent.run(prompt, event_stream_handler=simple_event_stream_handler)
        return result.output  # pragma: no cover

# clai/clai/__init__.py:9-11
def cli():
    """Run the clai CLI and exit."""
    _cli.cli_exit('clai')

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:135-139
    def visit_and_replace(
        self, visitor: Callable[[AbstractToolset[AgentDepsT]], AbstractToolset[AgentDepsT]]
    ) -> AbstractToolset[AgentDepsT]:
        """Run a visitor function on all "leaf" toolsets (i.e. those that implement their own tool listing and calling) and replace them in the hierarchy with the result of the function."""
        return visitor(self)