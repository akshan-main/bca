# Repository structure
.github/set_docs_main_preview_url.py
.github/set_docs_pr_preview_url.py
clai/clai/__init__.py
clai/clai/__main__.py
clai/update_readme.py
docs/.hooks/algolia.py
docs/.hooks/main.py
docs/.hooks/snippets.py
docs/.hooks/test_snippets.py
examples/pydantic_ai_examples/__main__.py
examples/pydantic_ai_examples/ag_ui/__init__.py
examples/pydantic_ai_examples/ag_ui/api/__init__.py
examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py
examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py
examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py
examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py
examples/pydantic_ai_examples/ag_ui/api/shared_state.py
examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py
examples/pydantic_ai_examples/bank_support.py
examples/pydantic_ai_examples/chat_app.py
examples/pydantic_ai_examples/data_analyst.py
examples/pydantic_ai_examples/evals/__init__.py
examples/pydantic_ai_examples/evals/agent.py
examples/pydantic_ai_examples/evals/custom_evaluators.py
examples/pydantic_ai_examples/evals/example_01_generate_dataset.py
examples/pydantic_ai_examples/evals/example_02_add_custom_evaluators.py
examples/pydantic_ai_examples/evals/example_03_unit_testing.py
examples/pydantic_ai_examples/evals/example_04_compare_models.py
examples/pydantic_ai_examples/evals/models.py
examples/pydantic_ai_examples/flight_booking.py
examples/pydantic_ai_examples/pydantic_model.py
examples/pydantic_ai_examples/question_graph.py
examples/pydantic_ai_examples/rag.py
examples/pydantic_ai_examples/roulette_wheel.py
examples/pydantic_ai_examples/slack_lead_qualifier/agent.py
examples/pydantic_ai_examples/slack_lead_qualifier/app.py
examples/pydantic_ai_examples/slack_lead_qualifier/functions.py
examples/pydantic_ai_examples/slack_lead_qualifier/modal.py
examples/pydantic_ai_examples/slack_lead_qualifier/models.py
examples/pydantic_ai_examples/slack_lead_qualifier/slack.py
examples/pydantic_ai_examples/slack_lead_qualifier/store.py
examples/pydantic_ai_examples/sql_gen.py
examples/pydantic_ai_examples/stream_markdown.py
examples/pydantic_ai_examples/stream_whales.py
examples/pydantic_ai_examples/weather_agent.py
examples/pydantic_ai_examples/weather_agent_gradio.py
pydantic_ai_slim/pydantic_ai/__init__.py
pydantic_ai_slim/pydantic_ai/__main__.py
pydantic_ai_slim/pydantic_ai/_a2a.py
pydantic_ai_slim/pydantic_ai/_agent_graph.py
pydantic_ai_slim/pydantic_ai/_cli/__init__.py
pydantic_ai_slim/pydantic_ai/_cli/web.py
pydantic_ai_slim/pydantic_ai/_function_schema.py
pydantic_ai_slim/pydantic_ai/_griffe.py
pydantic_ai_slim/pydantic_ai/_instrumentation.py
pydantic_ai_slim/pydantic_ai/_json_schema.py
pydantic_ai_slim/pydantic_ai/_mcp.py
pydantic_ai_slim/pydantic_ai/_otel_messages.py
pydantic_ai_slim/pydantic_ai/_output.py
pydantic_ai_slim/pydantic_ai/_parts_manager.py
pydantic_ai_slim/pydantic_ai/_run_context.py
pydantic_ai_slim/pydantic_ai/_ssrf.py
pydantic_ai_slim/pydantic_ai/_system_prompt.py
pydantic_ai_slim/pydantic_ai/_thinking_part.py
pydantic_ai_slim/pydantic_ai/_tool_manager.py
pydantic_ai_slim/pydantic_ai/_utils.py
pydantic_ai_slim/pydantic_ai/ag_ui.py
pydantic_ai_slim/pydantic_ai/agent/__init__.py
pydantic_ai_slim/pydantic_ai/agent/abstract.py
pydantic_ai_slim/pydantic_ai/agent/wrapper.py
pydantic_ai_slim/pydantic_ai/builtin_tools.py
pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py
pydantic_ai_slim/pydantic_ai/common_tools/exa.py
pydantic_ai_slim/pydantic_ai/common_tools/tavily.py
pydantic_ai_slim/pydantic_ai/concurrency.py
pydantic_ai_slim/pydantic_ai/direct.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/__init__.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_fastmcp_toolset.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp_server.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py
pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_util


# Relevant source code


# pydantic_evals/pydantic_evals/reporting/render_numbers.py:62-94
def default_render_number_diff(old: float | int, new: float | int) -> str | None:
    """Return a string representing the difference between old and new values.

    Rules:
      - If the two values are equal, return None.
      - For integers, return the raw difference (with a leading sign), e.g.:
            _default_format_number_diff(3, 4) -> '+1'
      - For floats (or a mix of float and int):
          * Compute the raw delta = new - old and format it with ABS_SIG_FIGS significant figures.
          * If `old` is nonzero, compute a relative change:
              - If |delta|/|old| ≤ 1, render the relative change as a percentage with
                PERC_DECIMALS decimal places, e.g. '+0.7 / +70.0%'.
              - If |delta|/|old| > 1, render a multiplier (new/old). Use one decimal place
                if the absolute multiplier is less than MULTIPLIER_ONE_DECIMAL_THRESHOLD,
                otherwise no decimals.
          * However, if the percentage rounds to 0.0% (e.g. '+0.0%'), return only the absolute diff.
          * Also, if |old| is below BASE_THRESHOLD and |delta| exceeds MULTIPLIER_DROP_FACTOR×|old|,
            drop the relative change indicator.
    """
    if old == new:
        return None

    if isinstance(old, int) and isinstance(new, int):
        diff_int = new - old
        return f'{diff_int:+d}'

    delta = new - old
    abs_diff_str = _render_signed(delta, ABS_SIG_FIGS)
    rel_diff_str = _render_relative(new, old, BASE_THRESHOLD)
    if rel_diff_str is None:
        return abs_diff_str
    else:
        return f'{abs_diff_str} / {rel_diff_str}'

# pydantic_graph/pydantic_graph/beta/decision.py:88-124
class DecisionBranch(Generic[SourceT]):
    """Represents a single branch within a decision node.

    Each branch defines the conditions under which it should be taken
    and the path to follow when those conditions are met.

    Note: with the current design, it is actually _critical_ that this class is invariant in SourceT for the sake
    of type-checking that inputs to a Decision are actually handled. See the `# type: ignore` comment in
    `tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch` for an example of how this works.
    """

    source: TypeOrTypeExpression[SourceT]
    """The expected type of data for this branch.

    This is necessary for exhaustiveness-checking when handling the inputs to a decision node."""

    matches: Callable[[Any], bool] | None
    """An optional predicate function used to determine whether input data matches this branch.

    If `None`, default logic is used which attempts to check the value for type-compatibility with the `source` type:
    * If `source` is `Any` or `object`, the branch will always match
    * If `source` is a `Literal` type, this branch will match if the value is one of the parametrizing literal values
    * If `source` is any other type, the value will be checked for matching using `isinstance`

    Inputs are tested against each branch of a decision node in order, and the path of the first matching branch is
    used to handle the input value.
    """

    path: Path
    """The execution path to follow when an input value matches this branch of a decision node.

    This can include transforming, mapping, and broadcasting the output before sending to the next node or nodes.

    The path can also include position-aware labels which are used when generating mermaid diagrams."""

    destinations: list[AnyDestinationNode]
    """The destination nodes that can be referenced by DestinationMarker in the path."""

# examples/pydantic_ai_examples/roulette_wheel.py:35-44
async def roulette_wheel(
    ctx: RunContext[Deps], square: int
) -> Literal['winner', 'loser']:
    """Check if the bet square is a winner.

    Args:
        ctx: The context containing the winning number.
        square: The number the player bet on.
    """
    return 'winner' if square == ctx.deps.winning_number else 'loser'

# pydantic_ai_slim/pydantic_ai/_a2a.py:276-307
    def _response_parts_to_a2a(self, parts: Sequence[ModelResponsePart]) -> list[Part]:
        """Convert pydantic-ai ModelResponsePart objects to A2A Part objects.

        This handles the conversion from pydantic-ai's internal response parts to
        A2A protocol parts. Different part types are handled as follows:
        - TextPart: Converted directly to A2A TextPart
        - ThinkingPart: Converted to TextPart with metadata indicating it's thinking
        - ToolCallPart: Skipped (internal to agent execution)

        Args:
            parts: List of ModelResponsePart objects from agent response

        Returns:
            List of A2A Part objects suitable for sending via A2A protocol
        """
        a2a_parts: list[Part] = []
        for part in parts:
            if isinstance(part, TextPart):
                a2a_parts.append(A2ATextPart(kind='text', text=part.content))
            elif isinstance(part, ThinkingPart):
                # Convert thinking to text with metadata
                a2a_parts.append(
                    A2ATextPart(
                        kind='text',
                        text=part.content,
                        metadata={'type': 'thinking', 'thinking_id': part.id, 'signature': part.signature},
                    )
                )
            elif isinstance(part, ToolCallPart):
                # Skip tool calls - they're internal to agent execution
                pass
        return a2a_parts

# pydantic_ai_slim/pydantic_ai/_ssrf.py:74-79
def is_cloud_metadata_ip(ip_str: str) -> bool:
    """Check if an IP address is a cloud metadata endpoint.

    These are always blocked for security reasons, even with allow_local=True.
    """
    return ip_str in _CLOUD_METADATA_IPS

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:1245-1352
    def to_ag_ui(
        self,
        *,
        # Agent.iter parameters
        output_type: OutputSpec[OutputDataT] | None = None,
        message_history: Sequence[_messages.ModelMessage] | None = None,
        deferred_tool_results: DeferredToolResults | None = None,
        model: models.Model | models.KnownModelName | str | None = None,
        deps: AgentDepsT = None,
        model_settings: ModelSettings | None = None,
        usage_limits: UsageLimits | None = None,
        usage: RunUsage | None = None,
        infer_name: bool = True,
        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,
        # Starlette
        debug: bool = False,
        routes: Sequence[BaseRoute] | None = None,
        middleware: Sequence[Middleware] | None = None,
        exception_handlers: Mapping[Any, ExceptionHandler] | None = None,
        on_startup: Sequence[Callable[[], Any]] | None = None,
        on_shutdown: Sequence[Callable[[], Any]] | None = None,
        lifespan: Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None = None,
    ) -> AGUIApp[AgentDepsT, OutputDataT]:
        """Returns an ASGI application that handles every AG-UI request by running the agent.

        Note that the `deps` will be the same for each request, with the exception of the AG-UI state that's
        injected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ag_ui.StateHandler] protocol.
        To provide different `deps` for each request (e.g. based on the authenticated user),
        use [`pydantic_ai.ag_ui.run_ag_ui`][pydantic_ai.ag_ui.run_ag_ui] or
        [`pydantic_ai.ag_ui.handle_ag_ui_request`][pydantic_ai.ag_ui.handle_ag_ui_request] instead.

        Example:
        ```python
        from pydantic_ai import Agent

        agent = Agent('openai:gpt-5.2')
        app = agent.to_ag_ui()
        ```

        The `app` is an ASGI application that can be used with any ASGI server.

        To run the application, you can use the following command:

        ```bash
        uvicorn app:app --host 0.0.0.0 --port 8000
        ```

        See [AG-UI docs](../ui/ag-ui.md) for more information.

        Args:
            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has
                no output validators since output validators would expect an argument that matches the agent's
                output type.
            message_history: History of the conversation so far.
            deferred_tool_results: Optional results for deferred tool calls in the message history.
            model: Optional model to use for this run, required if `model` was not set when creating the agent.
            deps: Optional dependencies to use for this run.
            model_settings: Optional settings to use for this model's request.
            usage_limits: Optional limits on model request count or token usage.
            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
            infer_name: Whether to try to infer the agent name from the call frame if it's not set.
            toolsets: Optional additional toolsets for this run.

            debug: Boolean indicating if debug tracebacks should be returned on errors.
            routes: A list of routes to serve incoming HTTP and WebSocket requests.
            middleware: A list of middleware to run for every request. A starlette application will always
                automatically include two middleware classes. `ServerErrorMiddleware` is added as the very
                outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.
                `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled
                exception cases occurring in the routing or endpoints.
            exception_handlers: A mapping of either integer status codes, or exception class types onto
                callables which handle the exceptions. Exception handler callables should be of the form
                `handler(request, exc) -> response` and may be either standard functions, or async functions.
            on_startup: A list of callables to run on application startup. Startup handler callables do not
                take any arguments, and may be either standard functions, or async functions.
            on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do
                not take any arguments, and may be either standard functions, or async functions.
            lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.
                This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or
                the other, not both.

        Returns:
            An ASGI application for running Pydantic AI agents with AG-UI protocol support.
        """
        from pydantic_ai.ui.ag_ui.app import AGUIApp

        return AGUIApp(
            agent=self,
            # Agent.iter parameters
            output_type=output_type,
            message_history=message_history,
            deferred_tool_results=deferred_tool_results,
            model=model,
            deps=deps,
            model_settings=model_settings,
            usage_limits=usage_limits,
            usage=usage,
            infer_name=infer_name,
            toolsets=toolsets,
            # Starlette
            debug=debug,
            routes=routes,
            middleware=middleware,
            exception_handlers=exception_handlers,
            on_startup=on_startup,
            on_shutdown=on_shutdown,
            lifespan=lifespan,
        )

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_toolset.py:31-67
def prefectify_toolset(
    toolset: AbstractToolset[AgentDepsT],
    mcp_task_config: TaskConfig,
    tool_task_config: TaskConfig,
    tool_task_config_by_name: dict[str, TaskConfig | None],
) -> AbstractToolset[AgentDepsT]:
    """Wrap a toolset to integrate it with Prefect.

    Args:
        toolset: The toolset to wrap.
        mcp_task_config: The Prefect task config to use for MCP server tasks.
        tool_task_config: The default Prefect task config to use for tool calls.
        tool_task_config_by_name: Per-tool task configuration. Keys are tool names, values are TaskConfig or None.
    """
    if isinstance(toolset, FunctionToolset):
        from ._function_toolset import PrefectFunctionToolset

        return PrefectFunctionToolset(
            wrapped=toolset,
            task_config=tool_task_config,
            tool_task_config=tool_task_config_by_name,
        )

    try:
        from pydantic_ai.mcp import MCPServer

        from ._mcp_server import PrefectMCPServer
    except ImportError:
        pass
    else:
        if isinstance(toolset, MCPServer):
            return PrefectMCPServer(
                wrapped=toolset,
                task_config=mcp_task_config,
            )

    return toolset

# pydantic_ai_slim/pydantic_ai/models/__init__.py:1069-1079
def check_allow_model_requests() -> None:
    """Check if model requests are allowed.

    If you're defining your own models that have costs or latency associated with their use, you should call this in
    [`Model.request`][pydantic_ai.models.Model.request] and [`Model.request_stream`][pydantic_ai.models.Model.request_stream].

    Raises:
        RuntimeError: If model requests are not allowed.
    """
    if not ALLOW_MODEL_REQUESTS:
        raise RuntimeError('Model requests are not allowed, since ALLOW_MODEL_REQUESTS is False')

# pydantic_ai_slim/pydantic_ai/profiles/zai.py:6-11
def zai_model_profile(model_name: str) -> ModelProfile | None:
    """The model profile for ZAI models.

    Currently returns None as ZAI model-specific properties are handled at the provider level.
    """
    return None

# pydantic_ai_slim/pydantic_ai/usage.py:99-101
    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

# pydantic_ai_slim/pydantic_ai/usage.py:354-364
    def has_token_limits(self) -> bool:
        """Returns `True` if this instance places any limits on token counts.

        If this returns `False`, the `check_tokens` method will never raise an error.

        This is useful because if we have token limits, we need to check them after receiving each streamed message.
        If there are no limits, we can skip that processing in the streaming response iterator.
        """
        return any(
            limit is not None for limit in (self.input_tokens_limit, self.output_tokens_limit, self.total_tokens_limit)
        )

# tests/models/xai_proto_cassettes.py:354-356
    def _can_replay(self) -> bool:
        """Check if there are more recorded interactions to replay."""
        return self.interaction_idx < len(self.cassette.interactions)

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:19-19
    content: str

# docs/.hooks/algolia.py:20-20
    pageID: str