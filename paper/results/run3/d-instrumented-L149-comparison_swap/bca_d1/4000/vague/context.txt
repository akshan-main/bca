## examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py

from pydantic import BaseModel, Field

## examples/pydantic_ai_examples/evals/custom_evaluators.py

from pydantic_evals.evaluators import (
    Evaluator,
    EvaluatorContext,
    EvaluatorOutput,
)

## pydantic_ai_slim/pydantic_ai/_json_schema.py

from abc import ABC, abstractmethod

## pydantic_ai_slim/pydantic_ai/direct.py

def _prepare_model(
    model: models.Model | models.KnownModelName | str,
    instrument: instrumented_models.InstrumentationSettings | bool | None,
) -> models.Model:
    model_instance = models.infer_model(model)

    if instrument is None:
        instrument = agent.Agent._instrument_default  # pyright: ignore[reportPrivateUsage]

    return instrumented_models.instrument_model(model_instance, instrument)

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_models.py

class CamelBaseModel(BaseModel, ABC):
    """Base model with camelCase aliases."""

    model_config = ConfigDict(alias_generator=to_camel, populate_by_name=True, extra='forbid')

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py

class BaseUIPart(CamelBaseModel, ABC):
    """Abstract base class for all UI parts."""

class StepStartUIPart(BaseUIPart):
    """A step boundary part of a message."""

    type: Literal['step-start'] = 'step-start'

## pydantic_evals/pydantic_evals/evaluators/common.py

class Equals(Evaluator[object, object, object]):
    """Check if the output exactly equals the provided value."""

    value: Any
    evaluation_name: str | None = field(default=None)

    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:
        return ctx.output == self.value

    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:
        return ctx.output == self.value

## pydantic_evals/pydantic_evals/evaluators/report_common.py

    n_thresholds: int = 100

## pydantic_evals/pydantic_evals/reporting/render_numbers.py

MULTIPLIER_ONE_DECIMAL_THRESHOLD = 100  # If |multiplier| is below this, use one decimal; otherwise, use none.

BASE_THRESHOLD = 1e-2  # If |old| is below this and delta is > MULTIPLIER_DROP_FACTOR * |old|, drop relative change.

def default_render_number(value: float | int) -> str:
    """The default logic for formatting numerical values in an Evaluation report.

    * If the value is an integer, format it as an integer.
    * If the value is a float, include at least one decimal place and at least 3 significant figures.
    """
    # If it's an int, just return its string representation.
    if isinstance(value, int):
        return f'{value:,d}'

    abs_val = abs(value)

    # Special case for zero:
    if abs_val == 0:
        return f'{value:,.{VALUE_SIG_FIGS}f}'

    if abs_val >= 1:
        # Count the digits in the integer part.
        digits = math.floor(math.log10(abs_val)) + 1
        # Number of decimals: at least one, and enough to reach 4 significant figures.
        decimals = max(1, VALUE_SIG_FIGS - digits)
    else:
        # For numbers between 0 and 1, determine the exponent.
        # For example: 0.1 -> log10(0.1) = -1, so we want -(-1) + 3 = 4 decimals.
        exponent = math.floor(math.log10(abs_val))
        decimals = -exponent + VALUE_SIG_FIGS - 1  # because the first nonzero digit is in the 10^exponent place.

    return f'{value:,.{decimals}f}'

def default_render_number_diff(old: float | int, new: float | int) -> str | None:
    """Return a string representing the difference between old and new values.

    Rules:
      - If the two values are equal, return None.
      - For integers, return the raw difference (with a leading sign), e.g.:
            _default_format_number_diff(3, 4) -> '+1'
      - For floats (or a mix of float and int):
          * Compute the raw delta = new - old and format it with ABS_SIG_FIGS significant figures.
          * If `old` is nonzero, compute a relative change:
              - If |delta|/|old| ≤ 1, render the relative change as a percentage with
                PERC_DECIMALS decimal places, e.g. '+0.7 / +70.0%'.
              - If |delta|/|old| > 1, render a multiplier (new/old). Use one decimal place
                if the absolute multiplier is less than MULTIPLIER_ONE_DECIMAL_THRESHOLD,
                otherwise no decimals.
          * However, if the percentage rounds to 0.0% (e.g. '+0.0%'), return only the absolute diff.
          * Also, if |old| is below BASE_THRESHOLD and |delta| exceeds MULTIPLIER_DROP_FACTOR×|old|,
            drop the relative change indicator.
    """
    if old == new:
        return None

    if isinstance(old, int) and isinstance(new, int):
        diff_int = new - old
        return f'{diff_int:+d}'

    delta = new - old
    abs_diff_str = _render_signed(delta, ABS_SIG_FIGS)
    rel_diff_str = _render_relative(new, old, BASE_THRESHOLD)
    if rel_diff_str is None:
        return abs_diff_str
    else:
        return f'{abs_diff_str} / {rel_diff_str}'

def default_render_duration_diff(old: float, new: float) -> str | None:
    """Format a duration difference (in seconds) with an explicit sign."""
    if old == new:
        return None

    abs_diff_str = _render_duration(new - old, True)
    rel_diff_str = _render_relative(new, old, BASE_THRESHOLD)
    if rel_diff_str is None:
        return abs_diff_str
    else:
        return f'{abs_diff_str} / {rel_diff_str}'

def _render_relative(new: float, base: float, small_base_threshold: float) -> str | None:
    # If we cannot compute a relative change, return just the diff.
    if base == 0:
        return None

    delta = new - base

    # For very small base values with huge changes, drop the relative indicator.
    if abs(base) < small_base_threshold and abs(delta) > MULTIPLIER_DROP_FACTOR * abs(base):
        return None

    # Compute the relative change as a percentage.
    rel_change = (delta / base) * 100
    perc_str = f'{rel_change:+.{PERC_DECIMALS}f}%'
    # If the percentage rounds to 0.0%, return only the absolute difference.
    if perc_str in ('+0.0%', '-0.0%'):
        return None

    # Decide whether to use percentage style or multiplier style.
    if abs(delta) / abs(base) <= 1:
        # Percentage style.
        return perc_str
    else:
        # Multiplier style.
        multiplier = new / base
        if abs(multiplier) < MULTIPLIER_ONE_DECIMAL_THRESHOLD:
            mult_str = f'{multiplier:,.1f}x'
        else:
            mult_str = f'{multiplier:,.0f}x'
        return mult_str

## pydantic_graph/pydantic_graph/beta/join.py

def reduce_sum(current: NumericT, inputs: NumericT) -> NumericT:
    """A reducer that sums numbers."""
    return current + inputs

## tests/conftest.py

SNAPSHOT_BYTES_COLLAPSE_THRESHOLD = 50

## tests/graph/beta/test_parent_forks.py

def test_parent_fork_self_loop():
    """Test parent fork identification with a self-loop at the join."""
    join_id = 'J'
    nodes = {'start', 'F', 'A', 'B', 'J', 'end'}
    start_ids = {'start'}
    fork_ids = {'F'}
    edges = {
        'start': ['F'],
        'F': ['A', 'B'],
        'A': ['J'],
        'B': ['J'],
        'J': ['J', 'end'],  # Self-loop
    }

    finder = ParentForkFinder(nodes, start_ids, fork_ids, edges)
    parent_fork = finder.find_parent_fork(join_id)

    # Self-loop means J is on a cycle avoiding F
    assert parent_fork is None

## tests/models/test_gemini.py

async def test_labels_are_ignored_with_gla_provider(allow_model_requests: None, gemini_api_key: str) -> None:
    m = GeminiModel('gemini-2.0-flash', provider=GoogleGLAProvider(api_key=gemini_api_key))
    agent = Agent(m)

    result = await agent.run(
        'What is the capital of France?',
        model_settings=GeminiModelSettings(gemini_labels={'environment': 'test', 'team': 'analytics'}),
    )
    assert result.output == snapshot('The capital of France is **Paris**.\n')

## tests/models/test_model_request_parameters.py

def test_model_request_parameters_are_serializable():
    params = ModelRequestParameters(
        function_tools=[],
        builtin_tools=[],
        output_mode='text',
        allow_text_output=True,
        output_tools=[],
        output_object=None,
    )
    dumped = ta.dump_python(params)
    assert dumped == snapshot(
        {
            'function_tools': [],
            'builtin_tools': [],
            'output_mode': 'text',
            'output_object': None,
            'output_tools': [],
            'prompted_output_template': None,
            'allow_text_output': True,
            'allow_image_output': False,
        }
    )
    assert ta.validate_python(dumped) == params

    params = ModelRequestParameters(
        function_tools=[ToolDefinition(name='test')],
        builtin_tools=[
            WebSearchTool(user_location=WebSearchUserLocation(city='New York', country='US')),
            CodeExecutionTool(),
            WebFetchTool(),
            ImageGenerationTool(size='1024x1024'),
            MemoryTool(),
            MCPServerTool(id='deepwiki', url='https://mcp.deepwiki.com/mcp'),
            MCPServerTool(id='github', url='https://api.githubcopilot.com/mcp'),
        ],
        output_mode='text',
        allow_text_output=True,
        output_tools=[ToolDefinition(name='final_result')],
        output_object=None,
    )
    dumped = ta.dump_python(params)
    assert dumped == snapshot(
        {
            'function_tools': [
                {
                    'name': 'test',
                    'parameters_json_schema': {'type': 'object', 'properties': {}},
                    'description': None,
                    'outer_typed_dict_key': None,
                    'strict': None,
                    'sequential': False,
                    'kind': 'function',
                    'metadata': None,
                    'timeout': None,
                }
            ],
            'builtin_tools': [
                {
                    'kind': 'web_search',
                    'search_context_size': 'medium',
                    'user_location': {'city': 'New York', 'country': 'US'},
                    'blocked_domains': None,
                    'allowed_domains': None,
                    'max_uses': None,
                },
                {'kind': 'code_execution'},
                {
                    'kind': 'web_fetch',
                    'max_uses': None,
                    'allowed_domains': None,
                    'blocked_domains': None,
                    'enable_citations': False,
                    'max_content_tokens': None,
                },
                {
                    'kind': 'image_generation',
                    'background': 'auto',
                    'input_fidelity': None,
                    'moderation': 'auto',
                    'output_compression': None,
                    'output_format': None,
                    'partial_images': 0,
                    'quality': 'auto',
                    'size': '1024x1024',
                    'aspect_ratio': None,
                },
                {'kind': 'memory'},
                {
                    'kind': 'mcp_server',
                    'id': 'deepwiki',
                    'url': 'https://mcp.deepwiki.com/mcp',
                    'authorization_token': None,
                    'description': None,
                    'allowed_tools': None,
                    'headers': None,
                },
                {
                    'kind': 'mcp_server',
                    'id': 'github',
                    'url': 'https://api.githubcopilot.com/mcp',
                    'authorization_token': None,
                    'description': None,
                    'allowed_tools': None,
                    'headers': None,
                },
            ],
            'output_mode': 'text',
            'output_object': None,
            'output_tools': [
                {
                    'name': 'final_result',
                    'parameters_json_schema': {'type': 'object', 'properties': {}},
                    'description': None,
                    'outer_typed_dict_key': None,
                    'strict': None,
                    'sequential': False,
                    'kind': 'function',
                    'metadata': None,
                    'timeout': None,
                }
            ],
            'prompted_output_template': None,
            'allow_text_output': True,
            'allow_image_output': False,
        }
    )
    assert ta.validate_python(dumped) == params

## tests/test_a2a.py

async def test_a2a_thinking_response():
    """Test that ModelResponse messages with ThinkingPart are properly handled."""

    def return_thinking_response(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        assert info.output_tools is not None
        # Create a response with thinking part and text part
        return ModelResponse(
            parts=[
                ThinkingPart(content='Let me think about this...', id='thinking_1'),
                PydanticAITextPart(content="Here's my response"),
            ]
        )

    thinking_model = FunctionModel(return_thinking_response)
    agent = Agent(model=thinking_model, output_type=str)
    app = agent.to_a2a()

    async with LifespanManager(app):
        transport = httpx.ASGITransport(app)
        async with httpx.AsyncClient(transport=transport) as http_client:
            a2a_client = A2AClient(http_client=http_client)

            message = Message(
                role='user',
                parts=[TextPart(text='Hello, world!', kind='text')],
                kind='message',
                message_id=str(uuid.uuid4()),
            )
            response = await a2a_client.send_message(message=message)
            assert 'error' not in response
            assert 'result' in response
            result = response['result']
            assert result['kind'] == 'task'

            task_id = result['id']

            # Wait for completion
            while task := await a2a_client.get_task(task_id):  # pragma: no branch
                if 'result' in task and task['result']['status']['state'] == 'completed':
                    result = task['result']
                    break
                await anyio.sleep(0.1)

            assert result == snapshot(
                {
                    'id': IsStr(),
                    'context_id': IsStr(),
                    'kind': 'task',
                    'status': {'state': 'completed', 'timestamp': IsDatetime(iso_string=True)},
                    'history': [
                        {
                            'role': 'user',
                            'parts': [{'kind': 'text', 'text': 'Hello, world!'}],
                            'kind': 'message',
                            'message_id': IsStr(),
                            'context_id': IsStr(),
                            'task_id': IsStr(),
                        },
                        {
                            'role': 'agent',
                            'parts': [
                                {
                                    'metadata': {'type': 'thinking', 'thinking_id': 'thinking_1', 'signature': None},
                                    'kind': 'text',
                                    'text': 'Let me think about this...',
                                },
                                {'kind': 'text', 'text': "Here's my response"},
                            ],
                            'kind': 'message',
                            'message_id': IsStr(),
                            'context_id': IsStr(),
                            'task_id': IsStr(),
                        },
                    ],
                    'artifacts': [
                        {
                            'artifact_id': IsStr(),
                            'name': 'result',
                            'parts': [{'kind': 'text', 'text': "Here's my response"}],
                        }
                    ],
                }
            )

## tests/test_agent.py

def test_instructions_decorator_with_parenthesis():
    agent = Agent('test')

    @agent.instructions()
    def instructions_2() -> str:
        return 'You are a helpful assistant.'

    result = agent.run_sync('Hello')
    assert result.all_messages()[0] == snapshot(
        ModelRequest(
            parts=[UserPromptPart(content='Hello', timestamp=IsNow(tz=timezone.utc))],
            timestamp=IsNow(tz=timezone.utc),
            instructions='You are a helpful assistant.',
            run_id=IsStr(),
        )
    )
