# pydantic_ai_slim/pydantic_ai/models/instrumented.py:40-57
MODEL_SETTING_ATTRIBUTES: tuple[
    Literal[
        'max_tokens',
        'top_p',
        'seed',
        'temperature',
        'presence_penalty',
        'frequency_penalty',
    ],
    ...,
] = (
    'max_tokens',
    'top_p',
    'seed',
    'temperature',
    'presence_penalty',
    'frequency_penalty',
)

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:11-11
from opentelemetry.util.types import AttributeValue

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# pydantic_evals/pydantic_evals/otel/span_tree.py:43-43
    has_attributes: dict[str, Any]

# pydantic_ai_slim/pydantic_ai/models/cerebras.py:52-52
    cerebras_disable_reasoning: bool

# pydantic_evals/pydantic_evals/otel/span_tree.py:44-44
    has_attribute_keys: list[str]

# tests/models/test_cerebras.py:37-49
async def test_cerebras_disable_reasoning_setting(allow_model_requests: None, cerebras_api_key: str):
    """Test that cerebras_disable_reasoning setting is properly transformed to extra_body.

    Note: disable_reasoning is only supported on reasoning models: zai-glm-4.6 and gpt-oss-120b.
    """
    provider = CerebrasProvider(api_key=cerebras_api_key)
    model = CerebrasModel('zai-glm-4.6', provider=provider)

    settings = CerebrasModelSettings(cerebras_disable_reasoning=True)
    response = await model_request(model, [ModelRequest.user_text_prompt('What is 2 + 2?')], model_settings=settings)

    text_part = cast(TextPart, response.parts[0])
    assert '4' in text_part.content

# tests/models/test_model_settings.py:121-144
def test_none_settings_in_hierarchy():
    """Test that None settings at any level don't break the merge hierarchy."""
    captured_settings = None

    def capture_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('captured')])

    # Model with no settings
    model = FunctionModel(capture_settings, settings=None)

    # Agent with settings
    agent_settings = ModelSettings(max_tokens=150, temperature=0.5)
    agent = Agent(model=model, model_settings=agent_settings)

    # Run with no additional settings
    result = agent.run_sync('test', model_settings=None)
    assert result.output == 'captured'

    # Should have agent settings
    assert captured_settings is not None
    assert captured_settings['max_tokens'] == 150
    assert captured_settings['temperature'] == 0.5

# pydantic_ai_slim/pydantic_ai/usage.py:99-101
    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# pydantic_evals/pydantic_evals/dataset.py:959-966
    def record_attribute(self, name: str, value: Any) -> None:
        """Record an attribute value.

        Args:
            name: The name of the attribute.
            value: The value of the attribute.
        """
        self.attributes[name] = value

# tests/test_prefect.py:1217-1248
async def test_disabled_tool():
    """Test that tools can be disabled via tool_task_config_by_name."""

    # Create an agent with a tool
    test_agent = Agent(TestModel(), name='test_disabled_tool')

    @test_agent.tool_plain
    def my_tool() -> str:
        return 'Tool executed'

    # Create PrefectAgent with the tool disabled
    test_prefect_agent = PrefectAgent(
        test_agent,
        tool_task_config_by_name={
            'my_tool': None,
        },
    )

    # Test outside a flow
    result = await test_prefect_agent.run('Call my_tool')
    messages = result.all_messages()
    assert any('my_tool' in str(msg) for msg in messages)

    # Test inside a flow to ensure disabled tools work there too
    @flow
    async def test_flow():
        result = await test_prefect_agent.run('Call my_tool')
        return result

    flow_result = await test_flow()
    flow_messages = flow_result.all_messages()
    assert any('my_tool' in str(msg) for msg in flow_messages)

# tests/test_usage_limits.py:258-284
def test_add_usages_with_none_detail_value():
    """Test that None values in details are skipped when incrementing usage."""
    usage = RunUsage(
        requests=1,
        input_tokens=10,
        output_tokens=20,
        details={'reasoning_tokens': 5},
    )

    # Create a usage with None in details (simulating model response with missing detail)
    incr_usage = RunUsage(
        requests=1,
        input_tokens=5,
        output_tokens=10,
    )
    # Manually set a None value in details to simulate edge case from model responses
    incr_usage.details = {'reasoning_tokens': None, 'other_tokens': 10}  # type: ignore[dict-item]

    result = usage + incr_usage
    assert result == snapshot(
        RunUsage(
            requests=2,
            input_tokens=15,
            output_tokens=30,
            details={'reasoning_tokens': 5, 'other_tokens': 10},
        )
    )

# pydantic_ai_slim/pydantic_ai/_utils.py:159-160
def is_set(t_or_unset: T | Unset) -> TypeGuard[T]:
    return t_or_unset is not UNSET

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:18-18
from .settings import EmbeddingSettings, merge_embedding_settings

# pydantic_evals/pydantic_evals/__init__.py:9-9
from .dataset import Case, Dataset, increment_eval_metric, set_eval_attribute

# pydantic_ai_slim/pydantic_ai/_utils.py:55-55
_disable_threads: ContextVar[bool] = ContextVar('_disable_threads', default=False)

# pydantic_ai_slim/pydantic_ai/_utils.py:156-156
UNSET = Unset()

# pydantic_ai_slim/pydantic_ai/_utils.py:156-156
UNSET = Unset()

# pydantic_ai_slim/pydantic_ai/_utils.py:156-156
UNSET = Unset()

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# pydantic_ai_slim/pydantic_ai/_utils.py:156-156
UNSET = Unset()

# pydantic_ai_slim/pydantic_ai/_utils.py:156-156
UNSET = Unset()

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# pydantic_ai_slim/pydantic_ai/_utils.py:156-156
UNSET = Unset()

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:298-303
    def system_instructions_attributes(self, instructions: str | None) -> dict[str, str]:
        if instructions and self.include_content:
            return {
                'gen_ai.system_instructions': json.dumps([_otel_messages.TextPart(type='text', content=instructions)]),
            }
        return {}

# tests/test_vercel_ai.py:4409-4434
    async def test_dump_provider_metadata_filters_none_values(self):
        """Test that dump_provider_metadata only includes non-None values."""

        # All None - should return None
        result = dump_provider_metadata(id=None, provider_name=None, provider_details=None)
        assert result is None

        # Some values
        result = dump_provider_metadata(id='test_id', provider_name=None, provider_details={'key': 'val'})
        assert result == {'pydantic_ai': {'id': 'test_id', 'provider_details': {'key': 'val'}}}

        # All values
        result = dump_provider_metadata(
            id='full_id',
            signature='sig',
            provider_name='provider',
            provider_details={'detail': 1},
        )
        assert result == {
            'pydantic_ai': {
                'id': 'full_id',
                'signature': 'sig',
                'provider_name': 'provider',
                'provider_details': {'detail': 1},
            }
        }

# tests/evals/test_utils.py:32-35
def test_unset():
    """Test Unset singleton."""
    assert isinstance(UNSET, Unset)
    assert UNSET is not Unset()  # note: we might want to change this and make it a true singleton..

# tests/graph/beta/test_graph_builder.py:431-444
async def test_validation_can_be_disabled():
    """Test that validation can be disabled with validate_graph_structure=False."""
    g = GraphBuilder(output_type=int)

    @g.step
    async def orphan_step(ctx: StepContext[None, None, None]) -> int:
        return 42  # pragma: no cover

    # Add the step to the graph but don't connect it to start
    # This would normally fail validation
    g.add(g.edge_from(orphan_step).to(g.end_node))

    # Should not raise an error when validation is disabled
    g.build(validate_graph_structure=False)

# pydantic_ai_slim/pydantic_ai/_griffe.py:173-178
def _disable_griffe_logging():
    # Hacky, but suggested here: https://github.com/mkdocstrings/griffe/issues/293#issuecomment-2167668117
    old_level = logging.root.getEffectiveLevel()
    logging.root.setLevel(logging.ERROR)
    yield
    logging.root.setLevel(old_level)

# tests/evals/test_multi_run.py:140-148
async def test_repeat_invalid_value():
    """repeat < 1 should raise ValueError."""

    async def task(inputs: str) -> str:
        return inputs  # pragma: no cover

    dataset = Dataset(cases=[Case(inputs='hello')])
    with pytest.raises(ValueError, match='repeat must be >= 1'):
        await dataset.evaluate(task, name='test', progress=False, repeat=0)

# tests/test_logfire.py:1253-1379
def test_include_tool_args_span_attributes(
    get_logfire_summary: Callable[[], LogfireSummary],
    include_content: bool,
    tool_error: bool,
) -> None:
    """Test that tool arguments are included/excluded in span attributes based on instrumentation settings."""

    instrumentation_settings = InstrumentationSettings(include_content=include_content)
    test_model = TestModel(seed=42)
    my_agent = Agent(model=test_model, instrument=instrumentation_settings)

    @my_agent.tool_plain
    async def add_numbers(x: int, y: int) -> int:
        """Add two numbers together."""
        if tool_error:
            raise ModelRetry('Tool error')
        return x + y

    try:
        result = my_agent.run_sync('Add 42 and 42')
        assert result.output == snapshot('{"add_numbers":84}')
    except UnexpectedModelBehavior:
        if not tool_error:
            raise  # pragma: no cover

    summary = get_logfire_summary()

    tool_attributes = next(
        attributes for attributes in summary.attributes.values() if attributes.get('gen_ai.tool.name') == 'add_numbers'
    )

    if include_content:
        if tool_error:
            assert tool_attributes == snapshot(
                {
                    'gen_ai.tool.name': 'add_numbers',
                    'gen_ai.tool.call.id': IsStr(),
                    'tool_arguments': '{"x":42,"y":42}',
                    'logfire.msg': 'running tool: add_numbers',
                    'logfire.json_schema': IsJson(
                        snapshot(
                            {
                                'type': 'object',
                                'properties': {
                                    'tool_arguments': {'type': 'object'},
                                    'tool_response': {'type': 'object'},
                                    'gen_ai.tool.name': {},
                                    'gen_ai.tool.call.id': {},
                                },
                            }
                        )
                    ),
                    'logfire.span_type': 'span',
                    'tool_response': """\
Tool error

Fix the errors and try again.\
""",
                    'logfire.level_num': 17,
                }
            )
        else:
            assert tool_attributes == snapshot(
                {
                    'gen_ai.tool.name': 'add_numbers',
                    'gen_ai.tool.call.id': IsStr(),
                    'tool_arguments': '{"x":42,"y":42}',
                    'tool_response': '84',
                    'logfire.msg': 'running tool: add_numbers',
                    'logfire.json_schema': IsJson(
                        snapshot(
                            {
                                'type': 'object',
                                'properties': {
                                    'tool_arguments': {'type': 'object'},
                                    'tool_response': {'type': 'object'},
                                    'gen_ai.tool.name': {},
                                    'gen_ai.tool.call.id': {},
                                },
                            }
                        )
                    ),
                    'logfire.span_type': 'span',
                }
            )
    else:
        if tool_error:
            assert tool_attributes == snapshot(
                {
                    'gen_ai.tool.name': 'add_numbers',
                    'gen_ai.tool.call.id': IsStr(),
                    'logfire.msg': 'running tool: add_numbers',
                    'logfire.json_schema': IsJson(
                        snapshot(
                            {
                                'type': 'object',
                                'properties': {
                                    'gen_ai.tool.name': {},
                                    'gen_ai.tool.call.id': {},
                                },
                            }
                        )
                    ),
                    'logfire.span_type': 'span',
                    'logfire.level_num': 17,
                }
            )
        else:
            assert tool_attributes == snapshot(
                {
                    'gen_ai.tool.name': 'add_numbers',
                    'gen_ai.tool.call.id': IsStr(),
                    'logfire.msg': 'running tool: add_numbers',
                    'logfire.json_schema': IsJson(
                        snapshot(
                            {
                                'type': 'object',
                                'properties': {
                                    'gen_ai.tool.name': {},
                                    'gen_ai.tool.call.id': {},
                                },
                            }
                        )
                    ),
                    'logfire.span_type': 'span',
                }
            )

# tests/evals/test_multi_run.py:49-69
async def test_repeat_1_produces_identical_behavior():
    """repeat=1 (default) should produce identical behavior to current â€” no source_case_name, case_groups() returns None."""
    call_count = 0

    async def task(inputs: str) -> str:
        nonlocal call_count
        call_count += 1
        return inputs.upper()

    dataset = Dataset(
        cases=[
            Case(name='case1', inputs='hello'),
            Case(name='case2', inputs='world'),
        ]
    )
    report = await dataset.evaluate(task, name='test', progress=False)

    assert call_count == 2
    assert len(report.cases) == 2
    assert all(c.source_case_name is None for c in report.cases)
    assert report.case_groups() is None

# pydantic_ai_slim/pydantic_ai/usage.py:68-93
    def opentelemetry_attributes(self) -> dict[str, int]:
        """Get the token usage values as OpenTelemetry attributes."""
        result: dict[str, int] = {}
        if self.input_tokens:
            result['gen_ai.usage.input_tokens'] = self.input_tokens
        if self.output_tokens:
            result['gen_ai.usage.output_tokens'] = self.output_tokens

        details = self.details.copy()
        if self.cache_write_tokens:
            details['cache_write_tokens'] = self.cache_write_tokens
        if self.cache_read_tokens:
            details['cache_read_tokens'] = self.cache_read_tokens
        if self.input_audio_tokens:
            details['input_audio_tokens'] = self.input_audio_tokens
        if self.cache_audio_read_tokens:
            details['cache_audio_read_tokens'] = self.cache_audio_read_tokens
        if self.output_audio_tokens:
            details['output_audio_tokens'] = self.output_audio_tokens
        if details:
            prefix = 'gen_ai.usage.details.'
            for key, value in details.items():
                # Skipping check for value since spec implies all detail values are relevant
                if value:
                    result[prefix + key] = value
        return result

# pydantic_ai_slim/pydantic_ai/direct.py:22-22
from . import agent, messages, models, settings

# pydantic_ai_slim/pydantic_ai/direct.py:22-22
from . import agent, messages, models, settings

# tests/test_mcp.py:1596-1601
async def test_client_sampling_disabled(run_context: RunContext[int]):
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'], allow_sampling=False)
    server.sampling_model = TestModel(custom_output_text='sampling model response')
    async with server:
        with pytest.raises(ModelRetry, match='Error executing tool use_sampling: Sampling not supported'):
            await server.direct_call_tool('use_sampling', {'foo': 'bar'})

# pydantic_graph/pydantic_graph/persistence/__init__.py:31-31
UNSET_SNAPSHOT_ID = '__unset__'

# pydantic_ai_slim/pydantic_ai/models/__init__.py:647-649
    def settings(self) -> ModelSettings | None:
        """Get the model settings."""
        return self._settings

# tests/test_dbos.py:1493-1493
    custom_setting: str

# tests/test_dbos.py:1493-1493
    custom_setting: str

# tests/test_dbos.py:1493-1493
    custom_setting: str

# pydantic_ai_slim/pydantic_ai/models/__init__.py:629-629
    _settings: ModelSettings | None = None

# tests/test_dbos.py:1507-1509
async def test_custom_model_settings(allow_model_requests: None, dbos: DBOS):
    result = await settings_dbos_agent.run('Give me those settings')
    assert result.output == snapshot("{'max_tokens': 123, 'custom_setting': 'custom_value'}")

# tests/test_mcp.py:344-349
async def test_log_level_unset(run_context: RunContext[int]):
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'])
    assert server.log_level is None
    async with server:
        result = await server.direct_call_tool('get_log_level', {})
        assert result == snapshot('unset')

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# tests/test_embeddings.py:1536-1552
async def test_settings():
    model_settings: EmbeddingSettings = {'dimensions': 128, 'from_model': True}  # pyright: ignore[reportAssignmentType]
    model = TestEmbeddingModel(settings=model_settings)
    assert model.settings == model_settings
    await Embedder(model).embed_query('Hello, world!')
    assert model.last_settings == snapshot({'dimensions': 128, 'from_model': True})

    embedder_settings: EmbeddingSettings = {'dimensions': 256, 'from_embedder': True}  # pyright: ignore[reportAssignmentType]
    embedder = Embedder(model, settings=embedder_settings)
    await embedder.embed_query('Hello, world!')
    assert model.last_settings == snapshot({'dimensions': 256, 'from_model': True, 'from_embedder': True})

    embed_settings: EmbeddingSettings = {'dimensions': 512, 'from_embed': True}  # pyright: ignore[reportAssignmentType]
    await embedder.embed_query('Hello, world!', settings=embed_settings)
    assert model.last_settings == snapshot(
        {'dimensions': 512, 'from_model': True, 'from_embedder': True, 'from_embed': True}
    )

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# tests/test_cli.py:276-280
def test_code_theme_unset(mocker: MockerFixture, env: TestEnv):
    env.set('OPENAI_API_KEY', 'test')
    mock_run_chat = mocker.patch('pydantic_ai._cli.run_chat')
    cli([])
    mock_run_chat.assert_awaited_once_with(True, IsInstance(Agent), IsInstance(Console), 'monokai', 'clai')

# tests/models/test_model_settings.py:30-43
def test_function_model_settings():
    """Test that FunctionModel correctly stores and returns settings."""
    settings = ModelSettings(max_tokens=200, temperature=0.7)

    def simple_response(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        return ModelResponse(parts=[TextPart('response')])  # pragma: no cover

    # Test with settings
    function_model = FunctionModel(simple_response, settings=settings)
    assert function_model.settings == settings

    # Test without settings
    function_model_no_settings = FunctionModel(simple_response)
    assert function_model_no_settings.settings is None

# tests/test_dbos.py:1500-1500
model_settings = CustomModelSettings(max_tokens=123, custom_setting='custom_value')

# tests/test_dbos.py:1503-1503
settings_agent = Agent(return_settings_model, name='settings_agent')

# tests/test_dbos.py:1500-1500
model_settings = CustomModelSettings(max_tokens=123, custom_setting='custom_value')

# tests/test_dbos.py:1503-1503
settings_agent = Agent(return_settings_model, name='settings_agent')

# tests/test_dbos.py:1500-1500
model_settings = CustomModelSettings(max_tokens=123, custom_setting='custom_value')

# tests/test_dbos.py:1503-1503
settings_agent = Agent(return_settings_model, name='settings_agent')

# tests/models/test_model_settings.py:18-27
def test_model_settings_property():
    """Test that the Model base class settings property works correctly."""
    # Test with settings
    settings = ModelSettings(max_tokens=100, temperature=0.5)
    test_model = TestModel(settings=settings)
    assert test_model.settings == settings

    # Test without settings
    test_model_no_settings = TestModel()
    assert test_model_no_settings.settings is None

# tests/test_dbos.py:1496-1497
def return_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
    return ModelResponse(parts=[TextPart(str(agent_info.model_settings))])

# tests/test_dbos.py:1496-1497
def return_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
    return ModelResponse(parts=[TextPart(str(agent_info.model_settings))])

# tests/test_dbos.py:1496-1497
def return_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
    return ModelResponse(parts=[TextPart(str(agent_info.model_settings))])

# tests/models/test_cerebras.py:52-69
async def test_cerebras_settings_transformation():
    """Test that CerebrasModelSettings are correctly transformed to OpenAIChatModelSettings."""
    # Test with disable_reasoning
    settings = CerebrasModelSettings(cerebras_disable_reasoning=True)
    transformed = _cerebras_settings_to_openai_settings(settings)
    extra_body = cast(dict[str, Any], transformed.get('extra_body', {}))
    assert extra_body.get('disable_reasoning') is True

    # Test without disable_reasoning (should not have extra_body)
    settings_empty = CerebrasModelSettings()
    transformed_empty = _cerebras_settings_to_openai_settings(settings_empty)
    assert transformed_empty.get('extra_body') is None

    # Test with disable_reasoning=False
    settings_false = CerebrasModelSettings(cerebras_disable_reasoning=False)
    transformed_false = _cerebras_settings_to_openai_settings(settings_false)
    extra_body_false = cast(dict[str, Any], transformed_false.get('extra_body', {}))
    assert extra_body_false.get('disable_reasoning') is False

# tests/models/test_model_settings.py:147-168
def test_empty_settings_objects():
    """Test that empty ModelSettings objects work correctly in the hierarchy."""
    captured_settings = None

    def capture_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('captured')])

    # All levels have empty settings
    model = FunctionModel(capture_settings, settings=ModelSettings())
    agent = Agent(model=model, model_settings=ModelSettings())

    # Run with one actual setting
    run_settings = ModelSettings(temperature=0.75)
    result = agent.run_sync('test', model_settings=run_settings)
    assert result.output == 'captured'

    # Should only have the run setting
    assert captured_settings is not None
    assert captured_settings.get('temperature') == 0.75
    assert len(captured_settings) == 1  # Only one setting should be present

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/_utils.py:140-143
class Some(Generic[T]):
    """Analogous to Rust's `Option::Some` type."""

    value: T

# tests/models/test_fallback.py:592-624
async def test_fallback_model_settings_merge():
    """Test that FallbackModel properly merges model settings from wrapped model and runtime settings."""

    def return_settings(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        return ModelResponse(parts=[TextPart(to_json(info.model_settings).decode())])

    base_model = FunctionModel(return_settings, settings=ModelSettings(temperature=0.1, max_tokens=1024))
    fallback_model = FallbackModel(base_model)

    # Test that base model settings are preserved when no additional settings are provided
    agent = Agent(fallback_model)
    result = await agent.run('Hello')
    assert result.output == IsJson({'max_tokens': 1024, 'temperature': 0.1})

    # Test that runtime model_settings are merged with base settings
    agent_with_settings = Agent(fallback_model, model_settings=ModelSettings(temperature=0.5, parallel_tool_calls=True))
    result = await agent_with_settings.run('Hello')
    expected = {'max_tokens': 1024, 'temperature': 0.5, 'parallel_tool_calls': True}
    assert result.output == IsJson(expected)

    # Test that run-time model_settings override both base and agent settings
    result = await agent_with_settings.run(
        'Hello', model_settings=ModelSettings(temperature=0.9, extra_headers={'runtime_setting': 'runtime_value'})
    )
    expected = {
        'max_tokens': 1024,
        'temperature': 0.9,
        'parallel_tool_calls': True,
        'extra_headers': {
            'runtime_setting': 'runtime_value',
        },
    }
    assert result.output == IsJson(expected)

# pydantic_ai_slim/pydantic_ai/embeddings/base.py:21-21
    _settings: EmbeddingSettings | None = None

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:135-135
    model_settings: ModelSettings | None

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:29-34
from .embeddings import (
    Embedder,
    EmbeddingModel,
    EmbeddingResult,
    EmbeddingSettings,
)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:82-82
    gemini_safety_settings: list[GeminiSafetySettings]

# pydantic_ai_slim/pydantic_ai/models/google.py:166-166
    google_safety_settings: list[SafetySettingDict]

# tests/test_mcp.py:2289-2301
async def test_tools_no_caching_when_disabled() -> None:
    """Test that list_tools() does not cache when cache_tools=False."""
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'], cache_tools=False)
    async with server:
        # First call - should not populate cache
        tools1 = await server.list_tools()
        assert len(tools1) > 0
        assert server._cached_tools is None  # pyright: ignore[reportPrivateUsage]

        # Second call - cache should still be None
        tools2 = await server.list_tools()
        assert tools2 == tools1
        assert server._cached_tools is None  # pyright: ignore[reportPrivateUsage]

# tests/test_settings.py:36-46
async def test_stop_settings(allow_model_requests: None, model: Model) -> None:
    agent = Agent(model=model, model_settings=ModelSettings(stop_sequences=['Paris']))
    result = await agent.run(
        'What is the capital of France? Give me an answer that contains the word "Paris", but is not the first word.'
    )

    # NOTE: Bedrock has a slightly different behavior. It will include the stop sequence in the response.
    if model.system == 'bedrock':
        assert result.output.endswith('Paris')
    else:
        assert 'Paris' not in result.output

# pydantic_evals/pydantic_evals/evaluators/common.py:197-197
    model_settings: ModelSettings | None = None

# pydantic_ai_slim/pydantic_ai/models/function.py:229-229
    model_settings: ModelSettings | None

# pydantic_ai_slim/pydantic_ai/models/gemini.py:77-98
class GeminiModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    gemini_safety_settings: list[GeminiSafetySettings]
    """Safety settings options for Gemini model request."""

    gemini_thinking_config: ThinkingConfig
    """Thinking is "on" by default in both the API and AI Studio.

    Being on by default doesn't mean the model will send back thoughts. For that, you would need to set `include_thoughts`
    to `True`. If you want to avoid the model spending any tokens on thinking, you can set `thinking_budget` to `0`.

    See more about it on <https://ai.google.dev/gemini-api/docs/thinking>.
    """

    gemini_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI provider.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """