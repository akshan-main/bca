## docs/.hooks/algolia.py

from typing_extensions import TypedDict

## examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py

from pydantic import BaseModel, Field

## examples/pydantic_ai_examples/pydantic_model.py

class MyModel(BaseModel):
    city: str
    country: str

## examples/pydantic_ai_examples/question_graph.py

from pydantic_graph import (
    BaseNode,
    End,
    Graph,
    GraphRunContext,
)

## pydantic_ai_slim/pydantic_ai/_utils.py

def generate_tool_call_id() -> str:
    """Generate a tool call id.

    Ensure that the tool call id is unique.
    """
    return f'pyd_ai_{uuid.uuid4().hex}'

def number_to_datetime(x: int | float) -> datetime:
    return _datetime_ta.validate_python(x)

## pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py

    def provider_name(self) -> str:
        return self.response.provider_name or ''  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py

    def provider_name(self) -> str:
        return self.response.provider_name or ''  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py

class _RequestParams:
    messages: list[ModelMessage]
    # `model_settings` can't be a `ModelSettings` because Temporal would end up dropping fields only defined on its subclasses.
    model_settings: dict[str, Any] | None
    model_request_parameters: ModelRequestParameters
    serialized_run_context: Any
    model_id: str | None = None

    def provider_name(self) -> str:
        return self.response.provider_name or ''  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/ext/langchain.py

from typing import Any, Protocol

## pydantic_ai_slim/pydantic_ai/messages.py

    provider_name: str | None = None

    provider_name: str | None = None

    provider_name: str | None = None

    provider_name: str | None = None

## pydantic_ai_slim/pydantic_ai/models/__init__.py

    def provider_name(self) -> str | None:
        """Get the provider name."""
        raise NotImplementedError()

def check_allow_model_requests() -> None:
    """Check if model requests are allowed.

    If you're defining your own models that have costs or latency associated with their use, you should call this in
    [`Model.request`][pydantic_ai.models.Model.request] and [`Model.request_stream`][pydantic_ai.models.Model.request_stream].

    Raises:
        RuntimeError: If model requests are not allowed.
    """
    if not ALLOW_MODEL_REQUESTS:
        raise RuntimeError('Model requests are not allowed, since ALLOW_MODEL_REQUESTS is False')

def get_user_agent() -> str:
    """Get the user agent string for the HTTP client."""
    from .. import __version__

    return f'pydantic-ai/{__version__}'

## pydantic_ai_slim/pydantic_ai/models/anthropic.py

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

def _map_web_search_tool_result_block(item: BetaWebSearchToolResultBlock, provider_name: str) -> BuiltinToolReturnPart:
    return BuiltinToolReturnPart(
        provider_name=provider_name,
        tool_name=WebSearchTool.kind,
        content=web_search_tool_result_content_ta.dump_python(item.content, mode='json'),
        tool_call_id=item.tool_use_id,
    )

def _map_web_fetch_tool_result_block(item: BetaWebFetchToolResultBlock, provider_name: str) -> BuiltinToolReturnPart:
    return BuiltinToolReturnPart(
        provider_name=provider_name,
        tool_name=WebFetchTool.kind,
        # Store just the content field (BetaWebFetchBlock) which has {content, type, url, retrieved_at}
        content=item.content.model_dump(mode='json'),
        tool_call_id=item.tool_use_id,
    )

## pydantic_ai_slim/pydantic_ai/models/bedrock.py

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

## pydantic_ai_slim/pydantic_ai/models/function.py

    def provider_name(self) -> None:
        """Get the provider name."""
        return None

## pydantic_ai_slim/pydantic_ai/models/gemini.py

class AuthProtocol(Protocol):
    """Abstract definition for Gemini authentication."""

    async def headers(self) -> dict[str, str]: ...

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

class _GeminiContent(TypedDict):
    role: Literal['user', 'model']
    parts: list[_GeminiPartUnion]

class _BasePart(TypedDict):
    thought: NotRequired[bool]
    """Indicates if the part is thought from the model."""

class _GeminiTextPart(_BasePart):
    text: str

class _GeminiThoughtPart(TypedDict):
    thought: bool
    thought_signature: Annotated[str, pydantic.Field(alias='thoughtSignature')]

def _function_call_part_from_call(tool: ToolCallPart) -> _GeminiFunctionCallPart:
    return _GeminiFunctionCallPart(function_call=_GeminiFunctionCall(name=tool.tool_name, args=tool.args_as_dict()))

class _GeminiFunctionCall(TypedDict):
    """See <https://ai.google.dev/api/caching#FunctionCall>."""

    name: str
    args: dict[str, Any]

def _response_part_from_response(name: str, response: dict[str, Any]) -> _GeminiFunctionResponsePart:
    return _GeminiFunctionResponsePart(function_response=_GeminiFunctionResponse(name=name, response=response))

class _GeminiTextContent(TypedDict):
    role: Literal['user', 'model']
    parts: list[_GeminiTextPart]

class _GeminiTools(TypedDict):
    function_declarations: Annotated[list[_GeminiFunction], pydantic.Field(alias='functionDeclarations')]

def _function_from_abstract_tool(tool: ToolDefinition) -> _GeminiFunction:
    json_schema = tool.parameters_json_schema
    f = _GeminiFunction(name=tool.name, description=tool.description or '', parameters_json_schema=json_schema)
    return f

class _GeminiToolConfig(TypedDict):
    function_calling_config: _GeminiFunctionCallingConfig

def _tool_config(function_names: list[str]) -> _GeminiToolConfig:
    return _GeminiToolConfig(
        function_calling_config=_GeminiFunctionCallingConfig(mode='ANY', allowed_function_names=function_names)
    )

class _GeminiFunctionCallingConfig(TypedDict):
    mode: Literal['ANY', 'AUTO']
    allowed_function_names: list[str]

## pydantic_ai_slim/pydantic_ai/models/google.py

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

def _map_executable_code(executable_code: ExecutableCode, provider_name: str, tool_call_id: str) -> BuiltinToolCallPart:
    return BuiltinToolCallPart(
        provider_name=provider_name,
        tool_name=CodeExecutionTool.kind,
        args=executable_code.model_dump(mode='json'),
        tool_call_id=tool_call_id,
    )

## pydantic_ai_slim/pydantic_ai/models/groq.py

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

## pydantic_ai_slim/pydantic_ai/models/huggingface.py

class HuggingFaceModelSettings(ModelSettings, total=False):
    """Settings used for a Hugging Face model request."""

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

## pydantic_ai_slim/pydantic_ai/models/mistral.py

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

## pydantic_ai_slim/pydantic_ai/models/openai.py

class _AzureError(BaseModel):
    code: str
    message: str
    innererror: _AzureInnerError | None = None

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

## pydantic_ai_slim/pydantic_ai/models/outlines.py

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

## pydantic_ai_slim/pydantic_ai/models/test.py

class _WrappedTextOutput:
    """A private wrapper class to tag an output that came from the custom_output_text field."""

    value: str | None

    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

def _get_string_usage(text: str) -> RequestUsage:
    response_tokens = _estimate_string_tokens(text)
    return RequestUsage(output_tokens=response_tokens)

## pydantic_ai_slim/pydantic_ai/models/xai.py

    def provider_name(self) -> str:
        """The model provider."""
        return self.system

def _map_json_object() -> chat_pb2.ResponseFormat:
    """Create a ResponseFormat for JSON object mode (prompted output)."""
    return chat_pb2.ResponseFormat(format_type=chat_pb2.FORMAT_TYPE_JSON_OBJECT)

## pydantic_graph/pydantic_graph/beta/graph.py

    def value(self) -> OutputT:
        return self._value

## tests/graph/test_mermaid.py

class Foo(BaseNode):
    async def run(self, ctx: GraphRunContext) -> Bar:
        return Bar()

class Bar(BaseNode[None, None, None]):
    async def run(self, ctx: GraphRunContext) -> End[None]:
        return End(None)

## tests/models/test_instrumented.py

    def provider_name(self) -> str:
        return 'openai'

async def test_instrumented_model_count_tokens(capfire: CaptureLogfire):
    messages: list[ModelMessage] = [ModelRequest(parts=[UserPromptPart('Hello, world!')], timestamp=IsDatetime())]
    model = InstrumentedModel(MyModel())
    usage = await model.count_tokens(
        messages, model_settings=ModelSettings(), model_request_parameters=ModelRequestParameters()
    )
    assert usage == RequestUsage(input_tokens=10)
