# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/otel/span_tree.py:244-366
    def _matches_query(self, query: SpanQuery) -> bool:  # noqa: C901
        """Check if the span matches the query conditions."""
        # Logical combinations
        if or_ := query.get('or_'):
            if len(query) > 1:
                raise ValueError("Cannot combine 'or_' conditions with other conditions at the same level")
            return any(self._matches_query(q) for q in or_)
        if not_ := query.get('not_'):
            if self._matches_query(not_):
                return False
        if and_ := query.get('and_'):
            results = [self._matches_query(q) for q in and_]
            if not all(results):
                return False
        # At this point, all existing ANDs and no existing ORs have passed, so it comes down to this condition

        # Name conditions
        if (name_equals := query.get('name_equals')) and self.name != name_equals:
            return False
        if (name_contains := query.get('name_contains')) and name_contains not in self.name:
            return False
        if (name_matches_regex := query.get('name_matches_regex')) and not re.match(name_matches_regex, self.name):
            return False

        # Attribute conditions
        if (has_attributes := query.get('has_attributes')) and not all(
            self.attributes.get(key) == value for key, value in has_attributes.items()
        ):
            return False
        if (has_attributes_keys := query.get('has_attribute_keys')) and not all(
            key in self.attributes for key in has_attributes_keys
        ):
            return False

        # Timing conditions
        if (min_duration := query.get('min_duration')) is not None:
            if not isinstance(min_duration, timedelta):
                min_duration = timedelta(seconds=min_duration)
            if self.duration < min_duration:
                return False
        if (max_duration := query.get('max_duration')) is not None:
            if not isinstance(max_duration, timedelta):
                max_duration = timedelta(seconds=max_duration)
            if self.duration > max_duration:
                return False

        # Children conditions
        if (min_child_count := query.get('min_child_count')) and len(self.children) < min_child_count:
            return False
        if (max_child_count := query.get('max_child_count')) and len(self.children) > max_child_count:
            return False
        if (some_child_has := query.get('some_child_has')) and not any(
            child._matches_query(some_child_has) for child in self.children
        ):
            return False
        if (all_children_have := query.get('all_children_have')) and not all(
            child._matches_query(all_children_have) for child in self.children
        ):
            return False
        if (no_child_has := query.get('no_child_has')) and any(
            child._matches_query(no_child_has) for child in self.children
        ):
            return False

        # Descendant conditions
        # The following local functions with cache decorators are used to avoid repeatedly evaluating these properties
        @cache
        def descendants():
            return self.descendants

        @cache
        def pruned_descendants():
            stop_recursing_when = query.get('stop_recursing_when')
            return (
                self._filter_descendants(lambda _: True, stop_recursing_when) if stop_recursing_when else descendants()
            )

        if (min_descendant_count := query.get('min_descendant_count')) and len(descendants()) < min_descendant_count:
            return False
        if (max_descendant_count := query.get('max_descendant_count')) and len(descendants()) > max_descendant_count:
            return False
        if (some_descendant_has := query.get('some_descendant_has')) and not any(
            descendant._matches_query(some_descendant_has) for descendant in pruned_descendants()
        ):
            return False
        if (all_descendants_have := query.get('all_descendants_have')) and not all(
            descendant._matches_query(all_descendants_have) for descendant in pruned_descendants()
        ):
            return False
        if (no_descendant_has := query.get('no_descendant_has')) and any(
            descendant._matches_query(no_descendant_has) for descendant in pruned_descendants()
        ):
            return False

        # Ancestor conditions
        # The following local functions with cache decorators are used to avoid repeatedly evaluating these properties
        @cache
        def ancestors():
            return self.ancestors

        @cache
        def pruned_ancestors():
            stop_recursing_when = query.get('stop_recursing_when')
            return self._filter_ancestors(lambda _: True, stop_recursing_when) if stop_recursing_when else ancestors()

        if (min_depth := query.get('min_depth')) and len(ancestors()) < min_depth:
            return False
        if (max_depth := query.get('max_depth')) and len(ancestors()) > max_depth:
            return False
        if (some_ancestor_has := query.get('some_ancestor_has')) and not any(
            ancestor._matches_query(some_ancestor_has) for ancestor in pruned_ancestors()
        ):
            return False
        if (all_ancestors_have := query.get('all_ancestors_have')) and not all(
            ancestor._matches_query(all_ancestors_have) for ancestor in pruned_ancestors()
        ):
            return False
        if (no_ancestor_has := query.get('no_ancestor_has')) and any(
            ancestor._matches_query(no_ancestor_has) for ancestor in pruned_ancestors()
        ):
            return False

        return True

# pydantic_evals/pydantic_evals/otel/span_tree.py:237-242
    def matches(self, query: SpanQuery | SpanPredicate) -> bool:
        """Check if the span node matches the query conditions or predicate."""
        if callable(query):
            return query(self)

        return self._matches_query(query)

# pydantic_ai_slim/pydantic_ai/__init__.py:115-128
from .toolsets import (
    AbstractToolset,
    ApprovalRequiredToolset,
    CombinedToolset,
    ExternalToolset,
    FilteredToolset,
    FunctionToolset,
    PrefixedToolset,
    PreparedToolset,
    RenamedToolset,
    ToolsetFunc,
    ToolsetTool,
    WrapperToolset,
)

# tests/graph/beta/test_decisions.py:178-207
async def test_decision_first_match_wins():
    """Test that the first matching branch is taken."""
    g = GraphBuilder(state_type=DecisionState, output_type=str)

    @g.step
    async def return_value(ctx: StepContext[DecisionState, None, None]) -> int:
        return 10

    @g.step
    async def branch_a(ctx: StepContext[DecisionState, None, int]) -> str:
        return 'Branch A'

    @g.step
    async def branch_b(ctx: StepContext[DecisionState, None, int]) -> str:
        return 'Branch B'  # pragma: no cover

    g.add(
        g.edge_from(g.start_node).to(return_value),
        g.edge_from(return_value).to(
            g.decision()
            # Both branches match, but A is first
            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 5).to(branch_a))
            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 0).to(branch_b))
        ),
        g.edge_from(branch_a, branch_b).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=DecisionState())
    assert result == 'Branch A'

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:141-150
    def filtered(
        self, filter_func: Callable[[RunContext[AgentDepsT], ToolDefinition], bool]
    ) -> FilteredToolset[AgentDepsT]:
        """Returns a new toolset that filters this toolset's tools using a filter function that takes the agent context and the tool definition.

        See [toolset docs](../toolsets.md#filtering-tools) for more information.
        """
        from .filtered import FilteredToolset

        return FilteredToolset(self, filter_func)

# tests/graph/beta/test_decisions.py:155-175
async def test_decision_all_types_match():
    """Test decision with a branch that matches all types."""
    g = GraphBuilder(state_type=DecisionState, output_type=str)

    @g.step
    async def return_value(ctx: StepContext[DecisionState, None, None]) -> int:
        return 100

    @g.step
    async def catch_all(ctx: StepContext[DecisionState, None, object]) -> str:
        return f'Caught: {ctx.inputs}'

    g.add(
        g.edge_from(g.start_node).to(return_value),
        g.edge_from(return_value).to(g.decision().branch(g.match(TypeExpression[object]).to(catch_all))),
        g.edge_from(catch_all).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=DecisionState())
    assert result == 'Caught: 100'

# tests/evals/test_otel.py:207-223
async def test_span_node_matches(span_tree: SpanTree):
    """Test the matches method of SpanNode."""
    root_node = span_tree.roots[0]
    child1_node = root_node.first_child(lambda node: node.name == 'child1')
    assert child1_node is not None

    # Test matches by name
    assert child1_node.matches(SpanQuery(name_equals='child1'))
    assert not child1_node.matches(SpanQuery(name_equals='child2'))

    # Test matches by attributes
    assert child1_node.matches(SpanQuery(has_attributes={'level': '1', 'type': 'important'}))
    assert not child1_node.matches(SpanQuery(has_attributes={'level': '2', 'type': 'important'}))

    # Test matches by both name and attributes
    assert child1_node.matches(SpanQuery(name_equals='child1', has_attributes={'type': 'important'}))
    assert not child1_node.matches(SpanQuery(name_equals='child1', has_attributes={'type': 'normal'}))

# pydantic_ai_slim/pydantic_ai/models/openai.py:236-263
def _check_azure_content_filter(e: APIStatusError, system: str, model_name: str) -> ModelResponse | None:
    """Check if the error is an Azure content filter error."""
    # Assign to Any to avoid 'dict[Unknown, Unknown]' inference in strict mode
    body_any: Any = e.body

    if system == 'azure' and e.status_code == 400 and isinstance(body_any, dict):
        try:
            error_data = _AzureErrorResponse.model_validate(body_any)

            if error_data.error.code == 'content_filter':
                provider_details: dict[str, Any] = {'finish_reason': 'content_filter'}

                if error_data.error.innererror:
                    provider_details['content_filter_result'] = (
                        error_data.error.innererror.content_filter_result.model_dump(exclude_none=True)
                    )

                return ModelResponse(
                    parts=[],  # Empty parts to trigger content filter error in agent graph
                    model_name=model_name,
                    timestamp=_utils.now_utc(),
                    provider_name=system,
                    finish_reason='content_filter',
                    provider_details=provider_details,
                )
        except ValidationError:
            pass
    return None

# pydantic_graph/pydantic_graph/beta/decision.py:104-104
    matches: Callable[[Any], bool] | None

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:115-128
from .toolsets import (
    AbstractToolset,
    ApprovalRequiredToolset,
    CombinedToolset,
    ExternalToolset,
    FilteredToolset,
    FunctionToolset,
    PrefixedToolset,
    PreparedToolset,
    RenamedToolset,
    ToolsetFunc,
    ToolsetTool,
    WrapperToolset,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:115-128
from .toolsets import (
    AbstractToolset,
    ApprovalRequiredToolset,
    CombinedToolset,
    ExternalToolset,
    FilteredToolset,
    FunctionToolset,
    PrefixedToolset,
    PreparedToolset,
    RenamedToolset,
    ToolsetFunc,
    ToolsetTool,
    WrapperToolset,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:115-128
from .toolsets import (
    AbstractToolset,
    ApprovalRequiredToolset,
    CombinedToolset,
    ExternalToolset,
    FilteredToolset,
    FunctionToolset,
    PrefixedToolset,
    PreparedToolset,
    RenamedToolset,
    ToolsetFunc,
    ToolsetTool,
    WrapperToolset,
)

# tests/graph/beta/test_graph_edge_cases.py:103-126
async def test_decision_no_matching_branch():
    """Test that decision raises RuntimeError when no branch matches."""
    g = GraphBuilder(state_type=MyState, output_type=str)

    @g.step
    async def return_unexpected(ctx: StepContext[MyState, None, None]) -> int:
        return 999

    @g.step
    async def handle_str(ctx: StepContext[MyState, None, str]) -> str:
        return f'Got: {ctx.inputs}'  # pragma: no cover

    # the purpose of this test is to test runtime behavior when you have this type failure, which is why
    # we have the `# type: ignore` below
    g.add(
        g.edge_from(g.start_node).to(return_unexpected),
        g.edge_from(return_unexpected).to(g.decision().branch(g.match(str).to(handle_str))),  # type: ignore
        g.edge_from(handle_str).to(g.end_node),
    )

    graph = g.build()

    with pytest.raises(RuntimeError, match='No branch matched'):
        await graph.run(state=MyState())

# pydantic_graph/pydantic_graph/beta/decision.py:149-149
    _matches: Callable[[Any], bool] | None

# pydantic_ai_slim/pydantic_ai/usage.py:99-101
    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

# pydantic_ai_slim/pydantic_ai/toolsets/filtered.py:19-19
    filter_func: Callable[[RunContext[AgentDepsT], ToolDefinition], bool]

# pydantic_evals/pydantic_evals/otel/span_tree.py:40-40
    name_matches_regex: str  # regex pattern

# pydantic_graph/pydantic_graph/beta/graph_builder.py:469-489
    def match(
        self,
        source: TypeOrTypeExpression[SourceT],
        *,
        matches: Callable[[Any], bool] | None = None,
    ) -> DecisionBranchBuilder[StateT, DepsT, SourceT, SourceT, Never]:
        """Create a decision branch matcher.

        Args:
            source: The type or type expression to match against
            matches: Optional custom matching function

        Returns:
            A DecisionBranchBuilder for constructing the branch
        """
        # Note, the following node_id really is just a placeholder and shouldn't end up in the final graph
        # This is why we don't expose a way for end users to override the value used here.
        node_id = NodeID(generate_placeholder_node_id('match_decision'))
        decision = Decision[StateT, DepsT, Never](id=node_id, branches=[], note=None)
        new_path_builder = PathBuilder[StateT, DepsT, SourceT](working_items=[])
        return DecisionBranchBuilder(decision=decision, source=source, matches=matches, path_builder=new_path_builder)

# tests/evals/test_otel.py:754-786
async def test_matches_function_directly():
    """Test the matches function directly with various SpanQuery combinations."""

    # Create a test span tree
    with context_subtree() as tree:
        with logfire.span('parent', level='1', category='main'):
            with logfire.span('child1', level='2', category='important'):
                pass
            with logfire.span('child2', level='2', category='normal'):
                pass
    assert isinstance(tree, SpanTree)

    parent_node = tree.roots[0]
    child1_node = parent_node.children[0]
    child2_node = parent_node.children[1]

    # Basic matches tests
    assert parent_node.matches({'name_equals': 'parent'})
    assert not child1_node.matches({'name_equals': 'parent'})

    # Test attribute matching
    assert parent_node.matches({'has_attributes': {'level': '1'}})
    assert not child1_node.matches({'has_attributes': {'level': '1'}})

    # Test logical combinations
    complex_query: SpanQuery = {'and_': [{'name_equals': 'child1'}, {'has_attributes': {'category': 'important'}}]}
    assert child1_node.matches(complex_query)
    assert not child2_node.matches(complex_query)

    # Test with descendants
    descendant_query: SpanQuery = {'some_child_has': {'name_equals': 'child1'}}
    assert parent_node.matches(descendant_query)
    assert not child1_node.matches(descendant_query)

# tests/graph/beta/test_decisions.py:58-86
async def test_decision_with_type_matching():
    """Test decision node matching by type."""
    g = GraphBuilder(state_type=DecisionState, output_type=str)

    @g.step
    async def return_int(ctx: StepContext[DecisionState, None, None]) -> int:
        return 42

    @g.step
    async def handle_int(ctx: StepContext[DecisionState, None, int]) -> str:
        return f'Got int: {ctx.inputs}'

    @g.step
    async def handle_str(ctx: StepContext[DecisionState, None, str]) -> str:
        return f'Got str: {ctx.inputs}'  # pragma: no cover

    g.add(
        g.edge_from(g.start_node).to(return_int),
        g.edge_from(return_int).to(
            g.decision()
            .branch(g.match(TypeExpression[int]).to(handle_int))
            .branch(g.match(TypeExpression[str]).to(handle_str))
        ),
        g.edge_from(handle_int, handle_str).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=DecisionState())
    assert result == 'Got int: 42'

# tests/graph/beta/test_graph_edge_cases.py:320-355
async def test_literal_branch_matching():
    """Test decision branch matching with Literal types."""
    g = GraphBuilder(state_type=MyState, output_type=str)

    @g.step
    async def choose_option(ctx: StepContext[MyState, None, None]) -> Literal['a', 'b', 'c']:
        return 'b'

    @g.step
    async def handle_a(ctx: StepContext[MyState, None, object]) -> str:
        return 'Chose A'  # pragma: no cover

    @g.step
    async def handle_b(ctx: StepContext[MyState, None, object]) -> str:
        return 'Chose B'

    @g.step
    async def handle_c(ctx: StepContext[MyState, None, object]) -> str:
        return 'Chose C'  # pragma: no cover

    from pydantic_graph.beta import TypeExpression

    g.add(
        g.edge_from(g.start_node).to(choose_option),
        g.edge_from(choose_option).to(
            g.decision()
            .branch(g.match(TypeExpression[Literal['a']]).to(handle_a))
            .branch(g.match(TypeExpression[Literal['b']]).to(handle_b))
            .branch(g.match(TypeExpression[Literal['c']]).to(handle_c))
        ),
        g.edge_from(handle_a, handle_b, handle_c).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=MyState())
    assert result == 'Chose B'

# tests/graph/beta/test_decisions.py:468-524
async def test_match_node():
    """Test using match_node() with BaseNode types in decisions.

    match_node() is designed for exhaustive matching of BaseNode return types
    in decision branches. Unlike match().to(), it doesn't require a .to() call
    since the destination is the BaseNode class itself.

    This is only necessary if you have a step that might return a v1-style node _or_ an
    arbitrary output that you want to route to another node using the builder API.
    """
    g = GraphBuilder(state_type=DecisionState, input_type=int, output_type=str)

    @dataclass
    class NodeStep(BaseNode[DecisionState, None, str]):
        value: int

        async def run(self, ctx: GraphRunContext[DecisionState, None]) -> End[str]:
            ctx.state.path_taken = 'path_a'
            return End(f'Path A: {self.value}')

    @g.step
    async def regular_step(ctx: StepContext[DecisionState, None, int]):
        ctx.state.path_taken = 'path_b'
        return f'Path B: {ctx.inputs}'

    @g.step
    async def route_to_node(ctx: StepContext[DecisionState, None, int]) -> NodeStep | int:
        # Route based on input value
        if ctx.inputs < 10:
            return NodeStep(ctx.inputs)
        else:
            return ctx.inputs

    # Use match_node to create decision branches for BaseNode types
    # Note: match_node doesn't require .to() - the node type IS the destination
    g.add(
        g.node(NodeStep),
        g.edge_from(g.start_node).to(route_to_node),
        g.edge_from(route_to_node).to(
            g.decision().branch(g.match_node(NodeStep)).branch(g.match(int).to(regular_step))
        ),
        g.edge_from(regular_step).to(g.end_node),
    )

    graph = g.build()

    # Test path A (value < 10)
    state_a = DecisionState()
    result_a = await graph.run(state=state_a, inputs=5)
    assert result_a == 'Path A: 5'
    assert state_a.path_taken == 'path_a'

    # Test path B (value >= 10)
    state_b = DecisionState()
    result_b = await graph.run(state=state_b, inputs=15)
    assert result_b == 'Path B: 15'
    assert state_b.path_taken == 'path_b'

# tests/test_agent.py:7701-7719
async def test_central_content_filter_handling():
    """
    Test that the agent graph correctly raises ContentFilterError
    when a model returns finish_reason='content_filter' AND empty content.
    """

    async def filtered_response(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        return ModelResponse(
            parts=[],
            model_name='test-model',
            finish_reason='content_filter',
            provider_details={'finish_reason': 'content_filter'},
        )

    model = FunctionModel(function=filtered_response, model_name='test-model')
    agent = Agent(model)

    with pytest.raises(ContentFilterError, match="Content filter triggered. Finish reason: 'content_filter'"):
        await agent.run('Trigger filter')

# pydantic_ai_slim/pydantic_ai/_utils.py:150-153
class Unset:
    """A singleton to represent an unset value."""

    pass

# tests/graph/beta/test_decisions.py:89-117
async def test_decision_with_custom_matcher():
    """Test decision node with custom matching function."""
    g = GraphBuilder(state_type=DecisionState, output_type=str)

    @g.step
    async def return_number(ctx: StepContext[DecisionState, None, None]) -> int:
        return 7

    @g.step
    async def even_path(ctx: StepContext[DecisionState, None, int]) -> str:
        return f'{ctx.inputs} is even'  # pragma: no cover

    @g.step
    async def odd_path(ctx: StepContext[DecisionState, None, int]) -> str:
        return f'{ctx.inputs} is odd'

    g.add(
        g.edge_from(g.start_node).to(return_number),
        g.edge_from(return_number).to(
            g.decision()
            .branch(g.match(TypeExpression[int], matches=lambda x: x % 2 == 0).to(even_path))
            .branch(g.match(TypeExpression[int], matches=lambda x: x % 2 == 1).to(odd_path))
        ),
        g.edge_from(even_path, odd_path).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=DecisionState())
    assert result == '7 is odd'

# tests/test_vercel_ai.py:4409-4434
    async def test_dump_provider_metadata_filters_none_values(self):
        """Test that dump_provider_metadata only includes non-None values."""

        # All None - should return None
        result = dump_provider_metadata(id=None, provider_name=None, provider_details=None)
        assert result is None

        # Some values
        result = dump_provider_metadata(id='test_id', provider_name=None, provider_details={'key': 'val'})
        assert result == {'pydantic_ai': {'id': 'test_id', 'provider_details': {'key': 'val'}}}

        # All values
        result = dump_provider_metadata(
            id='full_id',
            signature='sig',
            provider_name='provider',
            provider_details={'detail': 1},
        )
        assert result == {
            'pydantic_ai': {
                'id': 'full_id',
                'signature': 'sig',
                'provider_name': 'provider',
                'provider_details': {'detail': 1},
            }
        }

# clai/update_readme.py:12-29
def test_cli_help(capfd: pytest.CaptureFixture[str]):
    """Check README.md help output matches `clai --help`."""
    os.environ['COLUMNS'] = '150'
    with pytest.raises(SystemExit):
        cli(['--help'], prog_name='clai')

    help_output = capfd.readouterr().out.strip()
    help_output = re.sub(r'(Pydantic AI CLI v).+', r'\1...', help_output)

    this_dir = Path(__file__).parent
    readme = this_dir / 'README.md'
    content = readme.read_text(encoding='utf-8')

    new_content, count = re.subn('^(## Help\n+```).+?```', rf'\1\n{help_output}\n```', content, flags=re.M | re.S)
    assert count, 'help section not found'
    if new_content != content:
        readme.write_text(new_content, encoding='utf-8')
        pytest.fail('`clai --help` output changed.')

# pydantic_graph/pydantic_graph/beta/graph_builder.py:491-511
    def match_node(
        self,
        source: type[SourceNodeT],
        *,
        matches: Callable[[Any], bool] | None = None,
    ) -> DecisionBranch[SourceNodeT]:
        """Create a decision branch for BaseNode subclasses.

        This is similar to match() but specifically designed for matching
        against BaseNode types from the v1 system.

        Args:
            source: The BaseNode subclass to match against
            matches: Optional custom matching function

        Returns:
            A DecisionBranch for the BaseNode type
        """
        node = NodeStep(source)
        path = Path(items=[DestinationMarker(node.id)])
        return DecisionBranch(source=source, matches=matches, path=path, destinations=[node])

# tests/models/test_openai.py:3949-3982
async def test_openai_auto_mode_mismatched_field_uses_tags(allow_model_requests: None):
    """Test that auto mode falls back to tags when configured field doesn't match where reasoning comes from."""
    # Configure thinking_field as 'reasoning_content', but reasoning comes in 'reasoning'
    c = completion_message(
        ChatCompletionMessage.model_construct(content='response', reasoning='thought', role='assistant')
    )
    m = OpenAIChatModel(
        'foobar',
        provider=OpenAIProvider(openai_client=MockOpenAI.create_mock(c)),
        profile=OpenAIModelProfile(
            openai_chat_thinking_field='reasoning_content',
            openai_chat_send_back_thinking_parts='auto',
        ),
    )
    settings = ModelSettings()
    params = ModelRequestParameters()
    resp = await m.request(messages=[], model_settings=settings, model_request_parameters=params)

    thinking_parts = [p for p in resp.parts if isinstance(p, ThinkingPart)]
    assert len(thinking_parts) == 1
    assert thinking_parts[0].id == 'reasoning'

    # But when sending back, since id='reasoning' doesn't match configured 'reasoning_content', it should fall back to tags
    mapped = m._map_model_response(resp)  # type: ignore[reportPrivateUsage]
    assert mapped == snapshot(
        {
            'role': 'assistant',
            'content': """<think>
thought
</think>

response""",
        }
    )

# tests/models/test_model_settings.py:121-144
def test_none_settings_in_hierarchy():
    """Test that None settings at any level don't break the merge hierarchy."""
    captured_settings = None

    def capture_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('captured')])

    # Model with no settings
    model = FunctionModel(capture_settings, settings=None)

    # Agent with settings
    agent_settings = ModelSettings(max_tokens=150, temperature=0.5)
    agent = Agent(model=model, model_settings=agent_settings)

    # Run with no additional settings
    result = agent.run_sync('test', model_settings=None)
    assert result.output == 'captured'

    # Should have agent settings
    assert captured_settings is not None
    assert captured_settings['max_tokens'] == 150
    assert captured_settings['temperature'] == 0.5

# pydantic_evals/pydantic_evals/evaluators/common.py:260-260
    query: SpanQuery

# pydantic_evals/pydantic_evals/otel/span_tree.py:479-481
    def find(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:
        """Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order."""
        return list(self._filter(predicate))

# tests/graph/beta/test_edge_cases.py:352-384
async def test_step_that_modifies_deps():
    """Test that deps modifications don't persist (deps should be immutable)."""

    @dataclass
    class MutableDeps:
        value: int

    g = GraphBuilder(state_type=EdgeCaseState, deps_type=MutableDeps, output_type=int)

    @g.step
    async def try_modify_deps(ctx: StepContext[EdgeCaseState, MutableDeps, None]) -> int:
        original = ctx.deps.value
        # Attempt to modify (this DOES mutate the object, but that's user error)
        ctx.deps.value = 999
        return original

    @g.step
    async def check_deps(ctx: StepContext[EdgeCaseState, MutableDeps, int]) -> int:
        # Deps will show the mutation since it's the same object
        return ctx.deps.value

    g.add(
        g.edge_from(g.start_node).to(try_modify_deps),
        g.edge_from(try_modify_deps).to(check_deps),
        g.edge_from(check_deps).to(g.end_node),
    )

    graph = g.build()
    deps = MutableDeps(value=42)
    result = await graph.run(state=EdgeCaseState(), deps=deps)
    # The deps object was mutated (user responsibility to avoid this)
    assert result == 999
    assert deps.value == 999

# pydantic_evals/pydantic_evals/otel/span_tree.py:158-160
    def first_child(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:
        """Return the first immediate child that satisfies the given predicate, or None if none match."""
        return next(self._filter_children(predicate), None)

# pydantic_graph/pydantic_graph/persistence/file.py:104-106
    def should_set_types(self) -> bool:
        """Whether types need to be set."""
        return self._snapshots_type_adapter is None

# pydantic_evals/pydantic_evals/evaluators/common.py:263-267
    def evaluate(
        self,
        ctx: EvaluatorContext[object, object, object],
    ) -> bool:
        return ctx.span_tree.any(self.query)

# pydantic_evals/pydantic_evals/otel/span_tree.py:211-215
    def first_ancestor(
        self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None
    ) -> SpanNode | None:
        """Return the closest ancestor that satisfies the given predicate, or `None` if none match."""
        return next(self._filter_ancestors(predicate, stop_recursing_when), None)

# pydantic_ai_slim/pydantic_ai/toolsets/filtered.py:21-24
    async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:
        return {
            name: tool for name, tool in (await super().get_tools(ctx)).items() if self.filter_func(ctx, tool.tool_def)
        }

# tests/test_parts_manager.py:591-599
def test_handle_thinking_delta_wrong_part_type():
    manager = ModelResponsePartsManager()

    # Add a text part first
    list(manager.handle_text_delta(vendor_part_id='text', content='hello'))

    # Try to apply thinking delta to the text part - should raise error
    with pytest.raises(UnexpectedModelBehavior, match=r'Cannot apply a thinking delta to existing_part='):
        list(manager.handle_thinking_delta(vendor_part_id='text', content='thinking', signature=None))

# tests/models/test_xai.py:4518-4542
async def test_xai_cache_point_filtered(allow_model_requests: None):
    """Test that CachePoint in user prompt is filtered out."""
    response = create_response(content='Hello', usage=create_usage(prompt_tokens=5, completion_tokens=2))
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    # Run with a user prompt that includes a CachePoint (which should be filtered)
    result = await agent.run(['Hello', CachePoint(), ' world'])
    assert result.output == 'Hello'

    # Verify message was sent (CachePoint filtered out - only text items remain)
    assert get_mock_chat_create_kwargs(mock_client) == snapshot(
        [
            {
                'model': XAI_NON_REASONING_MODEL,
                'messages': [{'content': [{'text': 'Hello'}, {'text': ' world'}], 'role': 'ROLE_USER'}],
                'tools': None,
                'tool_choice': None,
                'response_format': None,
                'use_encrypted_content': False,
                'include': [],
            }
        ]
    )

# tests/models/test_google.py:4763-4776
async def test_cache_point_filtering():
    """Test that CachePoint is filtered out in Google internal method."""
    from pydantic_ai import CachePoint

    # Create a minimal GoogleModel instance to test _map_user_prompt
    model = GoogleModel('gemini-1.5-flash', provider=GoogleProvider(api_key='test-key'))

    # Test that CachePoint in a list is handled (triggers line 606)
    content = await model._map_user_prompt(UserPromptPart(content=['text before', CachePoint(), 'text after']))  # pyright: ignore[reportPrivateUsage]

    # CachePoint should be filtered out, only text content should remain
    assert len(content) == 2
    assert content[0] == {'text': 'text before'}
    assert content[1] == {'text': 'text after'}

# pydantic_evals/pydantic_evals/otel/span_tree.py:487-489
    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

# pydantic_ai_slim/pydantic_ai/toolsets/combined.py:18-22
class _CombinedToolsetTool(ToolsetTool[AgentDepsT]):
    """A tool definition for a combined toolset tools that keeps track of the source toolset and tool."""

    source_toolset: AbstractToolset[AgentDepsT]
    source_tool: ToolsetTool[AgentDepsT]

# tests/models/xai_proto_cassettes.py:576-576
    dirty_check: Any | None = None

# pydantic_evals/pydantic_evals/otel/span_tree.py:483-485
    def first(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:
        """Find the first node that matches a predicate, scanning from each root in DFS order. Returns `None` if not found."""
        return next(self._filter(predicate), None)

# pydantic_graph/pydantic_graph/persistence/in_mem.py:151-152
    def should_set_types(self) -> bool:
        return self._snapshots_type_adapter is None