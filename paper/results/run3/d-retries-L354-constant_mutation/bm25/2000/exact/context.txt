# pydantic_ai_slim/pydantic_ai/_tool_manager.py:42-42
    default_max_retries: int = 1

# pydantic_ai_slim/pydantic_ai/tools.py:272-272
    max_retries: int | None

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_dynamic_toolset.py:30-30
    max_retries: int

# pydantic_ai_slim/pydantic_ai/mcp.py:331-331
    max_retries: int

# pydantic_ai_slim/pydantic_ai/_run_context.py:63-63
    max_retries: int = 0

# pydantic_ai_slim/pydantic_ai/output.py:115-115
    max_retries: int | None

# pydantic_ai_slim/pydantic_ai/toolsets/abstract.py:53-53
    max_retries: int

# pydantic_ai_slim/pydantic_ai/_output.py:868-868
    max_retries: int

# pydantic_ai_slim/pydantic_ai/mcp.py:863-863
    max_retries: int

# pydantic_ai_slim/pydantic_ai/mcp.py:1030-1030
    max_retries: int

# pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py:74-74
    max_retries: int

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:44-44
    max_retries: int

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:165-165
    _max_tool_retries: int = dataclasses.field(repr=False)

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:164-164
    _max_result_retries: int = dataclasses.field(repr=False)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:139-139
    max_result_retries: int

# pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py:41-41
    default_max_tokens: int = 16_384

# tests/models/test_model_test.py:12-12
from annotated_types import Ge, Gt, Le, Lt, MaxLen, MinLen

# tests/test_tenacity.py:554-571
    def test_default_max_wait(self):
        """Test that default max_wait of 300 seconds is used."""
        wait_func = wait_retry_after()  # Use all defaults

        # Create HTTP status error with large Retry-After value
        request = httpx.Request('GET', 'https://example.com')
        response = Mock(spec=httpx.Response)
        response.headers = {'retry-after': '600'}  # 10 minutes
        http_error = httpx.HTTPStatusError('Rate limited', request=request, response=response)

        retry_state = Mock(spec=RetryCallState)
        retry_state.outcome = Mock()
        retry_state.outcome.failed = True
        retry_state.outcome.exception.return_value = http_error

        result = wait_func(retry_state)

        assert result == 300.0  # Capped at default max_wait

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_ai_slim/pydantic_ai/_ssrf.py:50-50
_MAX_REDIRECTS = 10

# pydantic_evals/pydantic_evals/otel/span_tree.py:75-75
    max_depth: int

# tests/models/test_model_test.py:449-456
def test_max_items():
    json_schema = {
        'type': 'array',
        'items': {'type': 'string'},
        'maxItems': 0,
    }
    data = _JsonSchemaTestData(json_schema).generate()
    assert data == snapshot([])

# pydantic_ai_slim/pydantic_ai/_ssrf.py:50-50
_MAX_REDIRECTS = 10

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:207-207
    max_uses: int | None = None

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:144-144
    max_uses: int | None = None

# tests/evals/test_evaluator_common.py:179-189
async def test_max_duration():
    """Test MaxDuration evaluator."""
    # Test with float seconds
    evaluator = MaxDuration(seconds=1.0)
    assert evaluator.evaluate(MockContext(duration=0.5)) is True
    assert evaluator.evaluate(MockContext(duration=1.5)) is False

    # Test with timedelta
    evaluator = MaxDuration(seconds=timedelta(seconds=1))
    assert evaluator.evaluate(MockContext(duration=0.5)) is True
    assert evaluator.evaluate(MockContext(duration=1.5)) is False

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:21-21
MULTIPLIER_DROP_FACTOR = 10  # Factor used with BASE_THRESHOLD to drop the multiplier.

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/settings.py:14-14
    max_tokens: int

# pydantic_evals/pydantic_evals/otel/span_tree.py:48-48
    max_duration: timedelta | float

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_utils.py:9-9
    max_attempts: int

# docs/.hooks/algolia.py:35-35
MAX_CONTENT_LENGTH = 90_000

# pydantic_ai_slim/pydantic_ai/concurrency.py:76-76
    max_queued: int | None = None

# pydantic_ai_slim/pydantic_ai/concurrency.py:75-75
    max_running: int

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:185-185
    max_tokens: int

# pydantic_evals/pydantic_evals/otel/span_tree.py:57-57
    max_child_count: int

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:94-94
    max_characters: int | None

# pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py:47-47
    max_results: int | None

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:170-170
    max_price: _OpenRouterMaxPrice

# pydantic_ai_slim/pydantic_ai/concurrency.py:161-163
    def max_running(self) -> int:
        """Maximum concurrent operations allowed."""
        return int(self._limiter.total_tokens)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# tests/evals/test_evaluators.py:520-541
async def test_max_duration_evaluator(test_context: EvaluatorContext[TaskInput, TaskOutput, TaskMetadata]):
    """Test the max_duration evaluator."""
    from datetime import timedelta

    # Test with duration under the maximum (using float seconds)
    evaluator = MaxDuration(seconds=0.2)  # test_context has duration=0.1
    result = evaluator.evaluate(test_context)
    assert result is True

    # Test with duration over the maximum
    evaluator = MaxDuration(seconds=0.05)
    result = evaluator.evaluate(test_context)
    assert result is False

    # Test with timedelta
    evaluator = MaxDuration(seconds=timedelta(milliseconds=200))
    result = evaluator.evaluate(test_context)
    assert result is True

    evaluator = MaxDuration(seconds=timedelta(milliseconds=50))
    result = evaluator.evaluate(test_context)
    assert result is False

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:243-243
    max_content_tokens: int | None = None

# pydantic_evals/pydantic_evals/otel/span_tree.py:68-68
    max_descendant_count: int

# pydantic_ai_slim/pydantic_ai/embeddings/test.py:115-116
    async def max_input_tokens(self) -> int | None:
        return 1024

# tests/models/test_huggingface.py:775-780
async def test_max_completion_tokens(allow_model_requests: None, model_name: str, huggingface_api_key: str):
    m = HuggingFaceModel(model_name, provider=HuggingFaceProvider(provider_name='nebius', api_key=huggingface_api_key))
    agent = Agent(m, model_settings=ModelSettings(max_tokens=100))

    result = await agent.run('hello')
    assert result.output == IsStr()

# tests/models/test_huggingface.py:775-780
async def test_max_completion_tokens(allow_model_requests: None, model_name: str, huggingface_api_key: str):
    m = HuggingFaceModel(model_name, provider=HuggingFaceProvider(provider_name='nebius', api_key=huggingface_api_key))
    agent = Agent(m, model_settings=ModelSettings(max_tokens=100))

    result = await agent.run('hello')
    assert result.output == IsStr()

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:19-19
MULTIPLIER_ONE_DECIMAL_THRESHOLD = 100  # If |multiplier| is below this, use one decimal; otherwise, use none.

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:204-205
    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self.model_name)

# pydantic_ai_slim/pydantic_ai/embeddings/google.py:192-193
    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self._model_name)

# pydantic_ai_slim/pydantic_ai/embeddings/openai.py:150-155
    async def max_input_tokens(self) -> int | None:
        if self.system != 'openai':
            return None

        # https://platform.openai.com/docs/guides/embeddings#embedding-models
        return 8192

# tests/test_embeddings.py:161-163
    async def test_max_input_tokens(self, embedder: Embedder):
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(8192)

# tests/test_embeddings.py:366-370
    async def test_max_input_tokens(self, co_api_key: str):
        model = CohereEmbeddingModel('embed-v4.0', provider=CohereProvider(api_key=co_api_key))
        embedder = Embedder(model)
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(128000)

# tests/test_embeddings.py:1261-1263
    async def test_max_input_tokens(self, embedder: Embedder):
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(2048)