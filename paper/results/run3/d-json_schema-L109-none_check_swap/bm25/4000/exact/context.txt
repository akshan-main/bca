# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# docs/.hooks/algolia.py:7-7
from typing import TYPE_CHECKING, cast

# pydantic_ai_slim/pydantic_ai/_function_schema.py:21-21
from typing_extensions import ParamSpec, TypeIs, TypeVar, get_type_hints

# pydantic_ai_slim/pydantic_ai/_function_schema.py:21-21
from typing_extensions import ParamSpec, TypeIs, TypeVar, get_type_hints

# pydantic_ai_slim/pydantic_ai/_function_schema.py:21-21
from typing_extensions import ParamSpec, TypeIs, TypeVar, get_type_hints

# pydantic_evals/pydantic_evals/evaluators/common.py:135-135
    type_name: str

# pydantic_graph/pydantic_graph/persistence/_utils.py:15-15
nodes_type_context: ContextVar[Sequence[type[BaseNode[Any, Any, Any]]]] = ContextVar('nodes_type_context')

# pydantic_graph/pydantic_graph/beta/graph.py:42-42
from pydantic_graph.beta.util import unpack_type_expression

# pydantic_graph/pydantic_graph/beta/graph.py:42-42
from pydantic_graph.beta.util import unpack_type_expression

# pydantic_graph/pydantic_graph/persistence/_utils.py:59-64
def set_nodes_type_context(nodes: Sequence[type[BaseNode[Any, Any, Any]]]) -> Iterator[None]:
    token = nodes_type_context.set(nodes)
    try:
        yield
    finally:
        nodes_type_context.reset(token)

# pydantic_graph/pydantic_graph/beta/graph.py:42-42
from pydantic_graph.beta.util import unpack_type_expression

# tests/test_tools.py:1718-1720
def test_output_type_empty():
    with pytest.raises(UserError, match='At least one output type must be provided.'):
        Agent(TestModel(), output_type=[])

# pydantic_ai_slim/pydantic_ai/_output.py:12-12
from pydantic._internal._typing_extra import get_function_type_hints

# pydantic_ai_slim/pydantic_ai/models/mistral.py:748-756
VALID_JSON_TYPE_MAPPING: dict[str, Any] = {
    'string': str,
    'integer': int,
    'number': float,
    'boolean': bool,
    'array': list,
    'object': dict,
    'null': type(None),
}

# tests/graph/beta/test_graph_edge_cases.py:129-153
async def test_decision_invalid_type_check():
    """Test decision branch with invalid type for isinstance check."""

    g = GraphBuilder(state_type=MyState, output_type=str)

    @g.step
    async def return_value(ctx: StepContext[MyState, None, None]) -> int:
        return 42

    @g.step
    async def handle_value(ctx: StepContext[MyState, None, int]) -> str:
        return str(ctx.inputs)

    # Try to use a non-type as a branch source - this might cause TypeError during isinstance check
    # Note: This is hard to trigger without directly constructing invalid decision branches
    # For now, just test normal union types work
    g.add(
        g.edge_from(g.start_node).to(return_value),
        g.edge_from(return_value).to(g.decision().branch(g.match(int).to(handle_value))),
        g.edge_from(handle_value).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=MyState())
    assert result == '42'

# pydantic_ai_slim/pydantic_ai/models/mistral.py:758-765
SIMPLE_JSON_TYPE_MAPPING = {
    'string': 'str',
    'integer': 'int',
    'number': 'float',
    'boolean': 'bool',
    'array': 'list',
    'null': 'None',
}

# tests/test_agent.py:1009-1045
def test_output_type_function():
    class Weather(BaseModel):
        temperature: float
        description: str

    def get_weather(city: str) -> Weather:
        return Weather(temperature=28.7, description='sunny')

    output_tools = None

    def call_tool(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        assert info.output_tools is not None

        nonlocal output_tools
        output_tools = info.output_tools

        args_json = '{"city": "Mexico City"}'
        return ModelResponse(parts=[ToolCallPart(info.output_tools[0].name, args_json)])

    agent = Agent(FunctionModel(call_tool), output_type=get_weather)
    result = agent.run_sync('Mexico City')
    assert result.output == snapshot(Weather(temperature=28.7, description='sunny'))
    assert output_tools == snapshot(
        [
            ToolDefinition(
                name='final_result',
                description='The final response which ends this conversation',
                parameters_json_schema={
                    'additionalProperties': False,
                    'properties': {'city': {'type': 'string'}},
                    'required': ['city'],
                    'type': 'object',
                },
                kind='output',
            )
        ]
    )

# tests/test_agent.py:4927-4933
def test_custom_output_type_sync() -> None:
    agent = Agent('test', output_type=Foo)

    assert agent.run_sync('Hello').output == snapshot(Foo(a=0, b='a'))
    assert agent.run_sync('Hello', output_type=Bar).output == snapshot(Bar(c=0, d='a'))
    assert agent.run_sync('Hello', output_type=str).output == snapshot('success (no tool calls)')
    assert agent.run_sync('Hello', output_type=int).output == snapshot(0)

# tests/test_agent.py:4936-4945
async def test_custom_output_type_async() -> None:
    agent = Agent('test')

    result = await agent.run('Hello')
    assert result.output == snapshot('success (no tool calls)')

    result = await agent.run('Hello', output_type=Foo)
    assert result.output == snapshot(Foo(a=0, b='a'))
    result = await agent.run('Hello', output_type=int)
    assert result.output == snapshot(0)

# pydantic_ai_slim/pydantic_ai/_utils.py:140-143
class Some(Generic[T]):
    """Analogous to Rust's `Option::Some` type."""

    value: T

# tests/test_agent.py:1343-1379
def test_output_type_async_function():
    class Weather(BaseModel):
        temperature: float
        description: str

    async def get_weather(city: str) -> Weather:
        return Weather(temperature=28.7, description='sunny')

    output_tools = None

    def call_tool(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        assert info.output_tools is not None

        nonlocal output_tools
        output_tools = info.output_tools

        args_json = '{"city": "Mexico City"}'
        return ModelResponse(parts=[ToolCallPart(info.output_tools[0].name, args_json)])

    agent = Agent(FunctionModel(call_tool), output_type=get_weather)
    result = agent.run_sync('Mexico City')
    assert result.output == snapshot(Weather(temperature=28.7, description='sunny'))
    assert output_tools == snapshot(
        [
            ToolDefinition(
                name='final_result',
                description='The final response which ends this conversation',
                parameters_json_schema={
                    'additionalProperties': False,
                    'properties': {'city': {'type': 'string'}},
                    'required': ['city'],
                    'type': 'object',
                },
                kind='output',
            )
        ]
    )

# tests/test_agent.py:4948-4956
def test_custom_output_type_invalid() -> None:
    agent = Agent('test')

    @agent.output_validator
    def validate_output(ctx: RunContext[None], o: Any) -> Any:  # pragma: no cover
        return o

    with pytest.raises(UserError, match='Cannot set a custom run `output_type` when the agent has output validators'):
        agent.run_sync('Hello', output_type=int)

# pydantic_graph/pydantic_graph/persistence/__init__.py:228-238
def build_snapshot_list_type_adapter(
    state_t: type[StateT], run_end_t: type[RunEndT]
) -> pydantic.TypeAdapter[list[Snapshot[StateT, RunEndT]]]:
    """Build a type adapter for a list of snapshots.

    This method should be called from within
    [`set_types`][pydantic_graph.persistence.BaseStatePersistence.set_types]
    where context variables will be set such that Pydantic can create a schema for
    [`NodeSnapshot.node`][pydantic_graph.persistence.NodeSnapshot.node].
    """
    return pydantic.TypeAdapter(list[Annotated[Snapshot[state_t, run_end_t], pydantic.Discriminator('kind')]])

# pydantic_graph/pydantic_graph/persistence/__init__.py:228-238
def build_snapshot_list_type_adapter(
    state_t: type[StateT], run_end_t: type[RunEndT]
) -> pydantic.TypeAdapter[list[Snapshot[StateT, RunEndT]]]:
    """Build a type adapter for a list of snapshots.

    This method should be called from within
    [`set_types`][pydantic_graph.persistence.BaseStatePersistence.set_types]
    where context variables will be set such that Pydantic can create a schema for
    [`NodeSnapshot.node`][pydantic_graph.persistence.NodeSnapshot.node].
    """
    return pydantic.TypeAdapter(list[Annotated[Snapshot[state_t, run_end_t], pydantic.Discriminator('kind')]])

# pydantic_graph/pydantic_graph/persistence/__init__.py:228-238
def build_snapshot_list_type_adapter(
    state_t: type[StateT], run_end_t: type[RunEndT]
) -> pydantic.TypeAdapter[list[Snapshot[StateT, RunEndT]]]:
    """Build a type adapter for a list of snapshots.

    This method should be called from within
    [`set_types`][pydantic_graph.persistence.BaseStatePersistence.set_types]
    where context variables will be set such that Pydantic can create a schema for
    [`NodeSnapshot.node`][pydantic_graph.persistence.NodeSnapshot.node].
    """
    return pydantic.TypeAdapter(list[Annotated[Snapshot[state_t, run_end_t], pydantic.Discriminator('kind')]])

# tests/graph/test_persistence.py:346-348
def test_snapshot_type_adapter_error():
    with pytest.raises(RuntimeError, match='Unable to build a Pydantic schema for `BaseNode` without setting'):
        build_snapshot_list_type_adapter(int, int)

# tests/test_agent.py:1672-1742
def test_output_type_structured_dict():
    PersonDict = StructuredDict(
        {
            'type': 'object',
            'properties': {
                'name': {'type': 'string'},
                'age': {'type': 'integer'},
            },
            'required': ['name', 'age'],
        },
        name='Person',
        description='A person',
    )
    AnimalDict = StructuredDict(
        {
            'type': 'object',
            'properties': {
                'name': {'type': 'string'},
                'species': {'type': 'string'},
            },
            'required': ['name', 'species'],
        },
        name='Animal',
        description='An animal',
    )

    output_tools = None

    def call_tool(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        assert info.output_tools is not None

        nonlocal output_tools
        output_tools = info.output_tools

        args_json = '{"name": "John Doe", "age": 30}'
        return ModelResponse(parts=[ToolCallPart(info.output_tools[0].name, args_json)])

    agent = Agent(
        FunctionModel(call_tool),
        output_type=[PersonDict, AnimalDict],
    )

    result = agent.run_sync('Generate a person')

    assert result.output == snapshot({'name': 'John Doe', 'age': 30})
    assert output_tools == snapshot(
        [
            ToolDefinition(
                name='final_result_Person',
                parameters_json_schema={
                    'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}},
                    'required': ['name', 'age'],
                    'title': 'Person',
                    'type': 'object',
                },
                description='A person',
                kind='output',
            ),
            ToolDefinition(
                name='final_result_Animal',
                parameters_json_schema={
                    'properties': {'name': {'type': 'string'}, 'species': {'type': 'string'}},
                    'required': ['name', 'species'],
                    'title': 'Animal',
                    'type': 'object',
                },
                description='An animal',
                kind='output',
            ),
        ]
    )

# tests/test_usage_limits.py:258-284
def test_add_usages_with_none_detail_value():
    """Test that None values in details are skipped when incrementing usage."""
    usage = RunUsage(
        requests=1,
        input_tokens=10,
        output_tokens=20,
        details={'reasoning_tokens': 5},
    )

    # Create a usage with None in details (simulating model response with missing detail)
    incr_usage = RunUsage(
        requests=1,
        input_tokens=5,
        output_tokens=10,
    )
    # Manually set a None value in details to simulate edge case from model responses
    incr_usage.details = {'reasoning_tokens': None, 'other_tokens': 10}  # type: ignore[dict-item]

    result = usage + incr_usage
    assert result == snapshot(
        RunUsage(
            requests=2,
            input_tokens=15,
            output_tokens=30,
            details={'reasoning_tokens': 5, 'other_tokens': 10},
        )
    )

# pydantic_graph/pydantic_graph/persistence/file.py:50-52
    _snapshots_type_adapter: pydantic.TypeAdapter[list[Snapshot[StateT, RunEndT]]] | None = field(
        default=None, init=False, repr=False
    )

# pydantic_graph/pydantic_graph/persistence/in_mem.py:97-99
    _snapshots_type_adapter: pydantic.TypeAdapter[list[Snapshot[StateT, RunEndT]]] | None = field(
        default=None, init=False, repr=False
    )

# tests/test_streaming.py:2286-2304
async def test_custom_output_type_default_str() -> None:
    agent = Agent('test')

    async with agent.run_stream('test') as result:
        response = await result.get_output()
        assert response == snapshot('success (no tool calls)')
    assert result.response == snapshot(
        ModelResponse(
            parts=[TextPart(content='success (no tool calls)')],
            usage=RequestUsage(input_tokens=51, output_tokens=4),
            model_name='test',
            timestamp=IsDatetime(),
            provider_name='test',
        )
    )

    async with agent.run_stream('test', output_type=OutputType) as result:
        response = await result.get_output()
        assert response == snapshot(OutputType(value='a'))

# tests/test_agent.py:1329-1331
def test_output_type_multiple_text_output(output_type: OutputSpec[str]):
    with pytest.raises(UserError, match='Only one `str` or `TextOutput` is allowed.'):
        Agent('test', output_type=output_type)

# pydantic_ai_slim/pydantic_ai/_function_schema.py:25-25
from ._utils import check_object_json_schema, is_async_callable, is_model_like, run_in_executor