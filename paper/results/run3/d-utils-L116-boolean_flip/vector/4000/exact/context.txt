# pydantic_ai_slim/pydantic_ai/_utils.py:124-133
def _contains_ref(obj: JsonSchemaValue | list[JsonSchemaValue]) -> bool:
    """Recursively check if an object contains any $ref keys."""
    items: Iterable[JsonSchemaValue]
    if isinstance(obj, dict):
        if '$ref' in obj:
            return True
        items = obj.values()
    else:
        items = obj
    return any(isinstance(item, dict | list) and _contains_ref(item) for item in items)  # pyright: ignore[reportUnknownArgumentType]

# tests/test_usage_limits.py:3-3
import operator

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_run_context.py:15-15
from . import _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_system_prompt.py:8-8
from . import _utils

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:20-30
from .. import (
    _agent_graph,
    _output,
    _system_prompt,
    _utils,
    concurrency as _concurrency,
    exceptions,
    messages as _messages,
    models,
    usage as _usage,
)

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:17-27
from .. import (
    _agent_graph,
    _system_prompt,
    _tool_manager,
    _utils,
    exceptions,
    messages as _messages,
    models,
    result,
    usage as _usage,
)

# pydantic_ai_slim/pydantic_ai/agent/wrapper.py:7-12
from .. import (
    _utils,
    messages as _messages,
    models,
    usage as _usage,
)

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py:10-17
from pydantic_ai import (
    AbstractToolset,
    AgentRunResultEvent,
    _utils,
    messages as _messages,
    models,
    usage as _usage,
)

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py:13-20
from pydantic_ai import (
    AbstractToolset,
    AgentRunResultEvent,
    _utils,
    messages as _messages,
    models,
    usage as _usage,
)

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py:18-25
from pydantic_ai import (
    AbstractToolset,
    AgentRunResultEvent,
    _utils,
    messages as _messages,
    models,
    usage as _usage,
)

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:9-9
from pydantic_ai import _utils

# pydantic_ai_slim/pydantic_ai/embeddings/openai.py:5-5
from pydantic_ai import _utils

# pydantic_ai_slim/pydantic_ai/embeddings/sentence_transformers.py:8-8
import pydantic_ai._utils as _utils

# pydantic_ai_slim/pydantic_ai/mcp.py:46-46
from . import _mcp, _utils, exceptions, messages, models

# pydantic_ai_slim/pydantic_ai/messages.py:25-25
from . import _otel_messages, _utils

# pydantic_ai_slim/pydantic_ai/models/__init__.py:22-22
from .. import _utils

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:17-42
from pydantic_ai import (
    AudioUrl,
    BinaryContent,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentUrl,
    FinishReason,
    ImageUrl,
    ModelMessage,
    ModelProfileSpec,
    ModelRequest,
    ModelResponse,
    ModelResponsePart,
    ModelResponseStreamEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
    _utils,
    usage,
)

# pydantic_ai_slim/pydantic_ai/models/function.py:14-14
from .. import _utils, usage

# pydantic_ai_slim/pydantic_ai/models/gemini.py:24-24
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/google.py:14-14
from .. import UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/groq.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:11-11
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/mistral.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils

# pydantic_ai_slim/pydantic_ai/models/openai.py:18-18
from .. import ModelAPIError, ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/outlines.py:17-17
from .. import UnexpectedModelBehavior, _utils

# pydantic_ai_slim/pydantic_ai/models/test.py:14-14
from .. import _utils

# pydantic_ai_slim/pydantic_ai/models/xai.py:13-13
from .. import _utils

# pydantic_ai_slim/pydantic_ai/output.py:12-12
from . import _utils, exceptions

# pydantic_ai_slim/pydantic_ai/result.py:12-12
from . import _utils, exceptions, messages as _messages, models

# pydantic_ai_slim/pydantic_ai/run.py:13-19
from . import (
    _agent_graph,
    _utils,
    exceptions,
    messages as _messages,
    usage as _usage,
)

# pydantic_ai_slim/pydantic_ai/tools.py:12-12
from . import _function_schema, _utils

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:10-10
from pydantic_ai import _utils

# pydantic_ai_slim/pydantic_ai/usage.py:12-12
from . import _utils

# pydantic_evals/pydantic_evals/evaluators/_base.py:13-13
from pydantic_ai import _utils

# pydantic_graph/pydantic_graph/beta/graph_builder.py:19-19
from pydantic_graph import _utils, exceptions

# pydantic_graph/pydantic_graph/graph.py:15-15
from . import _utils, exceptions, mermaid

# pydantic_graph/pydantic_graph/nodes.py:12-12
from . import _utils, exceptions

# pydantic_graph/pydantic_graph/persistence/__init__.py:13-13
from . import _utils

# pydantic_graph/pydantic_graph/persistence/file.py:16-26
from . import (
    BaseStatePersistence,
    EndSnapshot,
    NodeSnapshot,
    RunEndT,
    Snapshot,
    SnapshotStatus,
    StateT,
    _utils,
    build_snapshot_list_type_adapter,
)

# pydantic_graph/pydantic_graph/persistence/in_mem.py:19-28
from . import (
    BaseStatePersistence,
    EndSnapshot,
    NodeSnapshot,
    RunEndT,
    Snapshot,
    StateT,
    _utils,
    build_snapshot_list_type_adapter,
)

# tests/models/mock_async_stream.py:14-14
from pydantic_ai import _utils

# pydantic_ai_slim/pydantic_ai/_ssrf.py:182-204
def build_url_with_ip(resolved: ResolvedUrl) -> str:
    """Build a URL using a resolved IP address instead of the hostname.

    For IPv6 addresses, wraps them in brackets as required by URL syntax.
    """
    scheme = 'https' if resolved.is_https else 'http'
    default_port = 443 if resolved.is_https else 80

    # IPv6 addresses need brackets in URLs
    try:
        ip_obj = ipaddress.ip_address(resolved.resolved_ip)
        if isinstance(ip_obj, ipaddress.IPv6Address):
            host_part = f'[{resolved.resolved_ip}]'
        else:
            host_part = resolved.resolved_ip
    except ValueError:
        host_part = resolved.resolved_ip

    # Only include port if non-default
    if resolved.port != default_port:
        host_part = f'{host_part}:{resolved.port}'

    return urlunparse((scheme, host_part, resolved.path, '', '', ''))

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/app.py:42-148
    def __init__(
        self,
        agent: AbstractAgent[AgentDepsT, OutputDataT],
        *,
        # AGUIAdapter.dispatch_request parameters
        output_type: OutputSpec[Any] | None = None,
        message_history: Sequence[ModelMessage] | None = None,
        deferred_tool_results: DeferredToolResults | None = None,
        model: Model | KnownModelName | str | None = None,
        deps: AgentDepsT = None,
        model_settings: ModelSettings | None = None,
        usage_limits: UsageLimits | None = None,
        usage: RunUsage | None = None,
        infer_name: bool = True,
        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,
        builtin_tools: Sequence[AbstractBuiltinTool] | None = None,
        on_complete: OnCompleteFunc[Any] | None = None,
        # Starlette parameters
        debug: bool = False,
        routes: Sequence[BaseRoute] | None = None,
        middleware: Sequence[Middleware] | None = None,
        exception_handlers: Mapping[Any, ExceptionHandler] | None = None,
        on_startup: Sequence[Callable[[], Any]] | None = None,
        on_shutdown: Sequence[Callable[[], Any]] | None = None,
        lifespan: Lifespan[Self] | None = None,
    ) -> None:
        """An ASGI application that handles every request by running the agent and streaming the response.

        Note that the `deps` will be the same for each request, with the exception of the frontend state that's
        injected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ui.StateHandler] protocol.
        To provide different `deps` for each request (e.g. based on the authenticated user),
        use [`AGUIAdapter.run_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.run_stream] or
        [`AGUIAdapter.dispatch_request()`][pydantic_ai.ui.ag_ui.AGUIAdapter.dispatch_request] instead.

        Args:
            agent: The agent to run.

            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has
                no output validators since output validators would expect an argument that matches the agent's
                output type.
            message_history: History of the conversation so far.
            deferred_tool_results: Optional results for deferred tool calls in the message history.
            model: Optional model to use for this run, required if `model` was not set when creating the agent.
            deps: Optional dependencies to use for this run.
            model_settings: Optional settings to use for this model's request.
            usage_limits: Optional limits on model request count or token usage.
            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
            infer_name: Whether to try to infer the agent name from the call frame if it's not set.
            toolsets: Optional additional toolsets for this run.
            builtin_tools: Optional additional builtin tools for this run.
            on_complete: Optional callback function called when the agent run completes successfully.
                The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.

            debug: Boolean indicating if debug tracebacks should be returned on errors.
            routes: A list of routes to serve incoming HTTP and WebSocket requests.
            middleware: A list of middleware to run for every request. A starlette application will always
                automatically include two middleware classes. `ServerErrorMiddleware` is added as the very
                outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.
                `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled
                exception cases occurring in the routing or endpoints.
            exception_handlers: A mapping of either integer status codes, or exception class types onto
                callables which handle the exceptions. Exception handler callables should be of the form
                `handler(request, exc) -> response` and may be either standard functions, or async functions.
            on_startup: A list of callables to run on application startup. Startup handler callables do not
                take any arguments, and may be either standard functions, or async functions.
            on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do
                not take any arguments, and may be either standard functions, or async functions.
            lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.
                This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or
                the other, not both.
        """
        super().__init__(
            debug=debug,
            routes=routes,
            middleware=middleware,
            exception_handlers=exception_handlers,
            on_startup=on_startup,
            on_shutdown=on_shutdown,
            lifespan=lifespan,
        )

        async def run_agent(request: Request) -> Response:
            """Endpoint to run the agent with the provided input data."""
            # `dispatch_request` will store the frontend state from the request on `deps.state` (if it implements the `StateHandler` protocol),
            # so we need to copy the deps to avoid different requests mutating the same deps object.
            nonlocal deps
            if isinstance(deps, StateHandler):  # pragma: no branch
                deps = replace(deps)

            return await AGUIAdapter[AgentDepsT, OutputDataT].dispatch_request(
                request,
                agent=agent,
                output_type=output_type,
                message_history=message_history,
                deferred_tool_results=deferred_tool_results,
                model=model,
                deps=deps,
                model_settings=model_settings,
                usage_limits=usage_limits,
                usage=usage,
                infer_name=infer_name,
                toolsets=toolsets,
                builtin_tools=builtin_tools,
                on_complete=on_complete,
            )

        self.router.add_route('/', run_agent, methods=['POST'])

# pydantic_evals/pydantic_evals/otel/_errors.py:1-7
class SpanTreeRecordingError(Exception):
    """An exception that is used to provide the reason why a SpanTree was not recorded by `context_subtree`.

    This will either be due to missing dependencies or because a tracer provider had not been set.
    """

    pass

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:691-782
    def run_stream_sync(
        self,
        user_prompt: str | Sequence[_messages.UserContent] | None = None,
        *,
        output_type: OutputSpec[RunOutputDataT] | None = None,
        message_history: Sequence[_messages.ModelMessage] | None = None,
        deferred_tool_results: DeferredToolResults | None = None,
        model: models.Model | models.KnownModelName | str | None = None,
        deps: AgentDepsT = None,
        model_settings: ModelSettings | None = None,
        usage_limits: _usage.UsageLimits | None = None,
        usage: _usage.RunUsage | None = None,
        metadata: AgentMetadata[AgentDepsT] | None = None,
        infer_name: bool = True,
        toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,
        builtin_tools: Sequence[AbstractBuiltinTool | BuiltinToolFunc[AgentDepsT]] | None = None,
        event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,
    ) -> result.StreamedRunResultSync[AgentDepsT, Any]:
        """Run the agent with a user prompt in sync streaming mode.

        This is a convenience method that wraps [`run_stream()`][pydantic_ai.agent.AbstractAgent.run_stream] with `loop.run_until_complete(...)`.
        You therefore can't use this method inside async code or if there's an active event loop.

        This method builds an internal agent graph (using system prompts, tools and output schemas) and then
        runs the graph until the model produces output matching the `output_type`, for example text or structured data.
        At this point, a streaming run result object is yielded from which you can stream the output as it comes in,
        and -- once this output has completed streaming -- get the complete output, message history, and usage.

        As this method will consider the first output matching the `output_type` to be the final output,
        it will stop running the agent graph and will not execute any tool calls made by the model after this "final" output.
        If you want to always run the agent graph to completion and stream events and output at the same time,
        use [`agent.run()`][pydantic_ai.agent.AbstractAgent.run] with an `event_stream_handler` or [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] instead.

        Example:
        ```python
        from pydantic_ai import Agent

        agent = Agent('openai:gpt-5.2')

        def main():
            response = agent.run_stream_sync('What is the capital of the UK?')
            print(response.get_output())
            #> The capital of the UK is London.
        ```

        Args:
            user_prompt: User input to start/continue the conversation.
            output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no
                output validators since output validators would expect an argument that matches the agent's output type.
            message_history: History of the conversation so far.
            deferred_tool_results: Optional results for deferred tool calls in the message history.
            model: Optional model to use for this run, required if `model` was not set when creating the agent.
            deps: Optional dependencies to use for this run.
            model_settings: Optional settings to use for this model's request.
            usage_limits: Optional limits on model request count or token usage.
            usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.
            metadata: Optional metadata to attach to this run. Accepts a dictionary or a callable taking
                [`RunContext`][pydantic_ai.tools.RunContext]; merged with the agent's configured metadata.
            infer_name: Whether to try to infer the agent name from the call frame if it's not set.
            toolsets: Optional additional toolsets for this run.
            builtin_tools: Optional additional builtin tools for this run.
            event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.
                It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.
                Note that it does _not_ receive any events after the final result is found.

        Returns:
            The result of the run.
        """
        if infer_name and self.name is None:
            self._infer_name(inspect.currentframe())

        async def _consume_stream():
            async with self.run_stream(
                user_prompt,
                output_type=output_type,
                message_history=message_history,
                deferred_tool_results=deferred_tool_results,
                model=model,
                deps=deps,
                model_settings=model_settings,
                usage_limits=usage_limits,
                usage=usage,
                metadata=metadata,
                infer_name=infer_name,
                toolsets=toolsets,
                builtin_tools=builtin_tools,
                event_stream_handler=event_stream_handler,
            ) as stream_result:
                yield stream_result

        async_result = _utils.get_event_loop().run_until_complete(anext(_consume_stream()))
        return result.StreamedRunResultSync(async_result)

# pydantic_ai_slim/pydantic_ai/models/test.py:480-482
    def _bool_gen(self) -> bool:
        """Generate a boolean from a JSON Schema boolean."""
        return bool(self.seed % 2)

# pydantic_ai_slim/pydantic_ai/messages.py:553-566
    def identifier(self) -> str:
        """Identifier for the binary content, such as a unique ID.

        This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,
        and the tool can look up the file in question by iterating over the message history and finding the matching `BinaryContent`.

        This identifier is only automatically passed to the model when the `BinaryContent` is returned by a tool.
        If you're passing the `BinaryContent` as a user message, it's up to you to include a separate text part with the identifier,
        e.g. "This is file <identifier>:" preceding the `BinaryContent`.

        It's also included in inline-text delimiters for providers that require inlining text documents, so the model can
        distinguish multiple files.
        """
        return self._identifier or _multi_modal_content_identifier(self.data)

# pydantic_ai_slim/pydantic_ai/tools.py:235-235
A = TypeVar('A')

# tests/test_ssrf.py:290-293
    def test_relative_path(self) -> None:
        """Test that relative paths are resolved against the current URL."""
        result = resolve_redirect_url('https://example.com/old/path', 'new-file.txt')
        assert result == 'https://example.com/old/new-file.txt'

# tests/test_agent.py:150-150
    a: int