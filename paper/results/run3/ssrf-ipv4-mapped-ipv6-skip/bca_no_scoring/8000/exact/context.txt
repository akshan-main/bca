## examples/pydantic_ai_examples/chat_app.py

async def index() -> FileResponse:
    return FileResponse((THIS_DIR / 'chat_app.html'), media_type='text/html')

async def main_ts() -> FileResponse:
    """Get the raw typescript code, it's compiled in the browser, forgive me."""
    return FileResponse((THIS_DIR / 'chat_app.ts'), media_type='text/plain')

async def get_chat(database: Database = Depends(get_db)) -> Response:
    msgs = await database.get_messages()
    return Response(
        b'\n'.join(json.dumps(to_chat_message(m)).encode('utf-8') for m in msgs),
        media_type='text/plain',
    )

## examples/pydantic_ai_examples/data_analyst.py

def display(ctx: RunContext[AnalystAgentDeps], name: str) -> str:
    """Display at most 5 rows of the dataframe."""
    dataset = ctx.deps.get(name)
    return dataset.head().to_string()  # pyright: ignore[reportUnknownMemberType]

## examples/pydantic_ai_examples/rag.py

def slugify(value: str, separator: str, unicode: bool = False) -> str:
    """Slugify a string, to make it URL friendly."""
    # Taken unchanged from https://github.com/Python-Markdown/markdown/blob/3.7/markdown/extensions/toc.py#L38
    if not unicode:
        # Replace Extended Latin characters with ASCII, i.e. `žlutý` => `zluty`
        value = unicodedata.normalize('NFKD', value)
        value = value.encode('ascii', 'ignore').decode('ascii')
    value = re.sub(r'[^\w\s-]', '', value).strip().lower()
    return re.sub(rf'[{separator}\s]+', separator, value)

## pydantic_ai_slim/pydantic_ai/_a2a.py

from typing import Any, Generic, TypeVar

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

def get_captured_run_messages() -> _RunMessages:
    return _messages_ctx_var.get()

## pydantic_ai_slim/pydantic_ai/_griffe.py

def _infer_docstring_style(doc: str) -> DocstringStyle:
    """Simplistic docstring style inference."""
    for pattern, replacements, style in _docstring_style_patterns:
        matches = (
            re.search(pattern.format(replacement), doc, re.IGNORECASE | re.MULTILINE) for replacement in replacements
        )
        if any(matches):
            return style
    # fallback to google style
    return 'google'

## pydantic_ai_slim/pydantic_ai/_run_context.py

def get_current_run_context() -> RunContext[Any] | None:
    """Get the current run context, if one is set.

    Returns:
        The current [`RunContext`][pydantic_ai.tools.RunContext], or `None` if not in an agent run.
    """
    return _CURRENT_RUN_CONTEXT.get()

## pydantic_ai_slim/pydantic_ai/_ssrf.py

class ResolvedUrl:
    """Result of URL validation and DNS resolution."""

    resolved_ip: str
    """The resolved IP address to connect to."""

    hostname: str
    """The original hostname (used for Host header)."""

    port: int
    """The port number."""

    is_https: bool
    """Whether to use HTTPS."""

    path: str
    """The path including query string and fragment."""

def is_cloud_metadata_ip(ip_str: str) -> bool:
    """Check if an IP address is a cloud metadata endpoint.

    These are always blocked for security reasons, even with allow_local=True.
    """
    return ip_str in _CLOUD_METADATA_IPS

def is_private_ip(ip_str: str) -> bool:
    """Check if an IP address is in a private/internal range.

    Handles both IPv4 and IPv6 addresses, including IPv4-mapped IPv6 addresses.
    """
    try:
        ip = ipaddress.ip_address(ip_str)

        # Handle IPv4-mapped IPv6 addresses (e.g., ::ffff:192.168.1.1)
        if isinstance(ip, ipaddress.IPv6Address) and not ip.ipv4_mapped:
            ip = ip.ipv4_mapped

        return any(ip in network for network in _PRIVATE_NETWORKS)
    except ValueError:
        # Invalid IP address, treat as potentially dangerous
        return True

async def validate_and_resolve_url(url: str, allow_local: bool) -> ResolvedUrl:
    """Validate URL and resolve hostname to IP addresses.

    Performs protocol validation, DNS resolution, and IP validation.

    Args:
        url: The URL to validate.
        allow_local: Whether to allow private/internal IP addresses.

    Returns:
        ResolvedUrl with all the information needed to make the request.

    Raises:
        ValueError: If the URL fails validation.
    """
    hostname, path, port, is_https = extract_host_and_port(url)

    # Check if hostname is already an IP address
    try:
        # Handle IPv6 addresses in brackets
        ip_str = hostname.strip('[]')
        ipaddress.ip_address(ip_str)
        ips = [ip_str]
    except ValueError:
        # It's a hostname, resolve it
        ips = await resolve_hostname(hostname)

    # Validate all resolved IPs
    for ip in ips:
        # Cloud metadata IPs are always blocked
        if is_cloud_metadata_ip(ip):
            raise ValueError(f'Access to cloud metadata service ({ip}) is blocked for security reasons.')

        # Private IPs are blocked unless allow_local is True
        if not allow_local and is_private_ip(ip):
            raise ValueError(
                f'Access to private/internal IP address ({ip}) is blocked. '
                f'Use force_download="allow-local" to allow local network access.'
            )

    # Use the first resolved IP
    return ResolvedUrl(
        resolved_ip=ips[0],
        hostname=hostname,
        port=port,
        is_https=is_https,
        path=path,
    )

## pydantic_ai_slim/pydantic_ai/_utils.py

async def run_in_executor(func: Callable[_P, _R], *args: _P.args, **kwargs: _P.kwargs) -> _R:
    if _disable_threads.get():
        return func(*args, **kwargs)

    wrapped_func = partial(func, *args, **kwargs)
    return await run_sync(wrapped_func)

def _contains_ref(obj: JsonSchemaValue | list[JsonSchemaValue]) -> bool:
    """Recursively check if an object contains any $ref keys."""
    items: Iterable[JsonSchemaValue]
    if isinstance(obj, dict):
        if '$ref' in obj:
            return True
        items = obj.values()
    else:
        items = obj
    return any(isinstance(item, dict | list) and _contains_ref(item) for item in items)  # pyright: ignore[reportUnknownArgumentType]

def generate_tool_call_id() -> str:
    """Generate a tool call id.

    Ensure that the tool call id is unique.
    """
    return f'pyd_ai_{uuid.uuid4().hex}'

## pydantic_ai_slim/pydantic_ai/builtin_tools.py

def _tool_discriminator(tool_data: dict[str, Any] | AbstractBuiltinTool) -> str:
    if isinstance(tool_data, dict):
        return tool_data.get('kind', AbstractBuiltinTool.kind)
    else:
        return tool_data.kind

## pydantic_ai_slim/pydantic_ai/models/__init__.py

class DownloadedItem(TypedDict, Generic[DataT]):
    """The downloaded data and its type."""

    data: DataT
    """The downloaded data."""

    data_type: str
    """The type of data that was downloaded.

    Extracted from header "content-type", but defaults to the media type inferred from the file URL if content-type is "application/octet-stream".
    """

async def download_item(
    item: FileUrl,
    data_format: Literal['bytes', 'base64', 'base64_uri', 'text'] = 'bytes',
    type_format: Literal['mime', 'extension'] = 'mime',
) -> DownloadedItem[str] | DownloadedItem[bytes]:
    """Download an item by URL and return the content as a bytes object or a (base64-encoded) string.

    This function includes SSRF (Server-Side Request Forgery) protection:
    - Only http:// and https:// protocols are allowed
    - Private/internal IP addresses are blocked by default
    - Cloud metadata endpoints (169.254.169.254) are always blocked
    - Hostnames are resolved before requests to prevent DNS rebinding

    Set `item.force_download='allow-local'` to allow private IP addresses.

    Args:
        item: The item to download.
        data_format: The format to return the content in:
            - `bytes`: The raw bytes of the content.
            - `base64`: The base64-encoded content.
            - `base64_uri`: The base64-encoded content as a data URI.
            - `text`: The content as a string.
        type_format: The format to return the media type in:
            - `mime`: The media type as a MIME type.
            - `extension`: The media type as an extension.

    Raises:
        UserError: If the URL points to a YouTube video.
        ValueError: If the URL uses an unsupported protocol or targets a private/internal
            IP address (unless allow-local is set).
    """
    if isinstance(item, VideoUrl) and item.is_youtube:
        raise UserError('Downloading YouTube videos is not supported.')

    from .._ssrf import safe_download

    allow_local = item.force_download == 'allow-local'
    response = await safe_download(item.url, allow_local=allow_local)

    if content_type := response.headers.get('content-type'):
        content_type = content_type.split(';')[0]
        if content_type == 'application/octet-stream':
            content_type = None

    media_type = content_type or item.media_type

    data_type = media_type
    if type_format == 'extension':
        data_type = item.format

    data = response.content
    if data_format in ('base64', 'base64_uri'):
        data = base64.b64encode(data).decode('utf-8')
        if data_format == 'base64_uri':
            data = f'data:{media_type};base64,{data}'
        return DownloadedItem[str](data=data, data_type=data_type)
    elif data_format == 'text':
        return DownloadedItem[str](data=data.decode('utf-8'), data_type=data_type)
    else:
        return DownloadedItem[bytes](data=data, data_type=data_type)

def get_user_agent() -> str:
    """Get the user agent string for the HTTP client."""
    from .. import __version__

    return f'pydantic-ai/{__version__}'

def _customize_tool_def(transformer: type[JsonSchemaTransformer], tool_def: ToolDefinition):
    """Customize the tool definition using the given transformer.

    If the tool definition has `strict` set to None, the strictness will be inferred from the transformer.
    """
    schema_transformer = transformer(tool_def.parameters_json_schema, strict=tool_def.strict)
    parameters_json_schema = schema_transformer.walk()
    return replace(
        tool_def,
        parameters_json_schema=parameters_json_schema,
        strict=schema_transformer.is_strict_compatible if tool_def.strict is None else tool_def.strict,
    )

def _customize_output_object(transformer: type[JsonSchemaTransformer], output_object: OutputObjectDefinition):
    schema_transformer = transformer(output_object.json_schema, strict=output_object.strict)
    json_schema = schema_transformer.walk()
    return replace(
        output_object,
        json_schema=json_schema,
        strict=schema_transformer.is_strict_compatible if output_object.strict is None else output_object.strict,
    )

## pydantic_ai_slim/pydantic_ai/models/gemini.py

class _BasePart(TypedDict):
    thought: NotRequired[bool]
    """Indicates if the part is thought from the model."""

class _GeminiInlineDataPart(_BasePart):
    """See <https://ai.google.dev/api/caching#Blob>."""

    inline_data: Annotated[_GeminiInlineData, pydantic.Field(alias='inlineData')]

class _GeminiFileDataPart(_BasePart):
    file_data: Annotated[_GeminiFileData, pydantic.Field(alias='fileData')]

## pydantic_ai_slim/pydantic_ai/models/google.py

def _map_executable_code(executable_code: ExecutableCode, provider_name: str, tool_call_id: str) -> BuiltinToolCallPart:
    return BuiltinToolCallPart(
        provider_name=provider_name,
        tool_name=CodeExecutionTool.kind,
        args=executable_code.model_dump(mode='json'),
        tool_call_id=tool_call_id,
    )

## pydantic_ai_slim/pydantic_ai/models/xai.py

def _map_json_schema(o: OutputObjectDefinition) -> chat_pb2.ResponseFormat:
    """Convert OutputObjectDefinition to xAI ResponseFormat protobuf object."""
    return chat_pb2.ResponseFormat(
        format_type=chat_pb2.FORMAT_TYPE_JSON_SCHEMA,
        schema=json.dumps(o.json_schema),
    )

def _map_json_object() -> chat_pb2.ResponseFormat:
    """Create a ResponseFormat for JSON object mode (prompted output)."""
    return chat_pb2.ResponseFormat(format_type=chat_pb2.FORMAT_TYPE_JSON_OBJECT)

def _map_model_settings(model_settings: XaiModelSettings) -> dict[str, Any]:
    """Map pydantic_ai ModelSettings to xAI SDK parameters."""
    return {
        _XAI_MODEL_SETTINGS_MAPPING[key]: value
        for key, value in model_settings.items()
        if key in _XAI_MODEL_SETTINGS_MAPPING
    }

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py

def _json_dumps(obj: Any) -> str:
    """Dump an object to JSON string."""
    return to_json(obj).decode('utf-8')

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_utils.py

def load_provider_metadata(provider_metadata: ProviderMetadata | None) -> dict[str, Any]:
    """Load the Pydantic AI metadata from the provider metadata."""
    return provider_metadata.get(PROVIDER_METADATA_KEY, {}) if provider_metadata else {}

## pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py

def _stringify(value: Any) -> str:
    if isinstance(value, str):
        return value
    try:
        # If the value can be serialized to JSON, use that.
        # If that behavior is undesirable, the user could manually call repr on the arguments to the judge_* functions
        return to_json(value).decode()
    except Exception:
        return repr(value)

## pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py

def _set_exporter_context_id(context_id: str | None = None) -> typing.Iterator[str]:
    context_id = context_id or str(uuid.uuid4())
    token = _EXPORTER_CONTEXT_ID.set(context_id)
    try:
        yield context_id
    finally:
        _EXPORTER_CONTEXT_ID.reset(token)

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## pydantic_graph/pydantic_graph/persistence/_utils.py

def set_nodes_type_context(nodes: Sequence[type[BaseNode[Any, Any, Any]]]) -> Iterator[None]:
    token = nodes_type_context.set(nodes)
    try:
        yield
    finally:
        nodes_type_context.reset(token)

## tests/conftest.py

    def set(self, name: str, value: str) -> None:
        self.envars[name] = os.getenv(name)
        os.environ[name] = value

## tests/evals/test_dataset.py

class TaskInput(BaseModel):
    query: str

## tests/evals/test_otel.py

async def test_span_tree_any(span_tree: SpanTree):
    """Test the any() method of SpanTree."""
    # Test existence of a node by name
    assert span_tree.any(lambda node: node.name == 'grandchild2')

    # Test non-existence
    assert not span_tree.any(lambda node: node.name == 'non_existent')

    # Test existence by attribute
    assert span_tree.any(lambda node: node.attributes.get('type') == 'important')

async def test_or_cannot_be_mixed(span_tree: SpanTree):
    with pytest.raises(ValueError) as exc_info:
        span_tree.first({'name_equals': 'child1', 'or_': [SpanQuery(name_equals='child2')]})
    assert str(exc_info.value) == snapshot("Cannot combine 'or_' conditions with other conditions at the same level")

## tests/evals/test_report_evaluators.py

def _make_report(cases: list[ReportCase]) -> EvaluationReport:
    return EvaluationReport(name='test', cases=cases)

## tests/graph/beta/test_edge_cases.py

async def test_graph_with_no_steps():
    """Test a graph with no intermediate steps (direct start to end)."""
    g = GraphBuilder(input_type=int, output_type=int)

    g.add(g.edge_from(g.start_node).to(g.end_node))

    graph = g.build()
    result = await graph.run(inputs=42)
    assert result == 42

## tests/graph/beta/test_graph_builder.py

async def test_empty_graph():
    """Test that a minimal graph can be built and run."""
    g = GraphBuilder(input_type=int, output_type=int)

    g.add(g.edge_from(g.start_node).to(g.end_node))

    graph = g.build()
    result = await graph.run(inputs=42)
    assert result == 42

async def test_explicit_graph_name():
    """Test setting an explicit graph name."""
    g = GraphBuilder(name='ExplicitName', input_type=int, output_type=int)

    g.add(g.edge_from(g.start_node).to(g.end_node))

    graph = g.build()
    assert graph.name == 'ExplicitName'

## tests/graph/beta/test_graph_iteration.py

class IterState:
    counter: int = 0

## tests/json_body_serializer.py

def normalize_body(obj: Any) -> Any:
    """Recursively normalize smart characters in all strings within a data structure."""
    if isinstance(obj, str):
        return normalize_smart_chars(obj)
    elif isinstance(obj, dict):
        return {k: normalize_body(v) for k, v in obj.items()}
    elif isinstance(obj, list):  # pragma: no cover
        return [normalize_body(item) for item in obj]
    return obj  # pragma: no cover

## tests/mcp_server.py

async def get_image_resource() -> EmbeddedResource:
    data = Path(__file__).parent.joinpath('assets/kiwi.jpg').read_bytes()
    return EmbeddedResource(
        type='resource',
        resource=BlobResourceContents(
            uri=AnyUrl('resource://kiwi.jpg'),
            blob=base64.b64encode(data).decode('utf-8'),
            mimeType='image/jpeg',
        ),
    )

## tests/models/anthropic/test_output.py

class CityInfo(BaseModel):
    """Information about a city."""

    city: str
    country: str
    population: int

## tests/models/mock_xai.py

def _get_proto_finish_reason(finish_reason: FinishReason) -> sample_pb2.FinishReason:
    """Map pydantic-ai FinishReason to xAI proto FinishReason."""
    return {
        'stop': sample_pb2.FinishReason.REASON_STOP,
        'length': sample_pb2.FinishReason.REASON_MAX_LEN,
        'tool_call': sample_pb2.FinishReason.REASON_TOOL_CALLS,
        'content_filter': sample_pb2.FinishReason.REASON_STOP,
    }.get(finish_reason, sample_pb2.FinishReason.REASON_STOP)

## tests/models/test_anthropic.py

def test_init_with_provider_string(env: TestEnv):
    env.set('ANTHROPIC_API_KEY', 'env-api-key')
    model = AnthropicModel('claude-3-opus-latest', provider='anthropic')
    assert model.model_name == 'claude-3-opus-latest'
    assert model.client is not None

## tests/models/test_download_item.py

async def test_download_item_raises_user_error_with_unsupported_protocol(
    url: AudioUrl | DocumentUrl | ImageUrl | VideoUrl,
    protocol: str,
) -> None:
    with pytest.raises(ValueError, match=f'URL protocol "{protocol}" is not allowed'):
        _ = await download_item(url, data_format='bytes')

async def test_download_item_raises_user_error_with_youtube_url() -> None:
    with pytest.raises(UserError, match='Downloading YouTube videos is not supported.'):
        _ = await download_item(VideoUrl(url='https://youtu.be/lCdaVNyHtjU'), data_format='bytes')

async def test_download_item_application_octet_stream(disable_ssrf_protection_for_vcr: None) -> None:
    downloaded_item = await download_item(
        VideoUrl(
            url='https://raw.githubusercontent.com/pydantic/pydantic-ai/refs/heads/main/tests/assets/small_video.mp4'
        ),
        data_format='bytes',
    )
    assert downloaded_item['data_type'] == 'video/mp4'
    assert downloaded_item['data'] == IsInstance(bytes)

async def test_download_item_audio_mpeg(disable_ssrf_protection_for_vcr: None) -> None:
    downloaded_item = await download_item(
        AudioUrl(url='https://smokeshow.helpmanual.io/4l1l1s0s6q4741012x1w/common_voice_en_537507.mp3'),
        data_format='bytes',
    )
    assert downloaded_item['data_type'] == 'audio/mpeg'
    assert downloaded_item['data'] == IsInstance(bytes)

async def test_download_item_no_content_type(disable_ssrf_protection_for_vcr: None) -> None:
    downloaded_item = await download_item(
        DocumentUrl(url='https://raw.githubusercontent.com/pydantic/pydantic-ai/refs/heads/main/docs/help.md'),
        data_format='text',
    )
    assert downloaded_item['data_type'] == 'text/markdown'
    assert downloaded_item['data'] == IsStr()

## tests/models/test_gemini.py

class AsyncByteStreamList(httpx.AsyncByteStream):
    data: list[bytes]

    async def __aiter__(self) -> AsyncIterator[bytes]:
        for chunk in self.data:
            yield chunk

def gemini_response(content: _GeminiContent, finish_reason: Literal['STOP'] | None = 'STOP') -> _GeminiResponse:
    candidate = _GeminiCandidates(content=content, index=0, safety_ratings=[])
    if finish_reason:  # pragma: no branch
        candidate['finish_reason'] = finish_reason
    return _GeminiResponse(candidates=[candidate], usage_metadata=example_usage(), model_version='gemini-1.5-flash-123')

## tests/models/test_openai.py

async def test_valid_response(env: TestEnv, allow_model_requests: None):
    """VCR recording is of a valid response."""
    env.set('OPENAI_API_KEY', 'foobar')
    agent = Agent('openai:gpt-4o')

    result = await agent.run('What is the capital of France?')
    assert result.output == snapshot('The capital of France is Paris.')

def test_transformer_adds_properties_to_object_schemas():
    """OpenAI drops object schemas without a 'properties' key. The transformer must add it."""

    schema = {'type': 'object', 'additionalProperties': {'type': 'string'}}
    result = OpenAIJsonSchemaTransformer(schema, strict=None).walk()

    assert result['properties'] == {}

## tests/profiles/test_google.py

def test_const_string_infers_type():
    """When converting const to enum, type should be inferred for string values."""
    schema = {'const': 'hello'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': ['hello'], 'type': 'string'})

def test_const_integer_infers_type():
    """When converting const to enum, type should be inferred for integer values."""
    schema = {'const': 42}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': [42], 'type': 'integer'})

def test_const_float_infers_type():
    """When converting const to enum, type should be inferred for float values."""
    schema = {'const': 3.14}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': [3.14], 'type': 'number'})

def test_const_boolean_infers_type():
    """When converting const to enum, type should be inferred for boolean values."""
    schema = {'const': True}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': [True], 'type': 'boolean'})

def test_const_false_boolean_infers_type():
    """When converting const to enum, type should be inferred for False boolean."""
    schema = {'const': False}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': [False], 'type': 'boolean'})

def test_const_preserves_existing_type():
    """When const has an existing type field, it should be preserved."""
    schema = {'const': 'hello', 'type': 'string'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': ['hello'], 'type': 'string'})

def test_const_array_does_not_infer_type():
    """When const is an array, type cannot be inferred and should not be added."""
    schema = {'const': [1, 2, 3]}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': [[1, 2, 3]]})

def test_removes_schema_field():
    """$schema field should be removed."""
    schema = {'$schema': 'http://json-schema.org/draft-07/schema#', 'type': 'string'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert '$schema' not in transformed
    assert transformed == snapshot({'type': 'string'})

def test_removes_title_field():
    """title field should be removed."""
    schema = {'title': 'MyString', 'type': 'string'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert 'title' not in transformed
    assert transformed == snapshot({'type': 'string'})

def test_removes_discriminator_field():
    """discriminator field should be removed."""
    schema = {'discriminator': {'propertyName': 'type'}, 'type': 'object'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert 'discriminator' not in transformed
    assert transformed == snapshot({'type': 'object'})

def test_removes_examples_field():
    """examples field should be removed."""
    schema = {'examples': ['foo', 'bar'], 'type': 'string'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert 'examples' not in transformed
    assert transformed == snapshot({'type': 'string'})

def test_removes_exclusive_min_max():
    """exclusiveMinimum and exclusiveMaximum should be removed."""
    schema = {'type': 'integer', 'exclusiveMinimum': 0, 'exclusiveMaximum': 100}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert 'exclusiveMinimum' not in transformed
    assert 'exclusiveMaximum' not in transformed
    assert transformed == snapshot({'type': 'integer'})

def test_format_moved_to_description():
    """format should be moved to description for string types."""
    schema = {'type': 'string', 'format': 'date-time'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert 'format' not in transformed
    assert transformed == snapshot({'type': 'string', 'description': 'Format: date-time'})

def test_format_appended_to_existing_description():
    """format should be appended to existing description."""
    schema = {'type': 'string', 'format': 'email', 'description': 'User email address'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert 'format' not in transformed
    assert transformed == snapshot({'type': 'string', 'description': 'User email address (format: email)'})

## tests/providers/test_alibaba_provider.py

def test_alibaba_provider_env_key(env: TestEnv):
    env.set('ALIBABA_API_KEY', 'env-key')
    provider = AlibabaProvider()
    assert provider.client.api_key == 'env-key'

def test_alibaba_provider_dashscope_env_key(env: TestEnv):
    env.remove('ALIBABA_API_KEY')
    env.set('DASHSCOPE_API_KEY', 'dashscope-key')
    provider = AlibabaProvider()
    assert provider.client.api_key == 'dashscope-key'

def test_alibaba_provider_env_key_precedence(env: TestEnv):
    # ALIBABA_API_KEY takes precedence over DASHSCOPE_API_KEY
    env.set('ALIBABA_API_KEY', 'alibaba-key')
    env.set('DASHSCOPE_API_KEY', 'dashscope-key')
    provider = AlibabaProvider()
    assert provider.client.api_key == 'alibaba-key'

def test_infer_provider(env: TestEnv):
    # infer_provider instantiates the class, so we need an env var or it raises UserError
    env.set('ALIBABA_API_KEY', 'key')
    provider = infer_provider('alibaba')
    assert isinstance(provider, AlibabaProvider)

## tests/providers/test_bedrock.py

def test_bedrock_provider(env: TestEnv):
    env.set('AWS_DEFAULT_REGION', 'us-east-1')
    provider = BedrockProvider()
    assert isinstance(provider, BedrockProvider)
    assert provider.name == 'bedrock'
    assert provider.base_url == 'https://bedrock-runtime.us-east-1.amazonaws.com'

def test_bedrock_provider_model_profile_all_geo_prefixes(env: TestEnv, prefix: str):
    """Test that all cross-region inference geo prefixes are correctly handled."""
    env.set('AWS_DEFAULT_REGION', 'us-east-1')
    provider = BedrockProvider()

    model_name = f'{prefix}.anthropic.claude-sonnet-4-5-20250929-v1:0'
    profile = provider.model_profile(model_name)

    assert profile is not None, f'model_profile returned None for {model_name}'

def test_bedrock_provider_model_profile_with_unknown_geo_prefix(env: TestEnv):
    env.set('AWS_DEFAULT_REGION', 'us-east-1')
    provider = BedrockProvider()

    model_name = 'narnia.anthropic.claude-sonnet-4-5-20250929-v1:0'
    profile = provider.model_profile(model_name)
    assert profile is None, f'model_profile returned {profile} for {model_name}'

## tests/providers/test_cerebras.py

def test_infer_cerebras_model(env: TestEnv):
    """Test that infer_model correctly creates a CerebrasModel from a model name string."""
    env.set('CEREBRAS_API_KEY', 'test-api-key')
    model = infer_model('cerebras:llama-3.3-70b')
    assert isinstance(model, CerebrasModel)
    assert model.model_name == 'llama-3.3-70b'

## tests/providers/test_google_gla.py

def test_api_key_arg(env: TestEnv):
    env.set('GEMINI_API_KEY', 'via-env-var')
    provider = GoogleGLAProvider(api_key='via-arg')
    assert provider.client.headers['x-goog-api-key'] == 'via-arg'
    assert provider.client.base_url == 'https://generativelanguage.googleapis.com/v1beta/models/'

def test_api_key_env_var(env: TestEnv):
    env.set('GEMINI_API_KEY', 'via-env-var')
    provider = GoogleGLAProvider()
    assert 'x-goog-api-key' in dict(provider.client.headers)

def test_api_key_empty(env: TestEnv):
    env.set('GEMINI_API_KEY', '')
    with pytest.raises(
        UserError,
        match=re.escape(
            'Set the `GEMINI_API_KEY` environment variable or pass it via `GoogleGLAProvider(api_key=...)`'
        ),
    ):
        GoogleGLAProvider()

## tests/providers/test_sambanova_provider.py

def test_sambanova_provider_env_key(env: TestEnv):
    env.set('SAMBANOVA_API_KEY', 'env-key')
    provider = SambaNovaProvider()
    assert provider.client.api_key == 'env-key'

def test_infer_provider(env: TestEnv):
    # infer_provider instantiates the class, so we need an env var or it raises UserError
    env.set('SAMBANOVA_API_KEY', 'key')
    provider = infer_provider('sambanova')
    assert isinstance(provider, SambaNovaProvider)

def test_sambanova_provider_env_base_url(env: TestEnv):
    env.set('SAMBANOVA_API_KEY', 'key')
    env.set('SAMBANOVA_BASE_URL', 'https://env.endpoint.com/v1')
    provider = SambaNovaProvider()
    assert provider.base_url == 'https://env.endpoint.com/v1'

## tests/test_ag_ui.py

def current_time() -> str:
    """Get the current time in ISO format.

    Returns:
        The current UTC time in ISO format string.
    """
    return '2023-06-21T12:08:45.485981+00:00'

## tests/test_agent.py

def test_model_requests_blocked(env: TestEnv):
    try:
        env.set('GEMINI_API_KEY', 'foobar')
        agent = Agent('google-gla:gemini-3-flash-preview', output_type=tuple[str, str], defer_model_check=True)

        with pytest.raises(RuntimeError, match='Model requests are not allowed, since ALLOW_MODEL_REQUESTS is False'):
            agent.run_sync('Hello')
    except ImportError:  # pragma: lax no cover
        pytest.skip('google-genai not installed')

def test_override_model(env: TestEnv):
    env.set('GEMINI_API_KEY', 'foobar')
    agent = Agent('google-gla:gemini-3-flash-preview', output_type=tuple[int, str], defer_model_check=True)

    with agent.override(model='test'):
        result = agent.run_sync('Hello')
        assert result.output == snapshot((0, 'a'))

def test_set_model(env: TestEnv):
    env.set('GEMINI_API_KEY', 'foobar')
    agent = Agent(output_type=tuple[int, str])

    agent.model = 'test'

    result = agent.run_sync('Hello')
    assert result.output == snapshot((0, 'a'))

## tests/test_cli.py

def test_cli_prompt(capfd: CaptureFixture[str], env: TestEnv):
    env.set('OPENAI_API_KEY', 'test')
    with cli_agent.override(model=TestModel(custom_output_text='# result\n\n```py\nx = 1\n```')):
        assert cli(['hello']) == 0
        assert capfd.readouterr().out.splitlines() == snapshot([IsStr(), '# result', '', 'py', 'x = 1', '/py'])
        assert cli(['--no-stream', 'hello']) == 0
        assert capfd.readouterr().out.splitlines() == snapshot([IsStr(), '# result', '', 'py', 'x = 1', '/py'])

def test_code_theme_unset(mocker: MockerFixture, env: TestEnv):
    env.set('OPENAI_API_KEY', 'test')
    mock_run_chat = mocker.patch('pydantic_ai._cli.run_chat')
    cli([])
    mock_run_chat.assert_awaited_once_with(True, IsInstance(Agent), IsInstance(Console), 'monokai', 'clai')

def test_code_theme_light(mocker: MockerFixture, env: TestEnv):
    env.set('OPENAI_API_KEY', 'test')
    mock_run_chat = mocker.patch('pydantic_ai._cli.run_chat')
    cli(['--code-theme=light'])
    mock_run_chat.assert_awaited_once_with(True, IsInstance(Agent), IsInstance(Console), 'default', 'clai')

def test_code_theme_dark(mocker: MockerFixture, env: TestEnv):
    env.set('OPENAI_API_KEY', 'test')
    mock_run_chat = mocker.patch('pydantic_ai._cli.run_chat')
    cli(['--code-theme=dark'])
    mock_run_chat.assert_awaited_once_with(True, IsInstance(Agent), IsInstance(Console), 'monokai', 'clai')

## tests/test_prefect.py

class SimpleDeps:
    value: str

## tests/test_settings.py

def test_specific_prefix_settings(settings: tuple[type[ModelSettings], str]):
    settings_cls, prefix = settings
    global_settings = set(ModelSettings.__annotations__.keys())
    specific_settings = set(settings_cls.__annotations__.keys()) - global_settings
    assert all(setting.startswith(prefix) for setting in specific_settings), (
        f'{prefix} is not a prefix for {specific_settings}'
    )

## tests/test_ssrf.py

    async def test_6to4_address_blocked(self) -> None:
        """Test that 6to4 addresses (which can embed private IPv4) are blocked."""
        # 2002:c0a8:0101::1 embeds 192.168.1.1
        with pytest.raises(ValueError, match='Access to private/internal IP address'):
            await validate_and_resolve_url('http://[2002:c0a8:0101::1]/path', allow_local=False)

## tests/test_temporal.py

    async def decode(
        self, payloads: Sequence[temporalio.api.common.v1.Payload]
    ) -> list[temporalio.api.common.v1.Payload]:  # pragma: no cover
        return list(payloads)

## tests/test_tools.py

async def get_json_schema(_messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
    if len(info.function_tools) == 1:
        r = info.function_tools[0]
        return ModelResponse(parts=[TextPart(pydantic_core.to_json(r).decode())])
    else:
        return ModelResponse(parts=[TextPart(pydantic_core.to_json(info.function_tools).decode())])
