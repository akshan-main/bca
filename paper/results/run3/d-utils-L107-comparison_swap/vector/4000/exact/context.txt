# pydantic_ai_slim/pydantic_ai/output.py:296-360
def StructuredDict(
    json_schema: JsonSchemaValue, name: str | None = None, description: str | None = None
) -> type[JsonSchemaValue]:
    """Returns a `dict[str, Any]` subclass with a JSON schema attached that will be used for structured output.

    Args:
        json_schema: A JSON schema of type `object` defining the structure of the dictionary content.
        name: Optional name of the structured output. If not provided, the `title` field of the JSON schema will be used if it's present.
        description: Optional description of the structured output. If not provided, the `description` field of the JSON schema will be used if it's present.

    Example:
    ```python {title="structured_dict.py"}
    from pydantic_ai import Agent, StructuredDict

    schema = {
        'type': 'object',
        'properties': {
            'name': {'type': 'string'},
            'age': {'type': 'integer'}
        },
        'required': ['name', 'age']
    }

    agent = Agent('openai:gpt-5.2', output_type=StructuredDict(schema))
    result = agent.run_sync('Create a person')
    print(result.output)
    #> {'name': 'John Doe', 'age': 30}
    ```
    """
    json_schema = _utils.check_object_json_schema(json_schema)

    # Pydantic `TypeAdapter` fails when `object.__get_pydantic_json_schema__` has `$defs`, so we inline them
    # See https://github.com/pydantic/pydantic/issues/12145
    if '$defs' in json_schema:
        json_schema = InlineDefsJsonSchemaTransformer(json_schema).walk()
        if '$defs' in json_schema:
            raise exceptions.UserError(
                '`StructuredDict` does not currently support recursive `$ref`s and `$defs`. See https://github.com/pydantic/pydantic/issues/12145 for more information.'
            )

    if name:
        json_schema['title'] = name

    if description:
        json_schema['description'] = description

    class _StructuredDict(JsonSchemaValue):
        __is_model_like__ = True

        @classmethod
        def __get_pydantic_core_schema__(
            cls, source_type: Any, handler: GetCoreSchemaHandler
        ) -> core_schema.CoreSchema:
            return core_schema.dict_schema(
                keys_schema=core_schema.str_schema(),
                values_schema=core_schema.any_schema(),
            )

        @classmethod
        def __get_pydantic_json_schema__(
            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler
        ) -> JsonSchemaValue:
            return json_schema

    return _StructuredDict

# pydantic_ai_slim/pydantic_ai/result.py:152-154
    def get(self) -> _messages.ModelResponse:
        """Get the current state of the response."""
        return self._raw_stream_response.get()

# tests/models/test_model_names.py:142-142
    object: Literal['model']

# tests/test_usage_limits.py:3-3
import operator

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:35-36
    def get(self) -> ModelResponse:
        return self.response

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:44-45
    def get(self) -> ModelResponse:
        return self.response

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:53-54
    def get(self) -> ModelResponse:
        return self.response

# pydantic_evals/pydantic_evals/dataset.py:778-825
    def model_json_schema_with_evaluators(
        cls,
        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
        custom_report_evaluator_types: Sequence[type[ReportEvaluator[InputsT, OutputT, MetadataT]]] = (),
    ) -> dict[str, Any]:
        """Generate a JSON schema for this dataset type, including evaluator details.

        This is useful for generating a schema that can be used to validate YAML-format dataset files.

        Args:
            custom_evaluator_types: Custom evaluator classes to include in the schema.
            custom_report_evaluator_types: Custom report evaluator classes to include in the schema.

        Returns:
            A dictionary representing the JSON schema.
        """
        evaluator_schema_types = _build_evaluator_schema_types(
            _get_evaluator_registry(custom_evaluator_types, Evaluator, DEFAULT_EVALUATORS, 'evaluator')
        )
        report_evaluator_schema_types = _build_evaluator_schema_types(
            _get_evaluator_registry(
                custom_report_evaluator_types, ReportEvaluator, DEFAULT_REPORT_EVALUATORS, 'report evaluator'
            )
        )

        in_type, out_type, meta_type = cls._params()

        # Note: we shadow the `Case` and `Dataset` class names here to generate a clean JSON schema
        class Case(BaseModel, extra='forbid'):  # pyright: ignore[reportUnusedClass]  # this _is_ used below, but pyright doesn't seem to notice..
            name: str | None = None
            inputs: in_type  # pyright: ignore[reportInvalidTypeForm]
            metadata: meta_type | None = None  # pyright: ignore[reportInvalidTypeForm]
            expected_output: out_type | None = None  # pyright: ignore[reportInvalidTypeForm]
            if evaluator_schema_types:  # pragma: no branch
                evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007

        class Dataset(BaseModel, extra='forbid'):
            name: str | None = None
            cases: list[Case]
            if evaluator_schema_types:  # pragma: no branch
                evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007
            if report_evaluator_schema_types:  # pragma: no branch
                report_evaluators: list[Union[tuple(report_evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007

        json_schema = Dataset.model_json_schema()
        # See `_add_json_schema` below, since `$schema` is added to the JSON, it has to be supported in the JSON
        json_schema['properties']['$schema'] = {'type': 'string'}
        return json_schema

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:20-20
    type: Literal['tool_call']

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:15-15
    type: Literal['text']

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:47-47
    type: Literal['thinking']

# tests/example_modules/fake_database.py:8-10
    def get(self, name: str) -> int | None:
        if name == 'John Doe':
            return 123

# pydantic_evals/pydantic_evals/dataset.py:828-845
    def _save_schema(
        cls,
        path: Path | str,
        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
        custom_report_evaluator_types: Sequence[type[ReportEvaluator[InputsT, OutputT, MetadataT]]] = (),
    ):
        """Save the JSON schema for this dataset type to a file.

        Args:
            path: Path to save the schema to.
            custom_evaluator_types: Custom evaluator classes to include in the schema.
            custom_report_evaluator_types: Custom report evaluator classes to include in the schema.
        """
        path = Path(path)
        json_schema = cls.model_json_schema_with_evaluators(custom_evaluator_types, custom_report_evaluator_types)
        schema_content = to_json(json_schema, indent=2).decode() + '\n'
        if not path.exists() or path.read_text(encoding='utf-8') != schema_content:  # pragma: no branch
            path.write_text(schema_content, encoding='utf-8')

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:152-152
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:139-139
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:192-192
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:90-90
    type: Literal['tool-input-start'] = 'tool-input-start'

# pydantic_evals/pydantic_evals/reporting/analyses.py:21-21
    type: Literal['confusion_matrix'] = 'confusion_matrix'

# pydantic_evals/pydantic_evals/reporting/analyses.py:52-52
    type: Literal['precision_recall'] = 'precision_recall'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:78-78
    type: Literal['file'] = 'file'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:128-128
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:205-205
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:124-124
    type: Literal['tool-input-available'] = 'tool-input-available'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:136-136
    type: Literal['tool-input-error'] = 'tool-input-error'

# pydantic_evals/pydantic_evals/reporting/analyses.py:62-62
    type: Literal['scalar'] = 'scalar'

# pydantic_evals/pydantic_evals/reporting/analyses.py:73-73
    type: Literal['table'] = 'table'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:27-27
    type: Literal['text'] = 'text'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:67-67
    type: Literal['source-document'] = 'source-document'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:117-117
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:170-170
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:181-181
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:113-113
    type: Literal['tool-output-available'] = 'tool-output-available'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:42-42
    type: Literal['reasoning'] = 'reasoning'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:57-57
    type: Literal['source-url'] = 'source-url'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:149-149
    type: Literal['tool-output-error'] = 'tool-output-error'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:184-184
    type: Literal['source-document'] = 'source-document'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:203-203
    type: Annotated[str, Field(pattern=r'^data-')]

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:28-28
    type: Literal['tool_call_response']

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:41-41
    type: Literal['text-delta'] = 'text-delta'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:66-66
    type: Literal['reasoning-delta'] = 'reasoning-delta'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:174-174
    type: Literal['source-url'] = 'source-url'

# pydantic_ai_slim/pydantic_ai/direct.py:380-382
    def get(self) -> messages.ModelResponse:
        """Build a ModelResponse from the data received from the stream so far."""
        return self._ensure_stream_ready().get()

# pydantic_ai_slim/pydantic_ai/models/groq.py:665-665
    type: Literal['invalid_request_error']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:258-258
    type: Literal['reasoning.text', 'reasoning.summary', 'reasoning.encrypted']

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:108-108
    type: Annotated[str, Field(pattern=r'^data-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:33-33
    type: Literal['text-start'] = 'text-start'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:50-50
    type: Literal['text-end'] = 'text-end'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:58-58
    type: Literal['reasoning-start'] = 'reasoning-start'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:75-75
    type: Literal['reasoning-end'] = 'reasoning-end'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:105-105
    type: Literal['tool-input-delta'] = 'tool-input-delta'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:195-195
    type: Literal['file'] = 'file'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:224-224
    type: Literal['start'] = 'start'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:232-232
    type: Literal['finish'] = 'finish'

# pydantic_graph/pydantic_graph/beta/step.py:90-112
class StreamFunction(Protocol[StateT, DepsT, InputT, OutputT]):
    """Protocol for stream functions that can be executed in the graph.

    Stream functions are async callables that receive a step context and return an async iterator.

    Type Parameters:
        StateT: The type of the graph state
        DepsT: The type of the dependencies
        InputT: The type of the input data
        OutputT: The type of the output data
    """

    def __call__(self, ctx: StepContext[StateT, DepsT, InputT]) -> AsyncIterator[OutputT]:
        """Execute the stream function with the given context.

        Args:
            ctx: The step context containing state, dependencies, and inputs

        Returns:
            An async iterator yielding the streamed output
        """
        raise NotImplementedError
        yield

# pydantic_graph/pydantic_graph/graph.py:74-104
    def __init__(
        self,
        *,
        nodes: Sequence[type[BaseNode[StateT, DepsT, RunEndT]]],
        name: str | None = None,
        state_type: type[StateT] | _utils.Unset = _utils.UNSET,
        run_end_type: type[RunEndT] | _utils.Unset = _utils.UNSET,
        auto_instrument: bool = True,
    ):
        """Create a graph from a sequence of nodes.

        Args:
            nodes: The nodes which make up the graph, nodes need to be unique and all be generic in the same
                state type.
            name: Optional name for the graph, if not provided the name will be inferred from the calling frame
                on the first call to a graph method.
            state_type: The type of the state for the graph, this can generally be inferred from `nodes`.
            run_end_type: The type of the result of running the graph, this can generally be inferred from `nodes`.
            auto_instrument: Whether to create a span for the graph run and the execution of each node's run method.
        """
        self.name = name
        self._state_type = state_type
        self._run_end_type = run_end_type
        self.auto_instrument = auto_instrument

        parent_namespace = _utils.get_parent_namespace(inspect.currentframe())
        self.node_defs = {}
        for node in nodes:
            self._register_node(node, parent_namespace)

        self._validate_edges()

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:41-41
    type: Literal['binary']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:278-278
    type: Literal['reasoning.text']

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:83-83
    type: Literal['error'] = 'error'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:159-159
    type: Literal['tool-approval-request'] = 'tool-approval-request'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:254-254
    type: Literal['done'] = 'done'

# tests/models/test_model_names.py:121-121
    type: list[str]

# pydantic_graph/pydantic_graph/beta/step.py:66-87
class StepFunction(Protocol[StateT, DepsT, InputT, OutputT]):
    """Protocol for step functions that can be executed in the graph.

    Step functions are async callables that receive a step context and return a result.

    Type Parameters:
        StateT: The type of the graph state
        DepsT: The type of the dependencies
        InputT: The type of the input data
        OutputT: The type of the output data
    """

    def __call__(self, ctx: StepContext[StateT, DepsT, InputT]) -> Awaitable[OutputT]:
        """Execute the step function with the given context.

        Args:
            ctx: The step context containing state, dependencies, and inputs

        Returns:
            An awaitable that resolves to the step's output
        """
        raise NotImplementedError

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:36-36
    type: Literal['image-url', 'audio-url', 'video-url', 'document-url']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:264-264
    type: Literal['reasoning.summary']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:271-271
    type: Literal['reasoning.encrypted']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:356-356
    type: Literal['file']

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:102-102
    type: Literal['step-start'] = 'step-start'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:167-167
    type: Literal['tool-output-denied'] = 'tool-output-denied'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:212-212
    type: Literal['start-step'] = 'start-step'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:218-218
    type: Literal['finish-step'] = 'finish-step'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:240-240
    type: Literal['abort'] = 'abort'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:247-247
    type: Literal['message-metadata'] = 'message-metadata'

# examples/pydantic_ai_examples/data_analyst.py:20-25
    def get(self, ref: str) -> pd.DataFrame:
        if ref not in self.output:
            raise ModelRetry(
                f'Error: {ref} is not a valid variable reference. Check the previous messages and try again.'
            )
        return self.output[ref]

# pydantic_evals/pydantic_evals/evaluators/common.py:35-36
    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:
        return ctx.output == self.value

# pydantic_evals/pydantic_evals/evaluators/common.py:263-267
    def evaluate(
        self,
        ctx: EvaluatorContext[object, object, object],
    ) -> bool:
        return ctx.span_tree.any(self.query)

# examples/pydantic_ai_examples/evals/custom_evaluators.py:55-66
    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:
        return ctx.span_tree.any(
            SpanQuery(
                name_equals='agent run',
                has_attributes={'agent_name': self.agent_name},
                stop_recursing_when=SpanQuery(name_equals='agent run'),
                some_descendant_has=SpanQuery(
                    name_equals='running tool',
                    has_attributes={'gen_ai.tool.name': self.tool_name},
                ),
            )
        )

# pydantic_evals/pydantic_evals/evaluators/common.py:156-161
    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:
        duration = timedelta(seconds=ctx.duration)
        seconds = self.seconds
        if not isinstance(seconds, timedelta):
            seconds = timedelta(seconds=seconds)
        return duration <= seconds

# pydantic_evals/pydantic_evals/evaluators/common.py:78-128
    def evaluate(
        self,
        ctx: EvaluatorContext[object, object, object],
    ) -> EvaluationReason:
        # Convert objects to strings if requested
        failure_reason: str | None = None
        as_strings = self.as_strings or (isinstance(self.value, str) and isinstance(ctx.output, str))
        if as_strings:
            output_str = str(ctx.output)
            expected_str = str(self.value)

            if not self.case_sensitive:
                output_str = output_str.lower()
                expected_str = expected_str.lower()

            failure_reason: str | None = None
            if expected_str not in output_str:
                output_trunc = _truncated_repr(output_str, max_length=100)
                expected_trunc = _truncated_repr(expected_str, max_length=100)
                failure_reason = f'Output string {output_trunc} does not contain expected string {expected_trunc}'
            return EvaluationReason(value=failure_reason is None, reason=failure_reason)

        try:
            # Handle different collection types
            if isinstance(ctx.output, dict):
                if isinstance(self.value, dict):
                    # Cast to Any to avoid type checking issues
                    output_dict = cast(dict[Any, Any], ctx.output)  # pyright: ignore[reportUnknownMemberType]
                    expected_dict = cast(dict[Any, Any], self.value)  # pyright: ignore[reportUnknownMemberType]
                    for k in expected_dict:
                        if k not in output_dict:
                            k_trunc = _truncated_repr(k, max_length=30)
                            failure_reason = f'Output dictionary does not contain expected key {k_trunc}'
                            break
                        elif output_dict[k] != expected_dict[k]:
                            k_trunc = _truncated_repr(k, max_length=30)
                            output_v_trunc = _truncated_repr(output_dict[k], max_length=100)
                            expected_v_trunc = _truncated_repr(expected_dict[k], max_length=100)
                            failure_reason = f'Output dictionary has different value for key {k_trunc}: {output_v_trunc} != {expected_v_trunc}'
                            break
                else:
                    if self.value not in ctx.output:  # pyright: ignore[reportUnknownMemberType]
                        output_trunc = _truncated_repr(ctx.output, max_length=200)  # pyright: ignore[reportUnknownMemberType]
                        failure_reason = f'Output {output_trunc} does not contain provided value as a key'
            elif self.value not in ctx.output:  # pyright: ignore[reportOperatorIssue]  # will be handled by except block
                output_trunc = _truncated_repr(ctx.output, max_length=200)
                failure_reason = f'Output {output_trunc} does not contain provided value'
        except (TypeError, ValueError) as e:
            failure_reason = f'Containment check failed: {e}'

        return EvaluationReason(value=failure_reason is None, reason=failure_reason)

# pydantic_evals/pydantic_evals/evaluators/common.py:138-147
    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> EvaluationReason:
        output = ctx.output
        for cls in type(output).__mro__:
            if cls.__name__ == self.type_name or cls.__qualname__ == self.type_name:
                return EvaluationReason(value=True)

        reason = f'output is of type {type(output).__name__}'
        if type(output).__qualname__ != type(output).__name__:
            reason += f' (qualname: {type(output).__qualname__})'
        return EvaluationReason(value=False, reason=reason)

# pydantic_ai_slim/pydantic_ai/models/__init__.py:1014-1026
    def get(self) -> ModelResponse:
        """Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far."""
        return ModelResponse(
            parts=self._parts_manager.get_parts(),
            model_name=self.model_name,
            timestamp=self.timestamp,
            usage=self.usage(),
            provider_name=self.provider_name,
            provider_url=self.provider_url,
            provider_response_id=self.provider_response_id,
            provider_details=self.provider_details,
            finish_reason=self.finish_reason,
        )

# pydantic_evals/pydantic_evals/evaluators/common.py:45-48
    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool | dict[str, bool]:
        if ctx.expected_output is None:
            return {}  # Only compare if expected output is provided
        return ctx.output == ctx.expected_output

# pydantic_ai_slim/pydantic_ai/_output.py:18-18
from . import _function_schema, _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_run_context.py:15-15
from . import _utils, messages as _messages

# pydantic_ai_slim/pydantic_ai/_system_prompt.py:8-8
from . import _utils

# pydantic_ai_slim/pydantic_ai/agent/wrapper.py:7-12
from .. import (
    _utils,
    messages as _messages,
    models,
    usage as _usage,
)