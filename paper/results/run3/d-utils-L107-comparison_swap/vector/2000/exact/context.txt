# pydantic_ai_slim/pydantic_ai/output.py:296-360
def StructuredDict(
    json_schema: JsonSchemaValue, name: str | None = None, description: str | None = None
) -> type[JsonSchemaValue]:
    """Returns a `dict[str, Any]` subclass with a JSON schema attached that will be used for structured output.

    Args:
        json_schema: A JSON schema of type `object` defining the structure of the dictionary content.
        name: Optional name of the structured output. If not provided, the `title` field of the JSON schema will be used if it's present.
        description: Optional description of the structured output. If not provided, the `description` field of the JSON schema will be used if it's present.

    Example:
    ```python {title="structured_dict.py"}
    from pydantic_ai import Agent, StructuredDict

    schema = {
        'type': 'object',
        'properties': {
            'name': {'type': 'string'},
            'age': {'type': 'integer'}
        },
        'required': ['name', 'age']
    }

    agent = Agent('openai:gpt-5.2', output_type=StructuredDict(schema))
    result = agent.run_sync('Create a person')
    print(result.output)
    #> {'name': 'John Doe', 'age': 30}
    ```
    """
    json_schema = _utils.check_object_json_schema(json_schema)

    # Pydantic `TypeAdapter` fails when `object.__get_pydantic_json_schema__` has `$defs`, so we inline them
    # See https://github.com/pydantic/pydantic/issues/12145
    if '$defs' in json_schema:
        json_schema = InlineDefsJsonSchemaTransformer(json_schema).walk()
        if '$defs' in json_schema:
            raise exceptions.UserError(
                '`StructuredDict` does not currently support recursive `$ref`s and `$defs`. See https://github.com/pydantic/pydantic/issues/12145 for more information.'
            )

    if name:
        json_schema['title'] = name

    if description:
        json_schema['description'] = description

    class _StructuredDict(JsonSchemaValue):
        __is_model_like__ = True

        @classmethod
        def __get_pydantic_core_schema__(
            cls, source_type: Any, handler: GetCoreSchemaHandler
        ) -> core_schema.CoreSchema:
            return core_schema.dict_schema(
                keys_schema=core_schema.str_schema(),
                values_schema=core_schema.any_schema(),
            )

        @classmethod
        def __get_pydantic_json_schema__(
            cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler
        ) -> JsonSchemaValue:
            return json_schema

    return _StructuredDict

# pydantic_ai_slim/pydantic_ai/result.py:152-154
    def get(self) -> _messages.ModelResponse:
        """Get the current state of the response."""
        return self._raw_stream_response.get()

# tests/models/test_model_names.py:142-142
    object: Literal['model']

# tests/test_usage_limits.py:3-3
import operator

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:35-36
    def get(self) -> ModelResponse:
        return self.response

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:44-45
    def get(self) -> ModelResponse:
        return self.response

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:53-54
    def get(self) -> ModelResponse:
        return self.response

# pydantic_evals/pydantic_evals/dataset.py:778-825
    def model_json_schema_with_evaluators(
        cls,
        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
        custom_report_evaluator_types: Sequence[type[ReportEvaluator[InputsT, OutputT, MetadataT]]] = (),
    ) -> dict[str, Any]:
        """Generate a JSON schema for this dataset type, including evaluator details.

        This is useful for generating a schema that can be used to validate YAML-format dataset files.

        Args:
            custom_evaluator_types: Custom evaluator classes to include in the schema.
            custom_report_evaluator_types: Custom report evaluator classes to include in the schema.

        Returns:
            A dictionary representing the JSON schema.
        """
        evaluator_schema_types = _build_evaluator_schema_types(
            _get_evaluator_registry(custom_evaluator_types, Evaluator, DEFAULT_EVALUATORS, 'evaluator')
        )
        report_evaluator_schema_types = _build_evaluator_schema_types(
            _get_evaluator_registry(
                custom_report_evaluator_types, ReportEvaluator, DEFAULT_REPORT_EVALUATORS, 'report evaluator'
            )
        )

        in_type, out_type, meta_type = cls._params()

        # Note: we shadow the `Case` and `Dataset` class names here to generate a clean JSON schema
        class Case(BaseModel, extra='forbid'):  # pyright: ignore[reportUnusedClass]  # this _is_ used below, but pyright doesn't seem to notice..
            name: str | None = None
            inputs: in_type  # pyright: ignore[reportInvalidTypeForm]
            metadata: meta_type | None = None  # pyright: ignore[reportInvalidTypeForm]
            expected_output: out_type | None = None  # pyright: ignore[reportInvalidTypeForm]
            if evaluator_schema_types:  # pragma: no branch
                evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007

        class Dataset(BaseModel, extra='forbid'):
            name: str | None = None
            cases: list[Case]
            if evaluator_schema_types:  # pragma: no branch
                evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007
            if report_evaluator_schema_types:  # pragma: no branch
                report_evaluators: list[Union[tuple(report_evaluator_schema_types)]] = []  # pyright: ignore  # noqa: UP007

        json_schema = Dataset.model_json_schema()
        # See `_add_json_schema` below, since `$schema` is added to the JSON, it has to be supported in the JSON
        json_schema['properties']['$schema'] = {'type': 'string'}
        return json_schema

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:20-20
    type: Literal['tool_call']

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:15-15
    type: Literal['text']

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:47-47
    type: Literal['thinking']

# tests/example_modules/fake_database.py:8-10
    def get(self, name: str) -> int | None:
        if name == 'John Doe':
            return 123

# pydantic_evals/pydantic_evals/dataset.py:828-845
    def _save_schema(
        cls,
        path: Path | str,
        custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),
        custom_report_evaluator_types: Sequence[type[ReportEvaluator[InputsT, OutputT, MetadataT]]] = (),
    ):
        """Save the JSON schema for this dataset type to a file.

        Args:
            path: Path to save the schema to.
            custom_evaluator_types: Custom evaluator classes to include in the schema.
            custom_report_evaluator_types: Custom report evaluator classes to include in the schema.
        """
        path = Path(path)
        json_schema = cls.model_json_schema_with_evaluators(custom_evaluator_types, custom_report_evaluator_types)
        schema_content = to_json(json_schema, indent=2).decode() + '\n'
        if not path.exists() or path.read_text(encoding='utf-8') != schema_content:  # pragma: no branch
            path.write_text(schema_content, encoding='utf-8')

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:152-152
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:139-139
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:192-192
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:90-90
    type: Literal['tool-input-start'] = 'tool-input-start'

# pydantic_evals/pydantic_evals/reporting/analyses.py:21-21
    type: Literal['confusion_matrix'] = 'confusion_matrix'

# pydantic_evals/pydantic_evals/reporting/analyses.py:52-52
    type: Literal['precision_recall'] = 'precision_recall'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:78-78
    type: Literal['file'] = 'file'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:128-128
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:205-205
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:124-124
    type: Literal['tool-input-available'] = 'tool-input-available'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:136-136
    type: Literal['tool-input-error'] = 'tool-input-error'

# pydantic_evals/pydantic_evals/reporting/analyses.py:62-62
    type: Literal['scalar'] = 'scalar'

# pydantic_evals/pydantic_evals/reporting/analyses.py:73-73
    type: Literal['table'] = 'table'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:27-27
    type: Literal['text'] = 'text'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:67-67
    type: Literal['source-document'] = 'source-document'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:117-117
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:170-170
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:181-181
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:113-113
    type: Literal['tool-output-available'] = 'tool-output-available'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:42-42
    type: Literal['reasoning'] = 'reasoning'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:57-57
    type: Literal['source-url'] = 'source-url'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:149-149
    type: Literal['tool-output-error'] = 'tool-output-error'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:184-184
    type: Literal['source-document'] = 'source-document'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:203-203
    type: Annotated[str, Field(pattern=r'^data-')]

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:28-28
    type: Literal['tool_call_response']

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:41-41
    type: Literal['text-delta'] = 'text-delta'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:66-66
    type: Literal['reasoning-delta'] = 'reasoning-delta'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:174-174
    type: Literal['source-url'] = 'source-url'

# pydantic_ai_slim/pydantic_ai/direct.py:380-382
    def get(self) -> messages.ModelResponse:
        """Build a ModelResponse from the data received from the stream so far."""
        return self._ensure_stream_ready().get()

# pydantic_ai_slim/pydantic_ai/models/groq.py:665-665
    type: Literal['invalid_request_error']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:258-258
    type: Literal['reasoning.text', 'reasoning.summary', 'reasoning.encrypted']

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:108-108
    type: Annotated[str, Field(pattern=r'^data-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:33-33
    type: Literal['text-start'] = 'text-start'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:50-50
    type: Literal['text-end'] = 'text-end'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:58-58
    type: Literal['reasoning-start'] = 'reasoning-start'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:75-75
    type: Literal['reasoning-end'] = 'reasoning-end'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:105-105
    type: Literal['tool-input-delta'] = 'tool-input-delta'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:195-195
    type: Literal['file'] = 'file'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:224-224
    type: Literal['start'] = 'start'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:232-232
    type: Literal['finish'] = 'finish'

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:41-41
    type: Literal['binary']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:278-278
    type: Literal['reasoning.text']

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:83-83
    type: Literal['error'] = 'error'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:159-159
    type: Literal['tool-approval-request'] = 'tool-approval-request'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:254-254
    type: Literal['done'] = 'done'

# tests/models/test_model_names.py:121-121
    type: list[str]

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:36-36
    type: Literal['image-url', 'audio-url', 'video-url', 'document-url']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:264-264
    type: Literal['reasoning.summary']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:271-271
    type: Literal['reasoning.encrypted']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:356-356
    type: Literal['file']