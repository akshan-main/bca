## clai/clai/__init__.py

def cli():
    """Run the clai CLI and exit."""
    _cli.cli_exit('clai')

## examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py

from pydantic import BaseModel, Field

## examples/pydantic_ai_examples/chat_app.py

async def index() -> FileResponse:
    return FileResponse((THIS_DIR / 'chat_app.html'), media_type='text/html')

    timestamp: str

## examples/pydantic_ai_examples/evals/agent.py

    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

## examples/pydantic_ai_examples/flight_booking.py

    date: datetime.date

## examples/pydantic_ai_examples/question_graph.py

ask_agent = Agent('openai:gpt-5.2', output_type=str)

## examples/pydantic_ai_examples/rag.py

    id: int

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

    get_instructions: Callable[[RunContext[DepsT]], Awaitable[str | None]]

    request: _messages.ModelRequest

    model_response: _messages.ModelResponse

def get_captured_run_messages() -> _RunMessages:
    return _messages_ctx_var.get()

## pydantic_ai_slim/pydantic_ai/_function_schema.py

    json_schema: ObjectJsonSchema

## pydantic_ai_slim/pydantic_ai/_otel_messages.py

    type: Literal['text']

## pydantic_ai_slim/pydantic_ai/_output.py

    toolset: OutputToolset[Any] | None = None

## pydantic_ai_slim/pydantic_ai/_utils.py

def is_set(t_or_unset: T | Unset) -> TypeGuard[T]:
    return t_or_unset is not UNSET

def get_traceparent(x: AgentRun | AgentRunResult | GraphRun | GraphRunResult) -> str:
    return x._traceparent(required=False) or ''  # type: ignore[reportPrivateUsage]

def number_to_datetime(x: int | float) -> datetime:
    return _datetime_ta.validate_python(x)

## pydantic_ai_slim/pydantic_ai/agent/__init__.py

    instrument: InstrumentationSettings | bool | None

## pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py

    def __init__(
        self,
        wrapped: EmbeddingModel | str,
        options: InstrumentationSettings | None = None,
    ) -> None:
        super().__init__(wrapped)
        self.instrumentation_settings = options or InstrumentationSettings()

## pydantic_ai_slim/pydantic_ai/embeddings/settings.py

def merge_embedding_settings(
    base: EmbeddingSettings | None, overrides: EmbeddingSettings | None
) -> EmbeddingSettings | None:
    """Merge two sets of embedding settings, with overrides taking precedence.

    Args:
        base: Base settings (typically from the embedder or model).
        overrides: Settings that should override the base (typically per-call settings).

    Returns:
        Merged settings, or `None` if both inputs are `None`.
    """
    # Note: we may want merge recursively if/when we add non-primitive values
    if base and overrides:
        return base | overrides
    else:
        return base or overrides

## pydantic_ai_slim/pydantic_ai/mcp.py

    cwd: str | Path | None

## pydantic_ai_slim/pydantic_ai/models/function.py

    function: FunctionDef | None

    stream_function: StreamFunctionDef | None

## pydantic_ai_slim/pydantic_ai/models/gemini.py

def _function_call_part_from_call(tool: ToolCallPart) -> _GeminiFunctionCallPart:
    return _GeminiFunctionCallPart(function_call=_GeminiFunctionCall(name=tool.tool_name, args=tool.args_as_dict()))

## pydantic_ai_slim/pydantic_ai/models/groq.py

    error: _GroqToolUseFailedInnerError

## pydantic_ai_slim/pydantic_ai/models/instrumented.py

    def __init__(
        self,
        wrapped: Model | KnownModelName,
        options: InstrumentationSettings | None = None,
    ) -> None:
        super().__init__(wrapped)
        self.instrumentation_settings = options or InstrumentationSettings()

## pydantic_ai_slim/pydantic_ai/models/openrouter.py

    sort: Literal['price', 'throughput', 'latency']

## pydantic_ai_slim/pydantic_ai/retries.py

    sleep: Callable[[int | float], None | Awaitable[None]]

    wait: WaitBaseT

## pydantic_ai_slim/pydantic_ai/settings.py

def merge_model_settings(base: ModelSettings | None, overrides: ModelSettings | None) -> ModelSettings | None:
    """Merge two sets of model settings, preferring the overrides.

    A common use case is: merge_model_settings(<agent settings>, <run settings>)
    """
    # Note: we may want merge recursively if/when we add non-primitive values
    if base and overrides:
        return base | overrides
    else:
        return base or overrides

## pydantic_ai_slim/pydantic_ai/tools.py

A = TypeVar('A')

## pydantic_ai_slim/pydantic_ai/toolsets/abstract.py

    async def __aenter__(self) -> Self:
        """Enter the toolset context.

        This is where you can set up network connections in a concrete implementation.
        """
        return self

    async def __aexit__(self, *args: Any) -> bool | None:
        """Exit the toolset context.

        This is where you can tear down network connections in a concrete implementation.
        """
        return None

## pydantic_ai_slim/pydantic_ai/toolsets/function.py

    schema_generator: type[GenerateJsonSchema]

## pydantic_ai_slim/pydantic_ai/usage.py

    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

## pydantic_evals/pydantic_evals/evaluators/common.py

    query: SpanQuery

## pydantic_graph/pydantic_graph/beta/graph.py

    get_next_node_run_id: Callable[[], NodeRunID]

    get_next_task_id: Callable[[], TaskID]

## pydantic_graph/pydantic_graph/beta/graph_builder.py

    def add_edge(self, source: Source[T], destination: Destination[T], *, label: str | None = None) -> None:
        """Add a simple edge between two nodes.

        Args:
            source: The source node
            destination: The destination node
            label: Optional label for the edge
        """
        builder = self.edge_from(source)
        if label is not None:
            builder = builder.label(label)
        self.add(builder.to(destination))

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## tests/evals/test_report_evaluators.py

def _make_report(cases: list[ReportCase]) -> EvaluationReport:
    return EvaluationReport(name='test', cases=cases)

## tests/graph/beta/test_broadcast_and_spread.py

class CounterState:
    values: list[int] = field(default_factory=list[int])

    values: list[int] = field(default_factory=list[int])

## tests/graph/beta/test_decisions.py

class DecisionState:
    path_taken: str | None = None
    value: int = 0

## tests/graph/beta/test_edge_cases.py

class EdgeCaseState:
    value: int = 0
    error_raised: bool = False

## tests/graph/beta/test_edge_labels.py

class LabelState:
    value: int = 0

## tests/graph/beta/test_graph_builder.py

class SimpleState:
    counter: int = 0
    result: str | None = None

## tests/graph/beta/test_graph_edge_cases.py

class MyState:
    value: int = 0

## tests/graph/beta/test_graph_execution.py

class ExecutionState:
    log: list[str] = field(default_factory=list[str])
    counter: int = 0

## tests/graph/beta/test_graph_iteration.py

class IterState:
    counter: int = 0

## tests/graph/beta/test_v1_v2_integration.py

class IntegrationState:
    log: list[str] = field(default_factory=list[str])

## tests/models/test_gemini.py

def example_usage() -> _GeminiUsageMetaData:
    return _GeminiUsageMetaData(prompt_token_count=1, candidates_token_count=2, total_token_count=3)

## tests/models/test_groq.py

def text_chunk(text: str, finish_reason: FinishReason | None = None) -> chat.ChatCompletionChunk:
    return chunk([ChoiceDelta(content=text, role='assistant')], finish_reason=finish_reason)

## tests/models/test_mcp_sampling.py

    create_message: Any

def fake_session(create_message: Any) -> Any:
    return FakeSession(create_message)

## tests/models/test_model_function.py

async def test_pass_both():
    Agent(FunctionModel(return_last, stream_function=stream_text_function))

## tests/models/test_model_names.py

    object: Literal['model']

## tests/models/test_model_test.py

class AgentRunDeps:
    run_id: int

## tests/test_ag_ui.py

def uuid_str() -> str:
    """Generate a random UUID string."""
    return uuid.uuid4().hex

## tests/test_agent.py

class Person(BaseModel):
    name: str

def test_instructions_decorator_with_parenthesis():
    agent = Agent('test')

    @agent.instructions()
    def instructions_2() -> str:
        return 'You are a helpful assistant.'

    result = agent.run_sync('Hello')
    assert result.all_messages()[0] == snapshot(
        ModelRequest(
            parts=[UserPromptPart(content='Hello', timestamp=IsNow(tz=timezone.utc))],
            timestamp=IsNow(tz=timezone.utc),
            instructions='You are a helpful assistant.',
            run_id=IsStr(),
        )
    )

class UserContext:
    location: str | None

## tests/test_deps.py

class MyDeps:
    foo: int
    bar: int

## tests/test_logfire.py

def get_logfire_summary(capfire: CaptureLogfire) -> Callable[[], LogfireSummary]:
    def get_summary() -> LogfireSummary:
        return LogfireSummary(capfire)

    return get_summary

def test_logfire_metadata_values(
    get_logfire_summary: Callable[[], LogfireSummary],
    metadata: dict[str, Any] | Callable[[RunContext[Any]], dict[str, Any]],
    expected: dict[str, Any],
) -> None:
    agent = Agent(model=TestModel(), instrument=InstrumentationSettings(version=2), metadata=metadata)
    agent.run_sync('Hello')

    summary = get_logfire_summary()
    assert summary.attributes[0]['metadata'] == expected

class WeatherInfo(BaseModel):
    temperature: float
    description: str

## tests/test_prefect.py

class SimpleDeps:
    value: str

## tests/test_temporal.py

    values: list[int] = field(default_factory=list[int])

## tests/test_tools.py

def test_output_type_deferred_tool_requests_by_itself():
    with pytest.raises(UserError, match='At least one output type must be provided other than `DeferredToolRequests`.'):
        Agent(TestModel(), output_type=DeferredToolRequests)

def test_output_type_empty():
    with pytest.raises(UserError, match='At least one output type must be provided.'):
        Agent(TestModel(), output_type=[])
