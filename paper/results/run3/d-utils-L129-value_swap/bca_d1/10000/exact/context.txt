## pydantic_ai_slim/pydantic_ai/_a2a.py

from typing import Any, Generic, TypeVar

## pydantic_ai_slim/pydantic_ai/_utils.py

class PeekableAsyncStream(Generic[T]):
    """Wraps an async iterable of type T and allows peeking at the *next* item without consuming it.

    We only buffer one item at a time (the next item). Once that item is yielded, it is discarded.
    This is a single-pass stream.
    """

    def __init__(self, source: AsyncIterable[T]):
        self._source = source
        self._source_iter: AsyncIterator[T] | None = None
        self._buffer: T | Unset = UNSET
        self._exhausted = False

    async def peek(self) -> T | Unset:
        """Returns the next item that would be yielded without consuming it.

        Returns None if the stream is exhausted.
        """
        if self._exhausted:
            return UNSET

        # If we already have a buffered item, just return it.
        if not isinstance(self._buffer, Unset):
            return self._buffer

        # Otherwise, we need to fetch the next item from the underlying iterator.
        if self._source_iter is None:
            self._source_iter = aiter(self._source)

        try:
            self._buffer = await anext(self._source_iter)
        except StopAsyncIteration:
            self._exhausted = True
            return UNSET

        return self._buffer

    async def is_exhausted(self) -> bool:
        """Returns True if the stream is exhausted, False otherwise."""
        return isinstance(await self.peek(), Unset)

    def __aiter__(self) -> AsyncIterator[T]:
        # For a single-pass iteration, we can return self as the iterator.
        return self

    async def __anext__(self) -> T:
        """Yields the buffered item if present, otherwise fetches the next item from the underlying source.

        Raises StopAsyncIteration if the stream is exhausted.
        """
        if self._exhausted:
            raise StopAsyncIteration

        # If we have a buffered item, yield it.
        if not isinstance(self._buffer, Unset):
            item = self._buffer
            self._buffer = UNSET
            return item

        # Otherwise, fetch the next item from the source.
        if self._source_iter is None:
            self._source_iter = aiter(self._source)

        try:
            return await anext(self._source_iter)
        except StopAsyncIteration:
            self._exhausted = True
            raise

    async def peek(self) -> T | Unset:
        """Returns the next item that would be yielded without consuming it.

        Returns None if the stream is exhausted.
        """
        if self._exhausted:
            return UNSET

        # If we already have a buffered item, just return it.
        if not isinstance(self._buffer, Unset):
            return self._buffer

        # Otherwise, we need to fetch the next item from the underlying iterator.
        if self._source_iter is None:
            self._source_iter = aiter(self._source)

        try:
            self._buffer = await anext(self._source_iter)
        except StopAsyncIteration:
            self._exhausted = True
            return UNSET

        return self._buffer

    async def is_exhausted(self) -> bool:
        """Returns True if the stream is exhausted, False otherwise."""
        return isinstance(await self.peek(), Unset)

## pydantic_ai_slim/pydantic_ai/format_prompt.py

class _ToXml:
    data: Any
    item_tag: str
    none_str: str
    include_field_info: Literal['once'] | bool
    # a map of Pydantic and dataclasses Field paths to their metadata:
    # a field unique string representation and its class
    _fields_info: dict[str, tuple[str, FieldInfo | ComputedFieldInfo]] = field(
        default_factory=dict[str, tuple[str, FieldInfo | ComputedFieldInfo]]
    )
    # keep track of fields we have extracted attributes from
    _included_fields: set[str] = field(default_factory=set[str])
    # keep track of class names for dataclasses and Pydantic models, that occur in lists
    _element_names: dict[str, str] = field(default_factory=dict[str, str])
    # flag for parsing dataclasses and Pydantic models once
    _is_info_extracted: bool = False
    _FIELD_ATTRIBUTES = ('title', 'description')

    def to_xml(self, tag: str | None = None) -> ElementTree.Element:
        return self._to_xml(value=self.data, path='', tag=tag)

    def _to_xml(self, value: Any, path: str, tag: str | None = None) -> ElementTree.Element:
        element = self._create_element(self.item_tag if tag is None else tag, path)
        if self._set_scalar_text(element, value):
            return element
        if isinstance(value, Mapping):
            if tag is None and path in self._element_names:
                element.tag = self._element_names[path]
            self._mapping_to_xml(element, value, path)  # pyright: ignore[reportUnknownArgumentType]
            return element
        if is_dataclass(value) and not isinstance(value, type):
            self._init_structure_info()
            if tag is None:
                element.tag = value.__class__.__name__
            self._mapping_to_xml(element, asdict(value), path)
            return element
        if isinstance(value, BaseModel):
            self._init_structure_info()
            if tag is None:
                element.tag = value.__class__.__name__
            # by dumping the model we loose all metadata in nested data structures,
            # but we have collected it when called _init_structure_info
            try:
                mapping = value.model_dump(mode='json')
            except PydanticSerializationError as e:
                raise TypeError(f'Unsupported type for XML formatting: {e}') from e
            self._mapping_to_xml(element, mapping, path)
            return element
        if isinstance(value, Iterable):
            for n, item in enumerate(value):  # pyright: ignore[reportUnknownVariableType,reportUnknownArgumentType]
                element.append(self._to_xml(value=item, path=f'{path}.[{n}]' if path else f'[{n}]'))
            return element
        raise TypeError(f'Unsupported type for XML formatting: {type(value)}')

    def _set_scalar_text(self, element: ElementTree.Element, value: Any) -> bool:
        """Set element.text for scalar types. Return True if handled, False otherwise."""
        if value is None:
            element.text = self.none_str
        elif isinstance(value, str):
            element.text = value
        elif isinstance(value, bytes | bytearray):
            element.text = value.decode(errors='ignore')
        elif isinstance(value, bool | int | float | Enum):
            element.text = str(value)
        elif isinstance(value, date | time):
            element.text = value.isoformat()
        elif isinstance(value, timedelta):
            element.text = str(value)
        elif isinstance(value, Decimal):
            element.text = str(value)
        elif isinstance(value, UUID):
            element.text = str(value)
        else:
            return False
        return True

    def _create_element(self, tag: str, path: str) -> ElementTree.Element:
        element = ElementTree.Element(tag)
        if path in self._fields_info:
            field_repr, field_info = self._fields_info[path]
            if self.include_field_info and self.include_field_info != 'once' or field_repr not in self._included_fields:
                field_attributes = self._extract_attributes(field_info)
                for k, v in field_attributes.items():
                    element.set(k, v)
                self._included_fields.add(field_repr)
        return element

    def _init_structure_info(self):
        """Create maps with all data information (fields info and class names), if not already created."""
        if not self._is_info_extracted:
            self._parse_data_structures(self.data)
            self._is_info_extracted = True

    def _mapping_to_xml(
        self,
        element: ElementTree.Element,
        mapping: Mapping[Any, Any],
        path: str = '',
    ) -> None:
        for key, value in mapping.items():
            if isinstance(key, int):
                key = str(key)
            elif not isinstance(key, str):
                raise TypeError(f'Unsupported key type for XML formatting: {type(key)}, only str and int are allowed')
            element.append(self._to_xml(value=value, path=f'{path}.{key}' if path else key, tag=key))

    def _parse_data_structures(
        self,
        value: Any,
        path: str = '',
    ):
        """Parse data structures as dataclasses or Pydantic models to extract element names and attributes."""
        if value is None or isinstance(value, (str | int | float | date | time | timedelta | bytearray | bytes | bool)):
            return
        elif isinstance(value, Mapping):
            for k, v in value.items():  # pyright: ignore[reportUnknownVariableType]
                self._parse_data_structures(v, f'{path}.{k}' if path else f'{k}')
        elif is_dataclass(value) and not isinstance(value, type):
            self._element_names[path] = value.__class__.__name__
            for field in fields(value):
                new_path = f'{path}.{field.name}' if path else field.name
                if self.include_field_info and field.metadata:
                    attributes = {k: v for k, v in field.metadata.items() if k in self._FIELD_ATTRIBUTES}
                    if attributes:
                        field_repr = f'{value.__class__.__name__}.{field.name}'
                        self._fields_info[new_path] = (field_repr, FieldInfo(**attributes))
                self._parse_data_structures(getattr(value, field.name), new_path)
        elif isinstance(value, BaseModel):
            self._element_names[path] = value.__class__.__name__
            for model_fields in (value.__class__.model_fields, value.__class__.model_computed_fields):
                for field, info in model_fields.items():
                    new_path = f'{path}.{field}' if path else field
                    if self.include_field_info and (isinstance(info, ComputedFieldInfo) or not info.exclude):
                        field_repr = f'{value.__class__.__name__}.{field}'
                        self._fields_info[new_path] = (field_repr, info)
                    self._parse_data_structures(getattr(value, field), new_path)
        elif isinstance(value, Iterable):
            for n, item in enumerate(value):  # pyright: ignore[reportUnknownVariableType,reportUnknownArgumentType]
                new_path = f'{path}.[{n}]' if path else f'[{n}]'
                self._parse_data_structures(item, new_path)

    @classmethod
    def _extract_attributes(cls, info: FieldInfo | ComputedFieldInfo) -> dict[str, str]:
        return {attr: str(value) for attr in cls._FIELD_ATTRIBUTES if (value := getattr(info, attr, None)) is not None}

    def _to_xml(self, value: Any, path: str, tag: str | None = None) -> ElementTree.Element:
        element = self._create_element(self.item_tag if tag is None else tag, path)
        if self._set_scalar_text(element, value):
            return element
        if isinstance(value, Mapping):
            if tag is None and path in self._element_names:
                element.tag = self._element_names[path]
            self._mapping_to_xml(element, value, path)  # pyright: ignore[reportUnknownArgumentType]
            return element
        if is_dataclass(value) and not isinstance(value, type):
            self._init_structure_info()
            if tag is None:
                element.tag = value.__class__.__name__
            self._mapping_to_xml(element, asdict(value), path)
            return element
        if isinstance(value, BaseModel):
            self._init_structure_info()
            if tag is None:
                element.tag = value.__class__.__name__
            # by dumping the model we loose all metadata in nested data structures,
            # but we have collected it when called _init_structure_info
            try:
                mapping = value.model_dump(mode='json')
            except PydanticSerializationError as e:
                raise TypeError(f'Unsupported type for XML formatting: {e}') from e
            self._mapping_to_xml(element, mapping, path)
            return element
        if isinstance(value, Iterable):
            for n, item in enumerate(value):  # pyright: ignore[reportUnknownVariableType,reportUnknownArgumentType]
                element.append(self._to_xml(value=item, path=f'{path}.[{n}]' if path else f'[{n}]'))
            return element
        raise TypeError(f'Unsupported type for XML formatting: {type(value)}')

    def _set_scalar_text(self, element: ElementTree.Element, value: Any) -> bool:
        """Set element.text for scalar types. Return True if handled, False otherwise."""
        if value is None:
            element.text = self.none_str
        elif isinstance(value, str):
            element.text = value
        elif isinstance(value, bytes | bytearray):
            element.text = value.decode(errors='ignore')
        elif isinstance(value, bool | int | float | Enum):
            element.text = str(value)
        elif isinstance(value, date | time):
            element.text = value.isoformat()
        elif isinstance(value, timedelta):
            element.text = str(value)
        elif isinstance(value, Decimal):
            element.text = str(value)
        elif isinstance(value, UUID):
            element.text = str(value)
        else:
            return False
        return True

## pydantic_evals/pydantic_evals/_utils.py

class Unset:
    """A singleton to represent an unset value.

    Used to distinguish between explicitly set `None` values and values that were never set.

    Copied from pydantic_ai/_utils.py.
    """

    pass

def is_set(t_or_unset: T | Unset) -> TypeIs[T]:
    """Check if a value is set (not the UNSET singleton).

    Args:
        t_or_unset: The value to check, which may be the UNSET singleton or a regular value.

    Returns:
        True if the value is not UNSET, narrowing the type to T in a type-aware way.
    """
    return t_or_unset is not UNSET

## pydantic_evals/pydantic_evals/otel/span_tree.py

    def repr_xml(
        self,
        include_children: bool = True,
        include_trace_id: bool = False,
        include_span_id: bool = False,
        include_start_timestamp: bool = False,
        include_duration: bool = False,
    ) -> str:
        """Return an XML-like string representation of the node.

        Optionally includes children, trace_id, span_id, start_timestamp, and duration.
        """
        first_line_parts = [f'<SpanNode name={self.name!r}']
        if include_trace_id:
            first_line_parts.append(f"trace_id='{self.trace_id:032x}'")
        if include_span_id:
            first_line_parts.append(f"span_id='{self.span_id:016x}'")
        if include_start_timestamp:
            first_line_parts.append(f'start_timestamp={self.start_timestamp.isoformat()!r}')
        if include_duration:
            first_line_parts.append(f"duration='{self.duration}'")

        extra_lines: list[str] = []
        if include_children and self.children:
            first_line_parts.append('>')
            for child in self.children:
                extra_lines.append(
                    indent(
                        child.repr_xml(
                            include_children=include_children,
                            include_trace_id=include_trace_id,
                            include_span_id=include_span_id,
                            include_start_timestamp=include_start_timestamp,
                            include_duration=include_duration,
                        ),
                        '  ',
                    )
                )
            extra_lines.append('</SpanNode>')
        else:
            if self.children:
                first_line_parts.append('children=...')
            first_line_parts.append('/>')
        return '\n'.join([' '.join(first_line_parts), *extra_lines])

    def repr_xml(
        self,
        include_children: bool = True,
        include_trace_id: bool = False,
        include_span_id: bool = False,
        include_start_timestamp: bool = False,
        include_duration: bool = False,
    ) -> str:
        """Return an XML-like string representation of the tree, optionally including children, trace_id, span_id, duration, and timestamps."""
        if not self.roots:
            return '<SpanTree />'
        repr_parts = [
            '<SpanTree>',
            *[
                indent(
                    root.repr_xml(
                        include_children=include_children,
                        include_trace_id=include_trace_id,
                        include_span_id=include_span_id,
                        include_start_timestamp=include_start_timestamp,
                        include_duration=include_duration,
                    ),
                    '  ',
                )
                for root in self.roots
            ],
            '</SpanTree>',
        ]
        return '\n'.join(repr_parts)

## pydantic_evals/pydantic_evals/reporting/__init__.py

    def print(
        self,
        width: int | None = None,
        baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,
        *,
        console: Console | None = None,
        include_input: bool = False,
        include_metadata: bool = False,
        include_expected_output: bool = False,
        include_output: bool = False,
        include_durations: bool = True,
        include_total_duration: bool = False,
        include_removed_cases: bool = False,
        include_averages: bool = True,
        include_errors: bool = True,
        include_error_stacktrace: bool = False,
        include_evaluator_failures: bool = True,
        include_analyses: bool = True,
        input_config: RenderValueConfig | None = None,
        metadata_config: RenderValueConfig | None = None,
        output_config: RenderValueConfig | None = None,
        score_configs: dict[str, RenderNumberConfig] | None = None,
        label_configs: dict[str, RenderValueConfig] | None = None,
        metric_configs: dict[str, RenderNumberConfig] | None = None,
        duration_config: RenderNumberConfig | None = None,
        include_reasons: bool = False,
    ) -> None:
        """Print this report to the console, optionally comparing it to a baseline report.

        If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.
        """
        if console is None:  # pragma: no branch
            console = Console(width=width)

        metadata_panel = self._metadata_panel(baseline=baseline)
        renderable: RenderableType = self.console_table(
            baseline=baseline,
            include_input=include_input,
            include_metadata=include_metadata,
            include_expected_output=include_expected_output,
            include_output=include_output,
            include_durations=include_durations,
            include_total_duration=include_total_duration,
            include_removed_cases=include_removed_cases,
            include_averages=include_averages,
            include_evaluator_failures=include_evaluator_failures,
            input_config=input_config,
            metadata_config=metadata_config,
            output_config=output_config,
            score_configs=score_configs,
            label_configs=label_configs,
            metric_configs=metric_configs,
            duration_config=duration_config,
            include_reasons=include_reasons,
            with_title=not metadata_panel,
        )
        # Wrap table with experiment metadata panel if present
        if metadata_panel:
            renderable = Group(metadata_panel, renderable)
        console.print(renderable)
        if include_analyses and self.analyses:
            for analysis in self.analyses:
                console.print(_render_analysis(analysis))
        if include_evaluator_failures and self.report_evaluator_failures:
            console.print(
                Text('\nReport Evaluator Failures:', style='bold red'),
            )
            for failure in self.report_evaluator_failures:
                msg = f'  {failure.name}: {failure.error_message}'
                console.print(Text(msg, style='red'))
        if include_errors and self.failures:  # pragma: no cover
            failures_table = self.failures_table(
                include_input=include_input,
                include_metadata=include_metadata,
                include_expected_output=include_expected_output,
                include_error_message=True,
                include_error_stacktrace=include_error_stacktrace,
                input_config=input_config,
                metadata_config=metadata_config,
            )
            console.print(failures_table, style='red')

    def failures_table(
        self,
        *,
        include_input: bool = False,
        include_metadata: bool = False,
        include_expected_output: bool = False,
        include_error_message: bool = True,
        include_error_stacktrace: bool = True,
        input_config: RenderValueConfig | None = None,
        metadata_config: RenderValueConfig | None = None,
    ) -> Table:
        """Return a table containing the failures in this report."""
        renderer = EvaluationRenderer(
            include_input=include_input,
            include_metadata=include_metadata,
            include_expected_output=include_expected_output,
            include_output=False,
            include_durations=False,
            include_total_duration=False,
            include_removed_cases=False,
            include_averages=False,
            input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},
            metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},
            output_config=_DEFAULT_VALUE_CONFIG,
            score_configs={},
            label_configs={},
            metric_configs={},
            duration_config=_DEFAULT_DURATION_CONFIG,
            include_reasons=False,
            include_error_message=include_error_message,
            include_error_stacktrace=include_error_stacktrace,
            include_evaluator_failures=False,  # Not applicable for failures table
        )
        return renderer.build_failures_table(self)

    def build_failures_table(self, title: str) -> Table:
        """Build and return a Rich Table for the failures output."""
        table = Table(title=title, show_lines=True)
        table.add_column('Case ID', style='bold')
        if self.include_input:
            table.add_column('Inputs', overflow='fold')
        if self.include_metadata:
            table.add_column('Metadata', overflow='fold')
        if self.include_expected_output:
            table.add_column('Expected Output', overflow='fold')
        if self.include_error_message:
            table.add_column('Error Message', overflow='fold')
        if self.include_error_stacktrace:
            table.add_column('Error Stacktrace', overflow='fold')
        return table

## pydantic_graph/pydantic_graph/_utils.py

class Unset:
    """A singleton to represent an unset value.

    Copied from pydantic_ai/_utils.py.
    """

    pass

## pydantic_graph/pydantic_graph/beta/graph.py

    def value(self) -> OutputT:
        return self._value

## pydantic_graph/pydantic_graph/beta/join.py

class ReduceFirstValue(Generic[T]):
    """A reducer that returns the first value it encounters, and cancels all other tasks."""

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

    def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:
        """The reducer function."""
        ctx.cancel_sibling_tasks()
        return inputs

## tests/evals/test_otel.py

async def test_span_node_repr(span_tree: SpanTree):
    node = span_tree.first({'name_equals': 'child2'})
    assert node is not None

    leaf_node = span_tree.first({'name_equals': 'grandchild1'})
    assert str(leaf_node) == snapshot("<SpanNode name='grandchild1' span_id='0000000000000005' />")

    assert str(node) == snapshot("<SpanNode name='child2' span_id='0000000000000009'>...</SpanNode>")
    assert repr(node) == snapshot("""\
<SpanNode name='child2' >
  <SpanNode name='grandchild3' />
</SpanNode>\
""")
    assert node.repr_xml(include_children=False) == snapshot("<SpanNode name='child2' children=... />")
    assert node.repr_xml(include_span_id=True) == snapshot("""\
<SpanNode name='child2' span_id='0000000000000009' >
  <SpanNode name='grandchild3' span_id='000000000000000b' />
</SpanNode>\
""")
    assert node.repr_xml(include_trace_id=True) == snapshot("""\
<SpanNode name='child2' trace_id='00000000000000000000000000000001' >
  <SpanNode name='grandchild3' trace_id='00000000000000000000000000000001' />
</SpanNode>\
""")
    assert node.repr_xml(include_start_timestamp=True) == snapshot("""\
<SpanNode name='child2' start_timestamp='1970-01-01T00:00:08+00:00' >
  <SpanNode name='grandchild3' start_timestamp='1970-01-01T00:00:09+00:00' />
</SpanNode>\
""")
    assert node.repr_xml(include_duration=True) == snapshot("""\
<SpanNode name='child2' duration='0:00:03' >
  <SpanNode name='grandchild3' duration='0:00:01' />
</SpanNode>\
""")

## tests/evals/test_report_evaluators.py

def test_report_rendering_include_analyses_false():
    cases = [
        _make_report_case('c1', output='cat', expected_output='cat'),
    ]
    report = _make_report(cases)
    report.analyses = [
        ScalarResult(title='Accuracy', value=100.0, unit='%'),
    ]

    rendered = report.render(width=120, include_analyses=False)
    assert 'Accuracy: 100.0 %' not in rendered

def test_report_rendering_include_evaluator_failures_false():
    from pydantic_evals.evaluators.evaluator import EvaluatorFailure
    from pydantic_evals.evaluators.spec import EvaluatorSpec

    report = _make_report([_make_report_case('c1', output='x', expected_output='x')])
    report.report_evaluator_failures = [
        EvaluatorFailure(
            name='BrokenEvaluator',
            error_message='ValueError: oops',
            error_stacktrace='Traceback ...',
            source=EvaluatorSpec(name='BrokenEvaluator', arguments=None),
        ),
    ]

    rendered = report.render(width=120, include_evaluator_failures=False)
    assert 'Report Evaluator Failures' not in rendered
    assert 'BrokenEvaluator' not in rendered

## tests/evals/test_reporting.py

async def test_evaluation_renderer_with_failures(sample_report_case: ReportCase):
    """Test EvaluationRenderer with task failures."""
    from pydantic_evals.reporting import ReportCaseFailure

    failure = ReportCaseFailure(
        name='failed_case',
        inputs={'query': 'What is 10/0?'},
        metadata={'difficulty': 'impossible'},
        expected_output={'answer': 'undefined'},
        error_message='Division by zero',
        error_stacktrace='Traceback (most recent call last):\n  File "test.py", line 1\n    10/0\nZeroDivisionError: division by zero',
        trace_id='test-trace-failure',
        span_id='test-span-failure',
    )

    report = EvaluationReport(
        cases=[sample_report_case],
        failures=[failure],
        name='test_report_with_failures',
    )

    # Test with include_error_message=True, include_error_stacktrace=False
    failures_table = report.failures_table(
        include_input=True,
        include_metadata=True,
        include_expected_output=True,
        include_error_message=True,
        include_error_stacktrace=False,
    )

    assert render_table(failures_table) == snapshot("""\
                                                     Case Failures
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃ Case ID     ┃ Inputs                     ┃ Metadata                     ┃ Expected Output         ┃ Error Message    ┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ failed_case │ {'query': 'What is 10/0?'} │ {'difficulty': 'impossible'} │ {'answer': 'undefined'} │ Division by zero │
└─────────────┴────────────────────────────┴──────────────────────────────┴─────────────────────────┴──────────────────┘
""")

    # Test with both include_error_message=True and include_error_stacktrace=True
    failures_table_with_stacktrace = report.failures_table(
        include_input=False,
        include_metadata=False,
        include_expected_output=False,
        include_error_message=False,
        include_error_stacktrace=True,
    )

    assert render_table(failures_table_with_stacktrace) == snapshot("""\
                    Case Failures
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Case ID     ┃ Error Stacktrace                    ┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ failed_case │ Traceback (most recent call last):  │
│             │   File "test.py", line 1            │
│             │     10/0                            │
│             │ ZeroDivisionError: division by zero │
└─────────────┴─────────────────────────────────────┘
""")

async def test_evaluation_renderer_failures_without_error_message(sample_report_case: ReportCase):
    """Test failures table without error message."""
    from pydantic_evals.reporting import ReportCaseFailure

    # Create failure without error message
    failure = ReportCaseFailure(
        name='failed_case',
        inputs={'query': 'What is 10/0?'},
        metadata={'difficulty': 'impossible'},
        expected_output={'answer': 'undefined'},
        error_message='',  # Empty error message
        error_stacktrace='Traceback',
        trace_id='test-trace-failure',
        span_id='test-span-failure',
    )

    report = EvaluationReport(
        cases=[sample_report_case],
        failures=[failure],
        name='test_report_with_failures',
    )

    # Test with include_error_message=True even though message is empty
    failures_table = report.failures_table(
        include_input=True,
        include_metadata=False,
        include_expected_output=False,
        include_error_message=True,
        include_error_stacktrace=False,
    )

    # The test ensures build_failure_row covers the empty error_message branch
    assert render_table(failures_table) == snapshot("""\
                       Case Failures
┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Case ID     ┃ Inputs                     ┃ Error Message ┃
┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ failed_case │ {'query': 'What is 10/0?'} │ -             │
└─────────────┴────────────────────────────┴───────────────┘
""")

## tests/graph/beta/test_graph_edge_cases.py

async def test_parallel_reducer_stop_iteration_explicit_fork_ids():
    """Test reducer that raises StopIteration to cancel concurrent tasks."""

    g = GraphBuilder(output_type=int)

    @g.step
    async def generate_numbers(ctx: StepContext[None, None, None]) -> list[int]:
        return [1, 1, 1, 1, 1]

    stop_early_1 = g.join(ReduceFirstValue[int](), initial=0, parent_fork_id='map_1')
    stop_early_2 = g.join(ReduceFirstValue[int](), initial=0, parent_fork_id='map_2')
    collect = g.join(reduce_sum, initial=0)

    g.add(
        g.edge_from(g.start_node).to(generate_numbers),
        g.edge_from(generate_numbers).broadcast(
            lambda b: [
                b.map(fork_id='map_1').transform(lambda ctx: ctx.inputs * 10).to(stop_early_1),
                b.map(fork_id='map_2').to(stop_early_2),
            ]
        ),
        g.edge_from(stop_early_1, stop_early_2).to(collect),
        g.edge_from(collect).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run()

    # Result should be 11 because it should have stopped early on one input in the first fork and the *10 fork
    assert result == 11

async def test_parallel_reducer_stop_iteration_implicit_fork_ids():
    """Test reducer that raises StopIteration to cancel concurrent tasks."""

    g = GraphBuilder(output_type=int)

    @g.step
    async def generate_numbers(ctx: StepContext[None, None, None]) -> list[int]:
        return [1, 1, 1, 1, 1]

    stop_early_1 = g.join(ReduceFirstValue[int](), initial=0, preferred_parent_fork='closest')
    stop_early_2 = g.join(ReduceFirstValue[int](), initial=0, preferred_parent_fork='closest')
    collect = g.join(reduce_sum, initial=0)

    g.add(
        g.edge_from(g.start_node).to(generate_numbers),
        g.edge_from(generate_numbers).broadcast(
            lambda b: [
                b.map().transform(lambda ctx: ctx.inputs * 10).to(stop_early_1),
                b.map().to(stop_early_2),
            ]
        ),
        g.edge_from(stop_early_1, stop_early_2).to(collect),
        g.edge_from(collect).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run()

    # Result should be 11 because it should have stopped early on one input in the first fork and the *10 fork
    assert result == 11

## tests/graph/beta/test_graph_execution.py

async def test_reduce_first_value_task_cancellation():
    """Test that ReduceFirstValue properly cancels sibling tasks"""
    import asyncio

    g = GraphBuilder(state_type=ExecutionState, output_type=str)

    @g.step
    async def generate(ctx: StepContext[ExecutionState, None, None]) -> list[int]:
        return [1, 2, 3, 4, 5]

    @g.step
    async def slow_process(ctx: StepContext[ExecutionState, None, int]) -> str:
        if ctx.inputs == 1:
            # First one completes quickly
            await asyncio.sleep(0.01)
        else:
            # Others take longer (should be cancelled)
            await asyncio.sleep(10)
        ctx.state.log.append(f'completed-{ctx.inputs}')
        return f'result-{ctx.inputs}'

    first_join = g.join(ReduceFirstValue[str](), initial='')

    g.add(
        g.edge_from(g.start_node).to(generate),
        g.edge_from(generate).map().to(slow_process),
        g.edge_from(slow_process).to(first_join),
        g.edge_from(first_join).to(g.end_node),
    )

    graph = g.build()
    state = ExecutionState()
    result = await graph.run(state=state)

    # Should get a result
    assert result is not None and result.startswith('result-')
    # Not all tasks should have completed due to cancellation
    assert len(state.log) < 5

## tests/graph/beta/test_joins_and_reducers.py

async def test_reduce_first_value():
    """Test ReduceFirstValue cancels sibling tasks"""

    @dataclass
    class StateWithResults:
        results: list[str] = field(default_factory=list[str])

    g = GraphBuilder(state_type=StateWithResults, output_type=str)

    @g.step
    async def generate(ctx: StepContext[StateWithResults, None, None]) -> list[int]:
        return [1, 2, 3, 4, 5]

    @g.step
    async def slow_process(ctx: StepContext[StateWithResults, None, int]) -> str:
        # First task finishes quickly
        if ctx.inputs == 1:
            await asyncio.sleep(0.001)
        else:
            # Others take longer (should be cancelled)
            await asyncio.sleep(10)
        ctx.state.results.append(f'completed-{ctx.inputs}')
        return f'result-{ctx.inputs}'

    first_join = g.join(ReduceFirstValue[str](), initial='')

    g.add(
        g.edge_from(g.start_node).to(generate),
        g.edge_from(generate).map().to(slow_process),
        g.edge_from(slow_process).to(first_join),
        g.edge_from(first_join).to(g.end_node),
    )

    graph = g.build()
    state = StateWithResults()
    result = await graph.run(state=state)

    # Only the first value should be returned
    assert result.startswith('result-')
    # Due to cancellation, not all 5 tasks should complete
    # (though timing can be tricky, so we just verify we got a result)
    assert 'completed-1' in state.results

## tests/mcp_server.py

async def get_error(value: bool = False):
    if value:
        return 'This is not an error'

    raise ValueError('This is an error. Call the tool with True instead')

## tests/models/test_anthropic.py

async def test_anthropic_container_setting_false_ignores_history(allow_model_requests: None):
    """Test that anthropic_container=False ignores container_id from history."""
    c = completion_message([BetaTextBlock(text='world', type='text')], BetaUsage(input_tokens=5, output_tokens=10))
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(m)

    # Create a message history with a container_id
    history: list[ModelMessage] = [
        ModelRequest(parts=[UserPromptPart(content='hello')]),
        ModelResponse(
            parts=[TextPart(content='world')],
            provider_name='anthropic',
            provider_details={'container_id': 'container_should_be_ignored'},
        ),
    ]

    # Run with anthropic_container=False to force fresh container
    await agent.run(
        'follow up', message_history=history, model_settings=AnthropicModelSettings(anthropic_container=False)
    )

    completion_kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    # When anthropic_container=False, container should be OMIT (filtered out before sending to API)
    from anthropic import omit as OMIT

    assert completion_kwargs.get('container') is OMIT

## tests/profiles/test_anthropic.py

def test_strict_false_preserves_schema():
    """With strict=False, schemas are not transformed (only title/$schema removed)."""

    class User(BaseModel):
        username: Annotated[str, Field(min_length=3)]
        age: int

    original_schema = User.model_json_schema()
    transformer = AnthropicJsonSchemaTransformer(original_schema, strict=False)
    transformed = transformer.walk()

    assert transformer.is_strict_compatible is False
    # Constraints preserved, title removed
    assert transformed == snapshot(
        {
            'type': 'object',
            'properties': {
                'username': {'minLength': 3, 'type': 'string'},
                'age': {'type': 'integer'},
            },
            'required': ['username', 'age'],
        }
    )

## tests/test_temporal.py

class MockPayloadCodec(PayloadCodec):
    """A mock payload codec for testing (simulates encryption codec)."""

    async def encode(
        self, payloads: Sequence[temporalio.api.common.v1.Payload]
    ) -> list[temporalio.api.common.v1.Payload]:  # pragma: no cover
        return list(payloads)

    async def decode(
        self, payloads: Sequence[temporalio.api.common.v1.Payload]
    ) -> list[temporalio.api.common.v1.Payload]:  # pragma: no cover
        return list(payloads)

    async def decode(
        self, payloads: Sequence[temporalio.api.common.v1.Payload]
    ) -> list[temporalio.api.common.v1.Payload]:  # pragma: no cover
        return list(payloads)

## tests/test_utils.py

async def test_peekable_async_stream(peek_first: bool):
    async_stream = MockAsyncStream(iter([1, 2, 3]))
    peekable_async_stream = PeekableAsyncStream(async_stream)

    items: list[int] = []

    # We need to both peek before starting the stream, and not, to achieve full coverage
    if peek_first:
        assert not await peekable_async_stream.is_exhausted()
        assert await peekable_async_stream.peek() == 1

    async for item in peekable_async_stream:
        items.append(item)

        # The next line is included mostly for the sake of achieving coverage
        assert await peekable_async_stream.peek() == (item + 1 if item < 3 else UNSET)

    assert await peekable_async_stream.is_exhausted()
    assert await peekable_async_stream.peek() is UNSET
    assert items == [1, 2, 3]
