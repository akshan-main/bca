# pydantic_ai_slim/pydantic_ai/_output.py:533-533
    outer_typed_dict_key: str | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:851-858
    def model_response_object(self) -> dict[str, Any]:
        """Return a dictionary representation of the content, wrapping non-dict types appropriately."""
        # gemini supports JSON dict return values, but no other JSON types, hence we wrap anything else in a dict
        json_content = tool_return_ta.dump_python(self.content, mode='json')
        if isinstance(json_content, dict):
            return json_content  # type: ignore[reportUnknownReturn]
        else:
            return {'return_value': json_content}

# tests/graph/test_utils.py:39-39
global_obj = object()

# pydantic_ai_slim/pydantic_ai/_output.py:528-528
    object_def: OutputObjectDefinition

# pydantic_ai_slim/pydantic_ai/_griffe.py:10-10
from griffe import Docstring, DocstringSectionKind, GoogleOptions, Object as GriffeObject

# tests/models/test_model_names.py:142-142
    object: Literal['model']

# pydantic_ai_slim/pydantic_ai/_utils.py:36-36
from typing_inspection import typing_objects

# pydantic_ai_slim/pydantic_ai/_utils.py:36-36
from typing_inspection import typing_objects

# pydantic_graph/pydantic_graph/_utils.py:169-204
def infer_obj_name(obj: Any, *, depth: int) -> str | None:
    """Infer the variable name of an object from the calling frame's scope.

    This function examines the call stack to find what variable name was used
    for the given object in the calling scope. This is useful for automatic
    naming of objects based on their variable names.

    Args:
        obj: The object whose variable name to infer.
        depth: Number of stack frames to traverse upward from the current frame.

    Returns:
        The inferred variable name if found, None otherwise.

    Example:
        Usage should generally look like `infer_name(self, depth=2)` or similar.
    """
    target_frame = inspect.currentframe()
    if target_frame is None:
        return None  # pragma: no cover
    for _ in range(depth):
        target_frame = target_frame.f_back
        if target_frame is None:
            return None

    for name, item in target_frame.f_locals.items():
        if item is obj:
            return name

    if target_frame.f_locals != target_frame.f_globals:  # pragma: no branch
        # if we couldn't find the agent in locals and globals are a different dict, try globals
        for name, item in target_frame.f_globals.items():
            if item is obj:
                return name

    return None

# pydantic_ai_slim/pydantic_ai/_utils.py:36-36
from typing_inspection import typing_objects

# pydantic_graph/pydantic_graph/_utils.py:169-204
def infer_obj_name(obj: Any, *, depth: int) -> str | None:
    """Infer the variable name of an object from the calling frame's scope.

    This function examines the call stack to find what variable name was used
    for the given object in the calling scope. This is useful for automatic
    naming of objects based on their variable names.

    Args:
        obj: The object whose variable name to infer.
        depth: Number of stack frames to traverse upward from the current frame.

    Returns:
        The inferred variable name if found, None otherwise.

    Example:
        Usage should generally look like `infer_name(self, depth=2)` or similar.
    """
    target_frame = inspect.currentframe()
    if target_frame is None:
        return None  # pragma: no cover
    for _ in range(depth):
        target_frame = target_frame.f_back
        if target_frame is None:
            return None

    for name, item in target_frame.f_locals.items():
        if item is obj:
            return name

    if target_frame.f_locals != target_frame.f_globals:  # pragma: no branch
        # if we couldn't find the agent in locals and globals are a different dict, try globals
        for name, item in target_frame.f_globals.items():
            if item is obj:
                return name

    return None

# docs/.hooks/algolia.py:23-23
    objectID: str

# pydantic_ai_slim/pydantic_ai/_output.py:35-35
from .tools import GenerateToolJsonSchema, ObjectJsonSchema, ToolDefinition

# pydantic_ai_slim/pydantic_ai/_output.py:35-35
from .tools import GenerateToolJsonSchema, ObjectJsonSchema, ToolDefinition

# pydantic_ai_slim/pydantic_ai/_output.py:35-35
from .tools import GenerateToolJsonSchema, ObjectJsonSchema, ToolDefinition

# pydantic_ai_slim/pydantic_ai/_output.py:218-218
    object_def: OutputObjectDefinition | None = None

# tests/typed_deps.py:57-58
async def my_prepare_object(ctx: RunContext[object], tool_defn: ToolDefinition) -> None:
    pass

# pydantic_ai_slim/pydantic_ai/_output.py:532-670
class ObjectOutputProcessor(BaseObjectOutputProcessor[OutputDataT]):
    outer_typed_dict_key: str | None = None
    validator: SchemaValidator
    _function_schema: _function_schema.FunctionSchema | None = None

    def __init__(
        self,
        output: OutputTypeOrFunction[OutputDataT],
        *,
        name: str | None = None,
        description: str | None = None,
        strict: bool | None = None,
    ):
        if inspect.isfunction(output) or inspect.ismethod(output):
            self._function_schema = _function_schema.function_schema(output, GenerateToolJsonSchema)
            self.validator = self._function_schema.validator
            json_schema = self._function_schema.json_schema
            json_schema['description'] = self._function_schema.description
        else:
            json_schema_type_adapter: TypeAdapter[Any]
            validation_type_adapter: TypeAdapter[Any]
            if _utils.is_model_like(output):
                json_schema_type_adapter = validation_type_adapter = TypeAdapter(output)
            else:
                self.outer_typed_dict_key = 'response'
                output_type: type[OutputDataT] = cast(type[OutputDataT], output)

                response_data_typed_dict = TypedDict(  # noqa: UP013
                    'response_data_typed_dict',
                    {'response': output_type},  # pyright: ignore[reportInvalidTypeForm]
                )
                json_schema_type_adapter = TypeAdapter(response_data_typed_dict)

                # More lenient validator: allow either the native type or a JSON string containing it
                # i.e. `response: OutputDataT | Json[OutputDataT]`, as some models don't follow the schema correctly,
                # e.g. `BedrockConverseModel('us.meta.llama3-2-11b-instruct-v1:0')`
                response_validation_typed_dict = TypedDict(  # noqa: UP013
                    'response_validation_typed_dict',
                    {'response': output_type | Json[output_type]},  # pyright: ignore[reportInvalidTypeForm]
                )
                validation_type_adapter = TypeAdapter(response_validation_typed_dict)

            # Really a PluggableSchemaValidator, but it's API-compatible
            self.validator = cast(SchemaValidator, validation_type_adapter.validator)
            json_schema = _utils.check_object_json_schema(
                json_schema_type_adapter.json_schema(schema_generator=GenerateToolJsonSchema)
            )

            if self.outer_typed_dict_key:
                # including `response_data_typed_dict` as a title here doesn't add anything and could confuse the LLM
                json_schema.pop('title')

        if name is None and (json_schema_title := json_schema.get('title', None)):
            name = json_schema_title

        if json_schema_description := json_schema.pop('description', None):
            if description is None:
                description = json_schema_description
            else:
                description = f'{description}. {json_schema_description}'

        super().__init__(
            object_def=OutputObjectDefinition(
                name=name or getattr(output, '__name__', None),
                description=description,
                json_schema=json_schema,
                strict=strict,
            )
        )

    async def process(
        self,
        data: str | dict[str, Any] | None,
        *,
        run_context: RunContext[AgentDepsT],
        allow_partial: bool = False,
        wrap_validation_errors: bool = True,
    ) -> OutputDataT:
        """Process an output message, performing validation and (if necessary) calling the output function.

        Args:
            data: The output data to validate.
            run_context: The current run context.
            allow_partial: If true, allow partial validation.
            wrap_validation_errors: If true, wrap the validation errors in a retry message.

        Returns:
            Either the validated output data (left) or a retry message (right).
        """
        if isinstance(data, str):
            data = _utils.strip_markdown_fences(data)

        try:
            output = self.validate(data, allow_partial=allow_partial, validation_context=run_context.validation_context)
        except ValidationError as e:
            if wrap_validation_errors:
                m = _messages.RetryPromptPart(
                    content=e.errors(include_url=False),
                )
                raise ToolRetryError(m) from e
            else:
                raise

        output = await self.call(output, run_context, wrap_validation_errors)

        return output

    def validate(
        self,
        data: str | dict[str, Any] | None,
        *,
        allow_partial: bool = False,
        validation_context: Any | None = None,
    ) -> dict[str, Any]:
        pyd_allow_partial: Literal['off', 'trailing-strings'] = 'trailing-strings' if allow_partial else 'off'
        if isinstance(data, str):
            return self.validator.validate_json(
                data or '{}', allow_partial=pyd_allow_partial, context=validation_context
            )
        else:
            return self.validator.validate_python(
                data or {}, allow_partial=pyd_allow_partial, context=validation_context
            )

    async def call(
        self,
        output: dict[str, Any],
        run_context: RunContext[AgentDepsT],
        wrap_validation_errors: bool = True,
    ) -> Any:
        if k := self.outer_typed_dict_key:
            output = output[k]

        if self._function_schema:
            output = await execute_traced_output_function(
                self._function_schema, run_context, output, wrap_validation_errors
            )

        return output

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/mcp.py:19-19
from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/mcp.py:19-19
from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# tests/models/test_model_settings.py:147-168
def test_empty_settings_objects():
    """Test that empty ModelSettings objects work correctly in the hierarchy."""
    captured_settings = None

    def capture_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('captured')])

    # All levels have empty settings
    model = FunctionModel(capture_settings, settings=ModelSettings())
    agent = Agent(model=model, model_settings=ModelSettings())

    # Run with one actual setting
    run_settings = ModelSettings(temperature=0.75)
    result = agent.run_sync('test', model_settings=run_settings)
    assert result.output == 'captured'

    # Should only have the run setting
    assert captured_settings is not None
    assert captured_settings.get('temperature') == 0.75
    assert len(captured_settings) == 1  # Only one setting should be present

# pydantic_ai_slim/pydantic_ai/models/test.py:421-440
    def _object_gen(self, schema: dict[str, Any]) -> dict[str, Any]:
        """Generate data for a JSON Schema object."""
        required = set(schema.get('required', []))

        data: dict[str, Any] = {}
        if properties := schema.get('properties'):
            for key, value in properties.items():
                if key in required:
                    data[key] = self._gen_any(value)

        if addition_props := schema.get('additionalProperties'):
            add_prop_key = 'additionalProperty'
            while add_prop_key in data:
                add_prop_key += '_'
            if addition_props is True:
                data[add_prop_key] = self._char()
            else:
                data[add_prop_key] = self._gen_any(addition_props)

        return data

# pydantic_ai_slim/pydantic_ai/_function_schema.py:25-25
from ._utils import check_object_json_schema, is_async_callable, is_model_like, run_in_executor

# pydantic_ai_slim/pydantic_ai/_function_schema.py:25-25
from ._utils import check_object_json_schema, is_async_callable, is_model_like, run_in_executor

# pydantic_ai_slim/pydantic_ai/models/__init__.py:1358-1365
def _customize_output_object(transformer: type[JsonSchemaTransformer], output_object: OutputObjectDefinition):
    schema_transformer = transformer(output_object.json_schema, strict=output_object.strict)
    json_schema = schema_transformer.walk()
    return replace(
        output_object,
        json_schema=json_schema,
        strict=schema_transformer.is_strict_compatible if output_object.strict is None else output_object.strict,
    )

# pydantic_ai_slim/pydantic_ai/_function_schema.py:25-25
from ._utils import check_object_json_schema, is_async_callable, is_model_like, run_in_executor

# pydantic_ai_slim/pydantic_ai/models/__init__.py:606-606
    output_object: OutputObjectDefinition | None = None

# pydantic_ai_slim/pydantic_ai/_json_schema.py:121-140
    def _handle_object(self, schema: JsonSchema) -> JsonSchema:
        if properties := schema.get('properties'):
            handled_properties = {}
            for key, value in properties.items():
                handled_properties[key] = self._handle(value)
            schema['properties'] = handled_properties

        if (additional_properties := schema.get('additionalProperties')) is not None:
            if isinstance(additional_properties, bool):
                schema['additionalProperties'] = additional_properties
            else:
                schema['additionalProperties'] = self._handle(additional_properties)

        if (pattern_properties := schema.get('patternProperties')) is not None:
            handled_pattern_properties = {}
            for key, value in pattern_properties.items():
                handled_pattern_properties[key] = self._handle(value)
            schema['patternProperties'] = handled_pattern_properties

        return schema

# pydantic_ai_slim/pydantic_ai/_output.py:527-528
class BaseObjectOutputProcessor(BaseOutputProcessor[OutputDataT]):
    object_def: OutputObjectDefinition

# pydantic_ai_slim/pydantic_ai/mcp.py:19-19
from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream

# pydantic_ai_slim/pydantic_ai/mcp.py:19-19
from anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream

# tests/graph/test_utils.py:16-25
def test_infer_obj_name():
    """Test inferring variable names from the calling frame."""
    my_object = object()
    # Depth 1 means we look at the frame calling infer_obj_name
    inferred = infer_obj_name(my_object, depth=1)
    assert inferred == 'my_object'

    # Test with object not in locals
    result = infer_obj_name(object(), depth=1)
    assert result is None

# pydantic_graph/pydantic_graph/beta/graph.py:16-16
from anyio import BrokenResourceError, CancelScope, create_memory_object_stream, create_task_group

# pydantic_ai_slim/pydantic_ai/models/xai.py:909-911
def _map_json_object() -> chat_pb2.ResponseFormat:
    """Create a ResponseFormat for JSON object mode (prompted output)."""
    return chat_pb2.ResponseFormat(format_type=chat_pb2.FORMAT_TYPE_JSON_OBJECT)

# tests/test_utils.py:55-117
def test_check_object_json_schema():
    object_schema = {'type': 'object', 'properties': {'a': {'type': 'string'}}}
    assert check_object_json_schema(object_schema) == object_schema

    assert check_object_json_schema(
        {
            '$defs': {
                'JsonModel': {
                    'properties': {
                        'type': {'title': 'Type', 'type': 'string'},
                        'items': {'anyOf': [{'type': 'string'}, {'type': 'null'}]},
                    },
                    'required': ['type', 'items'],
                    'title': 'JsonModel',
                    'type': 'object',
                }
            },
            '$ref': '#/$defs/JsonModel',
        }
    ) == {
        'properties': {
            'items': {'anyOf': [{'type': 'string'}, {'type': 'null'}]},
            'type': {'title': 'Type', 'type': 'string'},
        },
        'required': ['type', 'items'],
        'title': 'JsonModel',
        'type': 'object',
    }

    # Can't remove the recursive ref here:
    assert check_object_json_schema(
        {
            '$defs': {
                'JsonModel': {
                    'properties': {
                        'type': {'title': 'Type', 'type': 'string'},
                        'items': {'anyOf': [{'$ref': '#/$defs/JsonModel'}, {'type': 'null'}]},
                    },
                    'required': ['type', 'items'],
                    'title': 'JsonModel',
                    'type': 'object',
                }
            },
            '$ref': '#/$defs/JsonModel',
        }
    ) == {
        '$defs': {
            'JsonModel': {
                'properties': {
                    'items': {'anyOf': [{'$ref': '#/$defs/JsonModel'}, {'type': 'null'}]},
                    'type': {'title': 'Type', 'type': 'string'},
                },
                'required': ['type', 'items'],
                'title': 'JsonModel',
                'type': 'object',
            }
        },
        '$ref': '#/$defs/JsonModel',
    }

    array_schema = {'type': 'array', 'items': {'type': 'string'}}
    with pytest.raises(UserError, match='^Schema must be an object$'):
        check_object_json_schema(array_schema)

# pydantic_evals/pydantic_evals/reporting/__init__.py:1261-1275
    def _render_dict(
        self,
        case_dict: Mapping[str, EvaluationResult[T] | T],
        renderers: Mapping[str, _AbstractRenderer[T]],
        *,
        include_names: bool = True,
    ) -> str:
        diff_lines: list[str] = []
        for key, val in case_dict.items():
            value = cast(EvaluationResult[T], val).value if isinstance(val, EvaluationResult) else val
            rendered = renderers[key].render_value(key if include_names else None, value)
            if self.include_reasons and isinstance(val, EvaluationResult) and (reason := val.reason):
                rendered += f'\n  Reason: {reason}\n'
            diff_lines.append(rendered)
        return '\n'.join(diff_lines) if diff_lines else EMPTY_CELL_STR

# tests/graph/test_utils.py:28-36
def test_infer_obj_name_no_frame():
    """Test infer_obj_name when frame inspection fails."""
    # This is hard to trigger without mocking, but we can test that the function
    # returns None gracefully when it can't find the object
    some_obj = object()

    # Call with depth that would exceed the call stack
    result = infer_obj_name(some_obj, depth=1000)
    assert result is None

# pydantic_ai_slim/pydantic_ai/_output.py:21-34
from .output import (
    DeferredToolRequests,
    NativeOutput,
    OutputDataT,
    OutputMode,
    OutputObjectDefinition,
    OutputSpec,
    OutputTypeOrFunction,
    PromptedOutput,
    TextOutput,
    TextOutputFunc,
    ToolOutput,
    _OutputSpecItem,  # type: ignore[reportPrivateUsage]
)

# pydantic_ai_slim/pydantic_ai/profiles/__init__.py:34-34
    supports_json_object_output: bool = False

# pydantic_graph/pydantic_graph/_utils.py:169-204
def infer_obj_name(obj: Any, *, depth: int) -> str | None:
    """Infer the variable name of an object from the calling frame's scope.

    This function examines the call stack to find what variable name was used
    for the given object in the calling scope. This is useful for automatic
    naming of objects based on their variable names.

    Args:
        obj: The object whose variable name to infer.
        depth: Number of stack frames to traverse upward from the current frame.

    Returns:
        The inferred variable name if found, None otherwise.

    Example:
        Usage should generally look like `infer_name(self, depth=2)` or similar.
    """
    target_frame = inspect.currentframe()
    if target_frame is None:
        return None  # pragma: no cover
    for _ in range(depth):
        target_frame = target_frame.f_back
        if target_frame is None:
            return None

    for name, item in target_frame.f_locals.items():
        if item is obj:
            return name

    if target_frame.f_locals != target_frame.f_globals:  # pragma: no branch
        # if we couldn't find the agent in locals and globals are a different dict, try globals
        for name, item in target_frame.f_globals.items():
            if item is obj:
                return name

    return None

# tests/models/test_xai.py:4470-4515
async def test_xai_prompted_output_json_object(allow_model_requests: None):
    """Test prompted output uses json_object format."""

    class SimpleResult(BaseModel):
        answer: str

    response = create_response(content='{"answer": "42"}', usage=create_usage(prompt_tokens=10, completion_tokens=5))
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    # Use PromptedOutput explicitly - uses json_object mode when no tools
    agent: Agent[None, SimpleResult] = Agent(m, output_type=PromptedOutput(SimpleResult))

    result = await agent.run('What is the meaning of life?')
    assert result.output == SimpleResult(answer='42')

    # Verify response_format was set to json_object (not json_schema since it's prompted output)
    assert get_mock_chat_create_kwargs(mock_client) == snapshot(
        [
            {
                'model': XAI_NON_REASONING_MODEL,
                'messages': [
                    {
                        'content': [
                            {
                                'text': """\

Always respond with a JSON object that's compatible with this schema:

{"properties": {"answer": {"type": "string"}}, "required": ["answer"], "title": "SimpleResult", "type": "object"}

Don't include any text or Markdown fencing before or after.
"""
                            }
                        ],
                        'role': 'ROLE_SYSTEM',
                    },
                    {'content': [{'text': 'What is the meaning of life?'}], 'role': 'ROLE_USER'},
                ],
                'tools': None,
                'tool_choice': None,
                'response_format': {'format_type': 'FORMAT_TYPE_JSON_OBJECT'},
                'use_encrypted_content': False,
                'include': [],
            }
        ]
    )

# tests/profiles/test_google.py:86-98
def test_const_in_nested_object():
    """const should be properly converted in nested object properties."""

    class TaggedModel(BaseModel):
        tag: Literal['hello']
        value: str

    schema = TaggedModel.model_json_schema()
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    # The tag property should have both enum and type
    assert transformed['properties']['tag'] == snapshot({'enum': ['hello'], 'type': 'string'})

# tests/graph/test_utils.py:42-55
def test_infer_obj_name_locals_vs_globals():
    """Test infer_obj_name prefers locals over globals."""
    result = infer_obj_name(global_obj, depth=1)
    assert result == 'global_obj'

    # Assign a local name to the variable and ensure it is found with precedence over the global
    local_obj = global_obj
    result = infer_obj_name(global_obj, depth=1)
    assert result == 'local_obj'

    # If we unbind the local name, should find the global name again
    del local_obj
    result = infer_obj_name(global_obj, depth=1)
    assert result == 'global_obj'

# pydantic_evals/pydantic_evals/reporting/__init__.py:1243-1259
    def _render_dicts_diff(
        baseline_dict: dict[str, T],
        new_dict: dict[str, T],
        renderers: Mapping[str, _AbstractRenderer[T]],
        *,
        include_names: bool = True,
    ) -> str:
        keys: set[str] = set()
        keys.update(baseline_dict.keys())
        keys.update(new_dict.keys())
        diff_lines: list[str] = []
        for key in sorted(keys):
            old_val = baseline_dict.get(key)
            new_val = new_dict.get(key)
            rendered = renderers[key].render_diff(key if include_names else None, old_val, new_val)
            diff_lines.append(rendered)
        return '\n'.join(diff_lines) if diff_lines else EMPTY_CELL_STR

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:67-75
def _replace_toolsets(
    inputs: dict[str, Any],
) -> Any:
    """Replace Toolset objects with a dict containing only hashable fields."""
    inputs = inputs.copy()
    for key, value in inputs.items():
        if _is_toolset_tool(value):
            inputs[key] = {field.name: getattr(value, field.name) for field in fields(value) if field.name != 'toolset'}
    return inputs

# tests/models/test_openai.py:4153-4168
def test_azure_400_non_dict_body(allow_model_requests: None) -> None:
    """Test a 400 error from Azure where the body is not a dictionary."""
    mock_client = MockOpenAI.create_mock(
        APIStatusError(
            'Bad Request',
            response=httpx.Response(status_code=400, request=httpx.Request('POST', 'https://example.com/v1')),
            body='Raw string body',
        )
    )
    m = OpenAIChatModel('gpt-5-mini', provider=AzureProvider(openai_client=cast(AsyncAzureOpenAI, mock_client)))
    agent = Agent(m)

    with pytest.raises(ModelHTTPError) as exc_info:
        agent.run_sync('hello')

    assert exc_info.value.status_code == 400

# tests/models/test_openai.py:4332-4338
def test_transformer_adds_properties_to_object_schemas():
    """OpenAI drops object schemas without a 'properties' key. The transformer must add it."""

    schema = {'type': 'object', 'additionalProperties': {'type': 'string'}}
    result = OpenAIJsonSchemaTransformer(schema, strict=None).walk()

    assert result['properties'] == {}

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# pydantic_ai_slim/pydantic_ai/_output.py:656-670
    async def call(
        self,
        output: dict[str, Any],
        run_context: RunContext[AgentDepsT],
        wrap_validation_errors: bool = True,
    ) -> Any:
        if k := self.outer_typed_dict_key:
            output = output[k]

        if self._function_schema:
            output = await execute_traced_output_function(
                self._function_schema, run_context, output, wrap_validation_errors
            )

        return output

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:27-44
def _replace_run_context(
    inputs: dict[str, Any],
) -> Any:
    """Replace RunContext objects with a dict containing only hashable fields."""
    for key, value in inputs.items():
        if isinstance(value, RunContext):
            inputs[key] = {
                'retries': value.retries,
                'tool_call_id': value.tool_call_id,
                'tool_name': value.tool_name,
                'tool_call_approved': value.tool_call_approved,
                'tool_call_metadata': value.tool_call_metadata,
                'retry': value.retry,
                'max_retries': value.max_retries,
                'run_step': value.run_step,
            }

    return inputs

# pydantic_ai_slim/pydantic_ai/output.py:264-264
    name: str | None = None

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/format_prompt.py:4-4
from dataclasses import asdict, dataclass, field, fields, is_dataclass

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:11-12
def _is_dict(obj: Any) -> TypeGuard[dict[str, Any]]:
    return isinstance(obj, dict)

# tests/mcp_server.py:146-147
async def get_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

# pydantic_ai_slim/pydantic_ai/output.py:266-266
    strict: bool | None = None

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# docs/.hooks/algolia.py:10-10
from typing_extensions import TypedDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/_function_schema.py:14-14
from pydantic import ConfigDict

# pydantic_ai_slim/pydantic_ai/models/xai.py:901-906
def _map_json_schema(o: OutputObjectDefinition) -> chat_pb2.ResponseFormat:
    """Convert OutputObjectDefinition to xAI ResponseFormat protobuf object."""
    return chat_pb2.ResponseFormat(
        format_type=chat_pb2.FORMAT_TYPE_JSON_SCHEMA,
        schema=json.dumps(o.json_schema),
    )

# pydantic_ai_slim/pydantic_ai/_output.py:537-600
    def __init__(
        self,
        output: OutputTypeOrFunction[OutputDataT],
        *,
        name: str | None = None,
        description: str | None = None,
        strict: bool | None = None,
    ):
        if inspect.isfunction(output) or inspect.ismethod(output):
            self._function_schema = _function_schema.function_schema(output, GenerateToolJsonSchema)
            self.validator = self._function_schema.validator
            json_schema = self._function_schema.json_schema
            json_schema['description'] = self._function_schema.description
        else:
            json_schema_type_adapter: TypeAdapter[Any]
            validation_type_adapter: TypeAdapter[Any]
            if _utils.is_model_like(output):
                json_schema_type_adapter = validation_type_adapter = TypeAdapter(output)
            else:
                self.outer_typed_dict_key = 'response'
                output_type: type[OutputDataT] = cast(type[OutputDataT], output)

                response_data_typed_dict = TypedDict(  # noqa: UP013
                    'response_data_typed_dict',
                    {'response': output_type},  # pyright: ignore[reportInvalidTypeForm]
                )
                json_schema_type_adapter = TypeAdapter(response_data_typed_dict)

                # More lenient validator: allow either the native type or a JSON string containing it
                # i.e. `response: OutputDataT | Json[OutputDataT]`, as some models don't follow the schema correctly,
                # e.g. `BedrockConverseModel('us.meta.llama3-2-11b-instruct-v1:0')`
                response_validation_typed_dict = TypedDict(  # noqa: UP013
                    'response_validation_typed_dict',
                    {'response': output_type | Json[output_type]},  # pyright: ignore[reportInvalidTypeForm]
                )
                validation_type_adapter = TypeAdapter(response_validation_typed_dict)

            # Really a PluggableSchemaValidator, but it's API-compatible
            self.validator = cast(SchemaValidator, validation_type_adapter.validator)
            json_schema = _utils.check_object_json_schema(
                json_schema_type_adapter.json_schema(schema_generator=GenerateToolJsonSchema)
            )

            if self.outer_typed_dict_key:
                # including `response_data_typed_dict` as a title here doesn't add anything and could confuse the LLM
                json_schema.pop('title')

        if name is None and (json_schema_title := json_schema.get('title', None)):
            name = json_schema_title

        if json_schema_description := json_schema.pop('description', None):
            if description is None:
                description = json_schema_description
            else:
                description = f'{description}. {json_schema_description}'

        super().__init__(
            object_def=OutputObjectDefinition(
                name=name or getattr(output, '__name__', None),
                description=description,
                json_schema=json_schema,
                strict=strict,
            )
        )

# pydantic_ai_slim/pydantic_ai/_output.py:639-654
    def validate(
        self,
        data: str | dict[str, Any] | None,
        *,
        allow_partial: bool = False,
        validation_context: Any | None = None,
    ) -> dict[str, Any]:
        pyd_allow_partial: Literal['off', 'trailing-strings'] = 'trailing-strings' if allow_partial else 'off'
        if isinstance(data, str):
            return self.validator.validate_json(
                data or '{}', allow_partial=pyd_allow_partial, context=validation_context
            )
        else:
            return self.validator.validate_python(
                data or {}, allow_partial=pyd_allow_partial, context=validation_context
            )

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# tests/models/test_groq.py:502-504
class MyTypedDict(TypedDict, total=False):
    first: str
    second: str

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_utils.py:31-35
from typing_extensions import (
    ParamSpec,
    TypeIs,
    is_typeddict,
)

# pydantic_ai_slim/pydantic_ai/_output.py:534-534
    validator: SchemaValidator

# tests/example_modules/mcp_server.py:17-28
async def echo_deps(ctx: Context[ServerSessionT, LifespanContextT, RequestT]) -> dict[str, Any]:
    """Echo the run context.

    Args:
        ctx: Context object containing request and session information.

    Returns:
        Dictionary with an echo message and the deps.
    """

    deps: Any = getattr(ctx.request_context.meta, 'deps')
    return {'echo': 'This is an echo message', 'deps': deps}

# tests/example_modules/mcp_server.py:17-28
async def echo_deps(ctx: Context[ServerSessionT, LifespanContextT, RequestT]) -> dict[str, Any]:
    """Echo the run context.

    Args:
        ctx: Context object containing request and session information.

    Returns:
        Dictionary with an echo message and the deps.
    """

    deps: Any = getattr(ctx.request_context.meta, 'deps')
    return {'echo': 'This is an echo message', 'deps': deps}

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# tests/typed_agent.py:176-182
structured_dict = StructuredDict(
    {
        'type': 'object',
        'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}},
        'required': ['name', 'age'],
    }
)

# pydantic_ai_slim/pydantic_ai/output.py:263-263
    json_schema: ObjectJsonSchema

# pydantic_ai_slim/pydantic_ai/output.py:265-265
    description: str | None = None

# tests/models/test_openai.py:1700-1701
class MyNormalTypedDict(TypedDict):
    foo: str

# examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py:27-45
async def document_predict_state() -> list[CustomEvent]:
    """Enable document state prediction.

    Returns:
        CustomEvent containing the event to enable state prediction.
    """
    return [
        CustomEvent(
            type=EventType.CUSTOM,
            name='PredictState',
            value=[
                {
                    'state_key': 'document',
                    'tool': 'write_document',
                    'tool_argument': 'document',
                },
            ],
        ),
    ]

# tests/graph/beta/test_joins_and_reducers.py:84-107
async def test_reduce_dict_update():
    """Test reduce_dict_update that merges dictionaries."""
    g = GraphBuilder(state_type=SimpleState, output_type=dict[str, int])

    @g.step
    async def generate_keys(ctx: StepContext[SimpleState, None, None]) -> list[str]:
        return ['a', 'b', 'c']

    @g.step
    async def create_dict(ctx: StepContext[SimpleState, None, str]) -> dict[str, int]:
        return {ctx.inputs: len(ctx.inputs)}

    dict_join = g.join(reduce_dict_update, initial_factory=dict[str, int])

    g.add(
        g.edge_from(g.start_node).to(generate_keys),
        g.edge_from(generate_keys).map().to(create_dict),
        g.edge_from(create_dict).to(dict_join),
        g.edge_from(dict_join).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=SimpleState())
    assert result == {'a': 1, 'b': 1, 'c': 1}

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:140-171
class OpenRouterProviderConfig(TypedDict, total=False):
    """Represents the 'Provider' object from the OpenRouter API."""

    order: list[OpenRouterProviderName]
    """List of provider slugs to try in order (e.g. ["anthropic", "openai"]). [See details](https://openrouter.ai/docs/features/provider-routing#ordering-specific-providers)"""

    allow_fallbacks: bool
    """Whether to allow backup providers when the primary is unavailable. [See details](https://openrouter.ai/docs/features/provider-routing#disabling-fallbacks)"""

    require_parameters: bool
    """Only use providers that support all parameters in your request."""

    data_collection: Literal['allow', 'deny']
    """Control whether to use providers that may store data. [See details](https://openrouter.ai/docs/features/provider-routing#requiring-providers-to-comply-with-data-policies)"""

    zdr: bool
    """Restrict routing to only ZDR (Zero Data Retention) endpoints. [See details](https://openrouter.ai/docs/features/provider-routing#zero-data-retention-enforcement)"""

    only: list[OpenRouterProviderName]
    """List of provider slugs to allow for this request. [See details](https://openrouter.ai/docs/features/provider-routing#allowing-only-specific-providers)"""

    ignore: list[str]
    """List of provider slugs to skip for this request. [See details](https://openrouter.ai/docs/features/provider-routing#ignoring-providers)"""

    quantizations: list[Literal['int4', 'int8', 'fp4', 'fp6', 'fp8', 'fp16', 'bf16', 'fp32', 'unknown']]
    """List of quantization levels to filter by (e.g. ["int4", "int8"]). [See details](https://openrouter.ai/docs/features/provider-routing#quantization)"""

    sort: Literal['price', 'throughput', 'latency']
    """Sort providers by price or throughput. (e.g. "price" or "throughput"). [See details](https://openrouter.ai/docs/features/provider-routing#provider-sorting)"""

    max_price: _OpenRouterMaxPrice
    """The maximum pricing you want to pay for this request. [See details](https://openrouter.ai/docs/features/provider-routing#max-price)"""

# pydantic_graph/pydantic_graph/beta/join.py:118-121
def reduce_dict_update(current: dict[K, V], inputs: Mapping[K, V]) -> dict[K, V]:
    """A reducer that updates a dict."""
    current.update(inputs)
    return current

# pydantic_ai_slim/pydantic_ai/messages.py:1230-1241
    def args_as_dict(self) -> dict[str, Any]:
        """Return the arguments as a Python dictionary.

        This is just for convenience with models that require dicts as input.
        """
        if not self.args:
            return {}
        if isinstance(self.args, dict):
            return self.args
        args = pydantic_core.from_json(self.args)
        assert isinstance(args, dict), 'args should be a dict'
        return cast(dict[str, Any], args)

# pydantic_ai_slim/pydantic_ai/models/cerebras.py:99-116
def _cerebras_settings_to_openai_settings(model_settings: CerebrasModelSettings) -> OpenAIChatModelSettings:
    """Transforms a 'CerebrasModelSettings' object into an 'OpenAIChatModelSettings' object.

    Args:
        model_settings: The 'CerebrasModelSettings' object to transform.

    Returns:
        An 'OpenAIChatModelSettings' object with equivalent settings.
    """
    extra_body = cast(dict[str, Any], model_settings.get('extra_body', {}))

    if (disable_reasoning := model_settings.pop('cerebras_disable_reasoning', None)) is not None:
        extra_body['disable_reasoning'] = disable_reasoning

    if extra_body:
        model_settings['extra_body'] = extra_body

    return OpenAIChatModelSettings(**model_settings)  # type: ignore[reportCallIssue]

# pydantic_graph/pydantic_graph/beta/join.py:118-121
def reduce_dict_update(current: dict[K, V], inputs: Mapping[K, V]) -> dict[K, V]:
    """A reducer that updates a dict."""
    current.update(inputs)
    return current

# tests/models/test_openai.py:1709-1710
class MyPartialTypedDict(TypedDict, total=False):
    foo: str

# pydantic_evals/pydantic_evals/evaluators/spec.py:56-64
    def kwargs(self) -> dict[str, Any]:
        """Get the keyword arguments for the evaluator.

        Returns:
            A dictionary of keyword arguments if arguments is a dict, otherwise an empty dict.
        """
        if isinstance(self.arguments, dict):
            return self.arguments
        return {}

# pydantic_evals/pydantic_evals/otel/_context_in_memory_span_exporter.py:9-9
from weakref import WeakValueDictionary

# tests/models/test_openai.py:1704-1706
class MyOptionalTypedDict(TypedDict):
    foo: NotRequired[str]
    bar: str

# tests/models/test_openai.py:1753-1754
def tool_with_typed_dict(x: MyNormalTypedDict) -> str:
    return f'{x}'  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:70-72
def _json_dumps(obj: Any) -> str:
    """Dump an object to JSON string."""
    return to_json(obj).decode('utf-8')

# tests/evals/test_report_evaluators.py:632-651
def test_confusion_matrix_evaluator_metadata_key_with_non_dict():
    """ConfusionMatrixEvaluator with metadata key but non-dict metadata skips the case."""
    cases = [
        _make_report_case('c1', expected_output='A', metadata='some_string'),
        _make_report_case('c2', expected_output='B', metadata={'pred': 'B'}),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='metadata',
        predicted_key='pred',
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    # c1 should be skipped (non-dict metadata with key), only c2 used
    assert result.class_labels == ['B']
    assert result.matrix == [[1]]

# tests/typed_agent.py:183-183
structured_dict_agent = Agent(output_type=structured_dict)