## examples/pydantic_ai_examples/slack_lead_qualifier/agent.py

async def analyze_profile(profile: Profile) -> Analysis | None:
    result = await agent.run(profile.as_prompt())
    return result.output  ### [/analyze_profile]

## examples/pydantic_ai_examples/slack_lead_qualifier/store.py

    async def add(cls, analysis: Analysis):
        await cls._get_store().put.aio(analysis.profile.email, analysis.model_dump())

    async def list(cls) -> list[Analysis]:
        return [
            Analysis.model_validate(analysis)
            async for analysis in cls._get_store().values.aio()
        ]

    async def clear(cls):
        await cls._get_store().clear.aio()

    def _get_store(cls) -> modal.Dict:
        return modal.Dict.from_name('analyses', create_if_missing=True)  # type: ignore ### [/analysis_store]

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

    request: _messages.ModelRequest

    model_response: _messages.ModelResponse

## pydantic_ai_slim/pydantic_ai/_cli/__init__.py

    def __init__(self, special_suggestions: list[str] | None = None):
        super().__init__()
        self.special_suggestions = special_suggestions or []

## pydantic_ai_slim/pydantic_ai/_json_schema.py

    def transform(self, schema: JsonSchema) -> JsonSchema:
        """Make changes to the schema."""
        return schema

## pydantic_ai_slim/pydantic_ai/_otel_messages.py

    type: Literal['text']

## pydantic_ai_slim/pydantic_ai/_output.py

    def mode(self) -> OutputMode:
        return 'auto'

    def label(self) -> str:
        return "the agent's output tools"

## pydantic_ai_slim/pydantic_ai/_utils.py

class Unset:
    """A singleton to represent an unset value."""

    pass

def number_to_datetime(x: int | float) -> datetime:
    return _datetime_ta.validate_python(x)

## pydantic_ai_slim/pydantic_ai/agent/__init__.py

    instrument: InstrumentationSettings | bool | None

## pydantic_ai_slim/pydantic_ai/exceptions.py

class FallbackExceptionGroup(ExceptionGroup[Any]):
    """A group of exceptions that can be raised when all fallback models fail."""

## pydantic_ai_slim/pydantic_ai/messages.py

    def format(self) -> str:
        """The file format."""
        raise NotImplementedError

    def user_text_prompt(cls, user_prompt: str, *, instructions: str | None = None) -> ModelRequest:
        """Create a `ModelRequest` with a single user prompt as text."""
        return cls(parts=[UserPromptPart(user_prompt)], instructions=instructions)

## pydantic_ai_slim/pydantic_ai/models/__init__.py

def get_user_agent() -> str:
    """Get the user agent string for the HTTP client."""
    from .. import __version__

    return f'pydantic-ai/{__version__}'

## pydantic_ai_slim/pydantic_ai/models/fallback.py

def _default_fallback_condition_factory(exceptions: tuple[type[Exception], ...]) -> Callable[[Exception], bool]:
    """Create a default fallback condition for the given exceptions."""

    def fallback_condition(exception: Exception) -> bool:
        return isinstance(exception, exceptions)

    return fallback_condition

## pydantic_ai_slim/pydantic_ai/models/gemini.py

class _GeminiTextPart(_BasePart):
    text: str

## pydantic_ai_slim/pydantic_ai/models/groq.py

    error: _GroqToolUseFailedInnerError

## pydantic_ai_slim/pydantic_ai/models/openai.py

    def base_url(self) -> str:
        return str(self.client.base_url)

    def model_name(self) -> OpenAIModelName:
        """The model name."""
        return self._model_name

    def system(self) -> str:
        """The model provider."""
        return self._provider.name

    def supported_builtin_tools(cls) -> frozenset[type[AbstractBuiltinTool]]:
        """Return the set of builtin tool types this model can handle."""
        return frozenset({WebSearchTool})

    def system_prompt_role(self) -> OpenAISystemPromptRole | None:
        return OpenAIModelProfile.from_profile(self.profile).openai_system_prompt_role

    def _map_usage(self, response: chat.ChatCompletion) -> usage.RequestUsage:
        return _map_usage(response, self._provider.name, self._provider.base_url, self.model_name)

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:
        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]

class OpenAIModel(OpenAIChatModel):
    """Deprecated alias for `OpenAIChatModel`."""

## pydantic_ai_slim/pydantic_ai/models/test.py

    def generate(self) -> dict[str, Any]:
        """Generate data for the JSON schema."""
        return self._gen_any(self.schema)

## pydantic_ai_slim/pydantic_ai/providers/__init__.py

    def model_profile(self, model_name: str) -> ModelProfile | None:
        """The model profile for the named model, if available."""
        return None  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/providers/azure.py

    def name(self) -> str:
        return 'azure'

    def base_url(self) -> str:
        assert self._base_url is not None
        return self._base_url

    def client(self) -> AsyncOpenAI:
        return self._client

## pydantic_ai_slim/pydantic_ai/retries.py

    wait: WaitBaseT

    def close(self) -> None:
        self.wrapped.close()  # pragma: no cover

    async def aclose(self) -> None:
        await self.wrapped.aclose()

## pydantic_ai_slim/pydantic_ai/run.py

    def _node_to_task(self, node: _agent_graph.AgentNode[AgentDepsT, OutputDataT]) -> GraphTaskRequest:
        return GraphTaskRequest(NodeStep(type(node)).id, inputs=node, fork_stack=())

## pydantic_ai_slim/pydantic_ai/toolsets/abstract.py

    def validate_python(
        self, input: Any, *, allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False, **kwargs: Any
    ) -> Any: ...

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py

    def encode(self, sdk_version: int) -> str:
        return self.model_dump_json(by_alias=True, exclude_none=True)

## pydantic_ai_slim/pydantic_ai/usage.py

class Usage(RunUsage):
    """Deprecated alias for `RunUsage`."""

## pydantic_evals/pydantic_evals/_utils.py

class Unset:
    """A singleton to represent an unset value.

    Used to distinguish between explicitly set `None` values and values that were never set.

    Copied from pydantic_ai/_utils.py.
    """

    pass

## pydantic_evals/pydantic_evals/otel/span_tree.py

    def children(self) -> list[SpanNode]:
        return list(self.children_by_id.values())

    def descendants(self) -> list[SpanNode]:
        """Return all descendants of this node in DFS order."""
        return self.find_descendants(lambda _: True)

    def find_children(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:
        """Return all immediate children that satisfy the given predicate."""
        return list(self._filter_children(predicate))

    def first_child(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:
        """Return the first immediate child that satisfies the given predicate, or None if none match."""
        return next(self._filter_children(predicate), None)

    def any_child(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if there is at least one child that satisfies the predicate."""
        return self.first_child(predicate) is not None

    def _filter_children(self, predicate: SpanQuery | SpanPredicate) -> Iterator[SpanNode]:
        return (child for child in self.children if child.matches(predicate))

    def find_descendants(
        self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None
    ) -> list[SpanNode]:
        """Return all descendant nodes that satisfy the given predicate in DFS order."""
        return list(self._filter_descendants(predicate, stop_recursing_when))

    def find(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:
        """Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order."""
        return list(self._filter(predicate))

    def first(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:
        """Find the first node that matches a predicate, scanning from each root in DFS order. Returns `None` if not found."""
        return next(self._filter(predicate), None)

    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

    def _filter(self, predicate: SpanQuery | SpanPredicate) -> Iterator[SpanNode]:
        for node in self:
            if node.matches(predicate):
                yield node

## pydantic_graph/pydantic_graph/beta/graph.py

    def value(self) -> OutputT:
        return self._value

## pydantic_graph/pydantic_graph/beta/join.py

    def initial_factory(self):
        return self._initial_factory

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## pydantic_graph/pydantic_graph/beta/util.py

class Some(Generic[T]):
    """Container for explicitly present values in Maybe type pattern.

    This class represents a value that is definitely present, as opposed to None.
    It's part of the Maybe pattern, similar to Option/Maybe in functional programming,
    allowing distinction between "no value" (None) and "value is None" (Some(None)).
    """

    value: T
    """The wrapped value."""

## pydantic_graph/pydantic_graph/persistence/in_mem.py

    def dump_json(self, *, indent: int | None = None) -> bytes:
        """Dump the history to JSON bytes."""
        assert self._snapshots_type_adapter is not None, 'type adapter must be set to use `dump_json`'
        return self._snapshots_type_adapter.dump_json(self.history, indent=indent)

## tests/conftest.py

    def set(self, name: str, value: str) -> None:
        self.envars[name] = os.getenv(name)
        os.environ[name] = value

## tests/evals/test_dataset.py

class TaskOutput(BaseModel):
    answer: str
    confidence: float = 1.0

## tests/evals/test_otel.py

async def test_log_levels_and_exceptions():
    """Test recording different log levels and exceptions in spans."""
    with context_subtree() as tree:
        # Test different log levels
        with logfire.span('parent_span'):
            logfire.debug('Debug message')
            logfire.info('Info message')
            logfire.warn('Warning message')

            # Create child span with error
            with logfire.span('error_child') as error_span:
                logfire.error('Error occurred')
                # Record exception
                try:
                    raise ValueError('Test exception')
                except ValueError as e:
                    error_span.record_exception(e)
    assert isinstance(tree, SpanTree)

    # Verify log levels are preserved
    parent_span = tree.first(lambda node: node.name == 'parent_span')
    assert parent_span is not None

    # Find the error child span
    error_child = parent_span.first_child(lambda node: node.name == 'error_child')
    assert error_child is not None

    # Verify attributes reflect log levels and exceptions
    log_nodes = list(
        parent_span.find_descendants(
            lambda node: 'Debug message' in str(node.attributes)
            or 'Info message' in str(node.attributes)
            or 'Warning message' in str(node.attributes)
            or 'Error occurred' in str(node.attributes)
        )
    )
    assert len(log_nodes) > 0, 'Should have log messages as spans'

async def test_or_cannot_be_mixed(span_tree: SpanTree):
    with pytest.raises(ValueError) as exc_info:
        span_tree.first({'name_equals': 'child1', 'or_': [SpanQuery(name_equals='child2')]})
    assert str(exc_info.value) == snapshot("Cannot combine 'or_' conditions with other conditions at the same level")

## tests/graph/beta/test_broadcast_and_spread.py

class CounterState:
    values: list[int] = field(default_factory=list[int])

    values: list[int] = field(default_factory=list[int])

## tests/graph/beta/test_decisions.py

class DecisionState:
    path_taken: str | None = None
    value: int = 0

## tests/graph/beta/test_edge_cases.py

class EdgeCaseState:
    value: int = 0
    error_raised: bool = False

async def test_step_with_zero_value():
    """Test handling of zero values (ensure they're not confused with None/falsy)."""
    g = GraphBuilder(state_type=EdgeCaseState, output_type=int)

    @g.step
    async def return_zero(ctx: StepContext[EdgeCaseState, None, None]) -> int:
        return 0

    @g.step
    async def process_zero(ctx: StepContext[EdgeCaseState, None, int]) -> int:
        return ctx.inputs + 1

    g.add(
        g.edge_from(g.start_node).to(return_zero),
        g.edge_from(return_zero).to(process_zero),
        g.edge_from(process_zero).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=EdgeCaseState())
    assert result == 1

## tests/graph/beta/test_edge_labels.py

class LabelState:
    value: int = 0

## tests/graph/beta/test_graph_builder.py

class SimpleState:
    counter: int = 0
    result: str | None = None

## tests/graph/beta/test_graph_edge_cases.py

class MyState:
    value: int = 0

## tests/graph/beta/test_graph_execution.py

class ExecutionState:
    log: list[str] = field(default_factory=list[str])
    counter: int = 0

## tests/graph/beta/test_graph_iteration.py

class IterState:
    counter: int = 0

## tests/graph/beta/test_v1_v2_integration.py

class IntegrationState:
    log: list[str] = field(default_factory=list[str])

## tests/graph/test_mermaid.py

class Foo(BaseNode):
    async def run(self, ctx: GraphRunContext) -> Bar:
        return Bar()

class Bar(BaseNode[None, None, None]):
    async def run(self, ctx: GraphRunContext) -> End[None]:
        return End(None)

## tests/models/mock_openai.py

    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(completions=completions))

    def create_mock(cls, responses: MockResponse | Sequence[MockResponse]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(response=responses))

## tests/models/test_anthropic.py

    def create_mock(cls, messages_: MockAnthropicMessage | Sequence[MockAnthropicMessage]) -> AsyncAnthropic:
        return cast(AsyncAnthropic, cls(messages_=messages_))

## tests/models/test_cohere.py

    def create_mock(cls, completions: MockChatResponse | Sequence[MockChatResponse]) -> AsyncClientV2:
        return cast(AsyncClientV2, cls(completions=completions))

## tests/models/test_groq.py

    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncGroq:
        return cast(AsyncGroq, cls(completions=completions))

def text_chunk(text: str, finish_reason: FinishReason | None = None) -> chat.ChatCompletionChunk:
    return chunk([ChoiceDelta(content=text, role='assistant')], finish_reason=finish_reason)

## tests/models/test_huggingface.py

    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncInferenceClient:
        return cast(AsyncInferenceClient, cls(completions=completions))

## tests/models/test_mistral.py

    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> Mistral:
        return cast(Mistral, cls(completions=completions))

## tests/models/test_model_test.py

class AgentRunDeps:
    run_id: int

## tests/models/test_openai.py

def test_openai_model_profile():
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key='foobar'))
    assert isinstance(m.profile, OpenAIModelProfile)

def test_azure_400_non_dict_body(allow_model_requests: None) -> None:
    """Test a 400 error from Azure where the body is not a dictionary."""
    mock_client = MockOpenAI.create_mock(
        APIStatusError(
            'Bad Request',
            response=httpx.Response(status_code=400, request=httpx.Request('POST', 'https://example.com/v1')),
            body='Raw string body',
        )
    )
    m = OpenAIChatModel('gpt-5-mini', provider=AzureProvider(openai_client=cast(AsyncAzureOpenAI, mock_client)))
    agent = Agent(m)

    with pytest.raises(ModelHTTPError) as exc_info:
        agent.run_sync('hello')

    assert exc_info.value.status_code == 400

## tests/test_ag_ui.py

def uuid_str() -> str:
    """Generate a random UUID string."""
    return uuid.uuid4().hex

## tests/test_agent.py

class UserContext:
    location: str | None

## tests/test_deps.py

class MyDeps:
    foo: int
    bar: int

## tests/test_exceptions.py

def test_exceptions_hashable(exc_factory: Callable[[], Any]):
    """Test that all exception classes are hashable and usable as keys."""
    exc = exc_factory()

    # Does not raise TypeError
    _ = hash(exc)

    # Can be used in sets and dicts
    s = {exc}
    d = {exc: 'value'}

    assert exc in s
    assert d[exc] == 'value'

## tests/test_mcp.py

def model(openai_api_key: str) -> Model:
    return OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key=openai_api_key))

async def test_agent_with_server_not_running(agent: Agent, allow_model_requests: None):
    result = await agent.run('What is 0 degrees Celsius in Fahrenheit?')
    assert result.output == snapshot('0 degrees Celsius is 32.0 degrees Fahrenheit.')

## tests/test_temporal.py

    async def decode(
        self, payloads: Sequence[temporalio.api.common.v1.Payload]
    ) -> list[temporalio.api.common.v1.Payload]:  # pragma: no cover
        return list(payloads)

## tests/test_tenacity.py

    def test_retry_on_exception(self):
        """Test that exceptions trigger retries."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response = Mock(spec=httpx.Response)

        # Fail twice, succeed on third attempt
        mock_transport.handle_request.side_effect = [
            httpx.ConnectError('Connection failed'),
            httpx.ConnectError('Connection failed again'),
            mock_response,
        ]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),  # Very short wait for tests
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response
        assert mock_transport.handle_request.call_count == 3

## tests/test_ui.py

    def dump_messages(cls, messages: Sequence[ModelMessage]) -> list[ModelMessage]:
        return list(messages)

    def load_messages(cls, messages: Sequence[ModelMessage]) -> list[ModelMessage]:
        return list(messages)
