# pydantic_ai_slim/pydantic_ai/_ssrf.py:67-67
    is_https: bool

# tests/test_ssrf.py:235-240
    def test_http_default_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='203.0.113.50', hostname='example.com', port=80, is_https=False, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'http://203.0.113.50/path'

# tests/test_ssrf.py:242-247
    def test_https_default_port(self) -> None:
        resolved = ResolvedUrl(
            resolved_ip='203.0.113.50', hostname='example.com', port=443, is_https=True, path='/path'
        )
        url = build_url_with_ip(resolved)
        assert url == 'https://203.0.113.50/path'

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/models/gemini.py:77-98
class GeminiModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    gemini_safety_settings: list[GeminiSafetySettings]
    """Safety settings options for Gemini model request."""

    gemini_thinking_config: ThinkingConfig
    """Thinking is "on" by default in both the API and AI Studio.

    Being on by default doesn't mean the model will send back thoughts. For that, you would need to set `include_thoughts`
    to `True`. If you want to avoid the model spending any tokens on thinking, you can set `thinking_budget` to `0`.

    See more about it on <https://ai.google.dev/gemini-api/docs/thinking>.
    """

    gemini_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI provider.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """

# pydantic_ai_slim/pydantic_ai/models/gemini.py:77-98
class GeminiModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    gemini_safety_settings: list[GeminiSafetySettings]
    """Safety settings options for Gemini model request."""

    gemini_thinking_config: ThinkingConfig
    """Thinking is "on" by default in both the API and AI Studio.

    Being on by default doesn't mean the model will send back thoughts. For that, you would need to set `include_thoughts`
    to `True`. If you want to avoid the model spending any tokens on thinking, you can set `thinking_budget` to `0`.

    See more about it on <https://ai.google.dev/gemini-api/docs/thinking>.
    """

    gemini_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI provider.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """

# tests/test_dbos.py:1492-1493
class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

# tests/test_dbos.py:1492-1493
class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

# tests/test_dbos.py:1492-1493
class CustomModelSettings(ModelSettings, total=False):
    custom_setting: str

# pydantic_ai_slim/pydantic_ai/models/openai.py:372-373
class OpenAIModelSettings(OpenAIChatModelSettings, total=False):
    """Deprecated alias for `OpenAIChatModelSettings`."""

# pydantic_ai_slim/pydantic_ai/_ssrf.py:58-58
    resolved_ip: str

# pydantic_ai_slim/pydantic_ai/settings.py:96-96
    seed: int

# pydantic_ai_slim/pydantic_ai/models/groq.py:107-116
class GroqModelSettings(ModelSettings, total=False):
    """Settings used for a Groq model request."""

    # ALL FIELDS MUST BE `groq_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    groq_reasoning_format: Literal['hidden', 'raw', 'parsed']
    """The format of the reasoning output.

    See [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details.
    """

# pydantic_ai_slim/pydantic_ai/settings.py:52-52
    top_p: float

# pydantic_ai_slim/pydantic_ai/models/cohere.py:89-90
class CohereModelSettings(ModelSettings, total=False):
    """Settings used for a Cohere model request."""

# pydantic_ai_slim/pydantic_ai/models/gemini.py:77-98
class GeminiModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    gemini_safety_settings: list[GeminiSafetySettings]
    """Safety settings options for Gemini model request."""

    gemini_thinking_config: ThinkingConfig
    """Thinking is "on" by default in both the API and AI Studio.

    Being on by default doesn't mean the model will send back thoughts. For that, you would need to set `include_thoughts`
    to `True`. If you want to avoid the model spending any tokens on thinking, you can set `thinking_budget` to `0`.

    See more about it on <https://ai.google.dev/gemini-api/docs/thinking>.
    """

    gemini_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI provider.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """

# pydantic_ai_slim/pydantic_ai/models/google.py:161-194
class GoogleModelSettings(ModelSettings, total=False):
    """Settings used for a Gemini model request."""

    # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    google_safety_settings: list[SafetySettingDict]
    """The safety settings to use for the model.

    See <https://ai.google.dev/gemini-api/docs/safety-settings> for more information.
    """

    google_thinking_config: ThinkingConfigDict
    """The thinking configuration to use for the model.

    See <https://ai.google.dev/gemini-api/docs/thinking> for more information.
    """

    google_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI API.

    See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.
    """

    google_video_resolution: MediaResolution
    """The video resolution to use for the model.

    See <https://ai.google.dev/api/generate-content#MediaResolution> for more information.
    """

    google_cached_content: str
    """The name of the cached content to use for the model.

    See <https://ai.google.dev/gemini-api/docs/caching> for more information.
    """

# pydantic_ai_slim/pydantic_ai/_ssrf.py:55-71
class ResolvedUrl:
    """Result of URL validation and DNS resolution."""

    resolved_ip: str
    """The resolved IP address to connect to."""

    hostname: str
    """The original hostname (used for Host header)."""

    port: int
    """The port number."""

    is_https: bool
    """Whether to use HTTPS."""

    path: str
    """The path including query string and fragment."""

# pydantic_ai_slim/pydantic_ai/settings.py:72-72
    timeout: float | Timeout

# pydantic_ai_slim/pydantic_ai/models/mistral.py:114-115
class MistralModelSettings(ModelSettings, total=False):
    """Settings used for a Mistral model request."""

# pydantic_ai_slim/pydantic_ai/models/openai.py:301-368
class OpenAIChatModelSettings(ModelSettings, total=False):
    """Settings used for an OpenAI model request."""

    # ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    openai_reasoning_effort: ReasoningEffort
    """Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).

    Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
    result in faster responses and fewer tokens used on reasoning in a response.
    """

    openai_logprobs: bool
    """Include log probabilities in the response.

    For Chat models, these will be included in `ModelResponse.provider_details['logprobs']`.
    For Responses models, these will be included in the response output parts `TextPart.provider_details['logprobs']`.
    """

    openai_top_logprobs: int
    """Include log probabilities of the top n tokens in the response."""

    openai_store: bool | None
    """Whether or not to store the output of this request in OpenAI's systems.

    If `False`, OpenAI will not store the request for its own internal review or training.
    See [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat/create#chat-create-store)."""

    openai_user: str
    """A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse.

    See [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details.
    """

    openai_service_tier: Literal['auto', 'default', 'flex', 'priority']
    """The service tier to use for the model request.

    Currently supported values are `auto`, `default`, `flex`, and `priority`.
    For more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier).
    """

    openai_prediction: ChatCompletionPredictionContentParam
    """Enables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs).

    This feature is currently only supported for some OpenAI models.
    """

    openai_prompt_cache_key: str
    """Used by OpenAI to cache responses for similar requests to optimize your cache hit rates.

    See the [OpenAI Prompt Caching documentation](https://platform.openai.com/docs/guides/prompt-caching#how-it-works) for more information.
    """

    openai_prompt_cache_retention: Literal['in_memory', '24h']
    """The retention policy for the prompt cache. Set to 24h to enable extended prompt caching, which keeps cached prefixes active for longer, up to a maximum of 24 hours.

    See the [OpenAI Prompt Caching documentation](https://platform.openai.com/docs/guides/prompt-caching#how-it-works) for more information.
    """

    openai_continuous_usage_stats: bool
    """When True, enables continuous usage statistics in streaming responses.

    When enabled, the API returns cumulative usage data with each chunk rather than only at the end.
    This setting correctly handles the cumulative nature of these stats by using only the final
    usage values rather than summing all intermediate values.

    See [OpenAI's streaming documentation](https://platform.openai.com/docs/api-reference/chat/create#stream_options) for more information.
    """

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:164-237
class AnthropicModelSettings(ModelSettings, total=False):
    """Settings used for an Anthropic model request."""

    # ALL FIELDS MUST BE `anthropic_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    anthropic_metadata: BetaMetadataParam
    """An object describing metadata about the request.

    Contains `user_id`, an external identifier for the user who is associated with the request.
    """

    anthropic_thinking: BetaThinkingConfigParam
    """Determine whether the model should generate a thinking block.

    See [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information.
    """

    anthropic_cache_tool_definitions: bool | Literal['5m', '1h']
    """Whether to add `cache_control` to the last tool definition.

    When enabled, the last tool in the `tools` array will have `cache_control` set,
    allowing Anthropic to cache tool definitions and reduce costs.
    If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly.
    TTL is automatically omitted for Bedrock, as it does not support explicit TTL.
    See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.
    """

    anthropic_cache_instructions: bool | Literal['5m', '1h']
    """Whether to add `cache_control` to the last system prompt block.

    When enabled, the last system prompt will have `cache_control` set,
    allowing Anthropic to cache system instructions and reduce costs.
    If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly.
    TTL is automatically omitted for Bedrock, as it does not support explicit TTL.
    See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.
    """

    anthropic_cache_messages: bool | Literal['5m', '1h']
    """Convenience setting to enable caching for the last user message.

    When enabled, this automatically adds a cache point to the last content block
    in the final user message, which is useful for caching conversation history
    or context in multi-turn conversations.
    If `True`, uses TTL='5m'. You can also specify '5m' or '1h' directly.
    TTL is automatically omitted for Bedrock, as it does not support explicit TTL.

    Note: Uses 1 of Anthropic's 4 available cache points per request. Any additional CachePoint
    markers in messages will be automatically limited to respect the 4-cache-point maximum.
    See https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching for more information.
    """

    anthropic_effort: Literal['low', 'medium', 'high', 'max'] | None
    """The effort level for the model to use when generating a response.

    See [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/effort) for more information.
    """

    anthropic_container: BetaContainerParams | Literal[False]
    """Container configuration for multi-turn conversations.

    By default, if previous messages contain a container_id (from a prior response),
    it will be reused automatically.

    Set to `False` to force a fresh container (ignore any `container_id` from history).
    Set to a dict (e.g. `{'id': 'container_xxx'}`) to explicitly specify a container.
    """

    anthropic_betas: list[AnthropicBetaParam]
    """List of Anthropic beta features to enable for API requests.

    Each item can be a known beta name (e.g. 'interleaved-thinking-2025-05-14') or a custom string.
    Merged with auto-added betas (e.g. structured-outputs, builtin tools) and any betas from
    extra_headers['anthropic-beta']. See the Anthropic docs for available beta features.
    """

# pydantic_ai_slim/pydantic_ai/models/xai.py:102-102
    xai_user: str

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:201-238
class OpenRouterModelSettings(ModelSettings, total=False):
    """Settings used for an OpenRouter model request."""

    # ALL FIELDS MUST BE `openrouter_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    openrouter_models: list[str]
    """A list of fallback models.

    These models will be tried, in order, if the main model returns an error. [See details](https://openrouter.ai/docs/features/model-routing#the-models-parameter)
    """

    openrouter_provider: OpenRouterProviderConfig
    """OpenRouter routes requests to the best available providers for your model. By default, requests are load balanced across the top providers to maximize uptime.

    You can customize how your requests are routed using the provider object. [See more](https://openrouter.ai/docs/features/provider-routing)"""

    openrouter_preset: str
    """Presets allow you to separate your LLM configuration from your code.

    Create and manage presets through the OpenRouter web application to control provider routing, model selection, system prompts, and other parameters, then reference them in OpenRouter API requests. [See more](https://openrouter.ai/docs/features/presets)"""

    openrouter_transforms: list[OpenRouterTransforms]
    """To help with prompts that exceed the maximum context size of a model.

    Transforms work by removing or truncating messages from the middle of the prompt, until the prompt fits within the model's context window. [See more](https://openrouter.ai/docs/features/message-transforms)
    """

    openrouter_reasoning: OpenRouterReasoning
    """To control the reasoning tokens in the request.

    The reasoning config object consolidates settings for controlling reasoning strength across different models. [See more](https://openrouter.ai/docs/use-cases/reasoning-tokens)
    """

    openrouter_usage: OpenRouterUsageConfig
    """To control the usage of the model.

    The usage config object consolidates settings for enabling detailed usage information. [See more](https://openrouter.ai/docs/use-cases/usage-accounting)
    """

# pydantic_ai_slim/pydantic_ai/settings.py:14-14
    max_tokens: int

# pydantic_ai_slim/pydantic_ai/settings.py:137-137
    logit_bias: dict[str, int]

# pydantic_ai_slim/pydantic_ai/settings.py:173-173
    extra_body: object

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:106-107
class HuggingFaceModelSettings(ModelSettings, total=False):
    """Settings used for a Hugging Face model request."""

# pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py:19-25
class MCPSamplingModelSettings(ModelSettings, total=False):
    """Settings used for an MCP Sampling model request."""

    # ALL FIELDS MUST BE `mcp_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.

    mcp_model_preferences: ModelPreferences
    """Model preferences to use for MCP Sampling."""

# pydantic_ai_slim/pydantic_ai/settings.py:31-31
    temperature: float

# pydantic_ai_slim/pydantic_ai/settings.py:162-162
    extra_headers: dict[str, str]

# pydantic_ai_slim/pydantic_ai/models/xai.py:96-96
    xai_logprobs: bool

# pydantic_ai_slim/pydantic_ai/settings.py:147-147
    stop_sequences: list[str]

# pydantic_ai_slim/pydantic_ai/models/gemini.py:94-94
    gemini_labels: dict[str, str]

# pydantic_ai_slim/pydantic_ai/models/google.py:178-178
    google_labels: dict[str, str]

# pydantic_ai_slim/pydantic_ai/models/openai.py:329-329
    openai_user: str

# pydantic_ai_slim/pydantic_ai/settings.py:109-109
    presence_penalty: float

# pydantic_ai_slim/pydantic_ai/models/openai.py:323-323
    openai_store: bool | None

# pydantic_ai_slim/pydantic_ai/settings.py:123-123
    frequency_penalty: float

# tests/test_dbos.py:1493-1493
    custom_setting: str

# tests/test_dbos.py:1493-1493
    custom_setting: str

# tests/test_dbos.py:1493-1493
    custom_setting: str

# pydantic_ai_slim/pydantic_ai/models/xai.py:99-99
    xai_top_logprobs: int

# pydantic_ai_slim/pydantic_ai/settings.py:85-85
    parallel_tool_calls: bool

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:231-231
    anthropic_betas: list[AnthropicBetaParam]

# pydantic_ai_slim/pydantic_ai/models/xai.py:105-105
    xai_store_messages: bool

# pydantic_ai_slim/pydantic_ai/models/openai.py:313-313
    openai_logprobs: bool

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:215-215
    anthropic_effort: Literal['low', 'medium', 'high', 'max'] | None

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:234-234
    openrouter_usage: OpenRouterUsageConfig

# pydantic_ai_slim/pydantic_ai/_ssrf.py:64-64
    port: int

# pydantic_ai_slim/pydantic_ai/_ssrf.py:70-70
    path: str

# pydantic_ai_slim/pydantic_ai/_ssrf.py:55-71
class ResolvedUrl:
    """Result of URL validation and DNS resolution."""

    resolved_ip: str
    """The resolved IP address to connect to."""

    hostname: str
    """The original hostname (used for Host header)."""

    port: int
    """The port number."""

    is_https: bool
    """Whether to use HTTPS."""

    path: str
    """The path including query string and fragment."""

# pydantic_ai_slim/pydantic_ai/models/openai.py:342-342
    openai_prediction: ChatCompletionPredictionContentParam

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:206-206
    openrouter_models: list[str]

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:217-217
    openrouter_preset: str

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:169-169
    anthropic_metadata: BetaMetadataParam

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:175-175
    anthropic_thinking: BetaThinkingConfigParam

# pydantic_ai_slim/pydantic_ai/models/groq.py:112-112
    groq_reasoning_format: Literal['hidden', 'raw', 'parsed']

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:221-221
    anthropic_container: BetaContainerParams | Literal[False]

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:294-294
    bedrock_service_tier: ServiceTierTypeDef

# pydantic_ai_slim/pydantic_ai/models/xai.py:135-135
    xai_include_mcp_output: bool

# pydantic_ai_slim/pydantic_ai/models/google.py:190-190
    google_cached_content: str

# pydantic_ai_slim/pydantic_ai/models/openai.py:320-320
    openai_top_logprobs: int

# pydantic_ai_slim/pydantic_ai/models/openai.py:335-335
    openai_service_tier: Literal['auto', 'default', 'flex', 'priority']

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:212-212
    openrouter_provider: OpenRouterProviderConfig

# pydantic_ai_slim/pydantic_ai/models/cerebras.py:99-116
def _cerebras_settings_to_openai_settings(model_settings: CerebrasModelSettings) -> OpenAIChatModelSettings:
    """Transforms a 'CerebrasModelSettings' object into an 'OpenAIChatModelSettings' object.

    Args:
        model_settings: The 'CerebrasModelSettings' object to transform.

    Returns:
        An 'OpenAIChatModelSettings' object with equivalent settings.
    """
    extra_body = cast(dict[str, Any], model_settings.get('extra_body', {}))

    if (disable_reasoning := model_settings.pop('cerebras_disable_reasoning', None)) is not None:
        extra_body['disable_reasoning'] = disable_reasoning

    if extra_body:
        model_settings['extra_body'] = extra_body

    return OpenAIChatModelSettings(**model_settings)  # type: ignore[reportCallIssue]

# pydantic_ai_slim/pydantic_ai/models/openai.py:410-410
    openai_truncation: Literal['disabled', 'auto']

# pydantic_ai_slim/pydantic_ai/models/gemini.py:82-82
    gemini_safety_settings: list[GeminiSafetySettings]

# pydantic_ai_slim/pydantic_ai/models/gemini.py:85-85
    gemini_thinking_config: ThinkingConfig

# pydantic_ai_slim/pydantic_ai/models/google.py:166-166
    google_safety_settings: list[SafetySettingDict]

# pydantic_ai_slim/pydantic_ai/models/google.py:172-172
    google_thinking_config: ThinkingConfigDict

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:228-228
    openrouter_reasoning: OpenRouterReasoning

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:490-516
def _openrouter_settings_to_openai_settings(model_settings: OpenRouterModelSettings) -> OpenAIChatModelSettings:
    """Transforms a 'OpenRouterModelSettings' object into an 'OpenAIChatModelSettings' object.

    Args:
        model_settings: The 'OpenRouterModelSettings' object to transform.

    Returns:
        An 'OpenAIChatModelSettings' object with equivalent settings.
    """
    extra_body = cast(dict[str, Any], model_settings.get('extra_body', {}))

    if models := model_settings.pop('openrouter_models', None):
        extra_body['models'] = models
    if provider := model_settings.pop('openrouter_provider', None):
        extra_body['provider'] = provider
    if preset := model_settings.pop('openrouter_preset', None):
        extra_body['preset'] = preset
    if transforms := model_settings.pop('openrouter_transforms', None):
        extra_body['transforms'] = transforms
    if reasoning := model_settings.pop('openrouter_reasoning', None):
        extra_body['reasoning'] = reasoning
    if usage := model_settings.pop('openrouter_usage', None):
        extra_body['usage'] = usage

    model_settings['extra_body'] = extra_body

    return OpenAIChatModelSettings(**model_settings)  # type: ignore[reportCallIssue]

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:282-282
    bedrock_cache_messages: bool

# pydantic_ai_slim/pydantic_ai/models/xai.py:108-108
    xai_previous_response_id: str

# pydantic_ai_slim/pydantic_ai/models/google.py:184-184
    google_video_resolution: MediaResolution

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:222-222
    openrouter_transforms: list[OpenRouterTransforms]

# tests/test_ssrf.py:503-529
    async def test_relative_redirect_resolved(self) -> None:
        """Test that relative redirect URLs are resolved correctly."""
        redirect_response = AsyncMock()
        redirect_response.is_redirect = True
        redirect_response.headers = {'location': '/new-path/file.txt'}

        final_response = AsyncMock()
        final_response.is_redirect = False
        final_response.raise_for_status = lambda: None
        final_response.content = b'final content'

        with (
            patch('pydantic_ai._ssrf.run_in_executor') as mock_executor,
            patch('pydantic_ai._ssrf.cached_async_http_client') as mock_client_fn,
        ):
            mock_executor.return_value = [(2, 1, 6, '', ('93.184.215.14', 0))]

            mock_client = AsyncMock()
            mock_client.get.side_effect = [redirect_response, final_response]
            mock_client_fn.return_value = mock_client

            response = await safe_download('https://example.com/old-path/file.txt')
            assert response.content == b'final content'

            # Check that the second request was to the correct path
            second_call = mock_client.get.call_args_list[1]
            assert '/new-path/file.txt' in second_call[0][0]

# pydantic_ai_slim/pydantic_ai/_ssrf.py:61-61
    hostname: str

# pydantic_ai_slim/pydantic_ai/models/xai.py:90-139
class XaiModelSettings(ModelSettings, total=False):
    """Settings specific to xAI models.

    See [xAI SDK documentation](https://docs.x.ai/docs) for more details on these parameters.
    """

    xai_logprobs: bool
    """Whether to return log probabilities of the output tokens or not."""

    xai_top_logprobs: int
    """An integer between 0 and 20 specifying the number of most likely tokens to return at each position."""

    xai_user: str
    """A unique identifier representing your end-user, which can help xAI to monitor and detect abuse."""

    xai_store_messages: bool
    """Whether to store messages on xAI's servers for conversation continuity."""

    xai_previous_response_id: str
    """The ID of the previous response to continue the conversation."""

    xai_include_encrypted_content: bool
    """Whether to include the encrypted content in the response.

    Corresponds to the `use_encrypted_content` value of the model settings in the Responses API.
    """

    xai_include_code_execution_output: bool
    """Whether to include the code execution results in the response.

    Corresponds to the `code_interpreter_call.outputs` value of the `include` parameter in the Responses API.
    """

    xai_include_web_search_output: bool
    """Whether to include the web search results in the response.

    Corresponds to the `web_search_call.action.sources` value of the `include` parameter in the Responses API.
    """

    xai_include_inline_citations: bool
    """Whether to include inline citations in the response.

    Corresponds to the `inline_citations` option in the xAI `include` parameter.
    """

    xai_include_mcp_output: bool
    """Whether to include the MCP results in the response.

    Corresponds to the `mcp_call.outputs` value of the `include` parameter in the Responses API.
    """

# tests/models/test_cerebras.py:52-69
async def test_cerebras_settings_transformation():
    """Test that CerebrasModelSettings are correctly transformed to OpenAIChatModelSettings."""
    # Test with disable_reasoning
    settings = CerebrasModelSettings(cerebras_disable_reasoning=True)
    transformed = _cerebras_settings_to_openai_settings(settings)
    extra_body = cast(dict[str, Any], transformed.get('extra_body', {}))
    assert extra_body.get('disable_reasoning') is True

    # Test without disable_reasoning (should not have extra_body)
    settings_empty = CerebrasModelSettings()
    transformed_empty = _cerebras_settings_to_openai_settings(settings_empty)
    assert transformed_empty.get('extra_body') is None

    # Test with disable_reasoning=False
    settings_false = CerebrasModelSettings(cerebras_disable_reasoning=False)
    transformed_false = _cerebras_settings_to_openai_settings(settings_false)
    extra_body_false = cast(dict[str, Any], transformed_false.get('extra_body', {}))
    assert extra_body_false.get('disable_reasoning') is False

# pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py:24-24
    mcp_model_preferences: ModelPreferences

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:231-231
    bedrock_guardrail_config: GuardrailConfigurationTypeDef

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:243-243
    bedrock_request_metadata: dict[str, str]

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:255-255
    bedrock_prompt_variables: Mapping[str, PromptVariableValuesTypeDef]

# pydantic_ai_slim/pydantic_ai/models/openai.py:382-382
    openai_builtin_tools: Sequence[FileSearchToolParam | WebSearchToolParam | ComputerToolParam]

# pydantic_ai_slim/pydantic_ai/models/openai.py:306-306
    openai_reasoning_effort: ReasoningEffort

# pydantic_ai_slim/pydantic_ai/models/openai.py:348-348
    openai_prompt_cache_key: str

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:201-201
    anthropic_cache_messages: bool | Literal['5m', '1h']

# pydantic_ai_slim/pydantic_ai/models/openai.py:421-421
    openai_text_verbosity: Literal['low', 'medium', 'high']

# pydantic_ai_slim/pydantic_ai/models/cerebras.py:46-58
class CerebrasModelSettings(ModelSettings, total=False):
    """Settings used for a Cerebras model request.

    ALL FIELDS MUST BE `cerebras_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.
    """

    cerebras_disable_reasoning: bool
    """Disable reasoning for the model.

    This setting is only supported on reasoning models: `zai-glm-4.6` and `gpt-oss-120b`.

    See [the Cerebras docs](https://inference-docs.cerebras.ai/resources/openai#passing-non-standard-parameters) for more details.
    """

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:275-275
    bedrock_cache_instructions: bool

# pydantic_ai_slim/pydantic_ai/models/xai.py:129-129
    xai_include_inline_citations: bool

# pydantic_ai_slim/pydantic_ai/models/cerebras.py:52-52
    cerebras_disable_reasoning: bool

# pydantic_ai_slim/pydantic_ai/models/xai.py:111-111
    xai_include_encrypted_content: bool

# pydantic_ai_slim/pydantic_ai/models/xai.py:123-123
    xai_include_web_search_output: bool

# pydantic_ai_slim/pydantic_ai/models/openai.py:391-391
    openai_reasoning_summary: Literal['detailed', 'concise', 'auto']

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:191-191
    anthropic_cache_instructions: bool | Literal['5m', '1h']

# pydantic_ai_slim/pydantic_ai/models/openai.py:401-401
    openai_send_reasoning_ids: bool

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:267-267
    bedrock_cache_tool_definitions: bool

# pydantic_ai_slim/pydantic_ai/models/openai.py:354-354
    openai_prompt_cache_retention: Literal['in_memory', '24h']

# pydantic_ai_slim/pydantic_ai/models/openai.py:360-360
    openai_continuous_usage_stats: bool

# pydantic_ai_slim/pydantic_ai/models/openai.py:429-429
    openai_previous_response_id: Literal['auto'] | str

# pydantic_ai_slim/pydantic_ai/models/xai.py:117-117
    xai_include_code_execution_output: bool

# pydantic_ai_slim/pydantic_ai/models/openai.py:376-465
class OpenAIResponsesModelSettings(OpenAIChatModelSettings, total=False):
    """Settings used for an OpenAI Responses model request.

    ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.
    """

    openai_builtin_tools: Sequence[FileSearchToolParam | WebSearchToolParam | ComputerToolParam]
    """The provided OpenAI built-in tools to use.

    See [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details.
    """

    openai_reasoning_generate_summary: Literal['detailed', 'concise']
    """Deprecated alias for `openai_reasoning_summary`."""

    openai_reasoning_summary: Literal['detailed', 'concise', 'auto']
    """A summary of the reasoning performed by the model.

    This can be useful for debugging and understanding the model's reasoning process.
    One of `concise`, `detailed`, or `auto`.

    Check the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries)
    for more details.
    """

    openai_send_reasoning_ids: bool
    """Whether to send the unique IDs of reasoning, text, and function call parts from the message history to the model. Enabled by default for reasoning models.

    This can result in errors like `"Item 'rs_123' of type 'reasoning' was provided without its required following item."`
    if the message history you're sending does not match exactly what was received from the Responses API in a previous response,
    for example if you're using a [history processor](../../message-history.md#processing-message-history).
    In that case, you'll want to disable this.
    """

    openai_truncation: Literal['disabled', 'auto']
    """The truncation strategy to use for the model response.

    It can be either:
    - `disabled` (default): If a model response will exceed the context window size for a model, the
        request will fail with a 400 error.
    - `auto`: If the context of this response and previous ones exceeds the model's context window size,
        the model will truncate the response to fit the context window by dropping input items in the
        middle of the conversation.
    """

    openai_text_verbosity: Literal['low', 'medium', 'high']
    """Constrains the verbosity of the model's text response.

    Lower values will result in more concise responses, while higher values will
    result in more verbose responses. Currently supported values are `low`,
    `medium`, and `high`.
    """

    openai_previous_response_id: Literal['auto'] | str
    """The ID of a previous response from the model to use as the starting point for a continued conversation.

    When set to `'auto'`, the request automatically uses the most recent
    `provider_response_id` from the message history and omits earlier messages.

    This enables the model to use server-side conversation state and faithfully reference previous reasoning.
    See the [OpenAI Responses API documentation](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context)
    for more information.
    """

    openai_include_code_execution_outputs: bool
    """Whether to include the code execution results in the response.

    Corresponds to the `code_interpreter_call.outputs` value of the `include` parameter in the Responses API.
    """

    openai_include_web_search_sources: bool
    """Whether to include the web search results in the response.

    Corresponds to the `web_search_call.action.sources` value of the `include` parameter in the Responses API.
    """

    openai_include_file_search_results: bool
    """Whether to include the file search results in the response.

    Corresponds to the `file_search_call.results` value of the `include` parameter in the Responses API.
    """

    openai_include_raw_annotations: bool
    """Whether to include the raw annotations in `TextPart.provider_details`.

    When enabled, any annotations (e.g., citations from web search) will be available
    in the `provider_details['annotations']` field of text parts.
    This is opt-in since there may be overlap with native annotation support once
    added via https://github.com/pydantic/pydantic-ai/issues/3126.
    """

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:181-181
    anthropic_cache_tool_definitions: bool | Literal['5m', '1h']

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:237-237
    bedrock_performance_configuration: PerformanceConfigurationTypeDef

# pydantic_ai_slim/pydantic_ai/models/openai.py:458-458
    openai_include_raw_annotations: bool

# pydantic_ai_slim/pydantic_ai/models/xai.py:914-920
def _map_model_settings(model_settings: XaiModelSettings) -> dict[str, Any]:
    """Map pydantic_ai ModelSettings to xAI SDK parameters."""
    return {
        _XAI_MODEL_SETTINGS_MAPPING[key]: value
        for key, value in model_settings.items()
        if key in _XAI_MODEL_SETTINGS_MAPPING
    }

# pydantic_ai_slim/pydantic_ai/__init__.py:113-113
from .settings import ModelSettings

# pydantic_ai_slim/pydantic_ai/models/openai.py:388-388
    openai_reasoning_generate_summary: Literal['detailed', 'concise']

# pydantic_ai_slim/pydantic_ai/models/openai.py:446-446
    openai_include_web_search_sources: bool

# pydantic_ai_slim/pydantic_ai/models/openai.py:452-452
    openai_include_file_search_results: bool

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:261-261
    bedrock_additional_model_requests_fields: Mapping[str, Any]

# pydantic_ai_slim/pydantic_ai/models/openai.py:440-440
    openai_include_code_execution_outputs: bool

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:249-249
    bedrock_additional_model_response_fields_paths: list[str]

# tests/models/test_model_settings.py:147-168
def test_empty_settings_objects():
    """Test that empty ModelSettings objects work correctly in the hierarchy."""
    captured_settings = None

    def capture_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('captured')])

    # All levels have empty settings
    model = FunctionModel(capture_settings, settings=ModelSettings())
    agent = Agent(model=model, model_settings=ModelSettings())

    # Run with one actual setting
    run_settings = ModelSettings(temperature=0.75)
    result = agent.run_sync('test', model_settings=run_settings)
    assert result.output == 'captured'

    # Should only have the run setting
    assert captured_settings is not None
    assert captured_settings.get('temperature') == 0.75
    assert len(captured_settings) == 1  # Only one setting should be present

# pydantic_ai_slim/pydantic_ai/mcp.py:241-241
    tools_list_changed: bool = False

# pydantic_ai_slim/pydantic_ai/mcp.py:229-229
    prompts_list_changed: bool = False

# pydantic_ai_slim/pydantic_ai/mcp.py:235-235
    resources_list_changed: bool = False

# tests/test_ssrf.py:285-288
    def test_absolute_path(self) -> None:
        """Test that absolute paths are resolved against the current URL."""
        result = resolve_redirect_url('https://example.com/old/path', '/new/path')
        assert result == 'https://example.com/new/path'

# tests/test_ssrf.py:290-293
    def test_relative_path(self) -> None:
        """Test that relative paths are resolved against the current URL."""
        result = resolve_redirect_url('https://example.com/old/path', 'new-file.txt')
        assert result == 'https://example.com/old/new-file.txt'

# tests/test_mcp.py:2395-2402
async def test_server_capabilities_list_changed_fields() -> None:
    """Test that ServerCapabilities correctly parses listChanged fields."""
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'])
    async with server:
        caps = server.capabilities
        assert isinstance(caps.prompts_list_changed, bool)
        assert isinstance(caps.tools_list_changed, bool)
        assert isinstance(caps.resources_list_changed, bool)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque