# pydantic_ai_slim/pydantic_ai/providers/openrouter.py:23-23
from pydantic_ai.profiles.openai import OpenAIJsonSchemaTransformer, OpenAIModelProfile, openai_model_profile

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:27-27
from ...run import AgentRunResultEvent

# examples/pydantic_ai_examples/stream_markdown.py:12-12
from rich.console import Console, ConsoleOptions, RenderResult

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:580-584
        def _into_message_param(self) -> chat.ChatCompletionAssistantMessageParam:
            message_param = super()._into_message_param()
            if self.reasoning_details:
                message_param['reasoning_details'] = self.reasoning_details  # type: ignore[reportGeneralTypeIssues]
            return message_param

# pydantic_ai_slim/pydantic_ai/providers/github.py:40-41
    def base_url(self) -> str:
        return 'https://models.github.ai/inference'

# pydantic_ai_slim/pydantic_ai/embeddings/google.py:9-9
from .base import EmbeddingModel, EmbedInputType

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:15-38
from ..messages import (
    AudioUrl,
    BinaryContent,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentUrl,
    FilePart,
    FinishReason,
    ImageUrl,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelResponsePart,
    ModelResponseStreamEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
)

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py:43-45
    async def __aenter__(self) -> Self:
        await self.wrapped.__aenter__()
        return self

# tests/test_validation_context.py:107-122
def test_agent_output_validator_with_validation_context():
    """Test that the argument passed to the output validator is validated using the validation context."""

    agent = Agent(
        'test',
        output_type=Value,
        deps_type=Deps,
        validation_context=lambda ctx: ctx.deps.increment,
    )

    @agent.output_validator
    def identity(ctx: RunContext[Deps], v: Value) -> Value:
        return v

    result = agent.run_sync('', deps=Deps(increment=10))
    assert result.output.x == snapshot(10)

# pydantic_evals/pydantic_evals/reporting/__init__.py:1374-1374
    duration_config: RenderNumberConfig

# tests/evals/test_evaluators.py:335-350
async def test_equals_evaluator(test_context: EvaluatorContext[TaskInput, TaskOutput, TaskMetadata]):
    """Test the equals evaluator."""
    # Test with matching value
    evaluator = Equals(value=TaskOutput(answer='4'))
    result = evaluator.evaluate(test_context)
    assert result is True

    # Test with non-matching value
    evaluator = Equals(value=TaskOutput(answer='5'))
    result = evaluator.evaluate(test_context)
    assert result is False

    # Test with completely different type
    evaluator = Equals(value='not a TaskOutput')
    result = evaluator.evaluate(test_context)
    assert result is False

# tests/models/test_model_test.py:7-7
import re

# pydantic_ai_slim/pydantic_ai/models/function.py:298-298
    _timestamp: datetime = field(default_factory=_utils.now_utc)

# tests/models/test_huggingface.py:35-35
from pydantic_ai.result import RunUsage

# tests/test_embeddings.py:31-31
from .conftest import IsDatetime, IsFloat, IsInt, IsList, IsStr, try_import

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:57-59
class ChatMessage(TypedDict):
    role: Role
    parts: list[MessagePart]

# pydantic_ai_slim/pydantic_ai/providers/cohere.py:26-27
    def name(self) -> str:
        return 'cohere'

# tests/test_temporal.py:1048-1080
async def test_multiple_agents(allow_model_requests: None, client: Client):
    async with Worker(
        client,
        task_queue=TASK_QUEUE,
        workflows=[SimpleAgentWorkflow, ComplexAgentWorkflow],
        plugins=[AgentPlugin(simple_temporal_agent), AgentPlugin(complex_temporal_agent)],
    ):
        output = await client.execute_workflow(
            SimpleAgentWorkflow.run,
            args=['What is the capital of Mexico?'],
            id=SimpleAgentWorkflow.__name__,
            task_queue=TASK_QUEUE,
        )
        assert output == snapshot('The capital of Mexico is Mexico City.')

        output = await client.execute_workflow(
            ComplexAgentWorkflow.run,
            args=[
                'Tell me: the capital of the country; the weather there; the product name',
                Deps(country='Mexico'),
            ],
            id=ComplexAgentWorkflow.__name__,
            task_queue=TASK_QUEUE,
        )
        assert output == snapshot(
            Response(
                answers=[
                    Answer(label='Capital of the Country', answer='Mexico City'),
                    Answer(label='Weather in Mexico City', answer='Sunny'),
                    Answer(label='Product Name', answer='Pydantic AI'),
                ]
            )
        )

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py:29-36
from pydantic_ai.tools import (
    AgentDepsT,
    BuiltinToolFunc,
    DeferredToolResults,
    RunContext,
    Tool,
    ToolFuncEither,
)

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:249-249
    client: AsyncAnthropicClient = field(repr=False)

# pydantic_ai_slim/pydantic_ai/providers/groq.py:55-56
    def base_url(self) -> str:
        return str(self.client.base_url)

# pydantic_ai_slim/pydantic_ai/providers/fireworks.py:29-99
class FireworksProvider(Provider[AsyncOpenAI]):
    """Provider for Fireworks AI API."""

    @property
    def name(self) -> str:
        return 'fireworks'

    @property
    def base_url(self) -> str:
        return 'https://api.fireworks.ai/inference/v1'

    @property
    def client(self) -> AsyncOpenAI:
        return self._client

    def model_profile(self, model_name: str) -> ModelProfile | None:
        prefix_to_profile = {
            'llama': meta_model_profile,
            'qwen': qwen_model_profile,
            'deepseek': deepseek_model_profile,
            'mistral': mistral_model_profile,
            'gemma': google_model_profile,
        }

        prefix = 'accounts/fireworks/models/'

        profile = None
        if model_name.startswith(prefix):
            model_name = model_name[len(prefix) :]
            for provider, profile_func in prefix_to_profile.items():
                if model_name.startswith(provider):
                    profile = profile_func(model_name)
                    break

        # As the Fireworks API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,
        # unless json_schema_transformer is set explicitly
        return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)

    @overload
    def __init__(self) -> None: ...

    @overload
    def __init__(self, *, api_key: str) -> None: ...

    @overload
    def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...

    @overload
    def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...

    def __init__(
        self,
        *,
        api_key: str | None = None,
        openai_client: AsyncOpenAI | None = None,
        http_client: httpx.AsyncClient | None = None,
    ) -> None:
        api_key = api_key or os.getenv('FIREWORKS_API_KEY')
        if not api_key and openai_client is None:
            raise UserError(
                'Set the `FIREWORKS_API_KEY` environment variable or pass it via `FireworksProvider(api_key=...)`'
                'to use the Fireworks AI provider.'
            )

        if openai_client is not None:
            self._client = openai_client
        elif http_client is not None:
            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)
        else:
            http_client = cached_async_http_client(provider='fireworks')
            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)

# tests/models/test_gemini.py:74-87
async def test_model_simple(allow_model_requests: None):
    m = GeminiModel('gemini-1.5-flash', provider=GoogleGLAProvider(api_key='via-arg'))
    assert isinstance(m.client, httpx.AsyncClient)
    assert m.model_name == 'gemini-1.5-flash'
    assert 'x-goog-api-key' in m.client.headers

    mrp = ModelRequestParameters(
        function_tools=[], allow_text_output=True, output_tools=[], output_mode='text', output_object=None
    )
    mrp = m.customize_request_parameters(mrp)
    tools = m._get_tools(mrp)
    tool_config = m._get_tool_config(mrp, tools)
    assert tools is None
    assert tool_config is None

# pydantic_graph/pydantic_graph/persistence/__init__.py:79-79
    kind: Literal['end'] = 'end'

# tests/providers/test_nebius.py:10-10
from pydantic_ai.profiles.google import GoogleJsonSchemaTransformer, google_model_profile

# pydantic_graph/pydantic_graph/mermaid.py:261-261
    height: int

# pydantic_evals/pydantic_evals/dataset.py:228-228
    name: str | None = None

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:20-30
from .. import (
    _agent_graph,
    _output,
    _system_prompt,
    _utils,
    concurrency as _concurrency,
    exceptions,
    messages as _messages,
    models,
    usage as _usage,
)

# docs/.hooks/main.py:17-25
def on_page_markdown(markdown: str, page: Page, config: Config, files: Files) -> str:
    """Called on each file after it is read and before it is converted to HTML."""
    relative_path = DOCS_ROOT / page.file.src_uri
    markdown = inject_snippets(markdown, relative_path.parent)
    markdown = replace_uv_python_run(markdown)
    markdown = render_examples(markdown)
    markdown = render_video(markdown)
    markdown = create_gateway_toggle(markdown, relative_path)
    return markdown

# tests/providers/test_provider_names.py:80-82
def empty_env():
    with patch.dict(os.environ, {}, clear=True):
        yield

# pydantic_ai_slim/pydantic_ai/profiles/grok.py:1-1
from __future__ import annotations as _annotations

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:11-11
from typing_extensions import NotRequired, TypedDict

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:33-33
from ..result import AgentStream, FinalResult, StreamedRunResult

# pydantic_ai_slim/pydantic_ai/_run_context.py:83-85
    def last_attempt(self) -> bool:
        """Whether this is the last attempt at running this tool before an error is raised."""
        return self.retry == self.max_retries

# pydantic_evals/pydantic_evals/evaluators/_run_evaluator.py:16-23
from .evaluator import (
    EvaluationReason,
    EvaluationResult,
    EvaluationScalar,
    Evaluator,
    EvaluatorFailure,
    EvaluatorOutput,
)

# tests/models/test_mistral.py:275-366
async def test_three_completions(allow_model_requests: None):
    completions = [
        completion_message(
            MistralAssistantMessage(content='world'),
            usage=MistralUsageInfo(prompt_tokens=1, completion_tokens=1, total_tokens=1),
        ),
        completion_message(MistralAssistantMessage(content='hello again')),
        completion_message(MistralAssistantMessage(content='final message')),
    ]
    mock_client = MockMistralAI.create_mock(completions)
    model = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(model=model)

    result = await agent.run('hello')

    assert result.output == 'world'
    assert result.usage().input_tokens == 1
    assert result.usage().output_tokens == 1

    result = await agent.run('hello again', message_history=result.all_messages())
    assert result.output == 'hello again'
    assert result.usage().input_tokens == 1
    assert result.usage().output_tokens == 1

    result = await agent.run('final message', message_history=result.all_messages())
    assert result.output == 'final message'
    assert result.usage().input_tokens == 1
    assert result.usage().output_tokens == 1
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='hello', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='world')],
                usage=RequestUsage(input_tokens=1, output_tokens=1),
                model_name='mistral-large-123',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='mistral',
                provider_url='https://api.mistral.ai',
                provider_details={
                    'finish_reason': 'stop',
                    'timestamp': datetime(2024, 1, 1, 0, 0, tzinfo=timezone.utc),
                },
                provider_response_id='123',
                finish_reason='stop',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[UserPromptPart(content='hello again', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='hello again')],
                usage=RequestUsage(input_tokens=1, output_tokens=1),
                model_name='mistral-large-123',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='mistral',
                provider_url='https://api.mistral.ai',
                provider_details={
                    'finish_reason': 'stop',
                    'timestamp': datetime(2024, 1, 1, 0, 0, tzinfo=timezone.utc),
                },
                provider_response_id='123',
                finish_reason='stop',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[UserPromptPart(content='final message', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='final message')],
                usage=RequestUsage(input_tokens=1, output_tokens=1),
                model_name='mistral-large-123',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='mistral',
                provider_url='https://api.mistral.ai',
                provider_details={
                    'finish_reason': 'stop',
                    'timestamp': datetime(2024, 1, 1, 0, 0, tzinfo=timezone.utc),
                },
                provider_response_id='123',
                finish_reason='stop',
                run_id=IsStr(),
            ),
        ]
    )

# tests/test_ssrf.py:634-652
    async def test_timeout_parameter(self) -> None:
        """Test that timeout parameter is passed to client."""
        mock_response = AsyncMock()
        mock_response.is_redirect = False
        mock_response.raise_for_status = lambda: None

        with (
            patch('pydantic_ai._ssrf.run_in_executor') as mock_executor,
            patch('pydantic_ai._ssrf.cached_async_http_client') as mock_client_fn,
        ):
            mock_executor.return_value = [(2, 1, 6, '', ('93.184.215.14', 0))]

            mock_client = AsyncMock()
            mock_client.get.return_value = mock_response
            mock_client_fn.return_value = mock_client

            await safe_download('https://example.com/file.txt', timeout=60)

            mock_client_fn.assert_called_once_with(timeout=60)

# pydantic_ai_slim/pydantic_ai/toolsets/approval_required.py:11-11
from .abstract import ToolsetTool

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:530-537
    async def handle_builtin_tool_call_end(self, part: BuiltinToolCallPart) -> AsyncIterator[EventT]:
        """Handle the end of a `BuiltinToolCallPart`.

        Args:
            part: The builtin tool call part.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

# tests/test_tools.py:34-34
from pydantic_ai.exceptions import ApprovalRequired, CallDeferred, ModelRetry, UnexpectedModelBehavior

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:190-192
    instructions_functions: list[_system_prompt.SystemPromptRunner[DepsT]] = dataclasses.field(
        default_factory=list[_system_prompt.SystemPromptRunner[DepsT]]
    )

# tests/test_agent.py:2950-2955
async def test_agent_run_metadata_kwarg_dict() -> None:
    agent = Agent(TestModel(custom_output_text='kwarg dict output'))

    result = await agent.run('kwarg dict prompt', metadata={'env': 'run'})

    assert result.metadata == {'env': 'run'}

# pydantic_ai_slim/pydantic_ai/models/gemini.py:723-724
def _function_call_part_from_call(tool: ToolCallPart) -> _GeminiFunctionCallPart:
    return _GeminiFunctionCallPart(function_call=_GeminiFunctionCall(name=tool.tool_name, args=tool.args_as_dict()))

# pydantic_ai_slim/pydantic_ai/models/outlines.py:540-548
    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:
        async for content in self._response:
            for event in self._parts_manager.handle_text_delta(
                vendor_part_id='content',
                content=content,
                thinking_tags=self._model_profile.thinking_tags,
                ignore_leading_whitespace=self._model_profile.ignore_streamed_leading_whitespace,
            ):
                yield event

# examples/pydantic_ai_examples/slack_lead_qualifier/store.py:26-27
    async def clear(cls):
        await cls._get_store().clear.aio()

# tests/evals/test_report_evaluators.py:44-45
class TaskInput(BaseModel):
    text: str

# pydantic_ai_slim/pydantic_ai/_function_schema.py:300-302
def _is_call_ctx(annotation: Any) -> bool:
    """Return whether the annotation is the `RunContext` class, parameterized or not."""
    return annotation is RunContext or get_origin(annotation) is RunContext

# pydantic_ai_slim/pydantic_ai/messages.py:475-475
    _: KW_ONLY

# pydantic_evals/pydantic_evals/generation.py:19-19
from pydantic_evals.evaluators.evaluator import Evaluator