# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:10-10
from pydantic_ai.exceptions import UserError

# tests/models/test_bedrock.py:47-47
from pydantic_ai.models import ModelRequestParameters

# pydantic_graph/pydantic_graph/beta/graph.py:191-230
    async def run(
        self,
        *,
        state: StateT = None,
        deps: DepsT = None,
        inputs: InputT = None,
        span: AbstractContextManager[AbstractSpan] | None = None,
        infer_name: bool = True,
    ) -> OutputT:
        """Execute the graph and return the final output.

        This is the main entry point for graph execution. It runs the graph
        to completion and returns the final output value.

        Args:
            state: The graph state instance
            deps: The dependencies instance
            inputs: The input data for the graph
            span: Optional span for tracing/instrumentation
            infer_name: Whether to infer the graph name from the calling frame.

        Returns:
            The final output from the graph execution
        """
        if infer_name and self.name is None:
            inferred_name = infer_obj_name(self, depth=2)
            if inferred_name is not None:  # pragma: no branch
                self.name = inferred_name

        async with self.iter(state=state, deps=deps, inputs=inputs, span=span, infer_name=False) as graph_run:
            # Note: This would probably be better using `async for _ in graph_run`, but this tests the `next` method,
            # which I'm less confident will be implemented correctly if not used on the critical path. We can change it
            # once we have tests, etc.
            event: Any = None
            while True:
                try:
                    event = await graph_run.next(event)
                except StopAsyncIteration:
                    assert isinstance(event, EndMarker), 'Graph run should end with an EndMarker.'
                    return cast(EndMarker[OutputT], event).value

# tests/models/test_outlines.py:18-18
from pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior

# tests/models/test_bedrock.py:724-733
async def test_video_as_binary_content_input(
    allow_model_requests: None, video_content: BinaryContent, bedrock_provider: BedrockProvider
):
    m = BedrockConverseModel('us.amazon.nova-pro-v1:0', provider=bedrock_provider)
    agent = Agent(m, instructions='You are a helpful chatbot.')

    result = await agent.run(['Explain me this video', video_content])
    assert result.output == snapshot(
        'The video shows a camera set up on a tripod, pointed at a scenic view of a rocky landscape under a clear sky. The camera remains stationary throughout the video, capturing the same view without any changes.'
    )

# tests/test_tenacity.py:1-1
from __future__ import annotations as _annotations

# tests/providers/test_github.py:3-3
import httpx

# tests/mcp_server.py:164-165
async def get_none():
    return None

# pydantic_ai_slim/pydantic_ai/usage.py:9-9
from pydantic import AliasChoices, BeforeValidator, Field

# tests/test_fastmcp.py:360-382
    async def test_call_tool_with_text_content(
        self,
        fastmcp_toolset: FastMCPToolset[None],
        run_context: RunContext[None],
    ):
        """Test tool call that returns text content."""
        async with fastmcp_toolset:
            tools = await fastmcp_toolset.get_tools(run_context)
            text_tool = tools['text_tool']

            result = await fastmcp_toolset.call_tool(
                name='text_tool', tool_args={'message': 'Hello World'}, ctx=run_context, tool=text_tool
            )

            assert result == snapshot({'result': 'Echo: Hello World'})

            text_list_tool = tools['text_list_tool']

            result = await fastmcp_toolset.call_tool(
                name='text_list_tool', tool_args={'message': 'Hello World'}, ctx=run_context, tool=text_list_tool
            )

            assert result == snapshot(['Echo: Hello World', 'Echo: Hello World again'])

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py:39-58
    async def call_tool(
        self,
        name: str,
        tool_args: dict[str, Any],
        ctx: RunContext[AgentDepsT],
        tool: ToolsetTool[AgentDepsT],
    ) -> Any:
        """Call a tool, wrapped as a Prefect task with a descriptive name."""
        # Check if this specific tool has custom config or is disabled
        tool_specific_config = self._tool_task_config.get(name, default_task_config)
        if tool_specific_config is None:
            # None means this tool should not be wrapped as a task
            return await super().call_tool(name, tool_args, ctx, tool)

        # Merge tool-specific config with default config
        merged_config = self._task_config | tool_specific_config

        return await self._call_tool_task.with_options(name=f'Call Tool: {name}', **merged_config)(
            name, tool_args, ctx, tool
        )

# pydantic_evals/pydantic_evals/evaluators/spec.py:38-38
    arguments: None | tuple[Any] | dict[str, Any]

# pydantic_ai_slim/pydantic_ai/mcp.py:300-300
    log_level: mcp_types.LoggingLevel | None

# tests/graph/test_utils.py:28-36
def test_infer_obj_name_no_frame():
    """Test infer_obj_name when frame inspection fails."""
    # This is hard to trigger without mocking, but we can test that the function
    # returns None gracefully when it can't find the object
    some_obj = object()

    # Call with depth that would exceed the call stack
    result = infer_obj_name(some_obj, depth=1000)
    assert result is None

# tests/mcp_server.py:38-47
async def get_weather_forecast(location: str) -> str:
    """Get the weather forecast for a location.

    Args:
        location: The location to get the weather forecast for.

    Returns:
        The weather forecast for the location.
    """
    return f'The weather in {location} is sunny and 26 degrees Celsius.'

# tests/test_embeddings.py:858-874
    async def test_cohere_v4_batch_documents(self, bedrock_provider: BedrockProvider):
        """Test Cohere V4 batch embedding (multiple texts in single request)."""
        model = BedrockEmbeddingModel('cohere.embed-v4:0', provider=bedrock_provider)
        embedder = Embedder(model)
        result = await embedder.embed_documents(['hello', 'world'])
        assert result == snapshot(
            EmbeddingResult(
                embeddings=IsList(IsList(IsFloat(), length=1536), length=2),
                inputs=['hello', 'world'],
                input_type='document',
                model_name='cohere.embed-v4:0',
                provider_name='bedrock',
                timestamp=IsDatetime(),
                usage=RequestUsage(input_tokens=2),
                provider_response_id=IsStr(),
            )
        )

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:72-72
    url: str

# tests/models/test_anthropic.py:8173-8184
async def test_anthropic_count_tokens_with_no_messages(allow_model_requests: None):
    """Test count_tokens when messages_ is None (no exception configured)."""
    mock_client = cast(AsyncAnthropic, MockAnthropic())
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))

    result = await m.count_tokens(
        [ModelRequest.user_text_prompt('hello')],
        None,
        ModelRequestParameters(),
    )

    assert result.input_tokens == 10

# pydantic_ai_slim/pydantic_ai/tools.py:5-5
from typing import Annotated, Any, Concatenate, Generic, Literal, TypeAlias, cast

# tests/test_examples.py:21-21
from pytest_mock import MockerFixture

# pydantic_ai_slim/pydantic_ai/models/google.py:166-166
    google_safety_settings: list[SafetySettingDict]

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:15-38
from ..messages import (
    AudioUrl,
    BinaryContent,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentUrl,
    FilePart,
    FinishReason,
    ImageUrl,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelResponsePart,
    ModelResponseStreamEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
)

# tests/providers/test_github.py:52-55
def test_github_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='ghp_test_token')
    provider = GitHubProvider(openai_client=openai_client)
    assert provider.client == openai_client

# pydantic_graph/pydantic_graph/nodes.py:170-172
def generate_snapshot_id(node_id: str) -> str:
    # module method to allow mocking
    return f'{node_id}:{uuid4().hex}'

# tests/models/test_xai.py:74-93
from .mock_xai import (
    MockXai,
    create_code_execution_response,
    create_failed_builtin_tool_response,
    create_logprob,
    create_mcp_server_response,
    create_mixed_tools_response,
    create_response,
    create_response_with_tool_calls,
    create_response_without_usage,
    create_server_tool_call,
    create_stream_chunk,
    create_tool_call,
    create_usage,
    create_web_search_response,
    get_grok_reasoning_text_chunk,
    get_grok_text_chunk,
    get_grok_tool_chunk,
    get_mock_chat_create_kwargs,
)

# tests/models/xai_proto_cassettes.py:578-582
    def dump_if_recording(self) -> None:
        if self.cassette is None:
            return
        if self.dirty_check is None or bool(self.dirty_check()):
            self.cassette.dump(self.cassette_path)

# tests/providers/test_huggingface.py:1-1
from __future__ import annotations as _annotations

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:300-305
    def parse_response(
        self,
        response_body: dict[str, Any],
    ) -> tuple[list[Sequence[float]], str | None]:
        embedding = response_body['embedding']
        return [embedding], None

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:66-66
    type: Literal['reasoning-delta'] = 'reasoning-delta'

# tests/models/test_fallback.py:15-27
from pydantic_ai import (
    Agent,
    ModelAPIError,
    ModelHTTPError,
    ModelMessage,
    ModelProfile,
    ModelRequest,
    ModelResponse,
    TextPart,
    ToolCallPart,
    ToolDefinition,
    UserPromptPart,
)

# pydantic_graph/pydantic_graph/nodes.py:138-140
    def deep_copy(self) -> Self:
        """Returns a deep copy of the node."""
        return copy.deepcopy(self)

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:343-343
GEN_AI_REQUEST_MODEL_ATTRIBUTE = 'gen_ai.request.model'

# pydantic_ai_slim/pydantic_ai/mcp.py:365-365
    _write_stream: MemoryObjectSendStream[SessionMessage]

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:1289-1289
_messages_ctx_var: ContextVar[_RunMessages] = ContextVar('var')

# pydantic_ai_slim/pydantic_ai/result.py:721-732
    def stream_responses(self, *, debounce_by: float | None = 0.1) -> Iterator[tuple[_messages.ModelResponse, bool]]:
        """Stream the response as an iterable of Structured LLM Messages.

        Args:
            debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.
                Debouncing is particularly important for long structured responses to reduce the overhead of
                performing validation as each token is received.

        Returns:
            An iterable of the structured response message and whether that is the last message.
        """
        return _utils.sync_async_iterator(self._streamed_run_result.stream_responses(debounce_by=debounce_by))

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:28-28
    is_async: bool

# tests/test_temporal.py:2583-2588
graph_builder = GraphBuilder(
    name='parallel_test_graph',
    state_type=GraphState,
    input_type=int,
    output_type=list[int],
)

# tests/models/test_google.py:75-75
from pydantic_ai.output import NativeOutput, PromptedOutput, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# tests/test_dbos.py:938-938
unserializable_deps_agent = Agent(model, name='unserializable_deps_agent', deps_type=UnserializableDeps)

# pydantic_graph/pydantic_graph/graph.py:70-70
    _state_type: type[StateT] | _utils.Unset = field(repr=False)

# tests/models/test_xai.py:29-60
from pydantic_ai import (
    Agent,
    AudioUrl,
    BinaryContent,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CodeExecutionTool,
    DocumentUrl,
    FilePart,
    FinalResultEvent,
    ImageUrl,
    MCPServerTool,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
    WebSearchTool,
)

# tests/test_agent.py:18-52
from pydantic_ai import (
    AbstractToolset,
    Agent,
    AgentStreamEvent,
    AudioUrl,
    BinaryContent,
    BinaryImage,
    CallDeferred,
    CombinedToolset,
    DocumentUrl,
    ExternalToolset,
    FunctionToolset,
    ImageUrl,
    IncompleteToolCall,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelProfile,
    ModelRequest,
    ModelResponse,
    ModelResponsePart,
    ModelRetry,
    PrefixedToolset,
    RetryPromptPart,
    RunContext,
    SystemPromptPart,
    TextPart,
    ToolCallPart,
    ToolReturn,
    ToolReturnPart,
    UnexpectedModelBehavior,
    UserError,
    UserPromptPart,
    VideoUrl,
    capture_run_messages,
)

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:34-34
    id: str

# pydantic_ai_slim/pydantic_ai/models/xai.py:687-687
    _model_name: str

# tests/test_ui.py:15-34
from pydantic_ai.messages import (
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    FilePart,
    FinalResultEvent,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    ModelMessage,
    ModelRequest,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
)

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:102-107
class ToolInputDeltaChunk(BaseChunk):
    """Tool input delta chunk."""

    type: Literal['tool-input-delta'] = 'tool-input-delta'
    tool_call_id: str
    input_text_delta: str

# pydantic_graph/pydantic_graph/persistence/__init__.py:13-13
from . import _utils

# pydantic_ai_slim/pydantic_ai/retries.py:66-66
    stop: StopBaseT

# pydantic_ai_slim/pydantic_ai/models/openai.py:12-12
from typing import Any, Literal, cast, overload

# tests/test_history_processor.py:25-25
pytestmark = [pytest.mark.anyio]

# tests/models/test_gemini_vertex.py:6-6
from inline_snapshot import Is, snapshot

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:11-18
from ..concurrency import (
    AbstractConcurrencyLimiter,
    AnyConcurrencyLimit,
    ConcurrencyLimit,
    ConcurrencyLimiter,
    get_concurrency_context,
    normalize_to_limiter,
)

# tests/parts_from_messages.py:3-3
from pydantic_ai import ModelMessage, ModelRequestPart, ModelResponsePart

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:14-14
from typing_inspection.introspection import get_literal_values

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:16-16
from .. import __version__, usage as _usage

# tests/conftest.py:12-12
from dataclasses import dataclass

# tests/evals/test_evaluators.py:73-93
async def test_evaluator_spec_initialization():
    """Test initializing EvaluatorSpec."""
    # Simple form with just a name
    spec1 = EvaluatorSpec(name='MyEvaluator', arguments=None)
    assert spec1.name == 'MyEvaluator'
    assert spec1.args == ()
    assert spec1.kwargs == {}

    # Form with args - using a tuple with a single element containing a tuple
    args_tuple = cast(tuple[Any], (('arg1', 'arg2'),))
    spec2 = EvaluatorSpec(name='MyEvaluator', arguments=args_tuple)
    assert spec2.name == 'MyEvaluator'
    assert len(spec2.args) == 1
    assert spec2.args[0] == ('arg1', 'arg2')
    assert spec2.kwargs == {}

    # Form with kwargs
    spec3 = EvaluatorSpec(name='MyEvaluator', arguments={'key1': 'value1', 'key2': 'value2'})
    assert spec3.name == 'MyEvaluator'
    assert spec3.args == ()
    assert spec3.kwargs == {'key1': 'value1', 'key2': 'value2'}

# pydantic_ai_slim/pydantic_ai/embeddings/openai.py:68-68
    _model_name: OpenAIEmbeddingModelName = field(repr=False)

# pydantic_graph/pydantic_graph/mermaid.py:28-28
DEFAULT_HIGHLIGHT_CSS = 'fill:#fdff32'

# pydantic_ai_slim/pydantic_ai/providers/cohere.py:3-3
import os

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:163-163
    _prepare_output_tools: ToolsPrepareFunc[AgentDepsT] | None = dataclasses.field(repr=False)

# tests/models/anthropic/test_output.py:23-23
from pydantic_ai.exceptions import UserError

# pydantic_ai_slim/pydantic_ai/profiles/__init__.py:40-40
    supports_image_output: bool = False