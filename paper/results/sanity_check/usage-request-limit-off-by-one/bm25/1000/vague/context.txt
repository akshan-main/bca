# pydantic_ai_slim/pydantic_ai/usage.py:260-260
    request_limit: int | None = 50

# tests/models/test_outlines.py:807-835
def test_model_settings_vllm_offline(vllm_model_offline: OutlinesModel) -> None:  # pragma: no cover
    # unsupported arguments removed
    kwargs = vllm_model_offline.format_inference_kwargs(
        ModelSettings(
            timeout=1,
            parallel_tool_calls=True,
            stop_sequences=['.'],
            extra_headers={'Authorization': 'Bearer 123'},
        )
    )
    assert 'timeout' not in kwargs
    assert 'parallel_tool_calls' not in kwargs
    assert 'stop_sequences' not in kwargs
    assert 'extra_headers' not in kwargs

    # special keys are preserved and others are in sampling params
    kwargs = vllm_model_offline.format_inference_kwargs(
        ModelSettings(  # type: ignore[reportCallIssue]
            use_tqdm=True,
            lora_request='test',
            priority=1,
            temperature=1,
        )
    )
    assert kwargs['use_tqdm'] is True
    assert kwargs['lora_request'] == 'test'
    assert kwargs['priority'] == 1
    assert 'sampling_params' in kwargs
    assert 'temperature' in kwargs['sampling_params']

# pydantic_ai_slim/pydantic_ai/models/outlines.py:391-421
    def _format_vllm_offline_inference_kwargs(  # pragma: no cover
        self, model_settings: dict[str, Any]
    ) -> dict[str, Any]:
        """Select the model settings supported by the vLLMOffline model."""
        from vllm.sampling_params import (  # pyright: ignore[reportMissingImports]
            SamplingParams,  # pyright: ignore[reportUnknownVariableType]
        )

        supported_args = [
            'max_tokens',
            'temperature',
            'top_p',
            'seed',
            'presence_penalty',
            'frequency_penalty',
            'logit_bias',
            'extra_body',
        ]
        # The arguments that are part of the fields of `ModelSettings` must be put in a `SamplingParams` object and
        # provided through the `sampling_params` argument to vLLM
        sampling_params = model_settings.get('extra_body', {}).pop('sampling_params', SamplingParams())

        for key in supported_args:
            setattr(sampling_params, key, model_settings.get(key, None))

        filtered_settings = {
            'sampling_params': sampling_params,
            **model_settings.get('extra_body', {}),
        }

        return filtered_settings

# tests/models/test_model_settings.py:121-144
def test_none_settings_in_hierarchy():
    """Test that None settings at any level don't break the merge hierarchy."""
    captured_settings = None

    def capture_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('captured')])

    # Model with no settings
    model = FunctionModel(capture_settings, settings=None)

    # Agent with settings
    agent_settings = ModelSettings(max_tokens=150, temperature=0.5)
    agent = Agent(model=model, model_settings=agent_settings)

    # Run with no additional settings
    result = agent.run_sync('test', model_settings=None)
    assert result.output == 'captured'

    # Should have agent settings
    assert captured_settings is not None
    assert captured_settings['max_tokens'] == 150
    assert captured_settings['temperature'] == 0.5

# pydantic_ai_slim/pydantic_ai/usage.py:400-407
    def check_before_tool_call(self, projected_usage: RunUsage) -> None:
        """Raises a `UsageLimitExceeded` exception if the next tool call(s) would exceed the tool call limit."""
        tool_calls_limit = self.tool_calls_limit
        tool_calls = projected_usage.tool_calls
        if tool_calls_limit is not None and tool_calls > tool_calls_limit:
            raise UsageLimitExceeded(
                f'The next tool call(s) would exceed the tool_calls_limit of {tool_calls_limit} ({tool_calls=}).'
            )

# pydantic_ai_slim/pydantic_ai/usage.py:270-270
    count_tokens_before_request: bool = False

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_utils.py:10-10
    backoff_rate: float

# tests/models/test_outlines.py:188-190
def vllm_model_offline() -> OutlinesModel:  # pragma: no cover
    outlines_model = outlines.models.vllm_offline.from_vllm_offline(vllm.LLM('microsoft/Phi-3-mini-4k-instruct'))  # pyright: ignore[reportUnknownMemberType, reportUnknownArgumentType]
    return OutlinesModel(outlines_model, provider=OutlinesProvider())

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:261-261
    bedrock_additional_model_requests_fields: Mapping[str, Any]

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:56-56
    _limiter: AbstractConcurrencyLimiter