# tests/test_direct.py:11-18
from pydantic_ai.direct import (
    StreamedResponseSync,
    _prepare_model,  # pyright: ignore[reportPrivateUsage]
    model_request,
    model_request_stream,
    model_request_stream_sync,
    model_request_sync,
)

# examples/pydantic_ai_examples/weather_agent.py:31-31
    client: AsyncClient

# pydantic_ai_slim/pydantic_ai/_tool_manager.py:8-8
from typing import Any, Generic, Literal

# examples/pydantic_ai_examples/question_graph.py:13-13
import logfire

# tests/models/test_model_names.py:2-2
from collections.abc import Iterator

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:194-237
    async def request_stream(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
        run_context: RunContext[Any] | None = None,
    ) -> AsyncIterator[StreamedResponse]:
        if not workflow.in_workflow():
            async with super().request_stream(
                messages, model_settings, model_request_parameters, run_context
            ) as streamed_response:
                yield streamed_response
                return

        if run_context is None:
            raise UserError(
                'A Temporal model cannot be used with `pydantic_ai.direct.model_request_stream()` as it requires a `run_context`. Set an `event_stream_handler` on the agent and use `agent.run()` instead.'
            )

        # We can never get here without an `event_stream_handler`, as `TemporalAgent.run_stream` and `TemporalAgent.iter` raise an error saying to use `TemporalAgent.run` instead,
        # and that only calls `request_stream` if `event_stream_handler` is set.
        assert self.event_stream_handler is not None

        self._validate_model_request_parameters(model_request_parameters)

        model_id = self._current_model_id()
        serialized_run_context = self.run_context_type.serialize_run_context(run_context)
        model_name = model_id or f'{self.system}:{self.model_name}'
        activity_config: ActivityConfig = {'summary': f'request model: {model_name} (stream)', **self.activity_config}
        response = await workflow.execute_activity(
            activity=self.request_stream_activity,
            args=[
                _RequestParams(
                    messages=messages,
                    model_settings=cast(dict[str, Any] | None, model_settings),
                    model_request_parameters=model_request_parameters,
                    serialized_run_context=serialized_run_context,
                    model_id=model_id,
                ),
                run_context.deps,
            ],
            **activity_config,
        )
        yield TemporalStreamedResponse(model_request_parameters, response)

# pydantic_ai_slim/pydantic_ai/_function_schema.py:269-297
def _build_schema(
    fields: dict[str, core_schema.TypedDictField],
    var_kwargs_schema: core_schema.CoreSchema | None,
    gen_schema: _generate_schema.GenerateSchema,
    core_config: core_schema.CoreConfig,
) -> tuple[core_schema.CoreSchema, str | None]:
    """Generate a typed dict schema for function parameters.

    Args:
        fields: The fields to generate a typed dict schema for.
        var_kwargs_schema: The variable keyword arguments schema.
        gen_schema: The `GenerateSchema` instance.
        core_config: The core configuration.

    Returns:
        tuple of (generated core schema, single arg name).
    """
    if len(fields) == 1 and var_kwargs_schema is None:
        name = next(iter(fields))
        td_field = fields[name]
        if td_field['metadata']['is_model_like']:  # type: ignore
            return td_field['schema'], name

    td_schema = core_schema.typed_dict_schema(
        fields,
        config=core_config,
        extras_schema=gen_schema.generate_schema(var_kwargs_schema) if var_kwargs_schema else None,
    )
    return td_schema, None

# pydantic_ai_slim/pydantic_ai/messages.py:594-596
    def is_document(self) -> bool:
        """Return `True` if the media type is a document type."""
        return self.media_type in _document_format_lookup

# tests/test_toolsets.py:510-527
async def test_context_manager_failed_initialization():
    """Test if MCP servers stop if any MCP server fails to initialize."""
    try:
        from pydantic_ai.mcp import MCPServerStdio
    except ImportError:  # pragma: lax no cover
        pytest.skip('mcp is not installed')

    server1 = MCPServerStdio('python', ['-m', 'tests.mcp_server'])
    server2 = AsyncMock()
    server2.__aenter__.side_effect = InitializationError

    toolset = CombinedToolset([server1, server2])

    with pytest.raises(InitializationError):
        async with toolset:
            pass

    assert server1.is_running is False

# tests/test_embeddings.py:1516-1533
def test_sync():
    model = TestEmbeddingModel()
    embedder = Embedder(model)

    result = embedder.embed_query_sync('Hello, world!')
    assert isinstance(result, EmbeddingResult)

    result = embedder.embed_documents_sync(['hello', 'world'])
    assert isinstance(result, EmbeddingResult)

    result = embedder.embed_sync('Hello, world!', input_type='query')
    assert isinstance(result, EmbeddingResult)

    result = embedder.max_input_tokens_sync()
    assert isinstance(result, int)

    result = embedder.count_tokens_sync('Hello, world!')
    assert isinstance(result, int)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:691-691
    data: str

# pydantic_graph/pydantic_graph/beta/graph.py:78-78
    _value: OutputT

# tests/test_temporal.py:2545-2549
class FastMCPAgentWorkflow:
    @workflow.run
    async def run(self, prompt: str) -> str:
        result = await fastmcp_temporal_agent.run(prompt)
        return result.output

# tests/test_history_processor.py:7-17
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRequest,
    ModelRequestPart,
    ModelResponse,
    SystemPromptPart,
    TextPart,
    UserPromptPart,
    capture_run_messages,
)

# pydantic_ai_slim/pydantic_ai/messages.py:102-102
DocumentFormat: TypeAlias = Literal['csv', 'doc', 'docx', 'html', 'md', 'pdf', 'txt', 'xls', 'xlsx']

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py:31-31
from pydantic_ai.output import OutputDataT, OutputSpec

# pydantic_ai_slim/pydantic_ai/models/fallback.py:1-1
from __future__ import annotations as _annotations

# tests/models/test_cohere.py:12-27
from pydantic_ai import (
    Agent,
    ImageUrl,
    ModelAPIError,
    ModelHTTPError,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:57-59
class ChatMessage(TypedDict):
    role: Role
    parts: list[MessagePart]

# examples/pydantic_ai_examples/pydantic_model.py:20-22
class MyModel(BaseModel):
    city: str
    country: str

# pydantic_evals/pydantic_evals/otel/span_tree.py:88-88
    parent_span_id: int | None

# pydantic_ai_slim/pydantic_ai/providers/vercel.py:18-18
from pydantic_ai.profiles.openai import OpenAIJsonSchemaTransformer, OpenAIModelProfile, openai_model_profile

# pydantic_ai_slim/pydantic_ai/models/mistral.py:682-684
    def model_name(self) -> MistralModelName:
        """Get the model name of the response."""
        return self._model_name

# pydantic_ai_slim/pydantic_ai/mcp.py:159-159
    uri: str

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:175-175
    anthropic_thinking: BetaThinkingConfigParam

# tests/graph/test_file_persistence.py:21-21
from ..conftest import IsFloat, IsNow

# tests/models/test_xai.py:4872-4878
async def test_xai_provider_string_initialization(allow_model_requests: None, monkeypatch: pytest.MonkeyPatch):
    """Test that provider can be initialized with a string."""
    # This test verifies the infer_provider path when provider is a string
    monkeypatch.setenv('XAI_API_KEY', 'test-key-for-coverage')
    m = XaiModel(XAI_NON_REASONING_MODEL, provider='xai')
    assert m.model_name == XAI_NON_REASONING_MODEL
    assert m.system == 'xai'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_fastmcp_toolset.py:7-7
from pydantic_ai import ToolsetTool

# pydantic_ai_slim/pydantic_ai/providers/vercel.py:18-18
from pydantic_ai.profiles.openai import OpenAIJsonSchemaTransformer, OpenAIModelProfile, openai_model_profile

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:19-19
from pydantic_ai.tools import RunContext

# pydantic_ai_slim/pydantic_ai/providers/groq.py:9-9
from pydantic_ai.exceptions import UserError

# pydantic_ai_slim/pydantic_ai/models/mistral.py:97-97
MistralModelName = str | LatestMistralModelNames

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:158-158
    provider_executed: bool | None = None

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py:257-284
    def _temporal_overrides(
        self, *, model: models.Model | models.KnownModelName | str | None = None, force: bool = False
    ) -> Iterator[None]:
        """Context manager for workflow-specific overrides.

        When called outside a workflow, this is a no-op.
        When called inside a workflow, it overrides the model and toolsets.
        """
        if not workflow.in_workflow() and not force:
            yield
            return

        # We reset tools here as the temporalized function toolset is already in self._toolsets.
        # Override model and set the model for workflow execution
        with (
            super().override(model=self._temporal_model, toolsets=self._toolsets, tools=[]),
            self._temporal_model.using_model(model),
            _utils.disable_threads(),
        ):
            temporal_active_token = self._temporal_overrides_active.set(True)
            try:
                yield
            except PydanticSerializationError as e:
                raise UserError(
                    "The `deps` object failed to be serialized. Temporal requires all objects that are passed to activities to be serializable using Pydantic's `TypeAdapter`."
                ) from e
            finally:
                self._temporal_overrides_active.reset(temporal_active_token)

# tests/evals/test_dataset.py:83-85
class TaskMetadata(BaseModel):
    difficulty: str = 'easy'
    category: str = 'general'

# tests/models/anthropic/test_output.py:375-394
def test_strict_true_tool_native_output(
    allow_model_requests: None,
    anthropic_model: ANTHROPIC_MODEL_FIXTURE,
) -> None:
    """Tool with strict=True, NativeOutput â†’ beta header, tool has strict field + output_format."""
    model = anthropic_model('claude-sonnet-4-5')
    hook = create_header_verification_hook(expect_beta=True, test_name='test_strict_true_tool_native_output')
    model.client._client.event_hooks['request'].append(hook)  # pyright: ignore[reportPrivateUsage]

    agent = Agent(model, output_type=NativeOutput(CityInfo))

    @agent.tool_plain(strict=True)
    def lookup_country(city: str) -> str:
        return 'France' if city == 'Paris' else 'Unknown'

    result = agent.run_sync('Give me details about Paris')

    assert isinstance(result.output, CityInfo)
    if errors := hook.errors:  # type: ignore[attr-defined]
        assert False, '\n'.join(sorted(errors))

# tests/providers/test_litellm.py:62-135
def test_model_profile_with_different_models(mocker: MockerFixture):
    provider = LiteLLMProvider(api_key='test-key')

    # Create mocks for all profile functions
    from dataclasses import dataclass

    @dataclass
    class MockProfile:
        max_tokens: int = 4096
        supports_streaming: bool = True

    # Mock all profile functions
    mock_profiles = {
        'openai': mocker.patch('pydantic_ai.providers.litellm.openai_model_profile', return_value=MockProfile()),
        'anthropic': mocker.patch('pydantic_ai.providers.litellm.anthropic_model_profile', return_value=MockProfile()),
        'google': mocker.patch('pydantic_ai.providers.litellm.google_model_profile', return_value=MockProfile()),
        'meta': mocker.patch('pydantic_ai.providers.litellm.meta_model_profile', return_value=MockProfile()),
        'mistral': mocker.patch('pydantic_ai.providers.litellm.mistral_model_profile', return_value=MockProfile()),
        'cohere': mocker.patch('pydantic_ai.providers.litellm.cohere_model_profile', return_value=MockProfile()),
        'amazon': mocker.patch('pydantic_ai.providers.litellm.amazon_model_profile', return_value=MockProfile()),
        'deepseek': mocker.patch('pydantic_ai.providers.litellm.deepseek_model_profile', return_value=MockProfile()),
        'groq': mocker.patch('pydantic_ai.providers.litellm.groq_model_profile', return_value=MockProfile()),
        'grok': mocker.patch('pydantic_ai.providers.litellm.grok_model_profile', return_value=MockProfile()),
        'moonshotai': mocker.patch(
            'pydantic_ai.providers.litellm.moonshotai_model_profile', return_value=MockProfile()
        ),
        'qwen': mocker.patch('pydantic_ai.providers.litellm.qwen_model_profile', return_value=MockProfile()),
    }

    # Test models without provider prefix (should use openai profile)
    models_without_prefix = ['gpt-4', 'claude-sonnet-4-5', 'gemini-pro', 'llama2-70b']

    for model in models_without_prefix:
        profile = provider.model_profile(model)
        assert isinstance(profile, OpenAIModelProfile)
        assert profile.json_schema_transformer == OpenAIJsonSchemaTransformer

    # Verify openai_model_profile was called for each model without prefix
    assert mock_profiles['openai'].call_count == len(models_without_prefix)

    # Reset all call counts
    for mock in mock_profiles.values():
        mock.reset_mock()

    # Test all provider prefixes
    test_cases = [
        ('anthropic/claude-3-haiku', 'anthropic', 'claude-3-haiku'),
        ('openai/gpt-4-turbo', 'openai', 'gpt-4-turbo'),
        ('google/gemini-1.5-pro', 'google', 'gemini-1.5-pro'),
        ('mistralai/mistral-large', 'mistral', 'mistral-large'),
        ('mistral/mistral-7b', 'mistral', 'mistral-7b'),
        ('cohere/command-r', 'cohere', 'command-r'),
        ('amazon/titan-text', 'amazon', 'titan-text'),
        ('bedrock/claude-v2', 'amazon', 'claude-v2'),
        ('meta-llama/llama-3-8b', 'meta', 'llama-3-8b'),
        ('meta/llama-2-70b', 'meta', 'llama-2-70b'),
        ('groq/llama3-70b', 'groq', 'llama3-70b'),
        ('deepseek/deepseek-coder', 'deepseek', 'deepseek-coder'),
        ('moonshotai/moonshot-v1', 'moonshotai', 'moonshot-v1'),
        ('x-ai/grok-beta', 'grok', 'grok-beta'),
        ('qwen/qwen-72b', 'qwen', 'qwen-72b'),
    ]

    for model_name, expected_profile, expected_suffix in test_cases:
        profile = provider.model_profile(model_name)
        assert isinstance(profile, OpenAIModelProfile)
        assert profile.json_schema_transformer == OpenAIJsonSchemaTransformer
        # Verify the correct profile function was called with the correct suffix
        mock_profiles[expected_profile].assert_called_with(expected_suffix)
        mock_profiles[expected_profile].reset_mock()

    # Test unknown provider prefix (should fall back to openai)
    provider.model_profile('unknown-provider/some-model')
    mock_profiles['openai'].assert_called_once_with('unknown-provider/some-model')

# tests/test_a2a.py:9-9
from inline_snapshot import snapshot

# tests/models/test_outlines.py:163-170
def llamacpp_model() -> OutlinesModel:  # pragma: lax no cover
    outlines_model_llamacpp = outlines.models.llamacpp.from_llamacpp(
        llama_cpp.Llama.from_pretrained(  # pyright: ignore[reportUnknownMemberType]
            repo_id='M4-ai/TinyMistral-248M-v2-Instruct-GGUF',
            filename='TinyMistral-248M-v2-Instruct.Q4_K_M.gguf',
        )
    )
    return OutlinesModel(outlines_model_llamacpp, provider=OutlinesProvider())

# pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py:14-14
from pydantic_ai.exceptions import ModelRetry

# docs/.hooks/snippets.py:57-57
    highlights: list[LineRange]

# tests/models/test_gemini.py:16-16
from inline_snapshot import snapshot

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_fastmcp_toolset.py:3-3
from typing import Literal

# tests/graph/beta/test_parent_forks.py:6-6
from pydantic_graph.beta.parent_forks import ParentForkFinder

# tests/models/test_outlines.py:21-37
from pydantic_ai.messages import (
    AudioUrl,
    BinaryContent,
    BinaryImage,
    FilePart,
    ImageUrl,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:35-48
from .exceptions import (
    AgentRunError,
    ApprovalRequired,
    CallDeferred,
    ConcurrencyLimitExceeded,
    FallbackExceptionGroup,
    IncompleteToolCall,
    ModelAPIError,
    ModelHTTPError,
    ModelRetry,
    UnexpectedModelBehavior,
    UsageLimitExceeded,
    UserError,
)

# pydantic_ai_slim/pydantic_ai/models/openai.py:1048-1053
    def _map_model_response(self, message: ModelResponse) -> chat.ChatCompletionMessageParam:
        """Hook that determines how `ModelResponse` is mapped into `ChatCompletionMessageParam` objects before sending.

        Subclasses of `OpenAIChatModel` may override this method to provide their own mapping logic.
        """
        return self._MapModelResponseContext(self).map_assistant_message(message)

# examples/pydantic_ai_examples/weather_agent.py:94-101
async def main():
    async with AsyncClient() as client:
        logfire.instrument_httpx(client, capture_all=True)
        deps = Deps(client=client)
        result = await weather_agent.run(
            'What is the weather like in London and in Wiltshire?', deps=deps
        )
        print('Response:', result.output)

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:434-437
    def _stop_tracking_vendor_id(self, vendor_part_id: VendorId | None) -> None:
        """Stop tracking a vendor_part_id (no-op if None or not tracked)."""
        if vendor_part_id is not None:  # pragma: no branch
            self._vendor_id_to_part_index.pop(vendor_part_id, None)

# examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py:6-6
from __future__ import annotations

# pydantic_ai_slim/pydantic_ai/embeddings/sentence_transformers.py:34-34
    sentence_transformers_device: str

# tests/test_usage_limits.py:50-56
def test_response_token_limit() -> None:
    test_agent = Agent(
        TestModel(custom_output_text='Unfortunately, this response exceeds the response tokens limit by a few!')
    )

    with pytest.raises(UsageLimitExceeded, match=re.escape('Exceeded the output_tokens_limit of 5 (output_tokens=11)')):
        test_agent.run_sync('Hello', usage_limits=UsageLimits(output_tokens_limit=5))

# tests/test_format_as_xml.py:11-11
from inline_snapshot import snapshot

# pydantic_evals/pydantic_evals/reporting/__init__.py:90-90
    task_duration: float

# examples/pydantic_ai_examples/flight_booking.py:104-106
class SeatPreference(BaseModel):
    row: int = Field(ge=1, le=30)
    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']

# tests/test_ui.py:48-48
from pydantic_ai.toolsets import AbstractToolset, ExternalToolset

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:48-54
class ChatRequestExtra(BaseModel, extra='ignore', alias_generator=to_camel):
    """Extra data extracted from chat request."""

    model: str | None = None
    """Model ID selected by the user, e.g. 'openai:gpt-5'. Maps to JSON field 'model'."""
    builtin_tools: list[str] = []
    """Tool IDs selected by the user, e.g. ['web_search', 'code_execution']. Maps to JSON field 'builtinTools'."""

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:91-91
    num_results: int