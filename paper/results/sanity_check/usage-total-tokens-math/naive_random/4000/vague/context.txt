# tests/models/xai_proto_cassettes.py:350-350
    include_debug_json: bool = False

# tests/test_ui.py:172-173
    async def handle_builtin_tool_call_end(self, part: BuiltinToolCallPart) -> AsyncIterator[str]:
        yield f'</builtin-tool-call name={part.tool_name!r}>'

# tests/test_prefect.py:32-32
from pydantic_ai.models.function import AgentInfo, FunctionModel

# pydantic_ai_slim/pydantic_ai/providers/google_vertex.py:124-124
    credentials: BaseCredentials | ServiceAccountCredentials | None

# pydantic_ai_slim/pydantic_ai/providers/together.py:7-7
from openai import AsyncOpenAI

# pydantic_ai_slim/pydantic_ai/mcp.py:308-308
    log_handler: LoggingFnT | None

# tests/test_logfire.py:21-21
from pydantic_ai.toolsets.abstract import ToolsetTool

# pydantic_ai_slim/pydantic_ai/models/cohere.py:33-33
from ..tools import ToolDefinition

# pydantic_ai_slim/pydantic_ai/models/outlines.py:10-10
from contextlib import asynccontextmanager

# pydantic_ai_slim/pydantic_ai/providers/ollama.py:38-39
    def base_url(self) -> str:
        return str(self.client.base_url)

# pydantic_ai_slim/pydantic_ai/common_tools/tavily.py:31-31
    content: str

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:11-11
from pydantic_ai.models import OpenAIChatCompatibleProvider, OpenAIResponsesCompatibleProvider

# pydantic_graph/pydantic_graph/beta/graph.py:16-16
from anyio import BrokenResourceError, CancelScope, create_memory_object_stream, create_task_group

# pydantic_ai_slim/pydantic_ai/models/__init__.py:17-17
from typing import Any, Generic, Literal, TypeVar, get_args, overload

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:223-236
    def walk(self) -> JsonSchema:
        # Note: OpenAI does not support anyOf at the root in strict mode
        # However, we don't need to check for it here because we ensure in pydantic_ai._utils.check_object_json_schema
        # that the root schema either has type 'object' or is recursive.
        result = super().walk()

        # For recursive models, we need to tweak the schema to make it compatible with strict mode.
        # Because the following should never change the semantics of the schema we apply it unconditionally.
        if self.root_ref is not None:
            result.pop('$ref', None)  # We replace references to the self.root_ref with just '#' in the transform method
            root_key = re.sub(r'^#/\$defs/', '', self.root_ref)
            result.update(self.defs.get(root_key) or {})

        return result

# pydantic_graph/pydantic_graph/nodes.py:67-73
    def get_snapshot_id(self) -> str:
        if snapshot_id := getattr(self, '__snapshot_id', None):
            return snapshot_id
        else:
            snapshot_id = generate_snapshot_id(self.get_node_id())
            object.__setattr__(self, '__snapshot_id', snapshot_id)
            return snapshot_id

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_fastmcp_toolset.py:3-3
from pydantic_ai import ToolsetTool

# pydantic_ai_slim/pydantic_ai/_output.py:674-676
class UnionOutputResult:
    kind: str
    data: ObjectJsonSchema

# pydantic_ai_slim/pydantic_ai/run.py:21-21
from .tools import AgentDepsT

# tests/mcp_server.py:169-175
async def get_multiple_items():
    return [
        'This is a string',
        'Another string',
        {'foo': 'bar', 'baz': 123},
        await get_image(),
    ]

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py:4-4
from contextlib import AbstractAsyncContextManager, asynccontextmanager, contextmanager

# pydantic_ai_slim/pydantic_ai/exceptions.py:4-4
import sys

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:9-9
from .._json_schema import JsonSchema, JsonSchemaTransformer

# tests/test_temporal.py:2453-2455
    async def run(self, prompt: str) -> str:
        result = await web_search_temporal_agent.run(prompt)
        return result.output

# pydantic_ai_slim/pydantic_ai/providers/github.py:84-112
    def __init__(
        self,
        *,
        api_key: str | None = None,
        openai_client: AsyncOpenAI | None = None,
        http_client: httpx.AsyncClient | None = None,
    ) -> None:
        """Create a new GitHub Models provider.

        Args:
            api_key: The GitHub token to use for authentication. If not provided, the `GITHUB_API_KEY`
                environment variable will be used if available.
            openai_client: An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`.
            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.
        """
        api_key = api_key or os.getenv('GITHUB_API_KEY')
        if not api_key and openai_client is None:
            raise UserError(
                'Set the `GITHUB_API_KEY` environment variable or pass it via `GitHubProvider(api_key=...)`'
                ' to use the GitHub Models provider.'
            )

        if openai_client is not None:
            self._client = openai_client
        elif http_client is not None:
            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)
        else:
            http_client = cached_async_http_client(provider='github')
            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:57-57
    result: ToolReturnContent

# pydantic_ai_slim/pydantic_ai/embeddings/google.py:30-34
LatestGoogleVertexEmbeddingModelNames = Literal[
    'gemini-embedding-001',
    'text-embedding-005',
    'text-multilingual-embedding-002',
]

# tests/test_agent.py:6199-6295
def test_prepare_output_tools():
    @dataclass
    class AgentDeps:
        plan_presented: bool = False

    async def present_plan(ctx: RunContext[AgentDeps], plan: str) -> str:
        """
        Present the plan to the user.
        """
        ctx.deps.plan_presented = True
        return plan

    async def run_sql(ctx: RunContext[AgentDeps], purpose: str, query: str) -> str:
        """
        Run an SQL query.
        """
        return 'SQL query executed successfully'

    async def only_if_plan_presented(
        ctx: RunContext[AgentDeps], tool_defs: list[ToolDefinition]
    ) -> list[ToolDefinition]:
        return tool_defs if ctx.deps.plan_presented else []

    agent = Agent(
        model='test',
        deps_type=AgentDeps,
        tools=[present_plan],
        output_type=[ToolOutput(run_sql, name='run_sql')],
        prepare_output_tools=only_if_plan_presented,
    )

    result = agent.run_sync('Hello', deps=AgentDeps())
    assert result.output == snapshot('SQL query executed successfully')
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content='Hello',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ToolCallPart(
                        tool_name='present_plan',
                        args={'plan': 'a'},
                        tool_call_id=IsStr(),
                    )
                ],
                usage=RequestUsage(input_tokens=51, output_tokens=5),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='present_plan',
                        content='a',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ToolCallPart(
                        tool_name='run_sql',
                        args={'purpose': 'a', 'query': 'a'},
                        tool_call_id=IsStr(),
                    )
                ],
                usage=RequestUsage(input_tokens=52, output_tokens=12),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='run_sql',
                        content='Final result processed.',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:12-32
from ..messages import (
    AgentStreamEvent,
    BuiltinToolCallEvent,  # pyright: ignore[reportDeprecated]
    BuiltinToolCallPart,
    BuiltinToolResultEvent,  # pyright: ignore[reportDeprecated]
    BuiltinToolReturnPart,
    FilePart,
    FinalResultEvent,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturnPart,
)

# tests/graph/beta/test_paths.py:27-27
    value: int = 0

# pydantic_ai_slim/pydantic_ai/models/gemini.py:704-704
    file_uri: Annotated[str, pydantic.Field(alias='fileUri')]

# tests/test_temporal.py:2177-2304
async def test_temporal_agent_with_model_retry(allow_model_requests: None, client: Client):
    async with Worker(
        client,
        task_queue=TASK_QUEUE,
        workflows=[ModelRetryWorkflow],
        plugins=[AgentPlugin(model_retry_temporal_agent)],
    ):
        workflow = await client.start_workflow(
            ModelRetryWorkflow.run,
            args=['What is the weather in CDMX?'],
            id=ModelRetryWorkflow.__name__,
            task_queue=TASK_QUEUE,
        )
        result = await workflow.result()
        assert result.output == snapshot('The weather in Mexico City is currently sunny.')
        assert result.all_messages() == snapshot(
            [
                ModelRequest(
                    parts=[
                        UserPromptPart(
                            content='What is the weather in CDMX?',
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[
                        ToolCallPart(
                            tool_name='get_weather_in_city',
                            args='{"city":"CDMX"}',
                            tool_call_id=IsStr(),
                        )
                    ],
                    usage=RequestUsage(
                        input_tokens=47,
                        output_tokens=17,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={'finish_reason': 'tool_calls', 'timestamp': '2025-08-28T23:19:50Z'},
                    provider_response_id=IsStr(),
                    finish_reason='tool_call',
                    run_id=IsStr(),
                ),
                ModelRequest(
                    parts=[
                        RetryPromptPart(
                            content='Did you mean Mexico City?',
                            tool_name='get_weather_in_city',
                            tool_call_id=IsStr(),
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[
                        ToolCallPart(
                            tool_name='get_weather_in_city',
                            args='{"city":"Mexico City"}',
                            tool_call_id=IsStr(),
                        )
                    ],
                    usage=RequestUsage(
                        input_tokens=87,
                        output_tokens=17,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={'finish_reason': 'tool_calls', 'timestamp': '2025-08-28T23:19:51Z'},
                    provider_response_id=IsStr(),
                    finish_reason='tool_call',
                    run_id=IsStr(),
                ),
                ModelRequest(
                    parts=[
                        ToolReturnPart(
                            tool_name='get_weather_in_city',
                            content='sunny',
                            tool_call_id=IsStr(),
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[TextPart(content='The weather in Mexico City is currently sunny.')],
                    usage=RequestUsage(
                        input_tokens=116,
                        output_tokens=10,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={'finish_reason': 'stop', 'timestamp': '2025-08-28T23:19:52Z'},
                    provider_response_id=IsStr(),
                    finish_reason='stop',
                    run_id=IsStr(),
                ),
            ]
        )

# pydantic_ai_slim/pydantic_ai/embeddings/sentence_transformers.py:11-11
from .base import EmbeddingModel, EmbedInputType

# docs/.hooks/algolia.py:23-23
    objectID: str

# tests/graph/beta/test_graph_iteration.py:419-466
async def test_iter_turn_tasks_into_end_marker():
    """Test overriding a sequence of tasks with an EndMarker to terminate early."""
    g = GraphBuilder(state_type=IterState, output_type=str)

    @g.step
    async def step1(ctx: StepContext[IterState, None, None]) -> int:
        ctx.state.counter += 1
        return 10

    @g.step
    async def step2(ctx: StepContext[IterState, None, int]) -> int:  # pragma: no cover
        ctx.state.counter += 1
        return ctx.inputs * 2

    @g.step
    async def step3(ctx: StepContext[IterState, None, int]) -> str:  # pragma: no cover
        ctx.state.counter += 1
        return f'result: {ctx.inputs}'

    g.add(
        g.edge_from(g.start_node).to(step1),
        g.edge_from(step1).to(step2),
        g.edge_from(step2).to(step3),
        g.edge_from(step3).to(g.end_node),
    )

    graph = g.build()
    state = IterState()

    early_exit_done = False
    async with graph.iter(state=state) as run:
        while True:
            try:
                event = await run.next()
                assert isinstance(event, list)
                assert not early_exit_done
                # Check if we're about to execute step2
                assert any(task.node_id == NodeID('step2') for task in event)
                # Override with an EndMarker to terminate early
                early_exit_done = True
                await run.next(EndMarker('early_exit'))
            except StopAsyncIteration:
                break

    result = run.output
    assert result == 'early_exit'
    # Only step1 should have run
    assert state.counter == 1

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:10-10
from pydantic import Field

# tests/models/test_google.py:18-18
from pytest_mock import MockerFixture

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:10-10
from contextlib import asynccontextmanager, contextmanager

# pydantic_evals/pydantic_evals/reporting/__init__.py:1358-1358
    include_metadata: bool

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:59-59
    url: str

# pydantic_ai_slim/pydantic_ai/messages.py:661-661
    ttl: Literal['5m', '1h'] = '5m'

# pydantic_ai_slim/pydantic_ai/result.py:20-20
from ._run_context import AgentDepsT, RunContext

# tests/models/test_mistral.py:5-5
from dataclasses import dataclass

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:50-50
    type: Literal['text-end'] = 'text-end'

# tests/test_streaming.py:51-51
from pydantic_ai.models.function import AgentInfo, DeltaToolCall, DeltaToolCalls, FunctionModel

# docs/.hooks/main.py:98-99
def render_video(markdown: str) -> str:
    return re.sub(r'\{\{ *video\((["\'])(.+?)\1(?:, (\d+))?(?:, (\d+))?\) *\}\}', sub_cf_video, markdown)