# pydantic_ai_slim/pydantic_ai/usage.py:64-66
    def total_tokens(self) -> int:
        """Sum of `input_tokens + output_tokens`."""
        return self.input_tokens - self.output_tokens

# pydantic_ai_slim/pydantic_ai/usage.py:268-268
    total_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:43-43
    output_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:32-36
    output_tokens: Annotated[
        int,
        # `response_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('output_tokens', 'response_tokens')),
    ] = 0

# pydantic_ai_slim/pydantic_ai/models/gemini.py:895-895
    total_token_count: Annotated[int, pydantic.Field(alias='totalTokenCount')]

# pydantic_ai_slim/pydantic_ai/usage.py:197-197
    output_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:39-39
    input_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/models/gemini.py:629-629
    max_output_tokens: int

# pydantic_ai_slim/pydantic_ai/usage.py:191-191
    input_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:270-270
    count_tokens_before_request: bool = False

# pydantic_ai_slim/pydantic_ai/models/gemini.py:884-884
    token_count: Annotated[int, pydantic.Field(alias='tokenCount', default=0)]

# pydantic_ai_slim/pydantic_ai/usage.py:266-266
    output_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:20-24
    input_tokens: Annotated[
        int,
        # `request_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('input_tokens', 'request_tokens')),
    ] = 0

# tests/test_usage_limits.py:59-63
def test_total_token_limit() -> None:
    test_agent = Agent(TestModel(custom_output_text='This utilizes 4 tokens!'))

    with pytest.raises(UsageLimitExceeded, match=re.escape('Exceeded the total_tokens_limit of 50 (total_tokens=55)')):
        test_agent.run_sync('Hello', usage_limits=UsageLimits(total_tokens_limit=50))

# tests/models/test_bedrock.py:91-92
    def count_tokens(self, **_: Any) -> None:
        raise self._error

# pydantic_ai_slim/pydantic_ai/usage.py:182-182
    input_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/settings.py:14-14
    max_tokens: int

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:45-51
_MAX_INPUT_TOKENS: dict[CohereEmbeddingModelName, int] = {
    'embed-v4.0': 128000,
    'embed-english-v3.0': 512,
    'embed-english-light-v3.0': 512,
    'embed-multilingual-v3.0': 512,
    'embed-multilingual-light-v3.0': 512,
}

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py:71-82
_MAX_INPUT_TOKENS: dict[VoyageAIEmbeddingModelName, int] = {
    'voyage-4-large': 32000,
    'voyage-4': 32000,
    'voyage-4-lite': 32000,
    'voyage-3-large': 32000,
    'voyage-3.5': 32000,
    'voyage-3.5-lite': 32000,
    'voyage-code-3': 32000,
    'voyage-finance-2': 32000,
    'voyage-law-2': 16000,
    'voyage-code-2': 16000,
}

# pydantic_ai_slim/pydantic_ai/embeddings/google.py:48-52
_MAX_INPUT_TOKENS: dict[GoogleEmbeddingModelName, int] = {
    'gemini-embedding-001': 2048,
    'text-embedding-005': 2048,
    'text-multilingual-embedding-002': 2048,
}

# pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py:187-188
def _map_usage(total_tokens: int) -> RequestUsage:
    return RequestUsage(input_tokens=total_tokens)

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:243-243
    max_content_tokens: int | None = None

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:185-185
    max_tokens: int

# tests/models/test_anthropic.py:187-195
    async def messages_count_tokens(self, *_args: Any, **kwargs: Any) -> BetaMessageTokensCount:
        # check if we are configured to raise an exception
        if self.messages_ is not None:
            raise_if_exception(self.messages_ if not isinstance(self.messages_, Sequence) else self.messages_[0])

        # record the kwargs used
        self.chat_completion_kwargs.append({k: v for k, v in kwargs.items() if v is not NOT_GIVEN})

        return BetaMessageTokensCount(input_tokens=10)

# pydantic_ai_slim/pydantic_ai/models/test.py:38-38
from .function import _estimate_string_tokens, _estimate_usage  # pyright: ignore[reportPrivateUsage]

# pydantic_ai_slim/pydantic_ai/usage.py:264-264
    input_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/models/gemini.py:897-897
    thoughts_token_count: NotRequired[Annotated[int, pydantic.Field(alias='thoughtsTokenCount')]]

# tests/models/test_instrumented.py:111-117
    async def count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> RequestUsage:
        return RequestUsage(input_tokens=10)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:893-893
    prompt_token_count: Annotated[int, pydantic.Field(alias='promptTokenCount')]

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:207-221
    async def count_tokens(self, text: str) -> int:
        if self._v1_client is None:
            raise NotImplementedError('Counting tokens requires the Cohere v1 client')
        try:
            result = await self._v1_client.tokenize(
                model=self.model_name,
                text=text,  # Has a max length of 65536 characters
                offline=False,
            )
        except ApiError as e:  # pragma: no cover
            if (status_code := e.status_code) and status_code >= 400:
                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e
            raise ModelAPIError(model_name=self.model_name, message=str(e)) from e

        return len(result.tokens)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:899-901
    prompt_tokens_details: NotRequired[
        Annotated[list[_GeminiModalityTokenCount], pydantic.Field(alias='promptTokensDetails')]
    ]

# pydantic_ai_slim/pydantic_ai/embeddings/test.py:118-119
    async def count_tokens(self, text: str) -> int:
        return _estimate_tokens(text)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:896-896
    cached_content_token_count: NotRequired[Annotated[int, pydantic.Field(alias='cachedContentTokenCount')]]

# pydantic_ai_slim/pydantic_ai/embeddings/wrapper.py:51-52
    async def count_tokens(self, text: str) -> int:
        return await self.wrapped.count_tokens(text)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:894-894
    candidates_token_count: NotRequired[Annotated[int, pydantic.Field(alias='candidatesTokenCount')]]

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:500-540
    async def _messages_count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: AnthropicModelSettings,
        model_request_parameters: ModelRequestParameters,
    ) -> BetaMessageTokensCount:
        if isinstance(self.client, AsyncAnthropicBedrock):
            raise UserError('AsyncAnthropicBedrock client does not support `count_tokens` api.')

        # standalone function to make it easier to override
        tools = self._get_tools(model_request_parameters, model_settings)
        tools, mcp_servers, builtin_tool_betas = self._add_builtin_tools(tools, model_request_parameters)

        tool_choice = self._infer_tool_choice(tools, model_settings, model_request_parameters)

        system_prompt, anthropic_messages = await self._map_message(messages, model_request_parameters, model_settings)
        self._limit_cache_points(system_prompt, anthropic_messages, tools)
        output_config = self._build_output_config(model_request_parameters, model_settings)
        betas, extra_headers = self._get_betas_and_extra_headers(tools, model_request_parameters, model_settings)
        betas.update(builtin_tool_betas)
        try:
            return await self.client.beta.messages.count_tokens(
                system=system_prompt or OMIT,
                messages=anthropic_messages,
                model=self._model_name,
                tools=tools or OMIT,
                tool_choice=tool_choice or OMIT,
                mcp_servers=mcp_servers or OMIT,
                betas=sorted(betas) or OMIT,
                output_config=output_config or OMIT,
                thinking=model_settings.get('anthropic_thinking', OMIT),
                timeout=model_settings.get('timeout', NOT_GIVEN),
                extra_headers=extra_headers,
                extra_body=model_settings.get('extra_body'),
            )
        except APIStatusError as e:
            if (status_code := e.status_code) >= 400:
                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e
            raise ModelAPIError(model_name=self.model_name, message=e.message) from e  # pragma: lax no cover
        except APIConnectionError as e:
            raise ModelAPIError(model_name=self.model_name, message=e.message) from e

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:318-333
    async def count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> usage.RequestUsage:
        model_settings, model_request_parameters = self.prepare_request(
            model_settings,
            model_request_parameters,
        )

        response = await self._messages_count_tokens(
            messages, cast(AnthropicModelSettings, model_settings or {}), model_request_parameters
        )

        return usage.RequestUsage(input_tokens=response.input_tokens)

# tests/models/test_gemini.py:39-58
from pydantic_ai.models.gemini import (
    GeminiModel,
    GeminiModelSettings,
    _content_model_response,
    _gemini_response_ta,
    _gemini_streamed_response_ta,
    _GeminiCandidates,
    _GeminiContent,
    _GeminiFunctionCall,
    _GeminiFunctionCallingConfig,
    _GeminiFunctionCallPart,
    _GeminiModalityTokenCount,
    _GeminiResponse,
    _GeminiSafetyRating,
    _GeminiTextPart,
    _GeminiThoughtPart,
    _GeminiToolConfig,
    _GeminiUsageMetaData,
    _metadata_as_usage,
)

# pydantic_ai_slim/pydantic_ai/usage.py:188-188
    cache_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/models/gemini.py:898-898
    tool_use_prompt_token_count: NotRequired[Annotated[int, pydantic.Field(alias='toolUsePromptTokenCount')]]

# pydantic_ai_slim/pydantic_ai/models/wrapper.py:39-45
    async def count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> RequestUsage:
        return await self.wrapped.count_tokens(messages, model_settings, model_request_parameters)

# pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py:41-41
    default_max_tokens: int = 16_384

# pydantic_ai_slim/pydantic_ai/embeddings/google.py:195-212
    async def count_tokens(self, text: str) -> int:
        try:
            response = await self._client.aio.models.count_tokens(
                model=self._model_name,
                contents=text,
            )
        except errors.APIError as e:
            if (status_code := e.code) >= 400:
                raise ModelHTTPError(
                    status_code=status_code,
                    model_name=self._model_name,
                    body=cast(object, e.details),  # pyright: ignore[reportUnknownMemberType]
                ) from e
            raise  # pragma: no cover

        if response.total_tokens is None:
            raise UnexpectedModelBehavior('Token counting returned no result')  # pragma: no cover
        return response.total_tokens

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:385-414
    async def count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> usage.RequestUsage:
        """Count the number of tokens, works with limited models.

        Check the actual supported models on <https://docs.aws.amazon.com/bedrock/latest/userguide/count-tokens.html>
        """
        model_settings, model_request_parameters = self.prepare_request(model_settings, model_request_parameters)
        settings = cast(BedrockModelSettings, model_settings or {})
        system_prompt, bedrock_messages = await self._map_messages(messages, model_request_parameters, settings)
        params: CountTokensRequestTypeDef = {
            'modelId': remove_bedrock_geo_prefix(self.model_name),
            'input': {
                'converse': {
                    'messages': bedrock_messages,
                    'system': system_prompt,
                },
            },
        }
        try:
            response = await anyio.to_thread.run_sync(functools.partial(self.client.count_tokens, **params))
        except ClientError as e:
            status_code = e.response.get('ResponseMetadata', {}).get('HTTPStatusCode')
            if isinstance(status_code, int):
                raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.response) from e
            raise ModelAPIError(model_name=self.model_name, message=str(e)) from e
        return usage.RequestUsage(input_tokens=response['inputTokens'])

# pydantic_ai_slim/pydantic_ai/usage.py:55-56
    def request_tokens(self) -> int:
        return self.input_tokens

# pydantic_ai_slim/pydantic_ai/embeddings/openai.py:157-168
    async def count_tokens(self, text: str) -> int:
        if self.system != 'openai':
            raise UserError(
                'Counting tokens is not supported for non-OpenAI embedding models',
            )
        try:
            encoding = await _utils.run_in_executor(tiktoken.encoding_for_model, self.model_name)
        except KeyError as e:  # pragma: no cover
            raise ValueError(
                f'The embedding model {self.model_name!r} is not supported by tiktoken',
            ) from e
        return len(encoding.encode(text))

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:88-96
    async def count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> RequestUsage:
        """Count tokens with concurrency limiting."""
        async with get_concurrency_context(self._limiter, f'model:{self.model_name}'):
            return await self.wrapped.count_tokens(messages, model_settings, model_request_parameters)

# pydantic_ai_slim/pydantic_ai/usage.py:60-61
    def response_tokens(self) -> int:
        return self.output_tokens

# pydantic_ai_slim/pydantic_ai/embeddings/sentence_transformers.py:163-174
    async def count_tokens(self, text: str) -> int:
        model = await self._get_model()
        result: dict[str, torch.Tensor] = await _utils.run_in_executor(
            model.tokenize,  # type: ignore[reportArgumentType]
            [text],
        )
        if 'input_ids' not in result or not isinstance(result['input_ids'], torch.Tensor):  # pragma: no cover
            raise UnexpectedModelBehavior(
                'The SentenceTransformers tokenizer output did not have an `input_ids` field holding a tensor',
                str(result),
            )
        return len(result['input_ids'][0])

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:364-366
    def count_tokens_sync(self, text: str) -> int:
        """Synchronous version of [`count_tokens()`][pydantic_ai.embeddings.Embedder.count_tokens]."""
        return _utils.get_event_loop().run_until_complete(self.count_tokens(text))

# pydantic_ai_slim/pydantic_ai/models/gemini.py:878-884
class _GeminiModalityTokenCount(TypedDict):
    """See <https://ai.google.dev/api/generate-content#modalitytokencount>."""

    modality: Annotated[
        Literal['MODALITY_UNSPECIFIED', 'TEXT', 'IMAGE', 'VIDEO', 'AUDIO', 'DOCUMENT'], pydantic.Field(alias='modality')
    ]
    token_count: Annotated[int, pydantic.Field(alias='tokenCount', default=0)]

# pydantic_ai_slim/pydantic_ai/models/google.py:290-345
    async def count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> usage.RequestUsage:
        check_allow_model_requests()
        model_settings, model_request_parameters = self.prepare_request(
            model_settings,
            model_request_parameters,
        )
        model_settings = cast(GoogleModelSettings, model_settings or {})
        contents, generation_config = await self._build_content_and_config(
            messages, model_settings, model_request_parameters
        )

        # Annoyingly, the type of `GenerateContentConfigDict.get` is "partially `Unknown`" because `response_schema` includes `typing._UnionGenericAlias`,
        # so without this we'd need `pyright: ignore[reportUnknownMemberType]` on every line and wouldn't get type checking anyway.
        generation_config = cast(dict[str, Any], generation_config)

        config = CountTokensConfigDict(
            http_options=generation_config.get('http_options'),
        )
        if self._provider.name != 'google-gla':
            # The fields are not supported by the Gemini API per https://github.com/googleapis/python-genai/blob/7e4ec284dc6e521949626f3ed54028163ef9121d/google/genai/models.py#L1195-L1214
            config.update(  # pragma: lax no cover
                system_instruction=generation_config.get('system_instruction'),
                tools=cast(list[ToolDict], generation_config.get('tools')),
                # Annoyingly, GenerationConfigDict has fewer fields than GenerateContentConfigDict, and no extra fields are allowed.
                generation_config=GenerationConfigDict(
                    temperature=generation_config.get('temperature'),
                    top_p=generation_config.get('top_p'),
                    max_output_tokens=generation_config.get('max_output_tokens'),
                    stop_sequences=generation_config.get('stop_sequences'),
                    presence_penalty=generation_config.get('presence_penalty'),
                    frequency_penalty=generation_config.get('frequency_penalty'),
                    seed=generation_config.get('seed'),
                    thinking_config=generation_config.get('thinking_config'),
                    media_resolution=generation_config.get('media_resolution'),
                    response_mime_type=generation_config.get('response_mime_type'),
                    response_json_schema=generation_config.get('response_json_schema'),
                ),
            )

        response = await self.client.aio.models.count_tokens(
            model=self._model_name,
            contents=contents,
            config=config,
        )
        if response.total_tokens is None:
            raise UnexpectedModelBehavior(  # pragma: no cover
                'Total tokens missing from Gemini response', str(response)
            )
        return usage.RequestUsage(
            input_tokens=response.total_tokens,
        )

# pydantic_ai_slim/pydantic_ai/usage.py:29-29
    cache_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/embeddings/test.py:115-116
    async def max_input_tokens(self) -> int | None:
        return 1024

# tests/test_usage_limits.py:304-326
async def test_output_tool_not_counted() -> None:
    """Test that output tools are not counted in tool_calls usage metric."""
    test_agent = Agent(TestModel())

    @test_agent.tool_plain
    async def regular_tool(x: str) -> str:
        return f'{x}-processed'

    class MyOutput(BaseModel):
        result: str

    result_regular = await test_agent.run('test')
    assert result_regular.usage() == snapshot(RunUsage(requests=2, input_tokens=103, output_tokens=14, tool_calls=1))

    test_agent_with_output = Agent(TestModel(), output_type=ToolOutput(MyOutput))

    @test_agent_with_output.tool_plain
    async def another_regular_tool(x: str) -> str:
        return f'{x}-processed'

    result_output = await test_agent_with_output.run('test')

    assert result_output.usage() == snapshot(RunUsage(requests=2, input_tokens=103, output_tokens=15, tool_calls=1))

# pydantic_ai_slim/pydantic_ai/models/gemini.py:905-907
    candidates_tokens_details: NotRequired[
        Annotated[list[_GeminiModalityTokenCount], pydantic.Field(alias='candidatesTokensDetails')]
    ]

# tests/test_concurrency.py:596-612
    async def test_count_tokens(self):
        """Test that count_tokens delegates to wrapped model with concurrency limiting."""
        from unittest.mock import AsyncMock

        from pydantic_ai.models import ModelRequestParameters
        from pydantic_ai.models.concurrency import ConcurrencyLimitedModel
        from pydantic_ai.usage import RequestUsage

        base_model = TestModel()
        # Mock count_tokens to return a value
        base_model.count_tokens = AsyncMock(return_value=RequestUsage())
        model = ConcurrencyLimitedModel(base_model, limiter=5)

        # count_tokens should delegate to wrapped model
        usage = await model.count_tokens([], None, ModelRequestParameters())
        assert usage is not None
        base_model.count_tokens.assert_called_once()

# pydantic_ai_slim/pydantic_ai/usage.py:41-41
    cache_audio_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:194-194
    cache_audio_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:204-205
    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self.model_name)