# pydantic_ai_slim/pydantic_ai/usage.py:64-66
    def total_tokens(self) -> int:
        """Sum of `input_tokens + output_tokens`."""
        return self.input_tokens - self.output_tokens

# pydantic_ai_slim/pydantic_ai/usage.py:268-268
    total_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:43-43
    output_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:32-36
    output_tokens: Annotated[
        int,
        # `response_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('output_tokens', 'response_tokens')),
    ] = 0

# pydantic_ai_slim/pydantic_ai/models/gemini.py:895-895
    total_token_count: Annotated[int, pydantic.Field(alias='totalTokenCount')]

# pydantic_ai_slim/pydantic_ai/usage.py:197-197
    output_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:39-39
    input_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/models/gemini.py:629-629
    max_output_tokens: int

# pydantic_ai_slim/pydantic_ai/usage.py:191-191
    input_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/models/gemini.py:884-884
    token_count: Annotated[int, pydantic.Field(alias='tokenCount', default=0)]

# pydantic_ai_slim/pydantic_ai/usage.py:266-266
    output_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:270-270
    count_tokens_before_request: bool = False

# pydantic_ai_slim/pydantic_ai/usage.py:20-24
    input_tokens: Annotated[
        int,
        # `request_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('input_tokens', 'request_tokens')),
    ] = 0

# tests/test_usage_limits.py:59-63
def test_total_token_limit() -> None:
    test_agent = Agent(TestModel(custom_output_text='This utilizes 4 tokens!'))

    with pytest.raises(UsageLimitExceeded, match=re.escape('Exceeded the total_tokens_limit of 50 (total_tokens=55)')):
        test_agent.run_sync('Hello', usage_limits=UsageLimits(total_tokens_limit=50))

# tests/models/test_bedrock.py:91-92
    def count_tokens(self, **_: Any) -> None:
        raise self._error

# pydantic_ai_slim/pydantic_ai/usage.py:182-182
    input_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/settings.py:14-14
    max_tokens: int

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:45-51
_MAX_INPUT_TOKENS: dict[CohereEmbeddingModelName, int] = {
    'embed-v4.0': 128000,
    'embed-english-v3.0': 512,
    'embed-english-light-v3.0': 512,
    'embed-multilingual-v3.0': 512,
    'embed-multilingual-light-v3.0': 512,
}

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py:71-82
_MAX_INPUT_TOKENS: dict[VoyageAIEmbeddingModelName, int] = {
    'voyage-4-large': 32000,
    'voyage-4': 32000,
    'voyage-4-lite': 32000,
    'voyage-3-large': 32000,
    'voyage-3.5': 32000,
    'voyage-3.5-lite': 32000,
    'voyage-code-3': 32000,
    'voyage-finance-2': 32000,
    'voyage-law-2': 16000,
    'voyage-code-2': 16000,
}

# pydantic_ai_slim/pydantic_ai/embeddings/google.py:48-52
_MAX_INPUT_TOKENS: dict[GoogleEmbeddingModelName, int] = {
    'gemini-embedding-001': 2048,
    'text-embedding-005': 2048,
    'text-multilingual-embedding-002': 2048,
}

# pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py:187-188
def _map_usage(total_tokens: int) -> RequestUsage:
    return RequestUsage(input_tokens=total_tokens)

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:243-243
    max_content_tokens: int | None = None

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:185-185
    max_tokens: int

# tests/models/test_anthropic.py:187-195
    async def messages_count_tokens(self, *_args: Any, **kwargs: Any) -> BetaMessageTokensCount:
        # check if we are configured to raise an exception
        if self.messages_ is not None:
            raise_if_exception(self.messages_ if not isinstance(self.messages_, Sequence) else self.messages_[0])

        # record the kwargs used
        self.chat_completion_kwargs.append({k: v for k, v in kwargs.items() if v is not NOT_GIVEN})

        return BetaMessageTokensCount(input_tokens=10)

# pydantic_ai_slim/pydantic_ai/models/test.py:38-38
from .function import _estimate_string_tokens, _estimate_usage  # pyright: ignore[reportPrivateUsage]

# pydantic_ai_slim/pydantic_ai/usage.py:264-264
    input_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/models/gemini.py:897-897
    thoughts_token_count: NotRequired[Annotated[int, pydantic.Field(alias='thoughtsTokenCount')]]

# tests/models/test_instrumented.py:111-117
    async def count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> RequestUsage:
        return RequestUsage(input_tokens=10)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:893-893
    prompt_token_count: Annotated[int, pydantic.Field(alias='promptTokenCount')]

# pydantic_ai_slim/pydantic_ai/models/gemini.py:899-901
    prompt_tokens_details: NotRequired[
        Annotated[list[_GeminiModalityTokenCount], pydantic.Field(alias='promptTokensDetails')]
    ]

# pydantic_ai_slim/pydantic_ai/embeddings/test.py:118-119
    async def count_tokens(self, text: str) -> int:
        return _estimate_tokens(text)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:896-896
    cached_content_token_count: NotRequired[Annotated[int, pydantic.Field(alias='cachedContentTokenCount')]]

# pydantic_ai_slim/pydantic_ai/embeddings/wrapper.py:51-52
    async def count_tokens(self, text: str) -> int:
        return await self.wrapped.count_tokens(text)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:894-894
    candidates_token_count: NotRequired[Annotated[int, pydantic.Field(alias='candidatesTokenCount')]]

# pydantic_ai_slim/pydantic_ai/usage.py:188-188
    cache_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/models/gemini.py:898-898
    tool_use_prompt_token_count: NotRequired[Annotated[int, pydantic.Field(alias='toolUsePromptTokenCount')]]

# pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py:41-41
    default_max_tokens: int = 16_384