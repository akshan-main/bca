# src/cegraph/agent/__init__.py:3-3
from cegraph.agent.loop import AgentLoop

# src/cegraph/agent/loop.py:22-30
class AgentStep:
    """A single step in the agent loop."""

    iteration: int
    thought: str = ""
    tool_calls: list[ToolCall] = field(default_factory=list)
    tool_results: list[ToolResult] = field(default_factory=list)
    response: str = ""
    usage: dict[str, int] = field(default_factory=dict)

# src/cegraph/cli.py:526-594
def _run_agent(
    root: Path,
    config: ProjectConfig,
    graph,
    store,
    task: str,
    agent_mode: bool = True,
    auto_approve: bool = False,
):
    """Run the agent loop."""
    from cegraph.agent.loop import AgentLoop, AgentStep
    from cegraph.graph.query import GraphQuery
    from cegraph.llm.factory import create_provider
    from cegraph.search.hybrid import HybridSearch
    from cegraph.tools.definitions import get_all_tools

    # Check for API key (skip for local providers that don't need one)
    llm_config = config.llm
    if not llm_config.api_key and llm_config.provider not in ("local",):
        provider = llm_config.provider
        env_var = {"openai": "OPENAI_API_KEY", "anthropic": "ANTHROPIC_API_KEY"}.get(
            provider, f"{provider.upper()}_API_KEY"
        )
        console.error(
            f"No API key found for {provider}. "
            f"Set the {env_var} environment variable or configure it with:\n"
            f"  cegraph config set llm.api_key_env {env_var}"
        )
        store.close()
        sys.exit(1)

    try:
        llm = create_provider(llm_config)
    except Exception as e:
        console.error(str(e))
        store.close()
        sys.exit(1)

    query = GraphQuery(graph, store)
    search_engine = HybridSearch(root, graph)
    tools = get_all_tools(root, graph, query, search_engine)

    def on_step(step: AgentStep):
        console.show_agent_step(step)

    agent_loop = AgentLoop(
        llm=llm,
        tools=tools,
        project_name=config.name,
        max_iterations=config.agent.max_iterations,
        on_step=on_step,
    )

    console.info(f"Running {'agent' if agent_mode else 'Q&A'} for: {task}")
    console.console.print()

    result = asyncio.run(agent_loop.run(task))

    if not result.success:
        if result.error:
            console.error(result.error)

    console.console.print()
    console.info(
        f"Completed in {result.total_iterations} step(s), "
        f"~{result.total_tokens:,} tokens used"
    )

    store.close()

# src/cegraph/agent/loop.py:22-30
class AgentStep:
    """A single step in the agent loop."""

    iteration: int
    thought: str = ""
    tool_calls: list[ToolCall] = field(default_factory=list)
    tool_results: list[ToolResult] = field(default_factory=list)
    response: str = ""
    usage: dict[str, int] = field(default_factory=dict)

# src/cegraph/config.py:41-46
class AgentConfig(BaseModel):
    """Agent behavior configuration."""

    max_iterations: int = 150
    auto_verify: bool = True
    require_approval: bool = True

# src/cegraph/agent/loop.py:34-42
class AgentResult:
    """Final result from the agent loop."""

    answer: str
    steps: list[AgentStep]
    total_iterations: int
    total_tokens: int
    success: bool = True
    error: str = ""

# src/cegraph/cli.py:517-523
def agent(task: str, path: str | None, auto: bool):
    """Run an agentic task (coding, debugging, refactoring)."""
    root = _get_project_root(path)
    config = load_config(root)
    graph, store = _load_graph(root)

    _run_agent(root, config, graph, store, task, agent_mode=True, auto_approve=auto)

# src/cegraph/ui/console.py:86-118
    def show_agent_step(self, step: AgentStep) -> None:
        """Display an agent step with tool calls and results."""
        if step.thought:
            self.console.print(
                Panel(
                    Markdown(step.thought),
                    title=f"[bold]Step {step.iteration}[/bold] - Thinking",
                    border_style="blue",
                )
            )

        for tc in step.tool_calls:
            args_str = ", ".join(f"{k}={repr(v)}" for k, v in tc.arguments.items())
            self.console.print(
                f"  [yellow]â†’[/yellow] [bold]{tc.name}[/bold]({args_str})"
            )

        for tr in step.tool_results:
            # Truncate long results
            content = tr.content
            if len(content) > 500:
                content = content[:500] + "\n... (truncated)"
            style = "red" if tr.is_error else "dim"
            self.console.print(f"  [{style}]{content}[/{style}]")

        if step.response:
            self.console.print(
                Panel(
                    Markdown(step.response),
                    title="[bold green]Answer[/bold green]",
                    border_style="green",
                )
            )

# src/cegraph/agent/loop.py:18-18
from cegraph.tools.registry import ToolRegistry

# src/cegraph/agent/loop.py:57-71
    def __init__(
        self,
        llm: LLMProvider,
        tools: ToolRegistry,
        project_name: str = "",
        max_iterations: int = 15,
        on_step: Callable[[AgentStep], None] | None = None,
        on_approval_needed: Callable[[str], bool] | None = None,
    ) -> None:
        self.llm = llm
        self.tools = tools
        self.project_name = project_name
        self.max_iterations = max_iterations
        self.on_step = on_step
        self.on_approval_needed = on_approval_needed

# paper/experiments/run_all.py:12-12
from collections import defaultdict

# paper/experiments/run_all.py:12-12
from collections import defaultdict

# src/cegraph/config.py:29-38
    def api_key(self) -> str | None:
        if self.api_key_env:
            return os.environ.get(self.api_key_env)
        # Try common env vars
        env_map = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
        }
        env_var = env_map.get(self.provider, "")
        return os.environ.get(env_var)

# src/cegraph/exceptions.py:24-25
class ToolError(CeGraphError):
    """Agent tool execution errors."""

# tests/test_config.py:19-23
    def test_default_config(self):
        config = ProjectConfig()
        assert config.llm.provider == "anthropic"
        assert config.agent.max_iterations == 15
        assert len(config.indexer.exclude_patterns) > 0

# src/cegraph/context/_native.py:235-236
    def set_lines(self, node: int, start: int, end: int) -> None:
        self._lib.cag_graph_set_lines(self._handle, node, start, end)

# src/cegraph/cli.py:14-22
from cegraph.config import (
    GRAPH_DB_FILE,
    ProjectConfig,
    find_project_root,
    get_cegraph_dir,
    load_config,
    save_config,
    set_config_value,
)

# src/cegraph/cli.py:14-22
from cegraph.config import (
    GRAPH_DB_FILE,
    ProjectConfig,
    find_project_root,
    get_cegraph_dir,
    load_config,
    save_config,
    set_config_value,
)

# src/cegraph/agent/__init__.py:3-3
from cegraph.agent.loop import AgentLoop

# src/cegraph/context/_native.py:232-233
    def set_node_weight(self, node: int, weight: float) -> None:
        self._lib.cag_graph_set_node_weight(self._handle, node, weight)

# tests/test_cli.py:107-113
    def test_config_set(self, runner: CliRunner, indexed_project: Path):
        result = runner.invoke(
            main,
            ["config", "set", "llm.provider", "openai", "--path", str(indexed_project)],
        )
        assert result.exit_code == 0
        assert "Set" in result.output

# src/cegraph/agent/loop.py:16-16
from cegraph.agent.prompts import get_system_prompt

# tests/test_config.py:50-53
    def test_set_config_value(self):
        config = ProjectConfig()
        updated = set_config_value(config, "llm.provider", "openai")
        assert updated.llm.provider == "openai"

# tests/test_config.py:55-58
    def test_set_config_nested(self):
        config = ProjectConfig()
        updated = set_config_value(config, "agent.max_iterations", 20)
        assert updated.agent.max_iterations == 20

# src/cegraph/agent/loop.py:73-183
    async def run(self, task: str, context: str = "") -> AgentResult:
        """Run the agent loop for a given task.

        Args:
            task: The user's request/task.
            context: Additional context (e.g., file content, error messages).

        Returns:
            AgentResult with the final answer and step history.
        """
        messages: list[Message] = [
            Message(role="system", content=get_system_prompt(self.project_name)),
        ]

        # Add context if provided
        user_content = task
        if context:
            user_content = f"{task}\n\nContext:\n{context}"
        messages.append(Message(role="user", content=user_content))

        steps: list[AgentStep] = []
        total_tokens = 0

        for iteration in range(self.max_iterations):
            step = AgentStep(iteration=iteration + 1)

            try:
                response = await self.llm.complete(
                    messages=messages,
                    tools=self.tools.get_definitions(),
                    temperature=0.0,
                )
            except Exception as e:
                return AgentResult(
                    answer="",
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=False,
                    error=f"LLM error: {e}",
                )

            step.usage = response.usage
            total_tokens += sum(response.usage.values())

            if response.has_tool_calls:
                # Agent wants to use tools
                step.thought = response.content
                step.tool_calls = response.tool_calls

                # Add assistant message with tool calls
                messages.append(
                    Message(
                        role="assistant",
                        content=response.content,
                        tool_calls=response.tool_calls,
                    )
                )

                # Execute each tool call
                for tc in response.tool_calls:
                    result = await self.tools.execute(tc.name, tc.arguments)
                    tool_result = ToolResult(
                        tool_call_id=tc.id,
                        name=tc.name,
                        content=result,
                    )
                    step.tool_results.append(tool_result)

                    # Add tool result message
                    messages.append(
                        Message(
                            role="tool",
                            content=result,
                            tool_call_id=tc.id,
                            name=tc.name,
                        )
                    )

                # Notify step callback
                if self.on_step:
                    self.on_step(step)

                steps.append(step)

            else:
                # Agent is done - final answer
                step.response = response.content

                if self.on_step:
                    self.on_step(step)

                steps.append(step)

                return AgentResult(
                    answer=response.content,
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=True,
                )

        # Max iterations reached
        return AgentResult(
            answer="I reached the maximum number of iterations. Here's what I've found so far based on my analysis.",
            steps=steps,
            total_iterations=self.max_iterations,
            total_tokens=total_tokens,
            success=False,
            error="Max iterations reached",
        )

# src/cegraph/cli.py:14-22
from cegraph.config import (
    GRAPH_DB_FILE,
    ProjectConfig,
    find_project_root,
    get_cegraph_dir,
    load_config,
    save_config,
    set_config_value,
)

# tests/test_config.py:60-63
    def test_set_config_invalid_key(self):
        config = ProjectConfig()
        with pytest.raises(KeyError):
            set_config_value(config, "nonexistent.key", "value")

# src/cegraph/context/_native.py:84-145
def _setup_signatures(lib):
    """Define ctypes function signatures for type safety."""
    # Graph creation
    lib.cag_graph_create.argtypes = [ctypes.c_int32]
    lib.cag_graph_create.restype = ctypes.c_void_p

    lib.cag_graph_add_edge.argtypes = [ctypes.c_void_p, ctypes.c_int32, ctypes.c_int32, ctypes.c_float]
    lib.cag_graph_add_edge.restype = None

    lib.cag_graph_set_node_weight.argtypes = [ctypes.c_void_p, ctypes.c_int32, ctypes.c_float]
    lib.cag_graph_set_node_weight.restype = None

    lib.cag_graph_set_lines.argtypes = [ctypes.c_void_p, ctypes.c_int32, ctypes.c_int32, ctypes.c_int32]
    lib.cag_graph_set_lines.restype = None

    # Weighted BFS
    lib.cag_weighted_bfs.argtypes = [
        ctypes.c_void_p,                                    # graph
        ctypes.POINTER(ctypes.c_int32),                     # seed_nodes
        ctypes.POINTER(ctypes.c_float),                     # seed_scores
        ctypes.c_int32,                                     # num_seeds
        ctypes.c_int32,                                     # max_depth
        ctypes.c_float,                                     # min_score
        ctypes.c_float,                                     # backward_decay
        ctypes.POINTER(ctypes.c_int32),                     # out_nodes
        ctypes.POINTER(ctypes.c_float),                     # out_scores
        ctypes.POINTER(ctypes.c_int32),                     # out_depths
        ctypes.c_int32,                                     # max_results
    ]
    lib.cag_weighted_bfs.restype = ctypes.c_int32

    # Token estimation
    lib.cag_estimate_tokens.argtypes = [ctypes.c_char_p, ctypes.c_int32]
    lib.cag_estimate_tokens.restype = ctypes.c_int32

    # Topological sort
    lib.cag_topological_sort.argtypes = [
        ctypes.c_void_p,
        ctypes.POINTER(ctypes.c_int32),
        ctypes.c_int32,
        ctypes.POINTER(ctypes.c_int32),
    ]
    lib.cag_topological_sort.restype = ctypes.c_int32

    # Entity extraction
    lib.cag_extract_entities.argtypes = [
        ctypes.c_char_p,
        ctypes.c_int32,
        ctypes.POINTER(ctypes.c_int32),
        ctypes.POINTER(ctypes.c_int32),
        ctypes.POINTER(ctypes.c_int32),
        ctypes.c_int32,
    ]
    lib.cag_extract_entities.restype = ctypes.c_int32

    # Cleanup
    lib.cag_graph_destroy.argtypes = [ctypes.c_void_p]
    lib.cag_graph_destroy.restype = None

    # Version
    lib.cag_version.argtypes = []
    lib.cag_version.restype = ctypes.c_char_p

# src/cegraph/agent/loop.py:185-191
    async def ask(self, question: str) -> str:
        """Simple Q&A mode - ask a question about the codebase.

        Returns just the answer string.
        """
        result = await self.run(question)
        return result.answer

# tests/test_graph.py:77-98
    def test_rebuild_resets_state(self, tmp_path: Path):
        """Regression: reusing a builder must not accumulate stale nodes."""
        # First project
        p1 = tmp_path / "proj1"
        p1.mkdir()
        (p1 / "a.py").write_text("def a_func():\n    pass\n")

        builder = GraphBuilder()
        g1 = builder.build_from_directory(p1)
        g1_files = {d["path"] for _, d in g1.nodes(data=True) if d.get("type") == "file"}

        # Second project with different files
        p2 = tmp_path / "proj2"
        p2.mkdir()
        (p2 / "b.py").write_text("def b_func():\n    pass\n")

        g2 = builder.build_from_directory(p2)
        g2_files = {d["path"] for _, d in g2.nodes(data=True) if d.get("type") == "file"}

        # g2 should NOT contain files from g1
        assert "a.py" not in g2_files
        assert "b.py" in g2_files

# src/cegraph/context/engine.py:567-630
    def _expand_context_pagerank(
        self, seeds: list[dict], config: dict
    ) -> list[dict]:
        """Expand context via personalized PageRank from seeds.

        Instead of BFS, compute PPR with the seed set as the personalization
        vector.  This gives a global relevance score that accounts for the
        full graph structure, not just local neighborhoods.
        """
        min_score = config["min_score"]

        # Build personalization vector (seed nodes get their scores)
        personalization: dict[str, float] = {}
        for s in seeds:
            personalization[s["symbol_id"]] = s["score"]

        # Ensure all seed nodes exist in graph
        personalization = {
            k: v for k, v in personalization.items()
            if self.graph.has_node(k)
        }
        if not personalization:
            return []

        # Normalize
        total = sum(personalization.values())
        personalization = {k: v / total for k, v in personalization.items()}

        try:
            ppr = nx.pagerank(
                self.graph,
                alpha=0.85,
                personalization=personalization,
                max_iter=100,
                tol=1e-6,
            )
        except nx.NetworkXError:
            return self._expand_context(seeds, config)

        seed_reason = {s["symbol_id"]: s["reason"] for s in seeds}
        candidates: list[dict] = []

        for node_id, score in ppr.items():
            if score < min_score * 0.01:
                continue
            data = self.graph.nodes.get(node_id, {})
            if data.get("type") != "symbol":
                continue

            # Scale PPR scores to be comparable with BFS scores
            scaled_score = score * 100

            reason = seed_reason.get(node_id, "pagerank expansion")
            depth = 0 if node_id in personalization else 1
            candidates.append({
                "symbol_id": node_id,
                "score": scaled_score,
                "depth": depth,
                "reason": reason,
                "via": [],
            })

        candidates.sort(key=lambda x: x["score"], reverse=True)
        return candidates

# src/cegraph/context/_native.py:269-276
    def topological_sort(self, nodes: list[int]) -> list[int]:
        """Topological sort a subset of nodes."""
        n = len(nodes)
        c_nodes = (ctypes.c_int32 * n)(*nodes)
        out_order = (ctypes.c_int32 * n)()

        count = self._lib.cag_topological_sort(self._handle, c_nodes, n, out_order)
        return [out_order[i] for i in range(count)]

# src/cegraph/context/learned_weights.py:105-139
def _get_commit_file_changes(
    root: Path, max_commits: int
) -> list[set[str]]:
    """Extract per-commit changed file sets from git log."""
    result = subprocess.run(
        ["git", "log", f"--max-count={max_commits}", "--name-only", "--format=%H"],
        capture_output=True,
        text=True,
        cwd=root,
        timeout=30,
    )
    if result.returncode != 0:
        return []

    commits: list[set[str]] = []
    current_files: set[str] = set()

    for line in result.stdout.splitlines():
        line = line.strip()
        if not line:
            if current_files:
                commits.append(current_files)
                current_files = set()
            continue
        if re.match(r"^[0-9a-f]{40}$", line):
            if current_files:
                commits.append(current_files)
            current_files = set()
        else:
            current_files.add(line)

    if current_files:
        commits.append(current_files)

    return commits

# src/cegraph/context/engine.py:706-743
    def _closure_of(self, symbol_id: str, candidate_ids: set[str]) -> set[str]:
        """Compute D(v): the transitive set of hard dependencies for symbol v."""
        deps: set[str] = set()
        stack = [symbol_id]
        visited = {symbol_id}

        while stack:
            current = stack.pop()
            data = self.graph.nodes.get(current, {})

            # Forward edges: inherits, implements
            for succ in self.graph.successors(current):
                edge_data = self.graph.edges[current, succ]
                edge_kind = edge_data.get("kind", "")

                if edge_kind in _HARD_DEP_EDGES:
                    succ_data = self.graph.nodes.get(succ, {})
                    if succ_data.get("type") == "symbol" and succ not in visited:
                        deps.add(succ)
                        visited.add(succ)
                        stack.append(succ)

            # Upward containment: if this is a method, its parent class is needed
            if data.get("kind") in ("method", "function"):
                for pred in self.graph.predecessors(current):
                    edge_data = self.graph.edges[pred, current]
                    if edge_data.get("kind") == "contains":
                        pred_data = self.graph.nodes.get(pred, {})
                        if (
                            pred_data.get("type") == "symbol"
                            and pred_data.get("kind") == "class"
                            and pred not in visited
                        ):
                            deps.add(pred)
                            visited.add(pred)
                            stack.append(pred)

        return deps