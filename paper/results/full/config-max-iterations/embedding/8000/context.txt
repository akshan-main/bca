# src/cegraph/agent/__init__.py:3-3
from cegraph.agent.loop import AgentLoop

# src/cegraph/config.py:41-46
class AgentConfig(BaseModel):
    """Agent behavior configuration."""

    max_iterations: int = 150
    auto_verify: bool = True
    require_approval: bool = True

# src/cegraph/agent/loop.py:45-191
class AgentLoop:
    """ReAct agent loop that iteratively uses tools to complete tasks.

    The agent:
    1. Receives a task from the user
    2. Reasons about what to do next
    3. Calls tools to gather information or make changes
    4. Processes tool results
    5. Repeats until it has enough info to give a final answer
    6. Presents the answer/changes for user approval
    """

    def __init__(
        self,
        llm: LLMProvider,
        tools: ToolRegistry,
        project_name: str = "",
        max_iterations: int = 15,
        on_step: Callable[[AgentStep], None] | None = None,
        on_approval_needed: Callable[[str], bool] | None = None,
    ) -> None:
        self.llm = llm
        self.tools = tools
        self.project_name = project_name
        self.max_iterations = max_iterations
        self.on_step = on_step
        self.on_approval_needed = on_approval_needed

    async def run(self, task: str, context: str = "") -> AgentResult:
        """Run the agent loop for a given task.

        Args:
            task: The user's request/task.
            context: Additional context (e.g., file content, error messages).

        Returns:
            AgentResult with the final answer and step history.
        """
        messages: list[Message] = [
            Message(role="system", content=get_system_prompt(self.project_name)),
        ]

        # Add context if provided
        user_content = task
        if context:
            user_content = f"{task}\n\nContext:\n{context}"
        messages.append(Message(role="user", content=user_content))

        steps: list[AgentStep] = []
        total_tokens = 0

        for iteration in range(self.max_iterations):
            step = AgentStep(iteration=iteration + 1)

            try:
                response = await self.llm.complete(
                    messages=messages,
                    tools=self.tools.get_definitions(),
                    temperature=0.0,
                )
            except Exception as e:
                return AgentResult(
                    answer="",
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=False,
                    error=f"LLM error: {e}",
                )

            step.usage = response.usage
            total_tokens += sum(response.usage.values())

            if response.has_tool_calls:
                # Agent wants to use tools
                step.thought = response.content
                step.tool_calls = response.tool_calls

                # Add assistant message with tool calls
                messages.append(
                    Message(
                        role="assistant",
                        content=response.content,
                        tool_calls=response.tool_calls,
                    )
                )

                # Execute each tool call
                for tc in response.tool_calls:
                    result = await self.tools.execute(tc.name, tc.arguments)
                    tool_result = ToolResult(
                        tool_call_id=tc.id,
                        name=tc.name,
                        content=result,
                    )
                    step.tool_results.append(tool_result)

                    # Add tool result message
                    messages.append(
                        Message(
                            role="tool",
                            content=result,
                            tool_call_id=tc.id,
                            name=tc.name,
                        )
                    )

                # Notify step callback
                if self.on_step:
                    self.on_step(step)

                steps.append(step)

            else:
                # Agent is done - final answer
                step.response = response.content

                if self.on_step:
                    self.on_step(step)

                steps.append(step)

                return AgentResult(
                    answer=response.content,
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=True,
                )

        # Max iterations reached
        return AgentResult(
            answer="I reached the maximum number of iterations. Here's what I've found so far based on my analysis.",
            steps=steps,
            total_iterations=self.max_iterations,
            total_tokens=total_tokens,
            success=False,
            error="Max iterations reached",
        )

    async def ask(self, question: str) -> str:
        """Simple Q&A mode - ask a question about the codebase.

        Returns just the answer string.
        """
        result = await self.run(question)
        return result.answer

# src/cegraph/agent/loop.py:34-42
class AgentResult:
    """Final result from the agent loop."""

    answer: str
    steps: list[AgentStep]
    total_iterations: int
    total_tokens: int
    success: bool = True
    error: str = ""

# src/cegraph/agent/loop.py:22-30
class AgentStep:
    """A single step in the agent loop."""

    iteration: int
    thought: str = ""
    tool_calls: list[ToolCall] = field(default_factory=list)
    tool_results: list[ToolResult] = field(default_factory=list)
    response: str = ""
    usage: dict[str, int] = field(default_factory=dict)

# src/cegraph/cli.py:526-594
def _run_agent(
    root: Path,
    config: ProjectConfig,
    graph,
    store,
    task: str,
    agent_mode: bool = True,
    auto_approve: bool = False,
):
    """Run the agent loop."""
    from cegraph.agent.loop import AgentLoop, AgentStep
    from cegraph.graph.query import GraphQuery
    from cegraph.llm.factory import create_provider
    from cegraph.search.hybrid import HybridSearch
    from cegraph.tools.definitions import get_all_tools

    # Check for API key (skip for local providers that don't need one)
    llm_config = config.llm
    if not llm_config.api_key and llm_config.provider not in ("local",):
        provider = llm_config.provider
        env_var = {"openai": "OPENAI_API_KEY", "anthropic": "ANTHROPIC_API_KEY"}.get(
            provider, f"{provider.upper()}_API_KEY"
        )
        console.error(
            f"No API key found for {provider}. "
            f"Set the {env_var} environment variable or configure it with:\n"
            f"  cegraph config set llm.api_key_env {env_var}"
        )
        store.close()
        sys.exit(1)

    try:
        llm = create_provider(llm_config)
    except Exception as e:
        console.error(str(e))
        store.close()
        sys.exit(1)

    query = GraphQuery(graph, store)
    search_engine = HybridSearch(root, graph)
    tools = get_all_tools(root, graph, query, search_engine)

    def on_step(step: AgentStep):
        console.show_agent_step(step)

    agent_loop = AgentLoop(
        llm=llm,
        tools=tools,
        project_name=config.name,
        max_iterations=config.agent.max_iterations,
        on_step=on_step,
    )

    console.info(f"Running {'agent' if agent_mode else 'Q&A'} for: {task}")
    console.console.print()

    result = asyncio.run(agent_loop.run(task))

    if not result.success:
        if result.error:
            console.error(result.error)

    console.console.print()
    console.info(
        f"Completed in {result.total_iterations} step(s), "
        f"~{result.total_tokens:,} tokens used"
    )

    store.close()

# src/cegraph/cli.py:517-523
def agent(task: str, path: str | None, auto: bool):
    """Run an agentic task (coding, debugging, refactoring)."""
    root = _get_project_root(path)
    config = load_config(root)
    graph, store = _load_graph(root)

    _run_agent(root, config, graph, store, task, agent_mode=True, auto_approve=auto)

# src/cegraph/agent/loop.py:57-71
    def __init__(
        self,
        llm: LLMProvider,
        tools: ToolRegistry,
        project_name: str = "",
        max_iterations: int = 15,
        on_step: Callable[[AgentStep], None] | None = None,
        on_approval_needed: Callable[[str], bool] | None = None,
    ) -> None:
        self.llm = llm
        self.tools = tools
        self.project_name = project_name
        self.max_iterations = max_iterations
        self.on_step = on_step
        self.on_approval_needed = on_approval_needed

# src/cegraph/exceptions.py:24-25
class ToolError(CeGraphError):
    """Agent tool execution errors."""

# src/cegraph/ui/console.py:13-13
from cegraph.agent.loop import AgentStep

# paper/experiments/run_all.py:17-17
from paper.experiments.ablation import AblationResult, run_ablation

# src/cegraph/agent/loop.py:73-183
    async def run(self, task: str, context: str = "") -> AgentResult:
        """Run the agent loop for a given task.

        Args:
            task: The user's request/task.
            context: Additional context (e.g., file content, error messages).

        Returns:
            AgentResult with the final answer and step history.
        """
        messages: list[Message] = [
            Message(role="system", content=get_system_prompt(self.project_name)),
        ]

        # Add context if provided
        user_content = task
        if context:
            user_content = f"{task}\n\nContext:\n{context}"
        messages.append(Message(role="user", content=user_content))

        steps: list[AgentStep] = []
        total_tokens = 0

        for iteration in range(self.max_iterations):
            step = AgentStep(iteration=iteration + 1)

            try:
                response = await self.llm.complete(
                    messages=messages,
                    tools=self.tools.get_definitions(),
                    temperature=0.0,
                )
            except Exception as e:
                return AgentResult(
                    answer="",
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=False,
                    error=f"LLM error: {e}",
                )

            step.usage = response.usage
            total_tokens += sum(response.usage.values())

            if response.has_tool_calls:
                # Agent wants to use tools
                step.thought = response.content
                step.tool_calls = response.tool_calls

                # Add assistant message with tool calls
                messages.append(
                    Message(
                        role="assistant",
                        content=response.content,
                        tool_calls=response.tool_calls,
                    )
                )

                # Execute each tool call
                for tc in response.tool_calls:
                    result = await self.tools.execute(tc.name, tc.arguments)
                    tool_result = ToolResult(
                        tool_call_id=tc.id,
                        name=tc.name,
                        content=result,
                    )
                    step.tool_results.append(tool_result)

                    # Add tool result message
                    messages.append(
                        Message(
                            role="tool",
                            content=result,
                            tool_call_id=tc.id,
                            name=tc.name,
                        )
                    )

                # Notify step callback
                if self.on_step:
                    self.on_step(step)

                steps.append(step)

            else:
                # Agent is done - final answer
                step.response = response.content

                if self.on_step:
                    self.on_step(step)

                steps.append(step)

                return AgentResult(
                    answer=response.content,
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=True,
                )

        # Max iterations reached
        return AgentResult(
            answer="I reached the maximum number of iterations. Here's what I've found so far based on my analysis.",
            steps=steps,
            total_iterations=self.max_iterations,
            total_tokens=total_tokens,
            success=False,
            error="Max iterations reached",
        )

# paper/experiments/run_all.py:16-16
from paper.experiments.baselines import BaselineResult, run_comparison

# src/cegraph/ui/console.py:86-118
    def show_agent_step(self, step: AgentStep) -> None:
        """Display an agent step with tool calls and results."""
        if step.thought:
            self.console.print(
                Panel(
                    Markdown(step.thought),
                    title=f"[bold]Step {step.iteration}[/bold] - Thinking",
                    border_style="blue",
                )
            )

        for tc in step.tool_calls:
            args_str = ", ".join(f"{k}={repr(v)}" for k, v in tc.arguments.items())
            self.console.print(
                f"  [yellow]â†’[/yellow] [bold]{tc.name}[/bold]({args_str})"
            )

        for tr in step.tool_results:
            # Truncate long results
            content = tr.content
            if len(content) > 500:
                content = content[:500] + "\n... (truncated)"
            style = "red" if tr.is_error else "dim"
            self.console.print(f"  [{style}]{content}[/{style}]")

        if step.response:
            self.console.print(
                Panel(
                    Markdown(step.response),
                    title="[bold green]Answer[/bold green]",
                    border_style="green",
                )
            )

# src/cegraph/context/models.py:163-163
    CHARS_PER_TOKEN = 4.0

# src/cegraph/tools/registry.py:11-58
class ToolRegistry:
    """Registry that manages available agent tools.

    Tools are functions decorated with @tool that the LLM agent can call.
    """

    def __init__(self) -> None:
        self._tools: dict[str, Callable] = {}
        self._definitions: dict[str, ToolDefinition] = {}

    def register(self, func: Callable, definition: ToolDefinition) -> None:
        """Register a tool function with its definition."""
        self._tools[definition.name] = func
        self._definitions[definition.name] = definition

    def get(self, name: str) -> Callable | None:
        """Get a tool function by name."""
        return self._tools.get(name)

    def get_definition(self, name: str) -> ToolDefinition | None:
        """Get a tool definition by name."""
        return self._definitions.get(name)

    def list_tools(self) -> list[str]:
        """List all registered tool names."""
        return list(self._tools.keys())

    def get_definitions(self) -> list[ToolDefinition]:
        """Get all tool definitions (for passing to LLM)."""
        return list(self._definitions.values())

    async def execute(self, name: str, arguments: dict[str, Any]) -> str:
        """Execute a tool by name with the given arguments.

        Returns the result as a string.
        """
        func = self._tools.get(name)
        if func is None:
            return f"Error: Unknown tool '{name}'"

        try:
            if inspect.iscoroutinefunction(func):
                result = await func(**arguments)
            else:
                result = func(**arguments)
            return str(result) if result is not None else "Done."
        except Exception as e:
            return f"Error executing tool '{name}': {e}"

# src/cegraph/context/engine.py:1078-1133
    def estimate_savings(self, task: str, token_budget: int = 8000) -> dict:
        """Estimate how many tokens CAG saves vs naive approaches."""
        package = self.assemble(task, token_budget)

        entities = self._extract_entities(task)
        grep_files = set()
        grep_tokens = 0
        for entity in entities:
            for node_id, data in self.graph.nodes(data=True):
                if data.get("type") != "file":
                    continue
                fp = data.get("path", "")
                full_path = self.root / fp
                if full_path.exists():
                    try:
                        content = full_path.read_text(
                            encoding="utf-8", errors="replace"
                        )
                        if entity["name"].lower() in content.lower():
                            if fp not in grep_files:
                                grep_files.add(fp)
                                grep_tokens += TokenEstimator.estimate(content)
                    except OSError:
                        pass

        total_tokens = 0
        for node_id, data in self.graph.nodes(data=True):
            if data.get("type") == "file":
                fp = data.get("path", "")
                full_path = self.root / fp
                if full_path.exists():
                    try:
                        content = full_path.read_text(
                            encoding="utf-8", errors="replace"
                        )
                        total_tokens += TokenEstimator.estimate(content)
                    except OSError:
                        pass

        cag_t = package.total_tokens
        return {
            "task": task,
            "cag_tokens": cag_t,
            "cag_files": package.files_included,
            "cag_symbols": package.symbols_included,
            "grep_tokens": grep_tokens,
            "grep_files": len(grep_files),
            "all_files_tokens": total_tokens,
            "savings_vs_grep": (
                f"{max(0, (1 - cag_t / max(grep_tokens, 1)) * 100):.0f}%"
            ),
            "savings_vs_all": (
                f"{max(0, (1 - cag_t / max(total_tokens, 1)) * 100):.0f}%"
            ),
            "accelerated": self.is_accelerated,
        }

# tests/test_context.py:302-305
    def test_is_not_accelerated(self, cag_engine: ContextAssembler):
        """Test that C++ acceleration is reported correctly."""
        # In tests, C++ extension is typically not compiled
        assert isinstance(cag_engine.is_accelerated, bool)

# src/cegraph/llm/openai_provider.py:90-129
    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        client = self._get_client()
        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": self._format_messages(messages),
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        response = await client.chat.completions.create(**kwargs)
        choice = response.choices[0]

        tool_calls = []
        if choice.message.tool_calls:
            for tc in choice.message.tool_calls:
                try:
                    args = json.loads(tc.function.arguments)
                except json.JSONDecodeError:
                    args = {}
                tool_calls.append(
                    ToolCall(id=tc.id, name=tc.function.name, arguments=args)
                )

        return LLMResponse(
            content=choice.message.content or "",
            tool_calls=tool_calls,
            finish_reason=choice.finish_reason or "",
            usage={
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0,
            },
        )

# paper/experiments/baselines.py:118-204
def baseline_bm25(
    repo_path: Path,
    task: str,
    budget: int,
    graph,
) -> BaselineResult:
    """BM25 scoring over symbols, greedy packing by score."""
    start = time.time()

    query_terms = re.findall(r"\b([A-Za-z_]\w{2,})\b", task.lower())
    query_tf = Counter(query_terms)

    # Collect symbol documents
    symbols: list[dict] = []
    for node_id, data in graph.nodes(data=True):
        if data.get("type") != "symbol":
            continue
        name = data.get("name", "")
        qname = data.get("qualified_name", "")
        doc = data.get("docstring", "")
        text = f"{name} {qname} {doc}".lower()
        symbols.append({"id": node_id, "text": text, "data": data})

    n = len(symbols)
    if n == 0:
        return BaselineResult(
            method="bm25", task=task, budget=budget,
            tokens_used=0, symbols_selected=0, files_included=0,
            assembly_time_ms=0,
        )

    # IDF
    doc_freq: Counter[str] = Counter()
    for sym in symbols:
        terms_in_doc = set(re.findall(r"\b\w+\b", sym["text"]))
        for t in terms_in_doc:
            doc_freq[t] += 1

    k1 = 1.5
    b = 0.75
    avg_dl = sum(len(s["text"]) for s in symbols) / n

    # Score each symbol
    scored: list[tuple[float, dict]] = []
    for sym in symbols:
        dl = len(sym["text"])
        score = 0.0
        for term, qtf in query_tf.items():
            tf = sym["text"].count(term)
            df = doc_freq.get(term, 0)
            idf = math.log((n - df + 0.5) / (df + 0.5) + 1)
            tf_norm = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / avg_dl))
            score += idf * tf_norm * qtf
        if score > 0:
            scored.append((score, sym))

    scored.sort(key=lambda x: x[0], reverse=True)

    # Greedy packing
    selected: list[str] = []
    total_tokens = 0
    files = set()
    for score, sym in scored:
        line_start = sym["data"].get("line_start", 0)
        line_end = sym["data"].get("line_end", 0)
        line_count = max(1, line_end - line_start + 1)
        cost = TokenEstimator.estimate_lines(line_count)
        if total_tokens + cost > budget:
            continue
        selected.append(sym["data"].get("qualified_name", sym["id"]))
        total_tokens += cost
        fp = sym["data"].get("file_path", "")
        if fp:
            files.add(fp)

    elapsed = (time.time() - start) * 1000

    return BaselineResult(
        method="bm25",
        task=task,
        budget=budget,
        tokens_used=total_tokens,
        symbols_selected=len(selected),
        files_included=len(files),
        assembly_time_ms=round(elapsed, 1),
        selected_symbols=selected,
    )

# paper/experiments/baselines.py:350-374
def run_bca(
    repo_path: Path,
    task: str,
    budget: int,
    graph,
    query_engine: GraphQuery,
) -> BaselineResult:
    """Full BCA pipeline."""
    start = time.time()
    assembler = ContextAssembler(repo_path, graph, query_engine)
    package = assembler.assemble(task=task, token_budget=budget, strategy=ContextStrategy.SMART)
    elapsed = (time.time() - start) * 1000

    return BaselineResult(
        method="bca",
        task=task,
        budget=budget,
        tokens_used=package.total_tokens,
        symbols_selected=package.symbols_included,
        files_included=package.files_included,
        assembly_time_ms=round(elapsed, 1),
        selected_symbols=[
            item.qualified_name or item.name for item in package.items
        ],
    )

# src/cegraph/parser/models.py:34-34
    OVERRIDES = "overrides"

# paper/experiments/ablation.py:65-126
def run_ablation(
    repo_path: Path,
    task: str,
    budgets: list[int],
    ground_truth_symbols: list[str] | None = None,
) -> list[AblationResult]:
    """Run all ablation configs for a single task across budgets."""
    builder = GraphBuilder()
    graph = builder.build_from_directory(repo_path)
    query = GraphQuery(graph)

    results: list[AblationResult] = []

    for budget in budgets:
        for config_name, ablation_config in ABLATION_CONFIGS.items():
            assembler = ContextAssembler(repo_path, graph, query, ablation=ablation_config)

            start = time.time()
            package = assembler.assemble(
                task=task,
                token_budget=budget,
                strategy=ContextStrategy.SMART,
            )
            elapsed_ms = (time.time() - start) * 1000

            selected_syms = [item.qualified_name or item.name for item in package.items]

            # Compute recall against ground truth if provided
            recall = None
            if ground_truth_symbols:
                hits = sum(
                    1 for gt in ground_truth_symbols
                    if any(gt in s or s in gt for s in selected_syms)
                )
                recall = hits / len(ground_truth_symbols) if ground_truth_symbols else 0.0

            # Check closure violations (symbols whose deps are missing)
            closure_violations = 0
            if ablation_config.dependency_closure:
                selected_ids = {item.symbol_id for item in package.items}
                for item in package.items:
                    for succ in graph.successors(item.symbol_id):
                        edge_data = graph.edges[item.symbol_id, succ]
                        if edge_data.get("kind") in ("inherits", "implements"):
                            if succ not in selected_ids:
                                closure_violations += 1

            results.append(AblationResult(
                config_name=config_name,
                task=task,
                budget=budget,
                symbols_selected=package.symbols_included,
                symbols_available=package.symbols_available,
                tokens_used=package.total_tokens,
                budget_used_pct=package.budget_used_pct,
                assembly_time_ms=round(elapsed_ms, 1),
                selected_symbols=selected_syms,
                closure_violations=closure_violations,
                recall=recall,
            ))

    return results

# src/cegraph/llm/anthropic_provider.py:97-143
    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        client = self._get_client()
        system, formatted_msgs = self._format_messages(messages)

        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": formatted_msgs,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        if system:
            kwargs["system"] = system
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        response = await client.messages.create(**kwargs)

        content = ""
        tool_calls = []

        for block in response.content:
            if block.type == "text":
                content += block.text
            elif block.type == "tool_use":
                tool_calls.append(
                    ToolCall(
                        id=block.id,
                        name=block.name,
                        arguments=block.input if isinstance(block.input, dict) else {},
                    )
                )

        return LLMResponse(
            content=content,
            tool_calls=tool_calls,
            finish_reason=response.stop_reason or "",
            usage={
                "prompt_tokens": response.usage.input_tokens,
                "completion_tokens": response.usage.output_tokens,
            },
        )

# paper/experiments/benchmark.py:61-65
from paper.experiments.baselines import (
    baseline_bm25,
    baseline_grep,
    baseline_repo_map,
)

# paper/experiments/benchmark.py:61-65
from paper.experiments.baselines import (
    baseline_bm25,
    baseline_grep,
    baseline_repo_map,
)

# src/cegraph/llm/openai_provider.py:20-25
    def __init__(
        self, model: str = "gpt-4o", api_key: str | None = None, base_url: str | None = None
    ) -> None:
        super().__init__(model, api_key, base_url)
        self._client = None
        self._async_client = None

# src/cegraph/mcp/server.py:36-36
    PROTOCOL_VERSION = "2024-11-05"