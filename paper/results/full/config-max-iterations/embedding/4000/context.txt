# src/cegraph/agent/__init__.py:3-3
from cegraph.agent.loop import AgentLoop

# src/cegraph/config.py:41-46
class AgentConfig(BaseModel):
    """Agent behavior configuration."""

    max_iterations: int = 150
    auto_verify: bool = True
    require_approval: bool = True

# src/cegraph/agent/loop.py:45-191
class AgentLoop:
    """ReAct agent loop that iteratively uses tools to complete tasks.

    The agent:
    1. Receives a task from the user
    2. Reasons about what to do next
    3. Calls tools to gather information or make changes
    4. Processes tool results
    5. Repeats until it has enough info to give a final answer
    6. Presents the answer/changes for user approval
    """

    def __init__(
        self,
        llm: LLMProvider,
        tools: ToolRegistry,
        project_name: str = "",
        max_iterations: int = 15,
        on_step: Callable[[AgentStep], None] | None = None,
        on_approval_needed: Callable[[str], bool] | None = None,
    ) -> None:
        self.llm = llm
        self.tools = tools
        self.project_name = project_name
        self.max_iterations = max_iterations
        self.on_step = on_step
        self.on_approval_needed = on_approval_needed

    async def run(self, task: str, context: str = "") -> AgentResult:
        """Run the agent loop for a given task.

        Args:
            task: The user's request/task.
            context: Additional context (e.g., file content, error messages).

        Returns:
            AgentResult with the final answer and step history.
        """
        messages: list[Message] = [
            Message(role="system", content=get_system_prompt(self.project_name)),
        ]

        # Add context if provided
        user_content = task
        if context:
            user_content = f"{task}\n\nContext:\n{context}"
        messages.append(Message(role="user", content=user_content))

        steps: list[AgentStep] = []
        total_tokens = 0

        for iteration in range(self.max_iterations):
            step = AgentStep(iteration=iteration + 1)

            try:
                response = await self.llm.complete(
                    messages=messages,
                    tools=self.tools.get_definitions(),
                    temperature=0.0,
                )
            except Exception as e:
                return AgentResult(
                    answer="",
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=False,
                    error=f"LLM error: {e}",
                )

            step.usage = response.usage
            total_tokens += sum(response.usage.values())

            if response.has_tool_calls:
                # Agent wants to use tools
                step.thought = response.content
                step.tool_calls = response.tool_calls

                # Add assistant message with tool calls
                messages.append(
                    Message(
                        role="assistant",
                        content=response.content,
                        tool_calls=response.tool_calls,
                    )
                )

                # Execute each tool call
                for tc in response.tool_calls:
                    result = await self.tools.execute(tc.name, tc.arguments)
                    tool_result = ToolResult(
                        tool_call_id=tc.id,
                        name=tc.name,
                        content=result,
                    )
                    step.tool_results.append(tool_result)

                    # Add tool result message
                    messages.append(
                        Message(
                            role="tool",
                            content=result,
                            tool_call_id=tc.id,
                            name=tc.name,
                        )
                    )

                # Notify step callback
                if self.on_step:
                    self.on_step(step)

                steps.append(step)

            else:
                # Agent is done - final answer
                step.response = response.content

                if self.on_step:
                    self.on_step(step)

                steps.append(step)

                return AgentResult(
                    answer=response.content,
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=True,
                )

        # Max iterations reached
        return AgentResult(
            answer="I reached the maximum number of iterations. Here's what I've found so far based on my analysis.",
            steps=steps,
            total_iterations=self.max_iterations,
            total_tokens=total_tokens,
            success=False,
            error="Max iterations reached",
        )

    async def ask(self, question: str) -> str:
        """Simple Q&A mode - ask a question about the codebase.

        Returns just the answer string.
        """
        result = await self.run(question)
        return result.answer

# src/cegraph/agent/loop.py:34-42
class AgentResult:
    """Final result from the agent loop."""

    answer: str
    steps: list[AgentStep]
    total_iterations: int
    total_tokens: int
    success: bool = True
    error: str = ""

# src/cegraph/agent/loop.py:22-30
class AgentStep:
    """A single step in the agent loop."""

    iteration: int
    thought: str = ""
    tool_calls: list[ToolCall] = field(default_factory=list)
    tool_results: list[ToolResult] = field(default_factory=list)
    response: str = ""
    usage: dict[str, int] = field(default_factory=dict)

# src/cegraph/cli.py:526-594
def _run_agent(
    root: Path,
    config: ProjectConfig,
    graph,
    store,
    task: str,
    agent_mode: bool = True,
    auto_approve: bool = False,
):
    """Run the agent loop."""
    from cegraph.agent.loop import AgentLoop, AgentStep
    from cegraph.graph.query import GraphQuery
    from cegraph.llm.factory import create_provider
    from cegraph.search.hybrid import HybridSearch
    from cegraph.tools.definitions import get_all_tools

    # Check for API key (skip for local providers that don't need one)
    llm_config = config.llm
    if not llm_config.api_key and llm_config.provider not in ("local",):
        provider = llm_config.provider
        env_var = {"openai": "OPENAI_API_KEY", "anthropic": "ANTHROPIC_API_KEY"}.get(
            provider, f"{provider.upper()}_API_KEY"
        )
        console.error(
            f"No API key found for {provider}. "
            f"Set the {env_var} environment variable or configure it with:\n"
            f"  cegraph config set llm.api_key_env {env_var}"
        )
        store.close()
        sys.exit(1)

    try:
        llm = create_provider(llm_config)
    except Exception as e:
        console.error(str(e))
        store.close()
        sys.exit(1)

    query = GraphQuery(graph, store)
    search_engine = HybridSearch(root, graph)
    tools = get_all_tools(root, graph, query, search_engine)

    def on_step(step: AgentStep):
        console.show_agent_step(step)

    agent_loop = AgentLoop(
        llm=llm,
        tools=tools,
        project_name=config.name,
        max_iterations=config.agent.max_iterations,
        on_step=on_step,
    )

    console.info(f"Running {'agent' if agent_mode else 'Q&A'} for: {task}")
    console.console.print()

    result = asyncio.run(agent_loop.run(task))

    if not result.success:
        if result.error:
            console.error(result.error)

    console.console.print()
    console.info(
        f"Completed in {result.total_iterations} step(s), "
        f"~{result.total_tokens:,} tokens used"
    )

    store.close()

# src/cegraph/cli.py:517-523
def agent(task: str, path: str | None, auto: bool):
    """Run an agentic task (coding, debugging, refactoring)."""
    root = _get_project_root(path)
    config = load_config(root)
    graph, store = _load_graph(root)

    _run_agent(root, config, graph, store, task, agent_mode=True, auto_approve=auto)

# src/cegraph/agent/loop.py:57-71
    def __init__(
        self,
        llm: LLMProvider,
        tools: ToolRegistry,
        project_name: str = "",
        max_iterations: int = 15,
        on_step: Callable[[AgentStep], None] | None = None,
        on_approval_needed: Callable[[str], bool] | None = None,
    ) -> None:
        self.llm = llm
        self.tools = tools
        self.project_name = project_name
        self.max_iterations = max_iterations
        self.on_step = on_step
        self.on_approval_needed = on_approval_needed

# src/cegraph/exceptions.py:24-25
class ToolError(CeGraphError):
    """Agent tool execution errors."""

# src/cegraph/ui/console.py:13-13
from cegraph.agent.loop import AgentStep

# paper/experiments/run_all.py:17-17
from paper.experiments.ablation import AblationResult, run_ablation

# src/cegraph/agent/loop.py:73-183
    async def run(self, task: str, context: str = "") -> AgentResult:
        """Run the agent loop for a given task.

        Args:
            task: The user's request/task.
            context: Additional context (e.g., file content, error messages).

        Returns:
            AgentResult with the final answer and step history.
        """
        messages: list[Message] = [
            Message(role="system", content=get_system_prompt(self.project_name)),
        ]

        # Add context if provided
        user_content = task
        if context:
            user_content = f"{task}\n\nContext:\n{context}"
        messages.append(Message(role="user", content=user_content))

        steps: list[AgentStep] = []
        total_tokens = 0

        for iteration in range(self.max_iterations):
            step = AgentStep(iteration=iteration + 1)

            try:
                response = await self.llm.complete(
                    messages=messages,
                    tools=self.tools.get_definitions(),
                    temperature=0.0,
                )
            except Exception as e:
                return AgentResult(
                    answer="",
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=False,
                    error=f"LLM error: {e}",
                )

            step.usage = response.usage
            total_tokens += sum(response.usage.values())

            if response.has_tool_calls:
                # Agent wants to use tools
                step.thought = response.content
                step.tool_calls = response.tool_calls

                # Add assistant message with tool calls
                messages.append(
                    Message(
                        role="assistant",
                        content=response.content,
                        tool_calls=response.tool_calls,
                    )
                )

                # Execute each tool call
                for tc in response.tool_calls:
                    result = await self.tools.execute(tc.name, tc.arguments)
                    tool_result = ToolResult(
                        tool_call_id=tc.id,
                        name=tc.name,
                        content=result,
                    )
                    step.tool_results.append(tool_result)

                    # Add tool result message
                    messages.append(
                        Message(
                            role="tool",
                            content=result,
                            tool_call_id=tc.id,
                            name=tc.name,
                        )
                    )

                # Notify step callback
                if self.on_step:
                    self.on_step(step)

                steps.append(step)

            else:
                # Agent is done - final answer
                step.response = response.content

                if self.on_step:
                    self.on_step(step)

                steps.append(step)

                return AgentResult(
                    answer=response.content,
                    steps=steps,
                    total_iterations=iteration + 1,
                    total_tokens=total_tokens,
                    success=True,
                )

        # Max iterations reached
        return AgentResult(
            answer="I reached the maximum number of iterations. Here's what I've found so far based on my analysis.",
            steps=steps,
            total_iterations=self.max_iterations,
            total_tokens=total_tokens,
            success=False,
            error="Max iterations reached",
        )

# paper/experiments/run_all.py:16-16
from paper.experiments.baselines import BaselineResult, run_comparison

# src/cegraph/context/models.py:163-163
    CHARS_PER_TOKEN = 4.0

# tests/test_context.py:302-305
    def test_is_not_accelerated(self, cag_engine: ContextAssembler):
        """Test that C++ acceleration is reported correctly."""
        # In tests, C++ extension is typically not compiled
        assert isinstance(cag_engine.is_accelerated, bool)

# src/cegraph/parser/models.py:34-34
    OVERRIDES = "overrides"

# paper/experiments/benchmark.py:61-65
from paper.experiments.baselines import (
    baseline_bm25,
    baseline_grep,
    baseline_repo_map,
)

# paper/experiments/benchmark.py:61-65
from paper.experiments.baselines import (
    baseline_bm25,
    baseline_grep,
    baseline_repo_map,
)

# src/cegraph/mcp/server.py:36-36
    PROTOCOL_VERSION = "2024-11-05"

# tests/test_context.py:42-45
    def test_estimate_proportional(self):
        short = TokenEstimator.estimate("x = 1")
        long = TokenEstimator.estimate("x = 1\n" * 100)
        assert long > short