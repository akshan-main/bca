# Codebase Context for: detect_language returns "python3" instead of "python" for .py files, causing the parser to fail to select the correct Python parser.
# 60 symbols from 16 files (~7,333 tokens, 92% of budget)

## paper/experiments/ablation.py
# Included because: graph expansion (depth 3)

# [class] AblationResult (relevance: 0.35, depth: 3)
  27 | class AblationResult:
  28 |     """Result of a single ablation run."""
  29 | 
  30 |     config_name: str
  31 |     task: str
  32 |     budget: int
  33 |     symbols_selected: int
  34 |     symbols_available: int
  35 |     tokens_used: int
  36 |     budget_used_pct: float
  37 |     assembly_time_ms: float
  38 |     selected_symbols: list[str] = field(default_factory=list)
  39 |     closure_violations: int = 0
  40 |     recall: float | None = None  # Set if ground-truth symbols provided

## paper/experiments/baselines.py
# Included because: graph expansion (depth 3)

# [class] BaselineResult (relevance: 0.43, depth: 3)
  34 | class BaselineResult:
  35 |     """Result of a single baseline run."""
  36 | 
  37 |     method: str
  38 |     task: str
  39 |     budget: int
  40 |     tokens_used: int
  41 |     symbols_selected: int
  42 |     files_included: int
  43 |     assembly_time_ms: float
  44 |     selected_symbols: list[str] = field(default_factory=list)
  45 |     recall: float | None = None

## paper/experiments/benchmark.py
# Included because: graph expansion (depth 2); graph expansion (depth 3)

# [function] _vector_score_dense (relevance: 0.37, depth: 3)
 424 | def _vector_score_dense(query: str, symbols: list[dict]) -> list[float]:
 425 |     """Dense embedding scoring using sentence-transformers."""
 426 |     try:
 427 |         from sentence_transformers import SentenceTransformer
 428 |         import numpy as np
 429 |     except ImportError:
 430 |         print("sentence-transformers not installed, falling back to TF-IDF")
 431 |         return _vector_score_tfidf(query, symbols)
 432 | 
 433 |     model = SentenceTransformer("all-MiniLM-L6-v2")
 434 |     texts = [s["text"] for s in symbols]
 435 |     query_emb = model.encode([query], normalize_embeddings=True)
 436 |     doc_embs = model.encode(texts, normalize_embeddings=True, batch_size=64)
 437 |     scores = (doc_embs @ query_emb.T).flatten()
 438 |     return scores.tolist()

# [function] build_prompt (relevance: 0.35, depth: 3)
 591 | def build_prompt(context: str, task: str) -> str:
 592 |     """Build the user prompt from assembled context and task."""
 593 |     return f"""## Source Code Context
 594 | 
 595 | {context}
 596 | 
 597 | ## Bug Report
 598 | 
 599 | {task}
 600 | 
 601 | ## Fix
 602 | 
 603 | Produce SEARCH/REPLACE blocks to fix this bug."""

# [function] call_llm (relevance: 0.50, depth: 2)
 606 | async def call_llm(
 607 |     provider, context: str, task: str,
 608 | ) -> tuple[str, float, int, int]:
 609 |     """Call the LLM and return (response_text, time_ms, input_tokens, output_tokens)."""
 610 |     messages = [
 611 |         Message(role="system", content=SYSTEM_PROMPT),
 612 |         Message(role="user", content=build_prompt(context, task)),
 613 |     ]
 614 | 
 615 |     start = time.time()
 616 |     response: LLMResponse = await provider.complete(
 617 |         messages=messages,
 618 |         temperature=0.0,
 619 |         max_tokens=4096,
 620 |     )
 621 |     elapsed = (time.time() - start) * 1000
 622 | 
 623 |     input_tokens = response.usage.get("prompt_tokens", 0) or response.usage.get("input_tokens", 0)
 624 |     output_tokens = response.usage.get("completion_tokens", 0) or response.usage.get("output_tokens", 0)
 625 | 
 626 |     return (response.content, round(elapsed, 1), input_tokens, output_tokens)

## src/cegraph/cli.py
# Included because: graph expansion (depth 2); graph expansion (depth 3)

# [function] _get_project_root (relevance: 0.61, depth: 3)
  28 | def _get_project_root(path: str | None = None) -> Path:
  29 |     """Find the project root or error."""
  30 |     if path:
  31 |         root = Path(path).resolve()
  32 |         if not root.exists():
  33 |             console.error(f"Path does not exist: {path}")
  34 |             sys.exit(1)
  35 |         return root
  36 | 
  37 |     root = find_project_root()
  38 |     if root is None:
  39 |         console.error(
  40 |             "No CeGraph project found. Run 'cegraph init' first, "
  41 |             "or specify a path with --path."
  42 |         )
  43 |         sys.exit(1)
  44 |     return root

# [function] _load_graph (relevance: 0.59, depth: 3)
  47 | def _load_graph(root: Path):
  48 |     """Load the knowledge graph and related objects."""
  49 |     from cegraph.graph.store import GraphStore
  50 | 
  51 |     db_path = get_cegraph_dir(root) / GRAPH_DB_FILE
  52 |     store = GraphStore(db_path)
  53 |     graph = store.load()
  54 |     if graph is None:
  55 |         console.error("No index found. Run 'cegraph init' or 'cegraph reindex' first.")
  56 |         sys.exit(1)
  57 |     return graph, store

# [function] reindex (relevance: 0.30, depth: 3)
 100 | def reindex(path: str | None):
 101 |     """Rebuild the knowledge graph from scratch."""
 102 |     root = _get_project_root(path)
 103 |     config = load_config(root)
 104 |     _do_index(root, config)

# [function] _do_index (relevance: 0.66, depth: 2)
 107 | def _do_index(root: Path, config: ProjectConfig):
 108 |     """Index the codebase and build the knowledge graph."""
 109 |     from cegraph.graph.builder import GraphBuilder
 110 |     from cegraph.graph.store import GraphStore
 111 | 
 112 |     builder = GraphBuilder()
 113 | 
 114 |     console.info("Scanning and parsing source files...")
 115 |     start_time = time.time()
 116 | 
 117 |     with console.indexing_progress() as progress:
 118 |         task = progress.add_task("Indexing...", total=None)
 119 |         file_count = 0
 120 | 
 121 |         def on_progress(file_path: str, current: int, total: int):
 122 |             nonlocal file_count
 123 |             file_count = total
 124 |             progress.update(task, total=total, completed=current, description=f"Parsing {file_path}")
 125 | 
 126 |         graph = builder.build_from_directory(root, config, on_progress)
 127 | 
 128 |     elapsed = time.time() - start_time
 129 |     stats = builder.get_stats()
 130 | 
 131 |     console.success(f"Indexed {stats.get('files', 0)} files in {elapsed:.1f}s")
 132 |     console.show_stats(stats)
 133 | 
 134 |     # Persist the graph
 135 |     db_path = get_cegraph_dir(root) / GRAPH_DB_FILE
 136 |     store = GraphStore(db_path)
 137 |     store.save(graph, metadata={"stats": stats, "root": str(root)})
 138 |     store.close()
 139 | 
 140 |     console.success(f"Knowledge graph saved to .cegraph/")

# [function] status (relevance: 0.40, depth: 3)
 145 | def status(path: str | None):
 146 |     """Show the current index status and graph statistics."""
 147 |     root = _get_project_root(path)
 148 |     graph, store = _load_graph(root)
 149 | 
 150 |     stats = store.get_metadata("stats")
 151 |     if stats:
 152 |         console.banner()
 153 |         console.info(f"Project: {root.name}")
 154 |         console.show_stats(stats)
 155 |     else:
 156 |         from cegraph.graph.builder import GraphBuilder
 157 | 
 158 |         b = GraphBuilder()
 159 |         b.graph = graph
 160 |         console.show_stats(b.get_stats())
 161 |     store.close()

# [function] who_calls (relevance: 0.56, depth: 3)
 202 | def who_calls(symbol_name: str, path: str | None, depth: int):
 203 |     """Find all callers of a function or method."""
 204 |     root = _get_project_root(path)
 205 |     graph, store = _load_graph(root)
 206 | 
 207 |     from cegraph.graph.query import GraphQuery
 208 | 
 209 |     query = GraphQuery(graph, store)
 210 |     callers = query.who_calls(symbol_name, max_depth=depth)
 211 | 
 212 |     if callers:
 213 |         console.info(f"Callers of '{symbol_name}':")
 214 |         console.show_callers(callers, symbol_name)
 215 |     else:
 216 |         console.warning(f"No callers found for '{symbol_name}'")
 217 | 
 218 |     store.close()

# [function] impact (relevance: 0.34, depth: 3)
 224 | def impact(symbol_name: str, path: str | None):
 225 |     """Analyze the blast radius of changing a symbol."""
 226 |     root = _get_project_root(path)
 227 |     graph, store = _load_graph(root)
 228 | 
 229 |     from cegraph.graph.query import GraphQuery
 230 | 
 231 |     query = GraphQuery(graph, store)
 232 |     result = query.impact_of(symbol_name)
 233 |     console.show_impact(result)
 234 | 
 235 |     store.close()

# [function] ask (relevance: 0.32, depth: 3)
 504 | def ask(question: str, path: str | None):
 505 |     """Ask a question about the codebase (uses LLM + knowledge graph)."""
 506 |     root = _get_project_root(path)
 507 |     config = load_config(root)
 508 |     graph, store = _load_graph(root)
 509 | 
 510 |     _run_agent(root, config, graph, store, question, agent_mode=False)

# [function] agent (relevance: 0.32, depth: 3)
 517 | def agent(task: str, path: str | None, auto: bool):
 518 |     """Run an agentic task (coding, debugging, refactoring)."""
 519 |     root = _get_project_root(path)
 520 |     config = load_config(root)
 521 |     graph, store = _load_graph(root)
 522 | 
 523 |     _run_agent(root, config, graph, store, task, agent_mode=True, auto_approve=auto)

## src/cegraph/config.py
# Included because: required dependency of Relationship

# [import] BaseModel (relevance: 0.11, depth: 3)
  10 | from pydantic import BaseModel, Field

## src/cegraph/context/models.py
# Included because: required dependency of estimate; graph expansion (depth 3)

# [class] TokenEstimator (relevance: 0.25, depth: 4)
 159 | class TokenEstimator:
 160 |     """Estimate token counts for code."""
 161 | 
 162 |     # Rough heuristic: 1 token â‰ˆ 4 characters for code
 163 |     CHARS_PER_TOKEN = 4.0
 164 | 
 165 |     @classmethod
 166 |     def estimate(cls, text: str) -> int:
 167 |         """Estimate token count for a string."""
 168 |         return max(1, int(len(text) / cls.CHARS_PER_TOKEN))
 169 | 
 170 |     @classmethod
 171 |     def estimate_lines(cls, line_count: int, avg_line_length: int = 40) -> int:
 172 |         """Estimate tokens for a given number of lines."""
 173 |         return max(1, int(line_count * avg_line_length / cls.CHARS_PER_TOKEN))

# [method] TokenEstimator.estimate (relevance: 0.50, depth: 3)
 166 |     def estimate(cls, text: str) -> int:
 167 |         """Estimate token count for a string."""
 168 |         return max(1, int(len(text) / cls.CHARS_PER_TOKEN))

# [method] TokenEstimator.estimate_lines (relevance: 0.42, depth: 3)
 171 |     def estimate_lines(cls, line_count: int, avg_line_length: int = 40) -> int:
 172 |         """Estimate tokens for a given number of lines."""
 173 |         return max(1, int(line_count * avg_line_length / cls.CHARS_PER_TOKEN))

## src/cegraph/github/diff_parser.py
# Included because: graph expansion (depth 2); graph expansion (depth 3)

# [class] ChangedSymbol (relevance: 0.37, depth: 3)
  47 | class ChangedSymbol:
  48 |     """A symbol that was affected by the diff."""
  49 |     name: str
  50 |     qualified_name: str
  51 |     kind: str
  52 |     file_path: str
  53 |     line_start: int
  54 |     line_end: int
  55 |     change_type: str  # 'modified', 'added', 'deleted'
  56 |     lines_changed: int = 0

# [function] get_pr_diff (relevance: 0.44, depth: 2)
 205 | def get_pr_diff(root: Path) -> str:
 206 |     """Get the diff for the current PR (GitHub Actions context)."""
 207 |     import os
 208 |     base_ref = os.environ.get("GITHUB_BASE_REF", "main")
 209 |     return get_git_diff(root, base=f"origin/{base_ref}")

## src/cegraph/github/renderer.py
# Included because: graph expansion (depth 3)

# [function] _risk_badge (relevance: 0.43, depth: 3)
 137 | def _risk_badge(risk: float) -> tuple[str, str, str]:
 138 |     """Return (emoji, label, color) for a risk score."""
 139 |     if risk < 0.1:
 140 |         return ("ðŸŸ¢", "LOW", "green")
 141 |     elif risk < 0.2:
 142 |         return ("ðŸŸ¡", "LOW", "yellow")
 143 |     elif risk < 0.4:
 144 |         return ("ðŸŸ ", "MEDIUM", "orange")
 145 |     elif risk < 0.6:
 146 |         return ("ðŸ”´", "HIGH", "red")
 147 |     else:
 148 |         return ("â›”", "CRITICAL", "red")

# [function] _render_file_tree (relevance: 0.39, depth: 3)
 151 | def _render_file_tree(files: list[str]) -> list[str]:
 152 |     """Render a list of file paths as an ASCII tree."""
 153 |     if not files:
 154 |         return []
 155 | 
 156 |     # Build tree structure
 157 |     tree: dict = {}
 158 |     for fp in sorted(files):
 159 |         parts = fp.split("/")
 160 |         node = tree
 161 |         for part in parts:
 162 |             node = node.setdefault(part, {})
 163 | 
 164 |     lines: list[str] = []
 165 |     _render_tree_recursive(tree, "", lines, is_last=True, is_root=True)
 166 |     return lines

# [function] _footer (relevance: 0.35, depth: 3)
 192 | def _footer() -> str:
 193 |     return (
 194 |         "---\n"
 195 |         "*Powered by [CeGraph](https://github.com/cegraph-ai/cegraph) "
 196 |         "â€” CAG-driven code intelligence*"
 197 |     )

## src/cegraph/graph/query.py
# Included because: graph expansion (depth 3)

# [class] SymbolInfo (relevance: 0.32, depth: 3)
  13 | class SymbolInfo:
  14 |     """Rich information about a symbol."""
  15 | 
  16 |     id: str
  17 |     name: str
  18 |     qualified_name: str
  19 |     kind: str
  20 |     file_path: str
  21 |     line_start: int
  22 |     line_end: int
  23 |     signature: str
  24 |     docstring: str = ""
  25 |     callers: list[str] = field(default_factory=list)
  26 |     callees: list[str] = field(default_factory=list)
  27 |     children: list[str] = field(default_factory=list)
  28 |     parent: str = ""

## src/cegraph/parser/models.py
# Included because: matches 'detect_language'; graph expansion (depth 2)

# [class] Relationship (relevance: 0.22, depth: 2)
  63 | class Relationship(BaseModel):
  64 |     """A relationship between two symbols."""
  65 | 
  66 |     source: str  # symbol ID
  67 |     target: str  # symbol ID or unresolved name
  68 |     kind: RelKind
  69 |     file_path: str
  70 |     line: int
  71 |     resolved: bool = False  # whether target is a resolved symbol ID

# [function] detect_language (relevance: 1.10, depth: 0)
 100 | def detect_language(file_path: str) -> str | None:
 101 |     """Detect programming language from file extension."""
 102 |     from pathlib import Path
 103 | 
 104 |     ext = Path(file_path).suffix.lower()
 105 |     return EXTENSION_LANGUAGE_MAP.get(ext)

## src/cegraph/parser/python_parser.py
# Included because: graph expansion (depth 1); graph expansion (depth 2); graph expansion (depth 3); matches 'python'

# [function] parse_python_file (relevance: 1.12, depth: 0)
  17 | def parse_python_file(file_path: str, source: str | None = None) -> FileSymbols:
  18 |     """Parse a Python file and extract symbols and relationships."""
  19 |     if source is None:
  20 |         source = Path(file_path).read_text(encoding="utf-8", errors="replace")
  21 | 
  22 |     result = FileSymbols(file_path=file_path, language="python")
  23 | 
  24 |     try:
  25 |         tree = ast.parse(source, filename=file_path)
  26 |     except SyntaxError as e:
  27 |         result.errors.append(f"SyntaxError: {e}")
  28 |         return result
  29 | 
  30 |     lines = source.splitlines()
  31 |     _extract_from_module(tree, file_path, lines, result)
  32 |     return result

# [function] _get_docstring (relevance: 0.56, depth: 3)
  35 | def _get_docstring(node: ast.AST) -> str:
  36 |     """Extract docstring from a node if present."""
  37 |     if (
  38 |         isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module))
  39 |         and node.body
  40 |         and isinstance(node.body[0], ast.Expr)
  41 |         and isinstance(node.body[0].value, (ast.Constant,))
  42 |         and isinstance(node.body[0].value.value, str)
  43 |     ):
  44 |         return node.body[0].value.value.strip()
  45 |     return ""

# [function] _get_decorators (relevance: 0.56, depth: 3)
  48 | def _get_decorators(node: ast.FunctionDef | ast.AsyncFunctionDef | ast.ClassDef) -> list[str]:
  49 |     """Extract decorator names."""
  50 |     decorators = []
  51 |     for dec in node.decorator_list:
  52 |         if isinstance(dec, ast.Name):
  53 |             decorators.append(dec.id)
  54 |         elif isinstance(dec, ast.Attribute):
  55 |             decorators.append(ast.dump(dec))
  56 |         elif isinstance(dec, ast.Call):
  57 |             if isinstance(dec.func, ast.Name):
  58 |                 decorators.append(dec.func.id)
  59 |             elif isinstance(dec.func, ast.Attribute):
  60 |                 decorators.append(ast.dump(dec.func))
  61 |     return decorators

# [function] _get_function_signature (relevance: 0.54, depth: 3)
  64 | def _get_function_signature(
  65 |     node: ast.FunctionDef | ast.AsyncFunctionDef, lines: list[str]
  66 | ) -> str:
  67 |     """Extract the function signature from source lines."""
  68 |     start = node.lineno - 1
  69 |     sig_lines = []
  70 |     for i in range(start, min(start + 10, len(lines))):
  71 |         line = lines[i]
  72 |         sig_lines.append(line.strip())
  73 |         if ":" in line:
  74 |             # Check if we've reached the colon that ends the signature
  75 |             text = "".join(sig_lines)
  76 |             if text.count("(") <= text.count(")"):
  77 |                 break
  78 |     sig = " ".join(sig_lines)
  79 |     # Trim to just the def ... : part
  80 |     if ":" in sig:
  81 |         sig = sig[: sig.rindex(":") + 1]
  82 |     return sig

# [function] _extract_from_module (relevance: 0.84, depth: 1)
  85 | def _extract_from_module(
  86 |     tree: ast.Module,
  87 |     file_path: str,
  88 |     lines: list[str],
  89 |     result: FileSymbols,
  90 |     parent_name: str = "",
  91 |     parent_id: str = "",
  92 | ) -> None:
  93 |     """Recursively extract symbols from an AST module/class body."""
  94 | 
  95 |     for node in ast.iter_child_nodes(tree):
  96 |         if isinstance(node, (ast.Import, ast.ImportFrom)):
  97 |             _extract_import(node, file_path, result)
  98 | 
  99 |         elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
 100 |             _extract_function(node, file_path, lines, result, parent_name, parent_id)
 101 | 
 102 |         elif isinstance(node, ast.ClassDef):
 103 |             _extract_class(node, file_path, lines, result, parent_name, parent_id)
 104 | 
 105 |         elif isinstance(node, ast.Assign):
 106 |             _extract_assignment(node, file_path, result, parent_name, parent_id)

# [function] _extract_assignment (relevance: 0.67, depth: 2)
 259 | def _extract_assignment(
 260 |     node: ast.Assign,
 261 |     file_path: str,
 262 |     result: FileSymbols,
 263 |     parent_name: str,
 264 |     parent_id: str,
 265 | ) -> None:
 266 |     """Extract variable/constant assignments at module or class level."""
 267 |     for target in node.targets:
 268 |         name = _node_to_name(target)
 269 |         if not name:
 270 |             continue
 271 |         qualified = f"{parent_name}.{name}" if parent_name else name
 272 |         kind = SymbolKind.CONSTANT if name.isupper() else SymbolKind.VARIABLE
 273 | 
 274 |         symbol = Symbol(
 275 |             name=name,
 276 |             qualified_name=qualified,
 277 |             kind=kind,
 278 |             file_path=file_path,
 279 |             line_start=node.lineno,
 280 |             line_end=node.end_lineno or node.lineno,
 281 |             parent=parent_id,
 282 |         )
 283 |         result.symbols.append(symbol)

# [function] _extract_calls (relevance: 0.58, depth: 3)
 286 | def _extract_calls(
 287 |     node: ast.AST, file_path: str, caller_id: str, result: FileSymbols
 288 | ) -> None:
 289 |     """Walk a function body and extract all function calls."""
 290 |     for child in ast.walk(node):
 291 |         if isinstance(child, ast.Call):
 292 |             callee = _node_to_name(child.func)
 293 |             if callee:
 294 |                 result.relationships.append(
 295 |                     Relationship(
 296 |                         source=caller_id,
 297 |                         target=callee,
 298 |                         kind=RelKind.CALLS,
 299 |                         file_path=file_path,
 300 |                         line=child.lineno if hasattr(child, "lineno") else 0,
 301 |                     )
 302 |                 )

# [function] _node_to_name (relevance: 0.62, depth: 3)
 305 | def _node_to_name(node: ast.AST) -> str:
 306 |     """Convert an AST node to a dotted name string."""
 307 |     if isinstance(node, ast.Name):
 308 |         return node.id
 309 |     elif isinstance(node, ast.Attribute):
 310 |         parent = _node_to_name(node.value)
 311 |         if parent:
 312 |             return f"{parent}.{node.attr}"
 313 |         return node.attr
 314 |     elif isinstance(node, ast.Subscript):
 315 |         return _node_to_name(node.value)
 316 |     return ""

## src/cegraph/parser/tree_sitter_parser.py
# Included because: graph expansion (depth 2); graph expansion (depth 3)

# [function] is_available (relevance: 0.40, depth: 2)
  73 | def is_available(language: str | None = None) -> bool:
  74 |     """Check if tree-sitter and the required language grammar are available."""
  75 |     try:
  76 |         import tree_sitter  # noqa: F401
  77 |     except ImportError:
  78 |         return False
  79 | 
  80 |     if language is None:
  81 |         return True
  82 | 
  83 |     module_name = _TS_LANGUAGE_MODULES.get(language)
  84 |     if not module_name:
  85 |         return False
  86 | 
  87 |     try:
  88 |         __import__(module_name)
  89 |         return True
  90 |     except ImportError:
  91 |         return False

# [function] _get_language (relevance: 0.42, depth: 2)
  94 | def _get_language(lang: str):
  95 |     """Get a tree-sitter Language object for the given language."""
  96 |     from tree_sitter import Language
  97 | 
  98 |     module_name = _TS_LANGUAGE_MODULES.get(lang)
  99 |     if not module_name:
 100 |         raise ValueError(f"No tree-sitter grammar for language: {lang}")
 101 | 
 102 |     module = __import__(module_name)
 103 |     return Language(module.language())

# [function] parse_tree_sitter_file (relevance: 0.44, depth: 2)
 106 | def parse_tree_sitter_file(
 107 |     file_path: str, language: str, source: str | None = None
 108 | ) -> FileSymbols:
 109 |     """Parse a file using tree-sitter for accurate AST extraction."""
 110 |     from tree_sitter import Parser
 111 | 
 112 |     if source is None:
 113 |         source = Path(file_path).read_text(encoding="utf-8", errors="replace")
 114 | 
 115 |     result = FileSymbols(file_path=file_path, language=language)
 116 |     source_bytes = source.encode("utf-8")
 117 | 
 118 |     try:
 119 |         ts_language = _get_language(language)
 120 |         parser = Parser(ts_language)
 121 |         tree = parser.parse(source_bytes)
 122 |     except Exception as e:
 123 |         result.errors.append(f"tree-sitter parse error: {e}")
 124 |         return result
 125 | 
 126 |     _walk_tree(tree.root_node, file_path, language, source_bytes, result)
 127 |     return result

# [function] _extract_name (relevance: 0.35, depth: 3)
 207 | def _extract_name(node, language: str) -> str:
 208 |     """Extract the name of a symbol from its tree-sitter node."""
 209 |     # Look for a name/identifier child
 210 |     for child in node.children:
 211 |         if child.type in ("identifier", "name", "type_identifier", "property_identifier"):
 212 |             return child.text.decode("utf-8")
 213 |     # For some languages, try named children
 214 |     name_child = node.child_by_field_name("name")
 215 |     if name_child:
 216 |         return name_child.text.decode("utf-8")
 217 |     return ""

# [function] _extract_ts_import (relevance: 0.35, depth: 3)
 250 | def _extract_ts_import(node, file_path: str, source: bytes, result: FileSymbols) -> None:
 251 |     """Extract import information from a tree-sitter node."""
 252 |     text = node.text.decode("utf-8")
 253 |     result.imports.append(text)

## src/cegraph/search/hybrid.py
# Included because: graph expansion (depth 3)

# [function] _tokenize (relevance: 0.34, depth: 3)
 170 | def _tokenize(text: str) -> list[str]:
 171 |     """Simple tokenizer that splits on non-alphanumeric and camelCase."""
 172 |     import re
 173 | 
 174 |     # Split camelCase and snake_case
 175 |     text = re.sub(r"([a-z])([A-Z])", r"\1 \2", text)
 176 |     text = text.replace("_", " ").replace(".", " ")
 177 |     tokens = re.findall(r"[a-zA-Z]{2,}", text.lower())
 178 |     return tokens

## tests/conftest.py
# Included because: matches 'python'

# [function] sample_python_source (relevance: 0.84, depth: 0)
 143 | def sample_python_source() -> str:
 144 |     """Sample Python source code for parser testing."""
 145 |     return '''"""Sample module."""
 146 | 
 147 | import os
 148 | from typing import List, Optional
 149 | from pathlib import Path
 150 | 
 151 | 
 152 | CONSTANT_VALUE = 42
 153 | 
 154 | 
 155 | class BaseProcessor:
 156 |     """Base class for processors."""
 157 | 
 158 |     def __init__(self, name: str):
 159 |         self.name = name
 160 | 
 161 |     def process(self, data: List[str]) -> List[str]:
 162 |         """Process the data."""
 163 |         return [self._transform(item) for item in data]
 164 | 
 165 |     def _transform(self, item: str) -> str:
 166 |         """Transform a single item."""
 167 |         return item.strip()
 168 | 
 169 | 
 170 | class AdvancedProcessor(BaseProcessor):
 171 |     """Advanced processor with extra features."""
 172 | 
 173 |     def __init__(self, name: str, verbose: bool = False):
 174 |         super().__init__(name)
 175 |         self.verbose = verbose
 176 | 
 177 |     def process(self, data: List[str]) -> List[str]:
 178 |         """Process with logging."""
 179 |         if self.verbose:
 180 |             print(f"Processing {len(data)} items")
 181 |         return super().process(data)
 182 | 
 183 |     def batch_process(self, batches: List[List[str]]) -> List[List[str]]:
 184 |         """Process multiple batches."""
 185 |         return [self.process(batch) for batch in batches]
 186 | 
 187 | 
 188 | def create_processor(name: str, advanced: bool = False) -> BaseProcessor:
 189 |     """Factory function for creating processors."""
 190 |     if advanced:
 191 |         return AdvancedProcessor(name, verbose=True)
 192 |     return BaseProcessor(name)
 193 | 
 194 | 
 195 | def run_pipeline(items: List[str], processor_name: str = "default") -> List[str]:
 196 |     """Run the processing pipeline."""
 197 |     processor = create_processor(processor_name)
 198 |     result = processor.process(items)
 199 |     return result
 200 | '''

## tests/test_mcp.py
# Included because: graph expansion (depth 3)

# [function] mcp_server (relevance: 0.35, depth: 3)
  17 | def mcp_server(tmp_project: Path, tmp_path: Path):
  18 |     """Create an MCP server with a built graph."""
  19 |     builder = GraphBuilder()
  20 |     graph = builder.build_from_directory(tmp_project)
  21 | 
  22 |     db_path = tmp_path / "test.db"
  23 |     store = GraphStore(db_path)
  24 |     store.save(graph, metadata={"stats": builder.get_stats()})
  25 |     store.close()
  26 | 
  27 |     # Create .cegraph dir in the project
  28 |     cs_dir = tmp_project / ".cegraph"
  29 |     cs_dir.mkdir(exist_ok=True)
  30 |     # Copy DB to the expected location
  31 |     import shutil
  32 |     shutil.copy(db_path, cs_dir / "graph.db")
  33 | 
  34 |     server = MCPServer(root=tmp_project)
  35 |     return server

## tests/test_parser.py
# Included because: graph expansion (depth 1); graph expansion (depth 2); matches 'python'

# [class] TestLanguageDetection (relevance: 0.50, depth: 1)
  12 | class TestLanguageDetection:
  13 |     def test_python(self):
  14 |         assert detect_language("main.py") == "python"
  15 |         assert detect_language("types.pyi") == "python"
  16 | 
  17 |     def test_javascript(self):
  18 |         assert detect_language("app.js") == "javascript"
  19 |         assert detect_language("component.jsx") == "javascript"
  20 | 
  21 |     def test_typescript(self):
  22 |         assert detect_language("app.ts") == "typescript"
  23 |         assert detect_language("component.tsx") == "typescript"
  24 | 
  25 |     def test_go(self):
  26 |         assert detect_language("main.go") == "go"
  27 | 
  28 |     def test_rust(self):
  29 |         assert detect_language("main.rs") == "rust"
  30 | 
  31 |     def test_unknown(self):
  32 |         assert detect_language("readme.md") is None
  33 |         assert detect_language("data.json") is None
  34 |         # Languages without tree-sitter grammars are not supported
  35 |         assert detect_language("main.rb") is None
  36 |         assert detect_language("main.php") is None

# [method] TestLanguageDetection.test_python (relevance: 0.80, depth: 0)
  13 |     def test_python(self):
  14 |         assert detect_language("main.py") == "python"
  15 |         assert detect_language("types.pyi") == "python"

# [class] TestPythonParser (relevance: 1.06, depth: 0)
  39 | class TestPythonParser:
  40 |     def test_parse_functions(self, sample_python_source: str):
  41 |         result = parse_python_file("sample.py", sample_python_source)
  42 |         assert result.language == "python"
  43 |         assert len(result.errors) == 0
  44 | 
  45 |         # Check functions are found
  46 |         func_names = [
  47 |             s.name for s in result.symbols if s.kind == SymbolKind.FUNCTION
  48 |         ]
  49 |         assert "create_processor" in func_names
  50 |         assert "run_pipeline" in func_names
  51 | 
  52 |     def test_parse_classes(self, sample_python_source: str):
  53 |         result = parse_python_file("sample.py", sample_python_source)
  54 | 
  55 |         class_names = [
  56 |             s.name for s in result.symbols if s.kind == SymbolKind.CLASS
  57 |         ]
  58 |         assert "BaseProcessor" in class_names
  59 |         assert "AdvancedProcessor" in class_names
  60 | 
  61 |     def test_parse_methods(self, sample_python_source: str):
  62 |         result = parse_python_file("sample.py", sample_python_source)
  63 | 
  64 |         methods = [s for s in result.symbols if s.kind == SymbolKind.METHOD]
  65 |         method_names = [m.name for m in methods]
  66 |         assert "__init__" in method_names
  67 |         assert "process" in method_names
  68 |         assert "_transform" in method_names
  69 |         assert "batch_process" in method_names
  70 | 
  71 |     def test_parse_imports(self, sample_python_source: str):
  72 |         result = parse_python_file("sample.py", sample_python_source)
  73 |         assert "os" in result.imports
  74 |         assert "typing.List" in result.imports
  75 |         assert "pathlib.Path" in result.imports
  76 | 
  77 |     def test_parse_constants(self, sample_python_source: str):
  78 |         result = parse_python_file("sample.py", sample_python_source)
  79 |         constants = [
  80 |             s for s in result.symbols if s.kind == SymbolKind.CONSTANT
  81 |         ]
  82 |         assert any(c.name == "CONSTANT_VALUE" for c in constants)
  83 | 
  84 |     def test_parse_inheritance(self, sample_python_source: str):
  85 |         result = parse_python_file("sample.py", sample_python_source)
  86 |         inherits = [
  87 |             r for r in result.relationships if r.kind == RelKind.INHERITS
  88 |         ]
  89 |         assert any("AdvancedProcessor" in r.source and "BaseProcessor" in r.target for r in inherits)
  90 | 
  91 |     def test_parse_calls(self, sample_python_source: str):
  92 |         result = parse_python_file("sample.py", sample_python_source)
  93 |         calls = [r for r in result.relationships if r.kind == RelKind.CALLS]
  94 |         # run_pipeline calls create_processor
  95 |         assert any(
  96 |             "run_pipeline" in r.source and "create_processor" in r.target
  97 |             for r in calls
  98 |         )
  99 | 
 100 |     def test_parse_docstrings(self, sample_python_source: str):
 101 |         result = parse_python_file("sample.py", sample_python_source)
 102 |         funcs = {s.name: s for s in result.symbols if s.kind in (SymbolKind.FUNCTION, SymbolKind.METHOD)}
 103 |         assert "Factory function" in funcs["create_processor"].docstring
 104 |         assert "Transform a single item" in funcs["_transform"].docstring
 105 | 
 106 |     def test_parse_contains_relationships(self, sample_python_source: str):
 107 |         result = parse_python_file("sample.py", sample_python_source)
 108 |         contains = [r for r in result.relationships if r.kind == RelKind.CONTAINS]
 109 |         # BaseProcessor should contain process, _transform, __init__
 110 |         assert any("BaseProcessor" in r.source and "process" in r.target for r in contains)
 111 | 
 112 |     def test_syntax_error_handling(self):
 113 |         result = parse_python_file("bad.py", "def broken(:\n  pass")
 114 |         assert len(result.errors) > 0
 115 | 
 116 |     def test_empty_file(self):
 117 |         result = parse_python_file("empty.py", "")
 118 |         assert result.language == "python"
 119 |         assert len(result.symbols) == 0

# [method] TestPythonParser.test_parse_functions (relevance: 0.58, depth: 1)
  40 |     def test_parse_functions(self, sample_python_source: str):
  41 |         result = parse_python_file("sample.py", sample_python_source)
  42 |         assert result.language == "python"
  43 |         assert len(result.errors) == 0
  44 | 
  45 |         # Check functions are found
  46 |         func_names = [
  47 |             s.name for s in result.symbols if s.kind == SymbolKind.FUNCTION
  48 |         ]
  49 |         assert "create_processor" in func_names
  50 |         assert "run_pipeline" in func_names

# [method] TestPythonParser.test_parse_classes (relevance: 0.58, depth: 1)
  52 |     def test_parse_classes(self, sample_python_source: str):
  53 |         result = parse_python_file("sample.py", sample_python_source)
  54 | 
  55 |         class_names = [
  56 |             s.name for s in result.symbols if s.kind == SymbolKind.CLASS
  57 |         ]
  58 |         assert "BaseProcessor" in class_names
  59 |         assert "AdvancedProcessor" in class_names

# [method] TestPythonParser.test_parse_methods (relevance: 0.58, depth: 1)
  61 |     def test_parse_methods(self, sample_python_source: str):
  62 |         result = parse_python_file("sample.py", sample_python_source)
  63 | 
  64 |         methods = [s for s in result.symbols if s.kind == SymbolKind.METHOD]
  65 |         method_names = [m.name for m in methods]
  66 |         assert "__init__" in method_names
  67 |         assert "process" in method_names
  68 |         assert "_transform" in method_names
  69 |         assert "batch_process" in method_names

# [method] TestPythonParser.test_parse_imports (relevance: 0.58, depth: 1)
  71 |     def test_parse_imports(self, sample_python_source: str):
  72 |         result = parse_python_file("sample.py", sample_python_source)
  73 |         assert "os" in result.imports
  74 |         assert "typing.List" in result.imports
  75 |         assert "pathlib.Path" in result.imports

# [method] TestPythonParser.test_parse_constants (relevance: 0.58, depth: 1)
  77 |     def test_parse_constants(self, sample_python_source: str):
  78 |         result = parse_python_file("sample.py", sample_python_source)
  79 |         constants = [
  80 |             s for s in result.symbols if s.kind == SymbolKind.CONSTANT
  81 |         ]
  82 |         assert any(c.name == "CONSTANT_VALUE" for c in constants)

# [method] TestPythonParser.test_parse_inheritance (relevance: 0.58, depth: 1)
  84 |     def test_parse_inheritance(self, sample_python_source: str):
  85 |         result = parse_python_file("sample.py", sample_python_source)
  86 |         inherits = [
  87 |             r for r in result.relationships if r.kind == RelKind.INHERITS
  88 |         ]
  89 |         assert any("AdvancedProcessor" in r.source and "BaseProcessor" in r.target for r in inherits)

# [method] TestPythonParser.test_parse_calls (relevance: 0.58, depth: 1)
  91 |     def test_parse_calls(self, sample_python_source: str):
  92 |         result = parse_python_file("sample.py", sample_python_source)
  93 |         calls = [r for r in result.relationships if r.kind == RelKind.CALLS]
  94 |         # run_pipeline calls create_processor
  95 |         assert any(
  96 |             "run_pipeline" in r.source and "create_processor" in r.target
  97 |             for r in calls
  98 |         )

# [method] TestPythonParser.test_parse_docstrings (relevance: 0.58, depth: 1)
 100 |     def test_parse_docstrings(self, sample_python_source: str):
 101 |         result = parse_python_file("sample.py", sample_python_source)
 102 |         funcs = {s.name: s for s in result.symbols if s.kind in (SymbolKind.FUNCTION, SymbolKind.METHOD)}
 103 |         assert "Factory function" in funcs["create_processor"].docstring
 104 |         assert "Transform a single item" in funcs["_transform"].docstring

# [method] TestPythonParser.test_parse_contains_relationships (relevance: 0.58, depth: 1)
 106 |     def test_parse_contains_relationships(self, sample_python_source: str):
 107 |         result = parse_python_file("sample.py", sample_python_source)
 108 |         contains = [r for r in result.relationships if r.kind == RelKind.CONTAINS]
 109 |         # BaseProcessor should contain process, _transform, __init__
 110 |         assert any("BaseProcessor" in r.source and "process" in r.target for r in contains)

# [method] TestPythonParser.test_syntax_error_handling (relevance: 0.58, depth: 1)
 112 |     def test_syntax_error_handling(self):
 113 |         result = parse_python_file("bad.py", "def broken(:\n  pass")
 114 |         assert len(result.errors) > 0

# [method] TestPythonParser.test_empty_file (relevance: 0.58, depth: 1)
 116 |     def test_empty_file(self):
 117 |         result = parse_python_file("empty.py", "")
 118 |         assert result.language == "python"
 119 |         assert len(result.symbols) == 0

# [class] TestCoreParser (relevance: 0.44, depth: 1)
 154 | class TestCoreParser:
 155 |     def test_auto_detect_python(self, sample_python_source: str):
 156 |         result = parse_file("sample.py", sample_python_source)
 157 |         assert result is not None
 158 |         assert result.language == "python"
 159 | 
 160 |     def test_auto_detect_javascript(self, sample_js_source: str):
 161 |         result = parse_file("app.js", sample_js_source)
 162 |         assert result is not None
 163 |         assert result.language == "javascript"
 164 | 
 165 |     def test_unsupported_file(self):
 166 |         result = parse_file("readme.md", "# Hello")
 167 |         assert result is None

# [method] TestCoreParser.test_auto_detect_python (relevance: 0.80, depth: 0)
 155 |     def test_auto_detect_python(self, sample_python_source: str):
 156 |         result = parse_file("sample.py", sample_python_source)
 157 |         assert result is not None
 158 |         assert result.language == "python"

# [method] TestCoreParser.test_auto_detect_javascript (relevance: 0.29, depth: 2)
 160 |     def test_auto_detect_javascript(self, sample_js_source: str):
 161 |         result = parse_file("app.js", sample_js_source)
 162 |         assert result is not None
 163 |         assert result.language == "javascript"

# [method] TestCoreParser.test_unsupported_file (relevance: 0.29, depth: 2)
 165 |     def test_unsupported_file(self):
 166 |         result = parse_file("readme.md", "# Hello")
 167 |         assert result is None
