# src/cegraph/exceptions.py:20-21
class LLMError(CeGraphError):
    """LLM provider errors."""

# src/cegraph/config.py:18-38
class LLMConfig(BaseModel):
    """LLM provider configuration."""

    provider: str = "openai"
    model: str = "claude-sonnet-4-5-20250929"
    api_key_env: str = ""
    max_tokens: int = 4096
    temperature: float = 0.0
    base_url: str | None = None

    @property
    def api_key(self) -> str | None:
        if self.api_key_env:
            return os.environ.get(self.api_key_env)
        # Try common env vars
        env_map = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
        }
        env_var = env_map.get(self.provider, "")
        return os.environ.get(env_var)

# src/cegraph/llm/__init__.py:3-3
from cegraph.llm.base import LLMProvider, LLMResponse, Message, ToolCall, ToolResult

# src/cegraph/llm/openai_provider.py:8-14
from cegraph.llm.base import (
    LLMProvider,
    LLMResponse,
    Message,
    ToolCall,
    ToolDefinition,
)

# src/cegraph/agent/loop.py:17-17
from cegraph.llm.base import LLMProvider, LLMResponse, Message, ToolCall, ToolResult

# src/cegraph/llm/factory.py:6-6
from cegraph.llm.base import LLMProvider

# src/cegraph/llm/anthropic_provider.py:8-14
from cegraph.llm.base import (
    LLMProvider,
    LLMResponse,
    Message,
    ToolCall,
    ToolDefinition,
)

# src/cegraph/llm/base.py:59-87
class LLMProvider(ABC):
    """Abstract base for LLM providers."""

    def __init__(self, model: str, api_key: str | None = None, base_url: str | None = None) -> None:
        self.model = model
        self.api_key = api_key
        self.base_url = base_url

    @abstractmethod
    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        """Send a completion request to the LLM."""
        ...

    @abstractmethod
    async def stream(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> AsyncIterator[str]:
        """Stream a completion response."""
        ...

# src/cegraph/llm/base.py:62-65
    def __init__(self, model: str, api_key: str | None = None, base_url: str | None = None) -> None:
        self.model = model
        self.api_key = api_key
        self.base_url = base_url

# src/cegraph/llm/factory.py:5-5
from cegraph.config import LLMConfig

# paper/experiments/benchmark.py:53-53
from cegraph.config import LLMConfig

# src/cegraph/exceptions.py:32-39
class ProviderNotAvailable(LLMError):
    """Raised when an LLM provider's SDK is not installed."""

    def __init__(self, provider: str, package: str):
        super().__init__(
            f"Provider '{provider}' requires the '{package}' package. "
            f"Install it with: pip install cegraph[{provider}]"
        )

# src/cegraph/llm/openai_provider.py:20-25
    def __init__(
        self, model: str = "gpt-4o", api_key: str | None = None, base_url: str | None = None
    ) -> None:
        super().__init__(model, api_key, base_url)
        self._client = None
        self._async_client = None

# src/cegraph/llm/anthropic_provider.py:20-27
    def __init__(
        self,
        model: str = "claude-sonnet-4-5-20250929",
        api_key: str | None = None,
        base_url: str | None = None,
    ) -> None:
        super().__init__(model, api_key, base_url)
        self._client = None

# src/cegraph/agent/loop.py:17-17
from cegraph.llm.base import LLMProvider, LLMResponse, Message, ToolCall, ToolResult

# paper/experiments/benchmark.py:58-58
from cegraph.llm.base import LLMResponse, Message

# src/cegraph/llm/__init__.py:3-3
from cegraph.llm.base import LLMProvider, LLMResponse, Message, ToolCall, ToolResult