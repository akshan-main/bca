# src/cegraph/llm/openai_provider.py:17-152
class OpenAIProvider(LLMProvider):
    """Provider for OpenAI and OpenAI-compatible APIs (Ollama, vLLM, etc.)."""

    def __init__(
        self, model: str = "gpt-4o", api_key: str | None = None, base_url: str | None = None
    ) -> None:
        super().__init__(model, api_key, base_url)
        self._client = None
        self._async_client = None

    def _get_client(self):
        if self._async_client is None:
            try:
                from openai import AsyncOpenAI
            except ImportError:
                from cegraph.exceptions import ProviderNotAvailable
                raise ProviderNotAvailable("openai", "openai")

            kwargs: dict[str, Any] = {}
            if self.api_key:
                kwargs["api_key"] = self.api_key
            if self.base_url:
                kwargs["base_url"] = self.base_url
            self._async_client = AsyncOpenAI(**kwargs)
        return self._async_client

    def _format_messages(self, messages: list[Message]) -> list[dict]:
        """Convert our Message format to OpenAI's format."""
        result = []
        for msg in messages:
            if msg.role == "tool":
                result.append({
                    "role": "tool",
                    "content": msg.content,
                    "tool_call_id": msg.tool_call_id,
                })
            elif msg.tool_calls:
                tool_calls = []
                for tc in msg.tool_calls:
                    tool_calls.append({
                        "id": tc.id,
                        "type": "function",
                        "function": {
                            "name": tc.name,
                            "arguments": json.dumps(tc.arguments),
                        },
                    })
                result.append({
                    "role": msg.role,
                    "content": msg.content or None,
                    "tool_calls": tool_calls,
                })
            else:
                result.append({
                    "role": msg.role,
                    "content": msg.content,
                })
        return result

    def _format_tools(self, tools: list[ToolDefinition]) -> list[dict]:
        """Convert tool definitions to OpenAI's format."""
        return [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.parameters,
                },
            }
            for tool in tools
        ]

    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        client = self._get_client()
        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": self._format_messages(messages),
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        response = await client.chat.completions.create(**kwargs)
        choice = response.choices[0]

        tool_calls = []
        if choice.message.tool_calls:
            for tc in choice.message.tool_calls:
                try:
                    args = json.loads(tc.function.arguments)
                except json.JSONDecodeError:
                    args = {}
                tool_calls.append(
                    ToolCall(id=tc.id, name=tc.function.name, arguments=args)
                )

        return LLMResponse(
            content=choice.message.content or "",
            tool_calls=tool_calls,
            finish_reason=choice.finish_reason or "",
            usage={
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0,
            },
        )

    async def stream(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> AsyncIterator[str]:
        client = self._get_client()
        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": self._format_messages(messages),
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True,
        }
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        stream = await client.chat.completions.create(**kwargs)
        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

# src/cegraph/exceptions.py:20-21
class LLMError(CeGraphError):
    """LLM provider errors."""

# src/cegraph/config.py:18-38
class LLMConfig(BaseModel):
    """LLM provider configuration."""

    provider: str = "openai"
    model: str = "claude-sonnet-4-5-20250929"
    api_key_env: str = ""
    max_tokens: int = 4096
    temperature: float = 0.0
    base_url: str | None = None

    @property
    def api_key(self) -> str | None:
        if self.api_key_env:
            return os.environ.get(self.api_key_env)
        # Try common env vars
        env_map = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
        }
        env_var = env_map.get(self.provider, "")
        return os.environ.get(env_var)

# src/cegraph/agent/loop.py:17-17
from cegraph.llm.base import LLMProvider, LLMResponse, Message, ToolCall, ToolResult

# src/cegraph/llm/__init__.py:3-3
from cegraph.llm.base import LLMProvider, LLMResponse, Message, ToolCall, ToolResult

# src/cegraph/llm/openai_provider.py:8-14
from cegraph.llm.base import (
    LLMProvider,
    LLMResponse,
    Message,
    ToolCall,
    ToolDefinition,
)

# src/cegraph/llm/factory.py:6-6
from cegraph.llm.base import LLMProvider

# src/cegraph/llm/anthropic_provider.py:8-14
from cegraph.llm.base import (
    LLMProvider,
    LLMResponse,
    Message,
    ToolCall,
    ToolDefinition,
)

# src/cegraph/llm/base.py:59-87
class LLMProvider(ABC):
    """Abstract base for LLM providers."""

    def __init__(self, model: str, api_key: str | None = None, base_url: str | None = None) -> None:
        self.model = model
        self.api_key = api_key
        self.base_url = base_url

    @abstractmethod
    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        """Send a completion request to the LLM."""
        ...

    @abstractmethod
    async def stream(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> AsyncIterator[str]:
        """Stream a completion response."""
        ...

# src/cegraph/llm/base.py:62-65
    def __init__(self, model: str, api_key: str | None = None, base_url: str | None = None) -> None:
        self.model = model
        self.api_key = api_key
        self.base_url = base_url

# src/cegraph/llm/anthropic_provider.py:17-168
class AnthropicProvider(LLMProvider):
    """Provider for Anthropic's Claude models."""

    def __init__(
        self,
        model: str = "claude-sonnet-4-5-20250929",
        api_key: str | None = None,
        base_url: str | None = None,
    ) -> None:
        super().__init__(model, api_key, base_url)
        self._client = None

    def _get_client(self):
        if self._client is None:
            try:
                from anthropic import AsyncAnthropic
            except ImportError:
                from cegraph.exceptions import ProviderNotAvailable
                raise ProviderNotAvailable("anthropic", "anthropic")

            kwargs: dict[str, Any] = {}
            if self.api_key:
                kwargs["api_key"] = self.api_key
            if self.base_url:
                kwargs["base_url"] = self.base_url
            self._client = AsyncAnthropic(**kwargs)
        return self._client

    def _format_messages(self, messages: list[Message]) -> tuple[str, list[dict]]:
        """Convert our Message format to Anthropic's format.

        Returns (system_prompt, messages_list).
        """
        system = ""
        result = []

        for msg in messages:
            if msg.role == "system":
                system = msg.content
                continue

            if msg.role == "tool":
                result.append({
                    "role": "user",
                    "content": [
                        {
                            "type": "tool_result",
                            "tool_use_id": msg.tool_call_id,
                            "content": msg.content,
                        }
                    ],
                })
            elif msg.tool_calls:
                content: list[dict] = []
                if msg.content:
                    content.append({"type": "text", "text": msg.content})
                for tc in msg.tool_calls:
                    content.append({
                        "type": "tool_use",
                        "id": tc.id,
                        "name": tc.name,
                        "input": tc.arguments,
                    })
                result.append({"role": "assistant", "content": content})
            else:
                result.append({"role": msg.role, "content": msg.content})

        return system, result

    def _format_tools(self, tools: list[ToolDefinition]) -> list[dict]:
        """Convert tool definitions to Anthropic's format."""
        return [
            {
                "name": tool.name,
                "description": tool.description,
                "input_schema": tool.parameters,
            }
            for tool in tools
        ]

    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        client = self._get_client()
        system, formatted_msgs = self._format_messages(messages)

        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": formatted_msgs,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        if system:
            kwargs["system"] = system
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        response = await client.messages.create(**kwargs)

        content = ""
        tool_calls = []

        for block in response.content:
            if block.type == "text":
                content += block.text
            elif block.type == "tool_use":
                tool_calls.append(
                    ToolCall(
                        id=block.id,
                        name=block.name,
                        arguments=block.input if isinstance(block.input, dict) else {},
                    )
                )

        return LLMResponse(
            content=content,
            tool_calls=tool_calls,
            finish_reason=response.stop_reason or "",
            usage={
                "prompt_tokens": response.usage.input_tokens,
                "completion_tokens": response.usage.output_tokens,
            },
        )

    async def stream(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> AsyncIterator[str]:
        client = self._get_client()
        system, formatted_msgs = self._format_messages(messages)

        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": formatted_msgs,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        if system:
            kwargs["system"] = system
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        async with client.messages.stream(**kwargs) as stream:
            async for text in stream.text_stream:
                yield text

# paper/experiments/benchmark.py:53-53
from cegraph.config import LLMConfig

# src/cegraph/llm/factory.py:5-5
from cegraph.config import LLMConfig

# src/cegraph/exceptions.py:32-39
class ProviderNotAvailable(LLMError):
    """Raised when an LLM provider's SDK is not installed."""

    def __init__(self, provider: str, package: str):
        super().__init__(
            f"Provider '{provider}' requires the '{package}' package. "
            f"Install it with: pip install cegraph[{provider}]"
        )

# src/cegraph/llm/openai_provider.py:20-25
    def __init__(
        self, model: str = "gpt-4o", api_key: str | None = None, base_url: str | None = None
    ) -> None:
        super().__init__(model, api_key, base_url)
        self._client = None
        self._async_client = None

# src/cegraph/llm/openai_provider.py:27-41
    def _get_client(self):
        if self._async_client is None:
            try:
                from openai import AsyncOpenAI
            except ImportError:
                from cegraph.exceptions import ProviderNotAvailable
                raise ProviderNotAvailable("openai", "openai")

            kwargs: dict[str, Any] = {}
            if self.api_key:
                kwargs["api_key"] = self.api_key
            if self.base_url:
                kwargs["base_url"] = self.base_url
            self._async_client = AsyncOpenAI(**kwargs)
        return self._async_client

# src/cegraph/llm/anthropic_provider.py:20-27
    def __init__(
        self,
        model: str = "claude-sonnet-4-5-20250929",
        api_key: str | None = None,
        base_url: str | None = None,
    ) -> None:
        super().__init__(model, api_key, base_url)
        self._client = None

# src/cegraph/llm/openai_provider.py:90-129
    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        client = self._get_client()
        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": self._format_messages(messages),
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        response = await client.chat.completions.create(**kwargs)
        choice = response.choices[0]

        tool_calls = []
        if choice.message.tool_calls:
            for tc in choice.message.tool_calls:
                try:
                    args = json.loads(tc.function.arguments)
                except json.JSONDecodeError:
                    args = {}
                tool_calls.append(
                    ToolCall(id=tc.id, name=tc.function.name, arguments=args)
                )

        return LLMResponse(
            content=choice.message.content or "",
            tool_calls=tool_calls,
            finish_reason=choice.finish_reason or "",
            usage={
                "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                "completion_tokens": response.usage.completion_tokens if response.usage else 0,
            },
        )

# src/cegraph/llm/factory.py:9-44
def create_provider(config: LLMConfig) -> LLMProvider:
    """Create an LLM provider from configuration.

    Args:
        config: LLM configuration with provider, model, etc.

    Returns:
        An initialized LLM provider.

    Raises:
        ValueError: If the provider is unknown.
        ProviderNotAvailable: If the provider's SDK is not installed.
    """
    provider = config.provider.lower()

    if provider == "openai" or provider == "local":
        from cegraph.llm.openai_provider import OpenAIProvider

        return OpenAIProvider(
            model=config.model,
            api_key=config.api_key,
            base_url=config.base_url,
        )
    elif provider == "anthropic":
        from cegraph.llm.anthropic_provider import AnthropicProvider

        return AnthropicProvider(
            model=config.model,
            api_key=config.api_key,
            base_url=config.base_url,
        )
    else:
        raise ValueError(
            f"Unknown LLM provider: '{provider}'. "
            f"Supported providers: openai, anthropic, local"
        )

# src/cegraph/llm/anthropic_provider.py:97-143
    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        client = self._get_client()
        system, formatted_msgs = self._format_messages(messages)

        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": formatted_msgs,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        if system:
            kwargs["system"] = system
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        response = await client.messages.create(**kwargs)

        content = ""
        tool_calls = []

        for block in response.content:
            if block.type == "text":
                content += block.text
            elif block.type == "tool_use":
                tool_calls.append(
                    ToolCall(
                        id=block.id,
                        name=block.name,
                        arguments=block.input if isinstance(block.input, dict) else {},
                    )
                )

        return LLMResponse(
            content=content,
            tool_calls=tool_calls,
            finish_reason=response.stop_reason or "",
            usage={
                "prompt_tokens": response.usage.input_tokens,
                "completion_tokens": response.usage.output_tokens,
            },
        )

# src/cegraph/agent/loop.py:57-71
    def __init__(
        self,
        llm: LLMProvider,
        tools: ToolRegistry,
        project_name: str = "",
        max_iterations: int = 15,
        on_step: Callable[[AgentStep], None] | None = None,
        on_approval_needed: Callable[[str], bool] | None = None,
    ) -> None:
        self.llm = llm
        self.tools = tools
        self.project_name = project_name
        self.max_iterations = max_iterations
        self.on_step = on_step
        self.on_approval_needed = on_approval_needed

# src/cegraph/llm/anthropic_provider.py:29-43
    def _get_client(self):
        if self._client is None:
            try:
                from anthropic import AsyncAnthropic
            except ImportError:
                from cegraph.exceptions import ProviderNotAvailable
                raise ProviderNotAvailable("anthropic", "anthropic")

            kwargs: dict[str, Any] = {}
            if self.api_key:
                kwargs["api_key"] = self.api_key
            if self.base_url:
                kwargs["base_url"] = self.base_url
            self._client = AsyncAnthropic(**kwargs)
        return self._client

# src/cegraph/llm/openai_provider.py:8-14
from cegraph.llm.base import (
    LLMProvider,
    LLMResponse,
    Message,
    ToolCall,
    ToolDefinition,
)

# paper/experiments/benchmark.py:58-58
from cegraph.llm.base import LLMResponse, Message

# src/cegraph/llm/anthropic_provider.py:8-14
from cegraph.llm.base import (
    LLMProvider,
    LLMResponse,
    Message,
    ToolCall,
    ToolDefinition,
)

# src/cegraph/agent/loop.py:17-17
from cegraph.llm.base import LLMProvider, LLMResponse, Message, ToolCall, ToolResult

# src/cegraph/llm/__init__.py:3-3
from cegraph.llm.base import LLMProvider, LLMResponse, Message, ToolCall, ToolResult

# paper/experiments/benchmark.py:606-626
async def call_llm(
    provider, context: str, task: str,
) -> tuple[str, float, int, int]:
    """Call the LLM and return (response_text, time_ms, input_tokens, output_tokens)."""
    messages = [
        Message(role="system", content=SYSTEM_PROMPT),
        Message(role="user", content=build_prompt(context, task)),
    ]

    start = time.time()
    response: LLMResponse = await provider.complete(
        messages=messages,
        temperature=0.0,
        max_tokens=4096,
    )
    elapsed = (time.time() - start) * 1000

    input_tokens = response.usage.get("prompt_tokens", 0) or response.usage.get("input_tokens", 0)
    output_tokens = response.usage.get("completion_tokens", 0) or response.usage.get("output_tokens", 0)

    return (response.content, round(elapsed, 1), input_tokens, output_tokens)

# src/cegraph/llm/base.py:46-56
class LLMResponse(BaseModel):
    """Response from the LLM."""

    content: str = ""
    tool_calls: list[ToolCall] = Field(default_factory=list)
    finish_reason: str = ""
    usage: dict[str, int] = Field(default_factory=dict)

    @property
    def has_tool_calls(self) -> bool:
        return len(self.tool_calls) > 0

# src/cegraph/exceptions.py:35-39
    def __init__(self, provider: str, package: str):
        super().__init__(
            f"Provider '{provider}' requires the '{package}' package. "
            f"Install it with: pip install cegraph[{provider}]"
        )

# paper/experiments/benchmark.py:59-59
from cegraph.llm.factory import create_provider

# src/cegraph/llm/__init__.py:4-4
from cegraph.llm.factory import create_provider

# src/cegraph/llm/base.py:68-76
    async def complete(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> LLMResponse:
        """Send a completion request to the LLM."""
        ...

# src/cegraph/config.py:41-46
class AgentConfig(BaseModel):
    """Agent behavior configuration."""

    max_iterations: int = 15
    auto_verify: bool = True
    require_approval: bool = True

# src/cegraph/context/engine.py:116-128
class AblationConfig:
    """Toggle individual components for ablation studies.

    All features enabled by default. Disable to measure their contribution.
    """
    dependency_closure: bool = True    # Enforce D(v) ⊆ X constraint
    centrality_scoring: bool = True    # Boost high-connectivity symbols
    file_proximity: bool = True        # Boost symbols in same file as seeds
    kind_weights: bool = True          # Weight by symbol kind
    submodular_coverage: bool = True   # Diminishing returns scoring
    dependency_ordering: bool = True   # Topological sort output
    learned_weights: bool = False      # Learn edge weights from git history
    use_pagerank: bool = False         # Personalized PageRank instead of BFS

# src/cegraph/config.py:29-38
    def api_key(self) -> str | None:
        if self.api_key_env:
            return os.environ.get(self.api_key_env)
        # Try common env vars
        env_map = {
            "openai": "OPENAI_API_KEY",
            "anthropic": "ANTHROPIC_API_KEY",
        }
        env_var = env_map.get(self.provider, "")
        return os.environ.get(env_var)

# paper/experiments/ablation.py:20-20
from cegraph.context.engine import AblationConfig, ContextAssembler

# paper/experiments/benchmark.py:54-54
from cegraph.context.engine import AblationConfig, ContextAssembler

# src/cegraph/agent/__init__.py:3-3
from cegraph.agent.loop import AgentLoop

# src/cegraph/context/models.py:39-156
class ContextPackage(BaseModel):
    """The complete assembled context package ready for LLM consumption."""

    task: str
    strategy: ContextStrategy
    items: list[ContextItem] = Field(default_factory=list)
    seed_symbols: list[str] = Field(default_factory=list)
    total_tokens: int = 0
    token_budget: int = 0
    files_included: int = 0
    symbols_included: int = 0
    symbols_available: int = 0  # Total candidates before budget cut
    budget_used_pct: float = 0.0
    assembly_time_ms: float = 0.0

    def render(self, include_line_numbers: bool = True, include_metadata: bool = True) -> str:
        """Render the context package as a string for LLM consumption.

        This is the key output - a carefully structured text that gives the LLM
        exactly what it needs to understand the relevant code.
        """
        sections: list[str] = []

        if include_metadata:
            sections.append(f"# Codebase Context for: {self.task}")
            sections.append(
                f"# {self.symbols_included} symbols from {self.files_included} files "
                f"(~{self.total_tokens:,} tokens, {self.budget_used_pct:.0f}% of budget)"
            )
            sections.append("")

        # Group items by file
        by_file: dict[str, list[ContextItem]] = {}
        for item in self.items:
            by_file.setdefault(item.file_path, []).append(item)

        for file_path, items in sorted(by_file.items()):
            sections.append(f"## {file_path}")
            if include_metadata:
                reasons = set(i.reason for i in items if i.reason)
                if reasons:
                    sections.append(f"# Included because: {'; '.join(reasons)}")
            sections.append("")

            # Sort items by line number
            items.sort(key=lambda x: x.line_start)

            for item in items:
                if include_metadata:
                    sections.append(
                        f"# [{item.kind}] {item.qualified_name} "
                        f"(relevance: {item.relevance_score:.2f}, depth: {item.depth})"
                    )

                if include_line_numbers:
                    lines = item.source_code.splitlines()
                    for i, line in enumerate(lines):
                        sections.append(f"{item.line_start + i:4d} | {line}")
                else:
                    sections.append(item.source_code)

                sections.append("")

        return "\n".join(sections)

    def render_compact(self) -> str:
        """Render a compact version - signatures + docstrings only for secondary symbols."""
        sections: list[str] = []
        sections.append(f"# Context for: {self.task}")
        sections.append("")

        by_file: dict[str, list[ContextItem]] = {}
        for item in self.items:
            by_file.setdefault(item.file_path, []).append(item)

        for file_path, items in sorted(by_file.items()):
            sections.append(f"## {file_path}")
            items.sort(key=lambda x: x.line_start)

            for item in items:
                if item.depth == 0:
                    # Primary symbols: full source
                    sections.append(item.source_code)
                else:
                    # Secondary: signature + docstring only
                    sections.append(item.signature)
                    if item.docstring:
                        doc_preview = item.docstring[:150]
                        if len(item.docstring) > 150:
                            doc_preview += "..."
                        sections.append(f'    """{doc_preview}"""')
                sections.append("")

        return "\n".join(sections)

    def summary(self) -> str:
        """Human-readable summary of what's in the context."""
        lines = [
            f"CAG Context Package for: {self.task}",
            f"Strategy: {self.strategy.value}",
            f"Tokens: {self.total_tokens:,} / {self.token_budget:,} ({self.budget_used_pct:.0f}%)",
            f"Symbols: {self.symbols_included} included, {self.symbols_available} candidates",
            f"Files: {self.files_included}",
            f"Assembly time: {self.assembly_time_ms:.1f}ms",
            "",
            "Included symbols:",
        ]
        for item in self.items:
            marker = ">" if item.depth == 0 else " " * item.depth + "·"
            lines.append(
                f"  {marker} {item.qualified_name} ({item.kind}) "
                f"[{item.file_path}:{item.line_start}] "
                f"score={item.relevance_score:.2f} ~{item.token_estimate}tok"
            )
            if item.reason:
                lines.append(f"    reason: {item.reason}")

        return "\n".join(lines)

# src/cegraph/llm/openai_provider.py:131-152
    async def stream(
        self,
        messages: list[Message],
        tools: list[ToolDefinition] | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4096,
    ) -> AsyncIterator[str]:
        client = self._get_client()
        kwargs: dict[str, Any] = {
            "model": self.model,
            "messages": self._format_messages(messages),
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True,
        }
        if tools:
            kwargs["tools"] = self._format_tools(tools)

        stream = await client.chat.completions.create(**kwargs)
        async for chunk in stream:
            if chunk.choices and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

# tests/test_config.py:9-15
from cegraph.config import (
    ProjectConfig,
    find_project_root,
    load_config,
    save_config,
    set_config_value,
)

# src/cegraph/mcp/server.py:24-24
from cegraph.config import find_project_root, load_config, get_cegraph_dir, GRAPH_DB_FILE

# paper/experiments/run_all.py:12-12
from collections import defaultdict

# src/cegraph/context/learned_weights.py:13-13
from collections import Counter, defaultdict