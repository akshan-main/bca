# Codebase Context for: Default LLM provider is set to openai instead of anthropic, breaking configurations that rely on the anthropic default.
# 19 symbols from 11 files (~1,739 tokens, 87% of budget)

## paper/experiments/baselines.py
# Included because: graph expansion (depth 3)

# [class] BaselineResult (relevance: 0.23, depth: 3)
  34 | class BaselineResult:
  35 |     """Result of a single baseline run."""
  36 | 
  37 |     method: str
  38 |     task: str
  39 |     budget: int
  40 |     tokens_used: int
  41 |     symbols_selected: int
  42 |     files_included: int
  43 |     assembly_time_ms: float
  44 |     selected_symbols: list[str] = field(default_factory=list)
  45 |     recall: float | None = None

## paper/experiments/benchmark.py
# Included because: graph expansion (depth 2)

# [function] _vector_score_dense (relevance: 0.24, depth: 2)
 424 | def _vector_score_dense(query: str, symbols: list[dict]) -> list[float]:
 425 |     """Dense embedding scoring using sentence-transformers."""
 426 |     try:
 427 |         from sentence_transformers import SentenceTransformer
 428 |         import numpy as np
 429 |     except ImportError:
 430 |         print("sentence-transformers not installed, falling back to TF-IDF")
 431 |         return _vector_score_tfidf(query, symbols)
 432 | 
 433 |     model = SentenceTransformer("all-MiniLM-L6-v2")
 434 |     texts = [s["text"] for s in symbols]
 435 |     query_emb = model.encode([query], normalize_embeddings=True)
 436 |     doc_embs = model.encode(texts, normalize_embeddings=True, batch_size=64)
 437 |     scores = (doc_embs @ query_emb.T).flatten()
 438 |     return scores.tolist()

# [function] call_llm (relevance: 0.31, depth: 2)
 606 | async def call_llm(
 607 |     provider, context: str, task: str,
 608 | ) -> tuple[str, float, int, int]:
 609 |     """Call the LLM and return (response_text, time_ms, input_tokens, output_tokens)."""
 610 |     messages = [
 611 |         Message(role="system", content=SYSTEM_PROMPT),
 612 |         Message(role="user", content=build_prompt(context, task)),
 613 |     ]
 614 | 
 615 |     start = time.time()
 616 |     response: LLMResponse = await provider.complete(
 617 |         messages=messages,
 618 |         temperature=0.0,
 619 |         max_tokens=4096,
 620 |     )
 621 |     elapsed = (time.time() - start) * 1000
 622 | 
 623 |     input_tokens = response.usage.get("prompt_tokens", 0) or response.usage.get("input_tokens", 0)
 624 |     output_tokens = response.usage.get("completion_tokens", 0) or response.usage.get("output_tokens", 0)
 625 | 
 626 |     return (response.content, round(elapsed, 1), input_tokens, output_tokens)

## src/cegraph/cli.py
# Included because: graph expansion (depth 3)

# [function] _get_project_root (relevance: 0.41, depth: 3)
  28 | def _get_project_root(path: str | None = None) -> Path:
  29 |     """Find the project root or error."""
  30 |     if path:
  31 |         root = Path(path).resolve()
  32 |         if not root.exists():
  33 |             console.error(f"Path does not exist: {path}")
  34 |             sys.exit(1)
  35 |         return root
  36 | 
  37 |     root = find_project_root()
  38 |     if root is None:
  39 |         console.error(
  40 |             "No CeGraph project found. Run 'cegraph init' first, "
  41 |             "or specify a path with --path."
  42 |         )
  43 |         sys.exit(1)
  44 |     return root

# [function] _load_graph (relevance: 0.39, depth: 3)
  47 | def _load_graph(root: Path):
  48 |     """Load the knowledge graph and related objects."""
  49 |     from cegraph.graph.store import GraphStore
  50 | 
  51 |     db_path = get_cegraph_dir(root) / GRAPH_DB_FILE
  52 |     store = GraphStore(db_path)
  53 |     graph = store.load()
  54 |     if graph is None:
  55 |         console.error("No index found. Run 'cegraph init' or 'cegraph reindex' first.")
  56 |         sys.exit(1)
  57 |     return graph, store

## src/cegraph/config.py
# Included because: matches 'set'

# [function] set_config_value (relevance: 0.40, depth: 0)
 125 | def set_config_value(config: ProjectConfig, key: str, value: Any) -> ProjectConfig:
 126 |     """Set a nested config value using dot notation (e.g., 'llm.provider')."""
 127 |     parts = key.split(".")
 128 |     data = config.model_dump()
 129 |     target = data
 130 |     for part in parts[:-1]:
 131 |         if part not in target or not isinstance(target[part], dict):
 132 |             raise KeyError(f"Invalid config key: {key}")
 133 |         target = target[part]
 134 |     if parts[-1] not in target:
 135 |         raise KeyError(f"Invalid config key: {key}")
 136 |     target[parts[-1]] = value
 137 |     return ProjectConfig(**data)

## src/cegraph/context/_native.py
# Included because: graph expansion (depth 1)

# [function] _load_library (relevance: 0.38, depth: 1)
  55 | def _load_library():
  56 |     """Load the native library, or return None."""
  57 |     global _lib, _lib_path
  58 | 
  59 |     if _lib is not None:
  60 |         return _lib
  61 | 
  62 |     path = _find_library()
  63 |     if path is None:
  64 |         return None
  65 | 
  66 |     try:
  67 |         lib = ctypes.CDLL(path)
  68 | 
  69 |         # Verify it's the right library
  70 |         lib.cag_is_available.restype = ctypes.c_int32
  71 |         if lib.cag_is_available() != 1:
  72 |             return None
  73 | 
  74 |         # Set up function signatures
  75 |         _setup_signatures(lib)
  76 | 
  77 |         _lib = lib
  78 |         _lib_path = path
  79 |         return lib
  80 |     except (OSError, AttributeError):
  81 |         return None

## src/cegraph/context/models.py
# Included because: graph expansion (depth 2); required dependency of estimate_lines

# [class] TokenEstimator (relevance: 0.13, depth: 3)
 159 | class TokenEstimator:
 160 |     """Estimate token counts for code."""
 161 | 
 162 |     # Rough heuristic: 1 token â‰ˆ 4 characters for code
 163 |     CHARS_PER_TOKEN = 4.0
 164 | 
 165 |     @classmethod
 166 |     def estimate(cls, text: str) -> int:
 167 |         """Estimate token count for a string."""
 168 |         return max(1, int(len(text) / cls.CHARS_PER_TOKEN))
 169 | 
 170 |     @classmethod
 171 |     def estimate_lines(cls, line_count: int, avg_line_length: int = 40) -> int:
 172 |         """Estimate tokens for a given number of lines."""
 173 |         return max(1, int(line_count * avg_line_length / cls.CHARS_PER_TOKEN))

# [method] TokenEstimator.estimate_lines (relevance: 0.27, depth: 2)
 171 |     def estimate_lines(cls, line_count: int, avg_line_length: int = 40) -> int:
 172 |         """Estimate tokens for a given number of lines."""
 173 |         return max(1, int(line_count * avg_line_length / cls.CHARS_PER_TOKEN))

## src/cegraph/exceptions.py
# Included because: graph expansion (depth 2); graph expansion (depth 3); required dependency of LLMError

# [class] CeGraphError (relevance: 0.10, depth: 4)
   4 | class CeGraphError(Exception):
   5 |     """Base exception for all CeGraph errors."""

# [class] LLMError (relevance: 0.20, depth: 3)
  20 | class LLMError(CeGraphError):
  21 |     """LLM provider errors."""

# [class] ProviderNotAvailable (relevance: 0.25, depth: 2)
  32 | class ProviderNotAvailable(LLMError):
  33 |     """Raised when an LLM provider's SDK is not installed."""
  34 | 
  35 |     def __init__(self, provider: str, package: str):
  36 |         super().__init__(
  37 |             f"Provider '{provider}' requires the '{package}' package. "
  38 |             f"Install it with: pip install cegraph[{provider}]"
  39 |         )

## src/cegraph/github/diff_parser.py
# Included because: graph expansion (depth 2); graph expansion (depth 3)

# [class] ChangedSymbol (relevance: 0.17, depth: 3)
  47 | class ChangedSymbol:
  48 |     """A symbol that was affected by the diff."""
  49 |     name: str
  50 |     qualified_name: str
  51 |     kind: str
  52 |     file_path: str
  53 |     line_start: int
  54 |     line_end: int
  55 |     change_type: str  # 'modified', 'added', 'deleted'
  56 |     lines_changed: int = 0

# [function] get_pr_diff (relevance: 0.21, depth: 2)
 205 | def get_pr_diff(root: Path) -> str:
 206 |     """Get the diff for the current PR (GitHub Actions context)."""
 207 |     import os
 208 |     base_ref = os.environ.get("GITHUB_BASE_REF", "main")
 209 |     return get_git_diff(root, base=f"origin/{base_ref}")

## src/cegraph/github/renderer.py
# Included because: graph expansion (depth 3)

# [function] _risk_badge (relevance: 0.23, depth: 3)
 137 | def _risk_badge(risk: float) -> tuple[str, str, str]:
 138 |     """Return (emoji, label, color) for a risk score."""
 139 |     if risk < 0.1:
 140 |         return ("ðŸŸ¢", "LOW", "green")
 141 |     elif risk < 0.2:
 142 |         return ("ðŸŸ¡", "LOW", "yellow")
 143 |     elif risk < 0.4:
 144 |         return ("ðŸŸ ", "MEDIUM", "orange")
 145 |     elif risk < 0.6:
 146 |         return ("ðŸ”´", "HIGH", "red")
 147 |     else:
 148 |         return ("â›”", "CRITICAL", "red")

# [function] _footer (relevance: 0.15, depth: 3)
 192 | def _footer() -> str:
 193 |     return (
 194 |         "---\n"
 195 |         "*Powered by [CeGraph](https://github.com/cegraph-ai/cegraph) "
 196 |         "â€” CAG-driven code intelligence*"
 197 |     )

## src/cegraph/parser/models.py
# Included because: graph expansion (depth 2)

# [function] detect_language (relevance: 0.19, depth: 2)
 100 | def detect_language(file_path: str) -> str | None:
 101 |     """Detect programming language from file extension."""
 102 |     from pathlib import Path
 103 | 
 104 |     ext = Path(file_path).suffix.lower()
 105 |     return EXTENSION_LANGUAGE_MAP.get(ext)

## src/cegraph/parser/tree_sitter_parser.py
# Included because: graph expansion (depth 2); graph expansion (depth 3)

# [function] _get_language (relevance: 0.19, depth: 2)
  94 | def _get_language(lang: str):
  95 |     """Get a tree-sitter Language object for the given language."""
  96 |     from tree_sitter import Language
  97 | 
  98 |     module_name = _TS_LANGUAGE_MODULES.get(lang)
  99 |     if not module_name:
 100 |         raise ValueError(f"No tree-sitter grammar for language: {lang}")
 101 | 
 102 |     module = __import__(module_name)
 103 |     return Language(module.language())

# [function] _extract_ts_import (relevance: 0.15, depth: 3)
 250 | def _extract_ts_import(node, file_path: str, source: bytes, result: FileSymbols) -> None:
 251 |     """Extract import information from a tree-sitter node."""
 252 |     text = node.text.decode("utf-8")
 253 |     result.imports.append(text)
