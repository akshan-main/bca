# src/cegraph/llm/factory.py:9-44
def create_provider(config: LLMConfig) -> LLMProvider:
    """Create an LLM provider from configuration.

    Args:
        config: LLM configuration with provider, model, etc.

    Returns:
        An initialized LLM provider.

    Raises:
        ValueError: If the provider is unknown.
        ProviderNotAvailable: If the provider's SDK is not installed.
    """
    provider = config.provider.lower()

    if provider == "openai" or provider == "local":
        from cegraph.llm.openai_provider import OpenAIProvider

        return OpenAIProvider(
            model=config.model,
            api_key=config.api_key,
            base_url=config.base_url,
        )
    elif provider == "anthropic":
        from cegraph.llm.anthropic_provider import AnthropicProvider

        return AnthropicProvider(
            model=config.model,
            api_key=config.api_key,
            base_url=config.base_url,
        )
    else:
        raise ValueError(
            f"Unknown LLM provider: '{provider}'. "
            f"Supported providers: openai, anthropic, local"
        )

# src/cegraph/llm/anthropic_provider.py:86-95
    def _format_tools(self, tools: list[ToolDefinition]) -> list[dict]:
        """Convert tool definitions to Anthropic's format."""
        return [
            {
                "name": tool.name,
                "description": tool.description,
                "input_schema": tool.parameters,
            }
            for tool in tools
        ]

# src/cegraph/context/models.py:54-102
    def render(self, include_line_numbers: bool = True, include_metadata: bool = True) -> str:
        """Render the context package as a string for LLM consumption.

        This is the key output - a carefully structured text that gives the LLM
        exactly what it needs to understand the relevant code.
        """
        sections: list[str] = []

        if include_metadata:
            sections.append(f"# Codebase Context for: {self.task}")
            sections.append(
                f"# {self.symbols_included} symbols from {self.files_included} files "
                f"(~{self.total_tokens:,} tokens, {self.budget_used_pct:.0f}% of budget)"
            )
            sections.append("")

        # Group items by file
        by_file: dict[str, list[ContextItem]] = {}
        for item in self.items:
            by_file.setdefault(item.file_path, []).append(item)

        for file_path, items in sorted(by_file.items()):
            sections.append(f"## {file_path}")
            if include_metadata:
                reasons = set(i.reason for i in items if i.reason)
                if reasons:
                    sections.append(f"# Included because: {'; '.join(reasons)}")
            sections.append("")

            # Sort items by line number
            items.sort(key=lambda x: x.line_start)

            for item in items:
                if include_metadata:
                    sections.append(
                        f"# [{item.kind}] {item.qualified_name} "
                        f"(relevance: {item.relevance_score:.2f}, depth: {item.depth})"
                    )

                if include_line_numbers:
                    lines = item.source_code.splitlines()
                    for i, line in enumerate(lines):
                        sections.append(f"{item.line_start + i:4d} | {line}")
                else:
                    sections.append(item.source_code)

                sections.append("")

        return "\n".join(sections)

# src/cegraph/exceptions.py:20-21
class LLMError(CeGraphError):
    """LLM provider errors."""

# src/cegraph/tools/registry.py:38-40
    def get_definitions(self) -> list[ToolDefinition]:
        """Get all tool definitions (for passing to LLM)."""
        return list(self._definitions.values())