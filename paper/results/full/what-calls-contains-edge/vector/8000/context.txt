# src/cegraph/graph/query.py:162-183
    def what_calls(self, name: str) -> list[dict]:
        """Find all symbols called by the given symbol."""
        symbol_ids = self.find_symbol(name)
        results = []

        for sid in symbol_ids:
            for succ in self.graph.successors(sid):
                edge_data = self.graph.edges[sid, succ]
                if edge_data.get("kind") != "contains":
                    continue
                succ_data = self.graph.nodes.get(succ, {})
                if succ_data.get("type") != "symbol":
                    continue
                results.append({
                    "symbol_id": succ,
                    "name": succ_data.get("qualified_name", succ_data.get("name", "")),
                    "kind": succ_data.get("kind", ""),
                    "file_path": succ_data.get("file_path", ""),
                    "line": succ_data.get("line_start", 0),
                })

        return results

# src/cegraph/tools/definitions.py:91-102
    def what_calls(self, symbol_name: str) -> str:
        """Find all symbols called by a function/method."""
        results = self.query.what_calls(symbol_name)
        if not results:
            return f"No callees found for '{symbol_name}'"

        output = [f"`{symbol_name}` calls:"]
        for r in results:
            output.append(
                f"→ **{r['name']}** ({r['kind']}) at {r['file_path']}:{r['line']}"
            )
        return "\n".join(output)

# src/cegraph/context/engine.py:906-938
    def _marginal_utility(
        self, symbol_id: str, covered_edges: set[tuple[str, str]]
    ) -> float:
        """Compute marginal coverage gain of adding symbol_id.

        This makes the utility submodular: each new symbol covers some edges
        in the graph. As more symbols are selected, each new symbol covers
        fewer NEW edges → diminishing returns.

        Returns a value in [0, 1] representing the fraction of new edges.
        """
        new_edges = 0
        total_edges = 0

        for succ in self.graph.successors(symbol_id):
            succ_data = self.graph.nodes.get(succ, {})
            if succ_data.get("type") == "symbol":
                total_edges += 1
                edge_key = (symbol_id, succ)
                if edge_key not in covered_edges:
                    new_edges += 1

        for pred in self.graph.predecessors(symbol_id):
            pred_data = self.graph.nodes.get(pred, {})
            if pred_data.get("type") == "symbol":
                total_edges += 1
                edge_key = (pred, symbol_id)
                if edge_key not in covered_edges:
                    new_edges += 1

        if total_edges == 0:
            return 1.0
        return new_edges / total_edges

# examples/demo.py:10-10
from cegraph.graph.query import GraphQuery

# paper/experiments/ablation.py:23-23
from cegraph.graph.query import GraphQuery

# paper/experiments/baselines.py:30-30
from cegraph.graph.query import GraphQuery

# paper/experiments/benchmark.py:57-57
from cegraph.graph.query import GraphQuery

# src/cegraph/context/engine.py:49-49
from cegraph.graph.query import GraphQuery

# src/cegraph/graph/__init__.py:5-5
from cegraph.graph.query import GraphQuery

# src/cegraph/tools/definitions.py:13-13
from cegraph.graph.query import GraphQuery

# tests/test_context.py:17-17
from cegraph.graph.query import GraphQuery

# tests/test_graph.py:11-11
from cegraph.graph.query import GraphQuery

# tests/test_mcp.py:11-11
from cegraph.graph.query import GraphQuery

# tests/test_tools.py:10-10
from cegraph.graph.query import GraphQuery

# src/cegraph/llm/base.py:38-43
class ToolDefinition(BaseModel):
    """Definition of a tool the LLM can call."""

    name: str
    description: str
    parameters: dict[str, Any] = Field(default_factory=dict)

# src/cegraph/graph/query.py:122-160
    def who_calls(self, name: str, max_depth: int = 1) -> list[dict]:
        """Find all callers of a symbol, optionally going N levels deep.

        Returns list of {symbol_id, name, file_path, line, depth}
        """
        symbol_ids = self.find_symbol(name)
        if not symbol_ids:
            return []

        results = []
        visited = set()

        def _traverse(node_id: str, depth: int) -> None:
            if depth > max_depth or node_id in visited:
                return
            visited.add(node_id)

            for pred in self.graph.predecessors(node_id):
                edge_data = self.graph.edges[pred, node_id]
                if edge_data.get("kind") != "calls":
                    continue
                pred_data = self.graph.nodes.get(pred, {})
                if pred_data.get("type") != "symbol":
                    continue

                results.append({
                    "symbol_id": pred,
                    "name": pred_data.get("qualified_name", pred_data.get("name", "")),
                    "kind": pred_data.get("kind", ""),
                    "file_path": pred_data.get("file_path", ""),
                    "line": pred_data.get("line_start", 0),
                    "depth": depth,
                })
                _traverse(pred, depth + 1)

        for sid in symbol_ids:
            _traverse(sid, 1)

        return results

# src/cegraph/parser/models.py:10-22
class SymbolKind(str, Enum):
    """Types of code symbols."""

    MODULE = "module"
    CLASS = "class"
    FUNCTION = "function"
    METHOD = "method"
    VARIABLE = "variable"
    CONSTANT = "constant"
    IMPORT = "import"
    INTERFACE = "interface"
    ENUM = "enum"
    TYPE_ALIAS = "type_alias"

# src/cegraph/graph/builder.py:15-213
class GraphBuilder:
    """Builds and maintains a code knowledge graph.

    The graph has two types of nodes:
    - File nodes: represent source files
    - Symbol nodes: represent code symbols (functions, classes, etc.)

    Edges represent relationships (calls, imports, inherits, contains, etc.)
    """

    def __init__(self) -> None:
        self.graph = nx.DiGraph()
        self._file_hashes: dict[str, str] = {}
        self._unresolved: list[Relationship] = []

    def build_from_directory(
        self,
        root: str | Path,
        config: ProjectConfig | None = None,
        progress_callback: callable | None = None,
    ) -> nx.DiGraph:
        """Build the full knowledge graph from a directory.

        Args:
            root: Root directory to index.
            config: Project configuration.
            progress_callback: Optional callback(file_path, current, total).

        Returns:
            The constructed NetworkX directed graph.
        """
        root = Path(root).resolve()
        indexer_config = config.indexer if config else IndexerConfig()

        # Reset state so reusing a builder doesn't accumulate stale data
        # self.graph = nx.DiGraph()
        # self._file_hashes = {}
        # self._unresolved = []

        # Parse all files
        all_parsed = parse_directory(root, indexer_config, progress_callback)

        # Build graph from parsed results
        for file_symbols in all_parsed:
            self._add_file(file_symbols, root)

        # Resolve cross-file references
        self._resolve_references()

        return self.graph

    def _add_file(self, fs: FileSymbols, root: Path) -> None:
        """Add a file and its symbols to the graph."""
        file_path = fs.file_path

        # Add file node
        self.graph.add_node(
            f"file::{file_path}",
            type="file",
            path=file_path,
            language=fs.language,
            symbol_count=len(fs.symbols),
            import_count=len(fs.imports),
        )

        # Compute file hash for change detection
        try:
            full_path = root / file_path
            content = full_path.read_bytes()
            self._file_hashes[file_path] = hashlib.sha256(content).hexdigest()[:16]
        except OSError:
            pass

        # Add symbol nodes
        for symbol in fs.symbols:
            attrs = {
                "type": "symbol",
                "name": symbol.name,
                "qualified_name": symbol.qualified_name,
                "kind": symbol.kind.value,
                "file_path": symbol.file_path,
                "line_start": symbol.line_start,
                "line_end": symbol.line_end,
                "signature": symbol.signature,
                "docstring": symbol.docstring,
            }
            self.graph.add_node(symbol.id, **attrs)

            # Link symbol to its file
            self.graph.add_edge(
                f"file::{file_path}",
                symbol.id,
                kind="contains",
            )

        # Add relationships
        for rel in fs.relationships:
            if rel.resolved or self._try_resolve(rel):
                self.graph.add_edge(
                    rel.source,
                    rel.target,
                    kind=rel.kind.value,
                    file_path=rel.file_path,
                    line=rel.line,
                )
            else:
                self._unresolved.append(rel)

    def _try_resolve(self, rel: Relationship) -> bool:
        """Try to resolve a relationship's target to an existing node."""
        target = rel.target

        # Direct match
        if self.graph.has_node(target):
            return True

        # Try finding by name across all files
        for node_id, data in self.graph.nodes(data=True):
            if data.get("type") != "symbol":
                continue
            if data.get("name") == target or data.get("qualified_name") == target:
                rel.target = node_id
                rel.resolved = True
                return True

        return False

    def _resolve_references(self) -> None:
        """Try to resolve all unresolved references after the full graph is built."""
        still_unresolved = []

        # Build a lookup index: name -> [node_ids]
        name_index: dict[str, list[str]] = {}
        for node_id, data in self.graph.nodes(data=True):
            if data.get("type") != "symbol":
                continue
            name = data.get("name", "")
            qname = data.get("qualified_name", "")
            if name:
                name_index.setdefault(name, []).append(node_id)
            if qname and qname != name:
                name_index.setdefault(qname, []).append(node_id)

        for rel in self._unresolved:
            target = rel.target
            # Try exact match by name
            candidates = name_index.get(target, [])

            # Try dotted parts (e.g., "module.func" -> "func")
            if not candidates and "." in target:
                parts = target.split(".")
                candidates = name_index.get(parts[-1], [])

            if candidates:
                # Pick the best candidate (same file first, then any)
                best = None
                for c in candidates:
                    c_data = self.graph.nodes[c]
                    if c_data.get("file_path") == rel.file_path:
                        best = c
                        break
                if best is None:
                    best = candidates[0]

                self.graph.add_edge(
                    rel.source,
                    best,
                    kind=rel.kind.value,
                    file_path=rel.file_path,
                    line=rel.line,
                )
            else:
                still_unresolved.append(rel)

        self._unresolved = still_unresolved

    def get_stats(self) -> dict:
        """Get graph statistics."""
        node_types: dict[str, int] = {}
        edge_types: dict[str, int] = {}

        for _, data in self.graph.nodes(data=True):
            kind = data.get("kind", data.get("type", "unknown"))
            node_types[kind] = node_types.get(kind, 0) + 1

        for _, _, data in self.graph.edges(data=True):
            kind = data.get("kind", "unknown")
            edge_types[kind] = edge_types.get(kind, 0) + 1

        return {
            "total_nodes": self.graph.number_of_nodes(),
            "total_edges": self.graph.number_of_edges(),
            "node_types": node_types,
            "edge_types": edge_types,
            "files": node_types.get("file", 0),
            "functions": node_types.get("function", 0) + node_types.get("method", 0),
            "classes": node_types.get("class", 0),
            "unresolved_refs": len(self._unresolved),
        }

# src/cegraph/parser/models.py:25-36
class RelKind(str, Enum):
    """Types of relationships between symbols."""

    CALLS = "calls"
    IMPORTS = "imports"
    INHERITS = "inherits"
    IMPLEMENTS = "implements"
    USES = "uses"
    CONTAINS = "contains"
    OVERRIDES = "overrides"
    DECORATES = "decorates"
    TYPE_OF = "type_of"

# src/cegraph/context/engine.py:567-630
    def _expand_context_pagerank(
        self, seeds: list[dict], config: dict
    ) -> list[dict]:
        """Expand context via personalized PageRank from seeds.

        Instead of BFS, compute PPR with the seed set as the personalization
        vector.  This gives a global relevance score that accounts for the
        full graph structure, not just local neighborhoods.
        """
        min_score = config["min_score"]

        # Build personalization vector (seed nodes get their scores)
        personalization: dict[str, float] = {}
        for s in seeds:
            personalization[s["symbol_id"]] = s["score"]

        # Ensure all seed nodes exist in graph
        personalization = {
            k: v for k, v in personalization.items()
            if self.graph.has_node(k)
        }
        if not personalization:
            return []

        # Normalize
        total = sum(personalization.values())
        personalization = {k: v / total for k, v in personalization.items()}

        try:
            ppr = nx.pagerank(
                self.graph,
                alpha=0.85,
                personalization=personalization,
                max_iter=100,
                tol=1e-6,
            )
        except nx.NetworkXError:
            return self._expand_context(seeds, config)

        seed_reason = {s["symbol_id"]: s["reason"] for s in seeds}
        candidates: list[dict] = []

        for node_id, score in ppr.items():
            if score < min_score * 0.01:
                continue
            data = self.graph.nodes.get(node_id, {})
            if data.get("type") != "symbol":
                continue

            # Scale PPR scores to be comparable with BFS scores
            scaled_score = score * 100

            reason = seed_reason.get(node_id, "pagerank expansion")
            depth = 0 if node_id in personalization else 1
            candidates.append({
                "symbol_id": node_id,
                "score": scaled_score,
                "depth": depth,
                "reason": reason,
                "via": [],
            })

        candidates.sort(key=lambda x: x["score"], reverse=True)
        return candidates

# src/cegraph/graph/query.py:185-229
    def impact_of(self, name: str, max_depth: int = 3) -> dict:
        """Analyze the impact of changing a symbol.

        Returns a dict with:
        - direct_callers: immediate callers
        - transitive_callers: all callers up to max_depth
        - affected_files: set of files that could be affected
        - risk_score: rough risk assessment (0-1)
        """
        symbol_ids = self.find_symbol(name)
        if not symbol_ids:
            return {
                "symbol": name,
                "found": False,
                "direct_callers": [],
                "transitive_callers": [],
                "affected_files": [],
                "risk_score": 0.0,
            }

        direct = self.who_calls(name, max_depth=1)
        transitive = self.who_calls(name, max_depth=max_depth)

        affected_files = set()
        for item in transitive:
            if item.get("file_path"):
                affected_files.add(item["file_path"])
        # Also include the symbol's own file
        for sid in symbol_ids:
            data = self.graph.nodes.get(sid, {})
            if data.get("file_path"):
                affected_files.add(data["file_path"])

        # Risk score based on impact breadth
        total_files = sum(1 for _, d in self.graph.nodes(data=True) if d.get("type") == "file")
        risk_score = min(len(affected_files) / max(total_files, 1), 1.0)

        return {
            "symbol": name,
            "found": True,
            "direct_callers": direct,
            "transitive_callers": transitive,
            "affected_files": sorted(affected_files),
            "risk_score": round(risk_score, 3),
        }

# src/cegraph/context/engine.py:940-950
    def _update_coverage(
        self, symbol_id: str, covered_edges: set[tuple[str, str]]
    ) -> None:
        """Mark edges incident to symbol_id as covered."""
        for succ in self.graph.successors(symbol_id):
            if self.graph.nodes.get(succ, {}).get("type") == "symbol":
                covered_edges.add((symbol_id, succ))

        for pred in self.graph.predecessors(symbol_id):
            if self.graph.nodes.get(pred, {}).get("type") == "symbol":
                covered_edges.add((pred, symbol_id))

# src/cegraph/context/engine.py:204-272
    def assemble(
        self,
        task: str,
        token_budget: int = 8000,
        strategy: ContextStrategy = ContextStrategy.SMART,
        focus_files: list[str] | None = None,
    ) -> ContextPackage:
        """Assemble a budgeted context package for a given task.

        Args:
            task: Natural language description of the task.
            token_budget: Maximum tokens to include (budget B).
            strategy: How aggressively to expand context.
            focus_files: Optional list of files to prioritize.

        Returns:
            A ContextPackage with the selected symbols and their source code,
            ordered by dependency (definitions before usage).
        """
        start_time = time.time()
        config = _STRATEGY_CONFIG[strategy]

        # Phase 1: Extract entities from the task
        entities = self._extract_entities(task)

        # Phase 2: Find seed symbols in the graph
        seeds = self._find_seeds(entities, focus_files)

        # Phase 3: Expand context via graph traversal
        if self.ablation.use_pagerank and seeds:
            candidates = self._expand_context_pagerank(seeds, config)
        elif self._native_graph and seeds:
            candidates = self._expand_context_native(seeds, config)
        else:
            candidates = self._expand_context(seeds, config)

        # Phase 4: Score candidates
        scored = self._score_candidates(candidates, entities, seeds)

        # Phase 5: Compute dependency closures
        closures = self._compute_closures(scored)

        # Phase 6: Budgeted selection with closure constraints
        selected = self._budget_select(scored, closures, token_budget)

        # Phase 7: Load source code
        items = self._load_source(selected)

        # Phase 8: Dependency-safe ordering
        if self.ablation.dependency_ordering:
            items = self._dependency_order(items)

        elapsed_ms = (time.time() - start_time) * 1000
        total_tokens = sum(item.token_estimate for item in items)
        files = set(item.file_path for item in items)

        return ContextPackage(
            task=task,
            strategy=strategy,
            items=items,
            seed_symbols=[s["symbol_id"] for s in seeds],
            total_tokens=total_tokens,
            token_budget=token_budget,
            files_included=len(files),
            symbols_included=len(items),
            symbols_available=len(scored),
            budget_used_pct=round(total_tokens / max(token_budget, 1) * 100, 1),
            assembly_time_ms=round(elapsed_ms, 1),
        )

# src/cegraph/graph/store.py:447-467
    def get_callers(self, symbol_id: str) -> list[dict]:
        """Get all symbols that call the given symbol."""
        conn = self._get_conn()
        target_nid = self._resolve_text_id(conn, symbol_id)
        if target_nid is None:
            return []

        rows = conn.execute(
            """SELECT (sp.path || '::' || s.qualified_name) as id,
                      s.name, s.qualified_name, s.kind,
                      sp.path as file_path, s.line_start, s.line_end,
                      s.signature, s.docstring,
                      e.line as call_line, rp.path as call_file
               FROM edges e
               JOIN nodes s ON s.nid = e.source_nid AND s.node_type = 'symbol'
               JOIN path_map sp ON sp.pid = s.path_id
               LEFT JOIN path_map rp ON rp.pid = e.path_id
               WHERE e.target_nid = ? AND e.kind = 'calls'""",
            (target_nid,),
        ).fetchall()
        return [dict(row) for row in rows]

# paper/experiments/baselines.py:34-45
class BaselineResult:
    """Result of a single baseline run."""

    method: str
    task: str
    budget: int
    tokens_used: int
    symbols_selected: int
    files_included: int
    assembly_time_ms: float
    selected_symbols: list[str] = field(default_factory=list)
    recall: float | None = None

# paper/experiments/ablation.py:27-40
class AblationResult:
    """Result of a single ablation run."""

    config_name: str
    task: str
    budget: int
    symbols_selected: int
    symbols_available: int
    tokens_used: int
    budget_used_pct: float
    assembly_time_ms: float
    selected_symbols: list[str] = field(default_factory=list)
    closure_violations: int = 0
    recall: float | None = None  # Set if ground-truth symbols provided

# paper/experiments/baselines.py:252-343
def baseline_repo_map(
    repo_path: Path,
    task: str,
    budget: int,
    graph,
) -> BaselineResult:
    """Structural summary (file tree + signatures) plus relevant file content.

    Mimics aider's repo-map approach: generate a compact structural overview
    of the repo, then append full content of files matching the query.
    """
    start = time.time()

    # Phase 1: Build structural map (file tree + function/class signatures)
    map_lines: list[str] = []
    files_by_path: dict[str, list[dict]] = {}

    for node_id, data in graph.nodes(data=True):
        if data.get("type") != "symbol":
            continue
        fp = data.get("file_path", "")
        if not fp:
            continue
        files_by_path.setdefault(fp, []).append(data)

    for fp in sorted(files_by_path):
        map_lines.append(fp)
        symbols = sorted(files_by_path[fp], key=lambda d: d.get("line_start", 0))
        for sym in symbols:
            kind = sym.get("kind", "")
            name = sym.get("name", "")
            sig = sym.get("signature", "")
            if kind in ("class", "function", "method"):
                indent = "    " if kind == "method" else "  "
                display = sig if sig else f"{kind} {name}"
                map_lines.append(f"{indent}{display}")

    struct_map = "\n".join(map_lines)
    map_tokens = TokenEstimator.estimate(struct_map)

    # Truncate map if it exceeds budget
    if map_tokens > budget:
        chars_allowed = int(budget * TokenEstimator.CHARS_PER_TOKEN)
        struct_map = struct_map[:chars_allowed]
        map_tokens = budget

    # Phase 2: With remaining budget, add relevant file content
    remaining = budget - map_tokens
    keywords = set(re.findall(r"\b([A-Za-z_]\w{2,})\b", task))
    stop = {"the", "and", "for", "that", "this", "with", "from", "fix", "bug", "add"}
    keywords -= stop

    selected_symbols: list[str] = []
    files_used = set()
    content_tokens = 0

    if remaining > 0:
        for fp in sorted(files_by_path):
            full_path = repo_path / fp
            if not full_path.exists():
                continue
            try:
                content = full_path.read_text(encoding="utf-8", errors="replace")
            except OSError:
                continue

            if not any(kw.lower() in content.lower() for kw in keywords):
                continue

            file_tokens = TokenEstimator.estimate(content)
            if content_tokens + file_tokens > remaining:
                continue

            content_tokens += file_tokens
            files_used.add(fp)
            for sym in files_by_path.get(fp, []):
                qn = sym.get("qualified_name", sym.get("name", ""))
                if qn:
                    selected_symbols.append(qn)

    elapsed = (time.time() - start) * 1000

    return BaselineResult(
        method="repo_map",
        task=task,
        budget=budget,
        tokens_used=map_tokens + content_tokens,
        symbols_selected=len(selected_symbols),
        files_included=len(files_used),
        assembly_time_ms=round(elapsed, 1),
        selected_symbols=selected_symbols,
    )

# src/cegraph/graph/store.py:469-489
    def get_callees(self, symbol_id: str) -> list[dict]:
        """Get all symbols called by the given symbol."""
        conn = self._get_conn()
        source_nid = self._resolve_text_id(conn, symbol_id)
        if source_nid is None:
            return []

        rows = conn.execute(
            """SELECT (sp.path || '::' || s.qualified_name) as id,
                      s.name, s.qualified_name, s.kind,
                      sp.path as file_path, s.line_start, s.line_end,
                      s.signature, s.docstring,
                      e.line as call_line, rp.path as call_file
               FROM edges e
               JOIN nodes s ON s.nid = e.target_nid AND s.node_type = 'symbol'
               JOIN path_map sp ON sp.pid = s.path_id
               LEFT JOIN path_map rp ON rp.pid = e.path_id
               WHERE e.source_nid = ? AND e.kind = 'calls'""",
            (source_nid,),
        ).fetchall()
        return [dict(row) for row in rows]

# src/cegraph/cli.py:224-235
def impact(symbol_name: str, path: str | None):
    """Analyze the blast radius of changing a symbol."""
    root = _get_project_root(path)
    graph, store = _load_graph(root)

    from cegraph.graph.query import GraphQuery

    query = GraphQuery(graph, store)
    result = query.impact_of(symbol_name)
    console.show_impact(result)

    store.close()

# paper/experiments/baselines.py:118-204
def baseline_bm25(
    repo_path: Path,
    task: str,
    budget: int,
    graph,
) -> BaselineResult:
    """BM25 scoring over symbols, greedy packing by score."""
    start = time.time()

    query_terms = re.findall(r"\b([A-Za-z_]\w{2,})\b", task.lower())
    query_tf = Counter(query_terms)

    # Collect symbol documents
    symbols: list[dict] = []
    for node_id, data in graph.nodes(data=True):
        if data.get("type") != "symbol":
            continue
        name = data.get("name", "")
        qname = data.get("qualified_name", "")
        doc = data.get("docstring", "")
        text = f"{name} {qname} {doc}".lower()
        symbols.append({"id": node_id, "text": text, "data": data})

    n = len(symbols)
    if n == 0:
        return BaselineResult(
            method="bm25", task=task, budget=budget,
            tokens_used=0, symbols_selected=0, files_included=0,
            assembly_time_ms=0,
        )

    # IDF
    doc_freq: Counter[str] = Counter()
    for sym in symbols:
        terms_in_doc = set(re.findall(r"\b\w+\b", sym["text"]))
        for t in terms_in_doc:
            doc_freq[t] += 1

    k1 = 1.5
    b = 0.75
    avg_dl = sum(len(s["text"]) for s in symbols) / n

    # Score each symbol
    scored: list[tuple[float, dict]] = []
    for sym in symbols:
        dl = len(sym["text"])
        score = 0.0
        for term, qtf in query_tf.items():
            tf = sym["text"].count(term)
            df = doc_freq.get(term, 0)
            idf = math.log((n - df + 0.5) / (df + 0.5) + 1)
            tf_norm = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / avg_dl))
            score += idf * tf_norm * qtf
        if score > 0:
            scored.append((score, sym))

    scored.sort(key=lambda x: x[0], reverse=True)

    # Greedy packing
    selected: list[str] = []
    total_tokens = 0
    files = set()
    for score, sym in scored:
        line_start = sym["data"].get("line_start", 0)
        line_end = sym["data"].get("line_end", 0)
        line_count = max(1, line_end - line_start + 1)
        cost = TokenEstimator.estimate_lines(line_count)
        if total_tokens + cost > budget:
            continue
        selected.append(sym["data"].get("qualified_name", sym["id"]))
        total_tokens += cost
        fp = sym["data"].get("file_path", "")
        if fp:
            files.add(fp)

    elapsed = (time.time() - start) * 1000

    return BaselineResult(
        method="bm25",
        task=task,
        budget=budget,
        tokens_used=total_tokens,
        symbols_selected=len(selected),
        files_included=len(files),
        assembly_time_ms=round(elapsed, 1),
        selected_symbols=selected,
    )

# src/cegraph/context/models.py:171-173
    def estimate_lines(cls, line_count: int, avg_line_length: int = 40) -> int:
        """Estimate tokens for a given number of lines."""
        return max(1, int(line_count * avg_line_length / cls.CHARS_PER_TOKEN))