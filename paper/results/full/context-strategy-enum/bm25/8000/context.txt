# src/cegraph/context/models.py:15-15
    SMART = "balanced"  # Graph-expanded with relevance scoring (default)

# paper/experiments/ablation.py:21-21
from cegraph.context.models import ContextStrategy

# paper/experiments/ablation.py:21-21
from cegraph.context.models import ContextStrategy

# paper/experiments/ablation.py:21-21
from cegraph.context.models import ContextStrategy

# paper/experiments/ablation.py:21-21
from cegraph.context.models import ContextStrategy

# paper/experiments/ablation.py:21-21
from cegraph.context.models import ContextStrategy

# paper/experiments/ablation.py:21-21
from cegraph.context.models import ContextStrategy

# tests/test_context.py:48-52
class TestContextStrategy:
    def test_strategies_exist(self):
        assert ContextStrategy.PRECISE.value == "precise"
        assert ContextStrategy.SMART.value == "smart"
        assert ContextStrategy.THOROUGH.value == "thorough"

# paper/experiments/ablation.py:21-21
from cegraph.context.models import ContextStrategy

# src/cegraph/context/models.py:14-14
    PRECISE = "precise"  # Only directly relevant symbols

# src/cegraph/context/models.py:16-16
    THOROUGH = "thorough"  # Deep expansion, all related code

# src/cegraph/graph/builder.py:5-5
import hashlib

# tests/test_context.py:49-52
    def test_strategies_exist(self):
        assert ContextStrategy.PRECISE.value == "precise"
        assert ContextStrategy.SMART.value == "smart"
        assert ContextStrategy.THOROUGH.value == "thorough"

# src/cegraph/context/engine.py:90-109
_STRATEGY_CONFIG = {
    ContextStrategy.PRECISE: {
        "max_depth": 1,
        "min_score": 0.3,
        "include_callers": True,
        "include_callees": True,
    },
    ContextStrategy.SMART: {
        "max_depth": 3,
        "min_score": 0.1,
        "include_callers": True,
        "include_callees": True,
    },
    ContextStrategy.THOROUGH: {
        "max_depth": 5,
        "min_score": 0.05,
        "include_callers": True,
        "include_callees": True,
    },
}

# src/cegraph/cli.py:14-22
from cegraph.config import (
    GRAPH_DB_FILE,
    ProjectConfig,
    find_project_root,
    get_cegraph_dir,
    load_config,
    save_config,
    set_config_value,
)

# src/cegraph/cli.py:14-22
from cegraph.config import (
    GRAPH_DB_FILE,
    ProjectConfig,
    find_project_root,
    get_cegraph_dir,
    load_config,
    save_config,
    set_config_value,
)

# src/cegraph/llm/base.py:55-56
    def has_tool_calls(self) -> bool:
        return len(self.tool_calls) > 0

# src/cegraph/agent/loop.py:185-191
    async def ask(self, question: str) -> str:
        """Simple Q&A mode - ask a question about the codebase.

        Returns just the answer string.
        """
        result = await self.run(question)
        return result.answer

# src/cegraph/parser/python_parser.py:35-45
def _get_docstring(node: ast.AST) -> str:
    """Extract docstring from a node if present."""
    if (
        isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module))
        and node.body
        and isinstance(node.body[0], ast.Expr)
        and isinstance(node.body[0].value, (ast.Constant,))
        and isinstance(node.body[0].value.value, str)
    ):
        return node.body[0].value.value.strip()
    return ""

# tests/test_cli.py:19-24
def indexed_project(tmp_project: Path) -> Path:
    """Create a tmp_project that has been indexed."""
    runner = CliRunner()
    result = runner.invoke(main, ["init", "--path", str(tmp_project)])
    assert result.exit_code == 0, f"Init failed: {result.output}"
    return tmp_project

# tests/test_config.py:50-53
    def test_set_config_value(self):
        config = ProjectConfig()
        updated = set_config_value(config, "llm.provider", "openai")
        assert updated.llm.provider == "openai"

# src/cegraph/context/models.py:54-102
    def render(self, include_line_numbers: bool = True, include_metadata: bool = True) -> str:
        """Render the context package as a string for LLM consumption.

        This is the key output - a carefully structured text that gives the LLM
        exactly what it needs to understand the relevant code.
        """
        sections: list[str] = []

        if include_metadata:
            sections.append(f"# Codebase Context for: {self.task}")
            sections.append(
                f"# {self.symbols_included} symbols from {self.files_included} files "
                f"(~{self.total_tokens:,} tokens, {self.budget_used_pct:.0f}% of budget)"
            )
            sections.append("")

        # Group items by file
        by_file: dict[str, list[ContextItem]] = {}
        for item in self.items:
            by_file.setdefault(item.file_path, []).append(item)

        for file_path, items in sorted(by_file.items()):
            sections.append(f"## {file_path}")
            if include_metadata:
                reasons = set(i.reason for i in items if i.reason)
                if reasons:
                    sections.append(f"# Included because: {'; '.join(reasons)}")
            sections.append("")

            # Sort items by line number
            items.sort(key=lambda x: x.line_start)

            for item in items:
                if include_metadata:
                    sections.append(
                        f"# [{item.kind}] {item.qualified_name} "
                        f"(relevance: {item.relevance_score:.2f}, depth: {item.depth})"
                    )

                if include_line_numbers:
                    lines = item.source_code.splitlines()
                    for i, line in enumerate(lines):
                        sections.append(f"{item.line_start + i:4d} | {line}")
                else:
                    sections.append(item.source_code)

                sections.append("")

        return "\n".join(sections)

# src/cegraph/cli.py:14-22
from cegraph.config import (
    GRAPH_DB_FILE,
    ProjectConfig,
    find_project_root,
    get_cegraph_dir,
    load_config,
    save_config,
    set_config_value,
)

# tests/test_mcp.py:93-101
    def test_search_code(self, mcp_server: MCPServer):
        """Test the search_code tool."""
        result = mcp_server._dispatch("tools/call", {
            "name": "search_code",
            "arguments": {"query": "main"},
        })
        assert not result.get("isError")
        content = result["content"][0]["text"]
        assert "main" in content

# tests/test_parser.py:100-104
    def test_parse_docstrings(self, sample_python_source: str):
        result = parse_python_file("sample.py", sample_python_source)
        funcs = {s.name: s for s in result.symbols if s.kind in (SymbolKind.FUNCTION, SymbolKind.METHOD)}
        assert "Factory function" in funcs["create_processor"].docstring
        assert "Transform a single item" in funcs["_transform"].docstring

# src/cegraph/tools/definitions.py:38-60
    def search_code(self, query: str, file_pattern: str = "", max_results: int = 10) -> str:
        """Search for code in the repository matching a query."""
        results = self.search.search(query, file_pattern, max_results=max_results)
        if not results:
            return f"No results found for '{query}'"

        output = []
        for r in results:
            header = f"**{r.file_path}:{r.line_number}**"
            if r.symbol_name:
                header += f" (in `{r.symbol_name}`)"
            output.append(header)

            if r.context_before:
                for line in r.context_before:
                    output.append(f"  {line}")
            output.append(f"→ {r.line_content}")
            if r.context_after:
                for line in r.context_after:
                    output.append(f"  {line}")
            output.append("")

        return "\n".join(output)

# examples/demo.py:14-70
def main():
    # Point at any code directory
    project_root = Path(".")

    # 1. Build the knowledge graph
    print("Building knowledge graph...")
    builder = GraphBuilder()
    graph = builder.build_from_directory(project_root)

    stats = builder.get_stats()
    print(f"  Files: {stats['files']}")
    print(f"  Functions/Methods: {stats['functions']}")
    print(f"  Classes: {stats['classes']}")
    print(f"  Total nodes: {stats['total_nodes']}")
    print(f"  Total edges: {stats['total_edges']}")

    # 2. Query the graph
    query = GraphQuery(graph)

    # Find a symbol
    print("\n--- Finding 'GraphBuilder' ---")
    symbol_ids = query.find_symbol("GraphBuilder")
    for sid in symbol_ids:
        info = query.get_symbol_info(sid)
        if info:
            print(f"  {info.qualified_name} ({info.kind}) at {info.file_path}:{info.line_start}")
            print(f"  Callers: {len(info.callers)}")
            print(f"  Callees: {len(info.callees)}")

    # Who calls a function?
    print("\n--- Who calls 'parse_file'? ---")
    callers = query.who_calls("parse_file", max_depth=2)
    for c in callers:
        indent = "  " * c["depth"]
        print(f"{indent}  {c['name']} at {c['file_path']}:{c['line']}")

    # Impact analysis
    print("\n--- Impact of changing 'GraphQuery' ---")
    impact = query.impact_of("GraphQuery")
    print(f"  Risk score: {impact['risk_score']:.1%}")
    print(f"  Direct callers: {len(impact['direct_callers'])}")
    print(f"  Affected files: {len(impact['affected_files'])}")
    for f in impact["affected_files"]:
        print(f"    - {f}")

    # 3. Search code
    search = HybridSearch(project_root, graph)

    print("\n--- Searching for 'knowledge graph' ---")
    results = search.search("knowledge graph", max_results=5)
    for r in results:
        print(f"  {r.file_path}:{r.line_number} - {r.line_content.strip()[:80]}")

    print("\n--- Searching for class definitions ---")
    symbols = search.search_symbols("", kind="class", max_results=10)
    for s in symbols:
        print(f"  {s['qualified_name']} at {s['file_path']}:{s['line']}")

# src/cegraph/ui/console.py:49-51
    def code(self, text: str, language: str = "python") -> None:
        """Render syntax-highlighted code."""
        self.console.print(Syntax(text, language, theme="monokai", line_numbers=True))

# src/cegraph/cli.py:107-140
def _do_index(root: Path, config: ProjectConfig):
    """Index the codebase and build the knowledge graph."""
    from cegraph.graph.builder import GraphBuilder
    from cegraph.graph.store import GraphStore

    builder = GraphBuilder()

    console.info("Scanning and parsing source files...")
    start_time = time.time()

    with console.indexing_progress() as progress:
        task = progress.add_task("Indexing...", total=None)
        file_count = 0

        def on_progress(file_path: str, current: int, total: int):
            nonlocal file_count
            file_count = total
            progress.update(task, total=total, completed=current, description=f"Parsing {file_path}")

        graph = builder.build_from_directory(root, config, on_progress)

    elapsed = time.time() - start_time
    stats = builder.get_stats()

    console.success(f"Indexed {stats.get('files', 0)} files in {elapsed:.1f}s")
    console.show_stats(stats)

    # Persist the graph
    db_path = get_cegraph_dir(root) / GRAPH_DB_FILE
    store = GraphStore(db_path)
    store.save(graph, metadata={"stats": stats, "root": str(root)})
    store.close()

    console.success(f"Knowledge graph saved to .cegraph/")

# src/cegraph/map/app.py:397-401
def launch_map(root: Path, graph, query) -> None:
    """Launch the Live Code Map TUI."""
    check_textual()
    app = CodeMapApp(root=root, graph=graph, query=query)
    app.run()

# src/cegraph/tools/registry.py:42-58
    async def execute(self, name: str, arguments: dict[str, Any]) -> str:
        """Execute a tool by name with the given arguments.

        Returns the result as a string.
        """
        func = self._tools.get(name)
        if func is None:
            return f"Error: Unknown tool '{name}'"

        try:
            if inspect.iscoroutinefunction(func):
                result = await func(**arguments)
            else:
                result = func(**arguments)
            return str(result) if result is not None else "Done."
        except Exception as e:
            return f"Error executing tool '{name}': {e}"

# src/cegraph/github/diff_parser.py:47-56
class ChangedSymbol:
    """A symbol that was affected by the diff."""
    name: str
    qualified_name: str
    kind: str
    file_path: str
    line_start: int
    line_end: int
    change_type: str  # 'modified', 'added', 'deleted'
    lines_changed: int = 0

# src/cegraph/cli.py:168-195
def search(query: str, path: str | None, kind: str):
    """Search for code or symbols in the repository."""
    root = _get_project_root(path)
    graph, store = _load_graph(root)

    from cegraph.search.hybrid import HybridSearch

    search_engine = HybridSearch(root, graph)

    # Try symbol search first
    symbol_results = search_engine.search_symbols(query, kind=kind)
    if symbol_results:
        console.info(f"Found {len(symbol_results)} symbol(s) matching '{query}':")
        console.show_search_results(symbol_results)
    else:
        console.info(f"No symbol definitions found for '{query}', searching code...")
        code_results = search_engine.search(query)
        if code_results:
            for r in code_results:
                console.console.print(
                    f"  [cyan]{r.file_path}:{r.line_number}[/cyan] {r.line_content.strip()}"
                )
                if r.symbol_name:
                    console.console.print(f"    [dim]in {r.symbol_name}[/dim]")
        else:
            console.warning(f"No results found for '{query}'")

    store.close()

# src/cegraph/mcp/server.py:250-267
    def _tool_search_code(self, args: dict) -> str:
        self._ensure_graph()
        results = self._query.find_symbol(args["query"])
        if not results:
            return f"No symbols found matching '{args['query']}'"

        lines = []
        for sid in results[:20]:
            data = self._graph.nodes.get(sid, {})
            name = data.get("name", sid)
            kind = data.get("kind", "")
            fp = data.get("file_path", "")
            line = data.get("line_start", 0)
            if args.get("kind") and kind != args["kind"]:
                continue
            lines.append(f"  {name} ({kind}) at {fp}:{line}")

        return "\n".join(lines) if lines else f"No symbols found matching '{args['query']}'"

# src/cegraph/graph/store.py:491-497
    def get_metadata(self, key: str) -> str | None:
        """Get a metadata value."""
        conn = self._get_conn()
        row = conn.execute("SELECT value FROM metadata WHERE key = ?", (key,)).fetchone()
        if row:
            return json.loads(row["value"])
        return None

# tests/test_tools.py:23-27
    def test_search_code(self, tmp_project: Path):
        tools = self._build_tools(tmp_project)
        result = tools.search_code("calculate_total")
        assert "calculate_total" in result
        assert "No results" not in result

# src/cegraph/graph/store.py:447-467
    def get_callers(self, symbol_id: str) -> list[dict]:
        """Get all symbols that call the given symbol."""
        conn = self._get_conn()
        target_nid = self._resolve_text_id(conn, symbol_id)
        if target_nid is None:
            return []

        rows = conn.execute(
            """SELECT (sp.path || '::' || s.qualified_name) as id,
                      s.name, s.qualified_name, s.kind,
                      sp.path as file_path, s.line_start, s.line_end,
                      s.signature, s.docstring,
                      e.line as call_line, rp.path as call_file
               FROM edges e
               JOIN nodes s ON s.nid = e.source_nid AND s.node_type = 'symbol'
               JOIN path_map sp ON sp.pid = s.path_id
               LEFT JOIN path_map rp ON rp.pid = e.path_id
               WHERE e.target_nid = ? AND e.kind = 'calls'""",
            (target_nid,),
        ).fetchall()
        return [dict(row) for row in rows]

# src/cegraph/cli.py:504-510
def ask(question: str, path: str | None):
    """Ask a question about the codebase (uses LLM + knowledge graph)."""
    root = _get_project_root(path)
    config = load_config(root)
    graph, store = _load_graph(root)

    _run_agent(root, config, graph, store, question, agent_mode=False)

# src/cegraph/cli.py:71-95
def init(path: str | None, provider: str | None, model: str | None):
    """Initialize CeGraph for a repository. Indexes the codebase and builds the knowledge graph."""
    root = Path(path or ".").resolve()
    if not root.exists():
        console.error(f"Path does not exist: {root}")
        sys.exit(1)

    console.banner()
    console.info(f"Initializing CeGraph for: {root}")

    # Create/load config
    config = load_config(root)
    config.name = root.name
    config.root_path = str(root)

    if provider:
        config.llm.provider = provider
    if model:
        config.llm.model = model

    save_config(root, config)
    console.success("Configuration saved")

    # Build the knowledge graph
    _do_index(root, config)

# src/cegraph/agent/loop.py:18-18
from cegraph.tools.registry import ToolRegistry

# src/cegraph/context/models.py:166-168
    def estimate(cls, text: str) -> int:
        """Estimate token count for a string."""
        return max(1, int(len(text) / cls.CHARS_PER_TOKEN))

# examples/demo.py:9-9
from cegraph.graph.builder import GraphBuilder

# src/cegraph/parser/python_parser.py:305-316
def _node_to_name(node: ast.AST) -> str:
    """Convert an AST node to a dotted name string."""
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        parent = _node_to_name(node.value)
        if parent:
            return f"{parent}.{node.attr}"
        return node.attr
    elif isinstance(node, ast.Subscript):
        return _node_to_name(node.value)
    return ""

# paper/experiments/benchmark.py:837-976
async def run_benchmark(
    tasks: list[EvalTask],
    budgets: list[int],
    methods: list[str],
    llm_config: LLMConfig,
    output_dir: Path,
) -> list[EvalResult]:
    """Run the full benchmark: mutate → context → LLM → patch → test → result.

    For each task with a mutation:
      1. Apply mutation to create buggy code
      2. Build graph from buggy code
      3. Assemble context from buggy code using each method
      4. Send context + bug description to LLM
      5. Apply LLM's patch to a temp copy of the buggy code
      6. Run tests — pass means the LLM fixed the bug
      7. Restore original code
    """
    provider = create_provider(llm_config)
    output_dir.mkdir(parents=True, exist_ok=True)

    all_results: list[EvalResult] = []
    total_runs = len(tasks) * len(budgets) * len(methods)
    run_idx = 0

    for task in tasks:
        repo_path = Path(task.repo_path).resolve()

        print(f"\n{'='*60}")
        print(f"Task: {task.task_id}")
        print(f"Repo: {repo_path}")
        print(f"Description: {task.description[:80]}")
        print(f"{'='*60}")

        # Apply mutation to create buggy code
        original_content = None
        if task.mutation:
            original_content = _apply_mutation(repo_path, task.mutation)
            if original_content is None:
                print("  WARNING: mutation could not be applied, skipping task")
                continue
            print(f"  Mutation applied: {task.mutation['file']}")

        try:
            # Build graph from (possibly mutated) code
            builder = GraphBuilder()
            graph = builder.build_from_directory(repo_path)
            query = GraphQuery(graph)

            for budget in budgets:
                for method_name in methods:
                    run_idx += 1
                    print(f"\n  [{run_idx}/{total_runs}] {method_name} @ B={budget}")

                    method_fn = METHODS[method_name]

                    # 1. Assemble context from buggy code
                    try:
                        context, tokens_used, syms, files, asm_time = method_fn(
                            repo_path, task.description, budget, graph, query,
                        )
                    except Exception as e:
                        print(f"    assembly error: {e}")
                        all_results.append(EvalResult(
                            task_id=task.task_id, method=method_name, budget=budget,
                            tokens_used=0, symbols_selected=0, files_included=0,
                            assembly_time_ms=0, llm_time_ms=0,
                            llm_input_tokens=0, llm_output_tokens=0,
                            tests_passed=False, test_output="", patch="",
                            error=str(e),
                        ))
                        continue

                    print(f"    context: {tokens_used} tokens, {syms} symbols, {files} files ({asm_time}ms)")

                    # 2. Call LLM
                    try:
                        llm_response, llm_time, in_tok, out_tok = await call_llm(
                            provider, context, task.description,
                        )
                    except Exception as e:
                        print(f"    LLM error: {e}")
                        all_results.append(EvalResult(
                            task_id=task.task_id, method=method_name, budget=budget,
                            tokens_used=tokens_used, symbols_selected=syms,
                            files_included=files, assembly_time_ms=asm_time,
                            llm_time_ms=0, llm_input_tokens=0, llm_output_tokens=0,
                            tests_passed=False, test_output="", patch="",
                            error=str(e),
                        ))
                        continue

                    print(f"    LLM: {in_tok} in, {out_tok} out ({llm_time}ms)")

                    # 3. Extract edits
                    edits = extract_edits(llm_response)
                    patch = extract_patch(llm_response)
                    print(f"    edits: {len(edits)} blocks, {len(patch)} chars")

                    # 4. Apply edits to buggy code and test
                    passed, test_output = apply_and_test(
                        repo_path, llm_response, task.test_cmd, task.timeout,
                    )
                    status = "PASS" if passed else "FAIL"
                    print(f"    result: {status}")

                    result = EvalResult(
                        task_id=task.task_id,
                        method=method_name,
                        budget=budget,
                        tokens_used=tokens_used,
                        symbols_selected=syms,
                        files_included=files,
                        assembly_time_ms=asm_time,
                        llm_time_ms=llm_time,
                        llm_input_tokens=in_tok,
                        llm_output_tokens=out_tok,
                        tests_passed=passed,
                        test_output=test_output[-500:],
                        patch=patch[:2000],
                    )
                    all_results.append(result)

                    # Save per-run artifact
                    artifact_dir = output_dir / task.task_id / method_name / str(budget)
                    artifact_dir.mkdir(parents=True, exist_ok=True)
                    (artifact_dir / "context.txt").write_text(context[:50000])
                    (artifact_dir / "llm_response.txt").write_text(llm_response)
                    (artifact_dir / "patch.diff").write_text(patch)
                    (artifact_dir / "test_output.txt").write_text(test_output)
                    (artifact_dir / "result.json").write_text(
                        json.dumps(asdict(result), indent=2)
                    )
        finally:
            # Always restore original code
            if original_content is not None:
                _restore_mutation(repo_path, task.mutation, original_content)
                print(f"\n  Mutation restored: {task.mutation['file']}")

    return all_results

# tests/test_context.py:198-206
    def test_assemble_respects_budget(self, cag_engine: ContextAssembler):
        """Test that assembly respects the token budget."""
        small_budget = 100
        package = cag_engine.assemble(
            task="review the entire codebase",
            token_budget=small_budget,
            strategy=ContextStrategy.THOROUGH,
        )
        assert package.total_tokens <= small_budget * 1.5  # Allow small overshoot

# src/cegraph/exceptions.py:12-13
class ParserError(CeGraphError):
    """Code parsing errors."""

# tests/test_context.py:297-300
    def test_assembly_time_tracked(self, cag_engine: ContextAssembler):
        """Test that assembly time is tracked."""
        package = cag_engine.assemble("fix main", token_budget=8000)
        assert package.assembly_time_ms >= 0