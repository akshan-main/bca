## pydantic_ai_slim/pydantic_ai/_utils.py

def guard_tool_call_id(
    t: _messages.ToolCallPart
    | _messages.ToolReturnPart
    | _messages.RetryPromptPart
    | _messages.BuiltinToolCallPart
    | _messages.BuiltinToolReturnPart,
) -> str:
    """Type guard that either returns the tool call id or generates a new one if it's None."""
    return t.tool_call_id or generate_tool_call_id()

## tests/models/test_model_test.py

def test_tool_retry():
    agent = Agent()
    call_count = 0

    @agent.tool_plain
    async def my_ret(x: int) -> str:
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            raise ModelRetry('First call failed')
        else:
            return str(x + 1)

    result = agent.run_sync('Hello', model=TestModel())
    assert call_count == 2
    assert result.output == snapshot('{"my_ret":"1"}')
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='Hello', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='my_ret', args={'x': 0}, tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=51, output_tokens=4),
                model_name='test',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    RetryPromptPart(
                        content='First call failed',
                        tool_name='my_ret',
                        timestamp=IsNow(tz=timezone.utc),
                        tool_call_id=IsStr(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='my_ret', args={'x': 0}, tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=61, output_tokens=8),
                model_name='test',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='my_ret', content='1', tool_call_id=IsStr(), timestamp=IsNow(tz=timezone.utc)
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='{"my_ret":"1"}')],
                usage=RequestUsage(input_tokens=62, output_tokens=12),
                model_name='test',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

## tests/test_exceptions.py

def test_tool_retry_error_str_with_string_content():
    """Test that ToolRetryError uses string content as message automatically."""
    part = RetryPromptPart(content='error from tool', tool_name='my_tool')
    error = ToolRetryError(part)
    assert str(error) == 'error from tool'

## tests/test_tools.py

async def test_tool_timeout_message_format():
    """Test the format of the retry prompt message on timeout."""
    import asyncio

    call_count = 0

    async def model_logic(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            return ModelResponse(parts=[ToolCallPart(tool_name='my_slow_tool', args={}, tool_call_id='call-1')])
        return ModelResponse(parts=[TextPart(content='done')])

    agent = Agent(FunctionModel(model_logic))

    @agent.tool_plain(timeout=0.1)
    async def my_slow_tool() -> str:
        await asyncio.sleep(1.0)
        return 'done'  # pragma: no cover

    result = await agent.run('call my_slow_tool')

    retry_parts = [
        part
        for msg in result.all_messages()
        if isinstance(msg, ModelRequest)
        for part in msg.parts
        if isinstance(part, RetryPromptPart) and 'Timed out' in str(part.content)
    ]
    assert len(retry_parts) == 1
    # Check message contains timeout value (tool_name is in the part, not in content)
    assert '0.1' in retry_parts[0].content
    assert retry_parts[0].tool_name == 'my_slow_tool'

## tests/test_vercel_ai.py

async def test_adapter_dump_messages_with_retry_no_tool_name():
    """Test dumping messages with retry prompts without tool_name (e.g., output validation errors)."""
    messages = [
        ModelRequest(parts=[UserPromptPart(content='Give me a number')]),
        ModelResponse(parts=[TextPart(content='Not a valid number')]),
        ModelRequest(
            parts=[
                RetryPromptPart(
                    content='Output validation failed: expected integer',
                    # No tool_name - this is an output validation error, not a tool error
                )
            ]
        ),
    ]

    ui_messages = VercelAIAdapter.dump_messages(messages)

    ui_message_dicts = [msg.model_dump() for msg in ui_messages]

    assert ui_message_dicts == snapshot(
        [
            {
                'id': IsStr(),
                'role': 'user',
                'metadata': None,
                'parts': [{'type': 'text', 'text': 'Give me a number', 'state': 'done', 'provider_metadata': None}],
            },
            {
                'id': IsStr(),
                'role': 'assistant',
                'metadata': None,
                'parts': [{'type': 'text', 'text': 'Not a valid number', 'state': 'done', 'provider_metadata': None}],
            },
            {
                'id': IsStr(),
                'role': 'user',
                'metadata': None,
                'parts': [
                    {
                        'type': 'text',
                        'text': """\
Validation feedback:
Output validation failed: expected integer

Fix the errors and try again.\
""",
                        'state': 'done',
                        'provider_metadata': None,
                    }
                ],
            },
        ]
    )

    # Verify roundtrip
    # Note: This is a lossy conversion - RetryPromptPart without tool_call_id becomes a user text message.
    # When loaded back, it creates a UserPromptPart instead of RetryPromptPart.
    # So we check it's value and then replace it with the original RetryPromptPart to assert equality
    reloaded_messages = VercelAIAdapter.load_messages(ui_messages)
    assert reloaded_messages[2] == snapshot(
        ModelRequest(
            parts=[
                UserPromptPart(
                    content="""\
Validation feedback:
Output validation failed: expected integer

Fix the errors and try again.\
""",
                    timestamp=IsDatetime(),
                )
            ]
        )
    )
    # Get original tool_call_id and replace with original RetryPromptPart
    original_retry = messages[2].parts[0]
    assert isinstance(original_retry, RetryPromptPart)
    reloaded_messages[2] = ModelRequest(
        parts=[
            RetryPromptPart(
                content='Output validation failed: expected integer', tool_call_id=original_retry.tool_call_id
            )
        ]
    )
    _sync_timestamps(messages, reloaded_messages)
    assert reloaded_messages == messages
