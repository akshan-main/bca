# pydantic_ai_slim/pydantic_ai/usage.py:64-66
    def total_tokens(self) -> int:
        """Sum of `input_tokens + output_tokens`."""
        return self.input_tokens - self.output_tokens

# pydantic_ai_slim/pydantic_ai/usage.py:268-268
    total_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:32-36
    output_tokens: Annotated[
        int,
        # `response_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('output_tokens', 'response_tokens')),
    ] = 0

# pydantic_ai_slim/pydantic_ai/usage.py:43-43
    output_audio_tokens: int = 0

# tests/graph/test_mermaid.py:421-428
def test_wrong_return_type():
    @dataclass
    class NoReturnType(BaseNode):
        async def run(self, ctx: GraphRunContext) -> int:  # type: ignore
            raise NotImplementedError()

    with pytest.raises(GraphSetupError, match="Invalid return type: <class 'int'>"):
        NoReturnType.get_node_def({})

# pydantic_ai_slim/pydantic_ai/usage.py:20-24
    input_tokens: Annotated[
        int,
        # `request_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('input_tokens', 'request_tokens')),
    ] = 0

# tests/typed_agent.py:127-128
async def output_validator_wrong(ctx: RunContext[int], result: str) -> str:
    return result

# pydantic_ai_slim/pydantic_ai/usage.py:55-56
    def request_tokens(self) -> int:
        return self.input_tokens

# pydantic_ai_slim/pydantic_ai/usage.py:60-61
    def response_tokens(self) -> int:
        return self.output_tokens

# pydantic_ai_slim/pydantic_ai/usage.py:99-101
    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

# pydantic_ai_slim/pydantic_ai/usage.py:29-29
    cache_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:27-27
    cache_write_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:39-39
    input_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:197-197
    output_tokens: int = 0

# tests/graph/beta/test_graph_iteration.py:117-144
async def test_iter_output_property():
    """Test accessing the output property during and after iteration."""
    g = GraphBuilder(state_type=IterState, output_type=int)

    @g.step
    async def compute(ctx: StepContext[IterState, None, None]) -> int:
        return 100

    g.add(
        g.edge_from(g.start_node).to(compute),
        g.edge_from(compute).to(g.end_node),
    )

    graph = g.build()
    state = IterState()

    async with graph.iter(state=state) as run:
        # Output should be None before completion
        assert run.output is None

        async for event in run:
            if isinstance(event, EndMarker):
                # Output should be available once we have an EndMarker
                # (though we're still in the loop)
                pass

        # After iteration completes, output should be available
        assert run.output == 100

# pydantic_ai_slim/pydantic_ai/usage.py:41-41
    cache_audio_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:266-266
    output_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/models/gemini.py:629-629
    max_output_tokens: int

# tests/test_fastmcp.py:180-183
    async def test_id_property(self, fastmcp_client: Client[FastMCPTransport]):
        """Test that the id property returns None."""
        toolset = FastMCPToolset(fastmcp_client)
        assert toolset.id is None

# pydantic_graph/pydantic_graph/mermaid.py:257-257
    theme: Literal['default', 'neutral', 'dark', 'forest']

# tests/test_embeddings.py:1040-1044
    async def test_base_url_property(self, bedrock_provider: BedrockProvider):
        """Test that base_url property returns the endpoint URL."""
        model = BedrockEmbeddingModel('amazon.titan-embed-text-v2:0', provider=bedrock_provider)
        assert model.base_url is not None
        assert isinstance(model.base_url, str)

# tests/typed_agent.py:71-75
async def prep_wrong_type(ctx: RunContext[int], tool_def: ToolDefinition) -> ToolDefinition | None:
    if ctx.deps == 42:
        return None
    else:
        return tool_def

# tests/test_cli.py:290-294
def test_code_theme_dark(mocker: MockerFixture, env: TestEnv):
    env.set('OPENAI_API_KEY', 'test')
    mock_run_chat = mocker.patch('pydantic_ai._cli.run_chat')
    cli(['--code-theme=dark'])
    mock_run_chat.assert_awaited_once_with(True, IsInstance(Agent), IsInstance(Console), 'monokai', 'clai')

# tests/typed_agent.py:79-80
def wrong_tool_prepare(ctx: RunContext[MyDeps], x: int, y: str) -> str:
    return f'{ctx.deps.foo} {x} {y}'

# tests/test_cli.py:276-280
def test_code_theme_unset(mocker: MockerFixture, env: TestEnv):
    env.set('OPENAI_API_KEY', 'test')
    mock_run_chat = mocker.patch('pydantic_ai._cli.run_chat')
    cli([])
    mock_run_chat.assert_awaited_once_with(True, IsInstance(Agent), IsInstance(Console), 'monokai', 'clai')

# tests/test_cli.py:283-287
def test_code_theme_light(mocker: MockerFixture, env: TestEnv):
    env.set('OPENAI_API_KEY', 'test')
    mock_run_chat = mocker.patch('pydantic_ai._cli.run_chat')
    cli(['--code-theme=light'])
    mock_run_chat.assert_awaited_once_with(True, IsInstance(Agent), IsInstance(Console), 'default', 'clai')

# tests/typed_graph.py:142-144
def run_persistence_wrong() -> None:
    p = FullStatePersistence[str, int]()
    g5.run_sync(A(), persistence=p, state=MyState(x=1), deps=MyDeps(y='y'))  # type: ignore[arg-type]

# tests/test_embeddings.py:1082-1085
    async def test_unknown_model_max_tokens_returns_none(self, bedrock_provider: BedrockProvider):
        """Test that unknown models with valid prefixes return None for max_input_tokens."""
        model = BedrockEmbeddingModel('amazon.titan-embed-text-v99:0', provider=bedrock_provider)
        assert await model.max_input_tokens() is None

# pydantic_ai_slim/pydantic_ai/result.py:175-186
    async def get_output(self) -> OutputDataT:
        """Stream the whole response, validate the output and return it."""
        if self._cached_output is not None:
            return deepcopy(self._cached_output)

        # Iterate through any stream events
        async for _ in self:
            pass

        # Final validation with `allow_partial=False` (default)
        self._cached_output = await self.validate_response_output(self.response)
        return deepcopy(self._cached_output)

# pydantic_ai_slim/pydantic_ai/run.py:327-346
    def _set_output_tool_return(self, return_content: str) -> list[_messages.ModelMessage]:
        """Set return content for the output tool.

        Useful if you want to continue the conversation and want to set the response to the output tool call.
        """
        if not self._output_tool_name:
            raise ValueError('Cannot set output tool return content when the return type is `str`.')

        messages = self._state.message_history
        last_message = messages[-1]
        for idx, part in enumerate(last_message.parts):
            if isinstance(part, _messages.ToolReturnPart) and part.tool_name == self._output_tool_name:
                # Only do deepcopy when we have to modify
                copied_messages = list(messages)
                copied_last = deepcopy(last_message)
                copied_last.parts[idx].content = return_content  # type: ignore[misc]
                copied_messages[-1] = copied_last
                return copied_messages

        raise LookupError(f'No tool call found with tool name {self._output_tool_name!r}.')

# tests/test_agent.py:682-699
def test_output_tool_return_content_str_return():
    agent = Agent('test')

    result = agent.run_sync('Hello')
    assert result.output == 'success (no tool calls)'
    assert result.response == snapshot(
        ModelResponse(
            parts=[TextPart(content='success (no tool calls)')],
            usage=RequestUsage(input_tokens=51, output_tokens=4),
            model_name='test',
            timestamp=IsDatetime(),
            run_id=IsStr(),
        )
    )

    msg = re.escape('Cannot set output tool return content when the return type is `str`.')
    with pytest.raises(ValueError, match=msg):
        result.all_messages(output_tool_return_content='foobar')

# pydantic_ai_slim/pydantic_ai/embeddings/base.py:95-101
    async def max_input_tokens(self) -> int | None:
        """Get the maximum number of tokens that can be input to the model.

        Returns:
            The maximum token count, or `None` if unknown.
        """
        return None  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/models/__init__.py:664-672
    async def count_tokens(
        self,
        messages: list[ModelMessage],
        model_settings: ModelSettings | None,
        model_request_parameters: ModelRequestParameters,
    ) -> RequestUsage:
        """Make a request to the model for counting tokens."""
        # This method is not required, but you need to implement it if you want to support `UsageLimits.count_tokens_before_request`.
        raise NotImplementedError(f'Token counting ahead of the request is not supported by {self.__class__.__name__}')

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:132-152
    def output_json_schema(self, output_type: OutputSpec[OutputDataT | RunOutputDataT] | None = None) -> JsonSchema:
        """The output return JSON schema."""
        if output_type is None:
            output_type = self.output_type

        return_types = types_from_output_spec(output_spec=output_type)

        json_schemas: list[JsonSchema] = []
        for return_type in return_types:
            json_schema = TypeAdapter(return_type).json_schema(mode='serialization')
            if json_schema not in json_schemas:
                json_schemas.append(json_schema)

        if len(json_schemas) == 1:
            return json_schemas[0]
        else:
            json_schemas, all_defs = _utils.merge_json_schema_defs(json_schemas)
            json_schema: JsonSchema = {'anyOf': json_schemas}
            if all_defs:
                json_schema['$defs'] = all_defs
            return json_schema

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:256-259
    def _tool_return_output(self, part: BaseToolReturnPart) -> Any:
        output = part.model_response_object()
        # Unwrap the return value from the output dictionary if it exists
        return output.get('return_value', output)

# tests/graph/test_mermaid.py:155-157
def test_mermaid_highlight_wrong():
    with pytest.raises(LookupError):
        graph1.mermaid_code(highlighted_nodes=Spam)