# pydantic_ai_slim/pydantic_ai/embeddings/wrapper.py:5-5
from typing import TYPE_CHECKING

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# pydantic_ai_slim/pydantic_ai/output.py:3-3
from collections.abc import Awaitable, Callable, Sequence

# tests/test_prefect.py:1003-1003
dynamic_agent = Agent(name='dynamic_agent', model=test_model, deps_type=ToggleableDeps)

# pydantic_evals/pydantic_evals/otel/span_tree.py:172-176
    def find_descendants(
        self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None
    ) -> list[SpanNode]:
        """Return all descendant nodes that satisfy the given predicate in DFS order."""
        return list(self._filter_descendants(predicate, stop_recursing_when))

# pydantic_ai_slim/pydantic_ai/models/groq.py:14-14
from .._output import DEFAULT_OUTPUT_TOOL_NAME, OutputObjectDefinition

# pydantic_graph/pydantic_graph/beta/graph_builder.py:20-20
from pydantic_graph._utils import UNSET, Unset

# pydantic_ai_slim/pydantic_ai/providers/gateway.py:114-114
UpstreamProvider = ModelProvider | APIFlavor

# tests/graph/beta/test_graph_builder.py:255-271
async def test_duplicate_node_ids_error():
    """Test that duplicate node IDs raise a ValueError."""
    g = GraphBuilder(state_type=SimpleState, output_type=int)

    @g.step(node_id='duplicate_id')
    async def step_one(ctx: StepContext[SimpleState, None, None]) -> int:
        return 1  # pragma: no cover

    @g.step(node_id='duplicate_id')
    async def step_two(ctx: StepContext[SimpleState, None, None]) -> int:
        return 2  # pragma: no cover

    with pytest.raises(GraphBuildingError, match='All nodes must have unique node IDs'):
        g.add(
            g.edge_from(g.start_node).to(step_one),
            g.edge_from(g.start_node).to(step_two),
        )

# pydantic_ai_slim/pydantic_ai/output.py:76-133
class ToolOutput(Generic[OutputDataT]):
    """Marker class to use a tool for output and optionally customize the tool.

    Example:
    ```python {title="tool_output.py"}
    from pydantic import BaseModel

    from pydantic_ai import Agent, ToolOutput


    class Fruit(BaseModel):
        name: str
        color: str


    class Vehicle(BaseModel):
        name: str
        wheels: int


    agent = Agent(
        'openai:gpt-5.2',
        output_type=[
            ToolOutput(Fruit, name='return_fruit'),
            ToolOutput(Vehicle, name='return_vehicle'),
        ],
    )
    result = agent.run_sync('What is a banana?')
    print(repr(result.output))
    #> Fruit(name='banana', color='yellow')
    ```
    """

    output: OutputTypeOrFunction[OutputDataT]
    """An output type or function."""
    name: str | None
    """The name of the tool that will be passed to the model. If not specified and only one output is provided, `final_result` will be used. If multiple outputs are provided, the name of the output type or function will be added to the tool name."""
    description: str | None
    """The description of the tool that will be passed to the model. If not specified, the docstring of the output type or function will be used."""
    max_retries: int | None
    """The maximum number of retries for the tool."""
    strict: bool | None
    """Whether to use strict mode for the tool."""

    def __init__(
        self,
        type_: OutputTypeOrFunction[OutputDataT],
        *,
        name: str | None = None,
        description: str | None = None,
        max_retries: int | None = None,
        strict: bool | None = None,
    ):
        self.output = type_
        self.name = name
        self.description = description
        self.max_retries = max_retries
        self.strict = strict

# tests/graph/test_state.py:24-69
async def test_run_graph(mock_snapshot_id: object):
    @dataclass
    class MyState:
        x: int
        y: str

    @dataclass
    class Foo(BaseNode[MyState]):
        async def run(self, ctx: GraphRunContext[MyState]) -> Bar:
            ctx.state.x += 1
            return Bar()

    @dataclass
    class Bar(BaseNode[MyState, None, str]):
        async def run(self, ctx: GraphRunContext[MyState]) -> End[str]:
            ctx.state.y += 'y'
            return End(f'x={ctx.state.x} y={ctx.state.y}')

    graph = Graph(nodes=(Foo, Bar))
    assert graph.inferred_types == (MyState, str)
    state = MyState(1, '')
    sp = FullStatePersistence()
    result = await graph.run(Foo(), state=state, persistence=sp)
    assert result.output == snapshot('x=2 y=y')
    assert sp.history == snapshot(
        [
            NodeSnapshot(
                state=MyState(x=1, y=''),
                node=Foo(),
                start_ts=IsNow(tz=timezone.utc),
                duration=IsFloat(),
                status='success',
                id='Foo:1',
            ),
            NodeSnapshot(
                state=MyState(x=2, y=''),
                node=Bar(),
                start_ts=IsNow(tz=timezone.utc),
                duration=IsFloat(),
                status='success',
                id='Bar:2',
            ),
            EndSnapshot(state=MyState(x=2, y='y'), result=End(data='x=2 y=y'), ts=IsNow(tz=timezone.utc), id='end:3'),
        ]
    )
    assert state == MyState(x=2, y='y')

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:151-151
    _output_schema: _output.OutputSchema[OutputDataT] = dataclasses.field(repr=False)

# pydantic_evals/pydantic_evals/evaluators/_run_evaluator.py:16-23
from .evaluator import (
    EvaluationReason,
    EvaluationResult,
    EvaluationScalar,
    Evaluator,
    EvaluatorFailure,
    EvaluatorOutput,
)

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:528-530
    def timestamp(self) -> datetime:
        """Get the timestamp of the response."""
        return self._timestamp

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_mcp_server.py:35-37
    def tool_for_tool_def(self, tool_def: ToolDefinition) -> ToolsetTool[AgentDepsT]:
        assert isinstance(self.wrapped, MCPServer)
        return self.wrapped.tool_for_tool_def(tool_def)

# tests/graph/beta/test_edge_cases.py:5-5
from dataclasses import dataclass, field

# pydantic_ai_slim/pydantic_ai/messages.py:1181-1183
    def has_content(self) -> bool:
        """Return `True` if the file content is non-empty."""
        return bool(self.content.data)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:834-834
    final_result: result.FinalResult[NodeRunEndT]

# pydantic_ai_slim/pydantic_ai/models/__init__.py:24-24
from .._output import OutputObjectDefinition, StructuredTextOutputSchema

# pydantic_ai_slim/pydantic_ai/models/openai.py:60-60
from ..profiles.openai import SAMPLING_PARAMS, OpenAIModelProfile, OpenAISystemPromptRole

# pydantic_graph/pydantic_graph/beta/graph_builder.py:26-30
from pydantic_graph.beta.node import (
    EndNode,
    Fork,
    StartNode,
)

# tests/models/test_openai.py:1674-1674
    field: MyRecursiveDc | None

# tests/test_agent.py:6064-6113
def test_toolset_factory():
    toolset = FunctionToolset()

    @toolset.tool
    def foo() -> str:
        return 'Hello from foo'

    available_tools: list[str] = []

    async def prepare_tools(ctx: RunContext[None], tool_defs: list[ToolDefinition]) -> list[ToolDefinition]:
        nonlocal available_tools
        available_tools = [tool_def.name for tool_def in tool_defs]
        return tool_defs

    toolset_creation_counts: dict[str, int] = defaultdict(int)

    def via_toolsets_arg(ctx: RunContext[None]) -> AbstractToolset[None]:
        nonlocal toolset_creation_counts
        toolset_creation_counts['via_toolsets_arg'] += 1
        return toolset.prefixed('via_toolsets_arg')

    def respond(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(parts=[ToolCallPart('via_toolsets_arg_foo')])
        elif len(messages) == 3:
            return ModelResponse(parts=[ToolCallPart('via_toolset_decorator_foo')])
        else:
            return ModelResponse(parts=[TextPart('Done')])

    agent = Agent(FunctionModel(respond), toolsets=[via_toolsets_arg], prepare_tools=prepare_tools)

    @agent.toolset
    def via_toolset_decorator(ctx: RunContext[None]) -> AbstractToolset[None]:
        nonlocal toolset_creation_counts
        toolset_creation_counts['via_toolset_decorator'] += 1
        return toolset.prefixed('via_toolset_decorator')

    @agent.toolset(per_run_step=False)
    async def via_toolset_decorator_for_entire_run(ctx: RunContext[None]) -> AbstractToolset[None]:
        nonlocal toolset_creation_counts
        toolset_creation_counts['via_toolset_decorator_for_entire_run'] += 1
        return toolset.prefixed('via_toolset_decorator_for_entire_run')

    run_result = agent.run_sync('Hello')

    assert run_result._state.run_step == 3  # pyright: ignore[reportPrivateUsage]
    assert len(available_tools) == 3
    assert toolset_creation_counts == snapshot(
        defaultdict(int, {'via_toolsets_arg': 3, 'via_toolset_decorator': 3, 'via_toolset_decorator_for_entire_run': 1})
    )

# pydantic_ai_slim/pydantic_ai/usage.py:20-24
    input_tokens: Annotated[
        int,
        # `request_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('input_tokens', 'request_tokens')),
    ] = 0

# tests/graph/beta/test_graph_edge_cases.py:9-9
import pytest

# tests/test_mcp.py:15-26
from pydantic_ai import (
    BinaryContent,
    BinaryImage,
    ModelRequest,
    ModelResponse,
    RetryPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)

# tests/test_agent.py:76-76
from .conftest import IsDatetime, IsNow, IsStr, TestEnv

# examples/pydantic_ai_examples/stream_markdown.py:19-19
from pydantic_ai.models import KnownModelName

# pydantic_ai_slim/pydantic_ai/_run_context.py:79-79
    metadata: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/providers/xai.py:32-33
    def client(self) -> AsyncClient:
        return self._client

# pydantic_evals/pydantic_evals/dataset.py:51-51
from .reporting import EvaluationReport, ReportCase, ReportCaseAggregate, ReportCaseFailure

# tests/test_agent.py:5-5
from collections import defaultdict

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:11-25
from ...messages import (
    BaseToolReturnPart,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    FilePart,
    FinishReason as PydanticFinishReason,
    FunctionToolResultEvent,
    RetryPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
)

# tests/evals/test_dataset.py:819-831
async def test_serialization_to_yaml(example_dataset: Dataset[TaskInput, TaskOutput, TaskMetadata], tmp_path: Path):
    """Test serializing a dataset to YAML."""
    yaml_path = tmp_path / 'test_cases.yaml'
    example_dataset.to_file(yaml_path)

    assert yaml_path.exists()

    # Test loading back
    loaded_dataset = Dataset[TaskInput, TaskOutput, TaskMetadata].from_file(yaml_path)
    assert len(loaded_dataset.cases) == 2
    assert loaded_dataset.name == 'example'
    assert loaded_dataset.cases[0].name == 'case1'
    assert loaded_dataset.cases[0].inputs.query == 'What is 2+2?'

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:1147-1157
    def _map_usage(self, metadata: ConverseStreamMetadataEventTypeDef) -> usage.RequestUsage:
        input_tokens = metadata['usage']['inputTokens']
        output_tokens = metadata['usage']['outputTokens']
        cache_read_tokens = metadata['usage'].get('cacheReadInputTokens', 0)
        cache_write_tokens = metadata['usage'].get('cacheWriteInputTokens', 0)
        return usage.RequestUsage(
            input_tokens=input_tokens + cache_write_tokens + cache_read_tokens,
            output_tokens=output_tokens,
            cache_read_tokens=cache_read_tokens,
            cache_write_tokens=cache_write_tokens,
        )

# pydantic_ai_slim/pydantic_ai/models/gemini.py:822-822
    name: str

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:144-146
    def system(self) -> str:
        """The embedding model provider."""
        return self._provider.name

# tests/models/mock_xai.py:10-10
from collections.abc import Sequence

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:9-9
from .._json_schema import JsonSchema, JsonSchemaTransformer

# pydantic_ai_slim/pydantic_ai/providers/cerebras.py:10-10
from pydantic_ai.models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_dynamic_toolset.py:26-30
class _ToolInfo:
    """Serializable tool information returned from get_tools_activity."""

    tool_def: ToolDefinition
    max_retries: int

# pydantic_ai_slim/pydantic_ai/models/outlines.py:366-375
    def _format_mlxlm_inference_kwargs(  # pragma: no cover
        self, model_settings: dict[str, Any]
    ) -> dict[str, Any]:
        """Select the model settings supported by the MLXLM model."""
        supported_args = [
            'extra_body',
        ]
        filtered_settings = {k: model_settings[k] for k in supported_args if k in model_settings}

        return filtered_settings

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:347-372
    async def _reevaluate_dynamic_prompts(
        self, messages: list[_messages.ModelMessage], run_context: RunContext[DepsT]
    ) -> None:
        """Reevaluate any `SystemPromptPart` with dynamic_ref in the provided messages by running the associated runner function."""
        # Only proceed if there's at least one dynamic runner.
        if self.system_prompt_dynamic_functions:
            for msg in messages:
                if isinstance(msg, _messages.ModelRequest):
                    reevaluated_message_parts: list[_messages.ModelRequestPart] = []
                    for part in msg.parts:
                        if isinstance(part, _messages.SystemPromptPart) and part.dynamic_ref:
                            # Look up the runner by its ref
                            if runner := self.system_prompt_dynamic_functions.get(  # pragma: lax no cover
                                part.dynamic_ref
                            ):
                                # To enable dynamic system prompt refs in future runs, use a placeholder string
                                updated_part_content = await runner.run(run_context)
                                part = _messages.SystemPromptPart(
                                    updated_part_content or '', dynamic_ref=part.dynamic_ref
                                )

                        reevaluated_message_parts.append(part)

                    # Replace message parts with reevaluated ones to prevent mutating parts list
                    if reevaluated_message_parts != msg.parts:
                        msg.parts = reevaluated_message_parts

# tests/test_messages.py:121-124
def test_binary_content_image(media_type: str, format: str):
    binary_content = BinaryContent(data=b'Hello, world!', media_type=media_type)
    assert binary_content.is_image
    assert binary_content.format == format

# pydantic_graph/pydantic_graph/persistence/_utils.py:54-55
def now_utc() -> datetime:
    return datetime.now(tz=timezone.utc)

# pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py:45-45
    _: KW_ONLY

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:11-11
from ..exceptions import ModelRetry, UserError

# pydantic_graph/pydantic_graph/beta/graph_builder.py:24-24
from pydantic_graph.beta.join import Join, JoinNode, ReducerFunction

# tests/models/test_anthropic.py:8219-8257
async def test_anthropic_cache_messages_real_api(allow_model_requests: None, anthropic_api_key: str):
    """Test that anthropic_cache_messages setting adds cache_control and produces cache usage metrics.

    This test uses a cassette to verify the cache behavior without making real API calls in CI.
    When run with real API credentials, it demonstrates that:
    1. The first call with a long context creates a cache (cache_write_tokens > 0)
    2. Follow-up messages in the same conversation can read from that cache (cache_read_tokens > 0)
    """
    m = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(
        m,
        system_prompt='You are a helpful assistant.',
        model_settings=AnthropicModelSettings(
            anthropic_cache_messages=True,
        ),
    )

    # First call with a longer message - this will cache the message content
    result1 = await agent.run('Please explain what Python is and its main use cases. ' * 100)
    usage1 = result1.usage()

    # With anthropic_cache_messages, the first call should write cache for the last message
    # (cache_write_tokens > 0 indicates that caching occurred)
    assert usage1.requests == 1
    assert usage1.cache_write_tokens > 0
    assert usage1.output_tokens > 0

    # Continue the conversation - this message appends to history
    # The previous cached message should still be in the request
    result2 = await agent.run('Can you summarize that in one sentence?', message_history=result1.all_messages())
    usage2 = result2.usage()

    # The second call should potentially read from cache if the previous message is still cached
    # (cache_read_tokens > 0 when cache hit occurs)
    # (cache_write_tokens > 0 as new message is added to cache)
    assert usage2.requests == 1
    assert usage2.cache_read_tokens > 0
    assert usage2.cache_write_tokens > 0
    assert usage2.output_tokens > 0

# pydantic_evals/pydantic_evals/otel/span_tree.py:416-420
    def __str__(self) -> str:
        if self.children:
            return f"<SpanNode name={self.name!r} span_id='{self.span_id:016x}'>...</SpanNode>"
        else:
            return f"<SpanNode name={self.name!r} span_id='{self.span_id:016x}' />"

# tests/test_validation_context.py:7-17
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelResponse,
    NativeOutput,
    PromptedOutput,
    RunContext,
    TextPart,
    ToolCallPart,
    ToolOutput,
)

# pydantic_evals/pydantic_evals/dataset.py:24-24
from typing import TYPE_CHECKING, Any, Generic, Literal, Union, cast

# pydantic_evals/pydantic_evals/otel/span_tree.py:9-9
from typing import TYPE_CHECKING, Any

# pydantic_graph/pydantic_graph/nodes.py:10-10
from typing_extensions import Never, Self, TypeVar

# pydantic_ai_slim/pydantic_ai/messages.py:1297-1297
    parts: Sequence[ModelResponsePart]

# pydantic_evals/pydantic_evals/reporting/__init__.py:169-169
    labels: dict[str, dict[str, float]]

# tests/providers/test_nebius.py:12-12
from pydantic_ai.profiles.meta import meta_model_profile