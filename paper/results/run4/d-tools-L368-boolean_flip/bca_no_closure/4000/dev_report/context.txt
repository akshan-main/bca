## pydantic_ai_slim/pydantic_ai/_otel_messages.py

    id: str

## pydantic_ai_slim/pydantic_ai/agent/__init__.py

T = TypeVar('T')

    _function_toolset: FunctionToolset[AgentDepsT] = dataclasses.field(repr=False)

    def __init__(
        self,
        tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = [],
        *,
        max_retries: int = 1,
        timeout: float | None = None,
        id: str | None = None,
        output_schema: _output.OutputSchema[Any],
    ):
        self.output_schema = output_schema
        super().__init__(tools, max_retries=max_retries, timeout=timeout, id=id)

## tests/test_tools.py

def test_tool_no_ctx():
    agent = Agent(TestModel())

    with pytest.raises(UserError) as exc_info:

        @agent.tool  # pyright: ignore[reportArgumentType]
        def invalid_tool(x: int) -> str:  # pragma: no cover
            return 'Hello'

    assert str(exc_info.value) == snapshot(
        'Error generating schema for test_tool_no_ctx.<locals>.invalid_tool:\n'
        '  First parameter of tools that take context must be annotated with RunContext[...]'
    )

def test_tool_raises_call_deferred():
    agent = Agent(TestModel(), output_type=[str, DeferredToolRequests])

    @agent.tool_plain
    def my_tool(x: int) -> int:
        raise CallDeferred

    result = agent.run_sync('Hello')
    assert result.output == snapshot(
        DeferredToolRequests(calls=[ToolCallPart(tool_name='my_tool', args={'x': 0}, tool_call_id=IsStr())])
    )

def test_tool_raises_approval_required():
    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired
        return x * 42

    result = agent.run_sync('Hello')
    messages = result.all_messages()
    assert result.output == snapshot(
        DeferredToolRequests(approvals=[ToolCallPart(tool_name='my_tool', args={'x': 1}, tool_call_id='my_tool')])
    )

    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(approvals={'my_tool': ToolApproved(override_args={'x': 2})}),
    )
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content='Hello',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='my_tool', args={'x': 1}, tool_call_id='my_tool')],
                usage=RequestUsage(input_tokens=51, output_tokens=4),
                model_name='function:llm:',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='my_tool',
                        content=84,
                        tool_call_id='my_tool',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='Done!')],
                usage=RequestUsage(input_tokens=52, output_tokens=5),
                model_name='function:llm:',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
        ]
    )
    assert result.output == snapshot('Done!')

def test_approval_required_with_metadata():
    """Test that ApprovalRequired exception can carry metadata."""

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired(
                metadata={
                    'reason': 'High compute cost',
                    'estimated_time': '5 minutes',
                    'cost_usd': 100.0,
                }
            )
        return x * 42

    result = agent.run_sync('Hello')
    assert result.output == snapshot(
        DeferredToolRequests(
            approvals=[ToolCallPart(tool_name='my_tool', args={'x': 1}, tool_call_id=IsStr())],
            metadata={'my_tool': {'reason': 'High compute cost', 'estimated_time': '5 minutes', 'cost_usd': 100.0}},
        )
    )

    # Continue with approval
    messages = result.all_messages()
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(approvals={'my_tool': ToolApproved()}),
    )
    assert result.output == 'Done!'

def test_mixed_deferred_tools_with_metadata():
    """Test multiple deferred tools with different metadata."""

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('tool_a', {'x': 1}, tool_call_id='call_a'),
                    ToolCallPart('tool_b', {'y': 2}, tool_call_id='call_b'),
                    ToolCallPart('tool_c', {'z': 3}, tool_call_id='call_c'),
                ]
            )
        else:
            return ModelResponse(parts=[TextPart('Done!')])

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def tool_a(ctx: RunContext[None], x: int) -> int:
        raise CallDeferred(metadata={'type': 'external', 'priority': 'high'})

    @agent.tool
    def tool_b(ctx: RunContext[None], y: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired(metadata={'reason': 'Needs approval', 'level': 'manager'})
        return y * 10

    @agent.tool
    def tool_c(ctx: RunContext[None], z: int) -> int:
        raise CallDeferred  # No metadata

    result = agent.run_sync('Hello')
    assert isinstance(result.output, DeferredToolRequests)

    # Check that we have the right tools deferred
    assert len(result.output.calls) == 2  # tool_a and tool_c
    assert len(result.output.approvals) == 1  # tool_b

    # Check metadata
    assert result.output.metadata['call_a'] == {'type': 'external', 'priority': 'high'}
    assert result.output.metadata['call_b'] == {'reason': 'Needs approval', 'level': 'manager'}
    assert result.output.metadata.get('call_c', {}) == {}

    # Continue with results for all three tools
    messages = result.all_messages()
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(
            calls={'call_a': 10, 'call_c': 30},
            approvals={'call_b': ToolApproved()},
        ),
    )
    assert result.output == 'Done!'

def test_deferred_tool_call_approved_fails():
    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        return ModelResponse(
            parts=[
                ToolCallPart('foo', {'x': 0}, tool_call_id='foo'),
            ]
        )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    async def defer(ctx: RunContext[None], tool_def: ToolDefinition) -> ToolDefinition | None:
        return replace(tool_def, kind='external')

    @agent.tool_plain(prepare=defer)
    def foo(x: int) -> int:
        return x + 1  # pragma: no cover

    result = agent.run_sync('foo')
    assert result.output == snapshot(
        DeferredToolRequests(calls=[ToolCallPart(tool_name='foo', args={'x': 0}, tool_call_id='foo')])
    )

    with pytest.raises(RuntimeError, match='External tools cannot be called'):
        agent.run_sync(
            message_history=result.all_messages(),
            deferred_tool_results=DeferredToolResults(
                approvals={
                    'foo': True,
                },
            ),
        )

def test_retry_tool_until_last_attempt():
    model = TestModel()
    agent = Agent(model, retries=2)

    @agent.tool
    def always_fail(ctx: RunContext[None]) -> str:
        if ctx.last_attempt:
            return 'I guess you never learn'
        else:
            raise ModelRetry('Please try again.')

    result = agent.run_sync('Always fail!')
    assert result.output == snapshot('{"always_fail":"I guess you never learn"}')
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content='Always fail!',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='always_fail', args={}, tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=52, output_tokens=2),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    RetryPromptPart(
                        content='Please try again.',
                        tool_name='always_fail',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='always_fail', args={}, tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=62, output_tokens=4),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    RetryPromptPart(
                        content='Please try again.',
                        tool_name='always_fail',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='always_fail', args={}, tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=72, output_tokens=6),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='always_fail',
                        content='I guess you never learn',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='{"always_fail":"I guess you never learn"}')],
                usage=RequestUsage(input_tokens=77, output_tokens=14),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
        ]
    )

def test_tool_approved_with_metadata_and_override_args():
    """Test that DeferredToolResults.metadata works together with ToolApproved.override_args."""
    received_data: list[tuple[Any, int]] = []

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired()
        # Capture both the metadata and the argument
        received_data.append((ctx.tool_call_metadata, x))
        return x * 42

    # First run: get approval request
    result = agent.run_sync('Hello')
    messages = result.all_messages()
    assert isinstance(result.output, DeferredToolRequests)

    # Second run: provide approval with both metadata and override_args
    approval_metadata = {'approver': 'admin', 'notes': 'LGTM'}
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(
            approvals={
                'my_tool': ToolApproved(
                    override_args={'x': 100},
                )
            },
            metadata={'my_tool': approval_metadata},
        ),
    )

    assert result.output == 'Done!'
    # Verify both metadata and overridden args were received
    assert len(received_data) == 1
    assert received_data[0] == (approval_metadata, 100)

def test_tool_approved_without_metadata():
    """Test that tool_call_metadata is None when DeferredToolResults has no metadata for the tool."""
    received_metadata: list[Any] = []

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired()
        # Capture the tool_call_metadata from context
        received_metadata.append(ctx.tool_call_metadata)
        return x * 42

    # First run: get approval request
    result = agent.run_sync('Hello')
    messages = result.all_messages()

    # Second run: provide approval without metadata (using ToolApproved() or True)
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(approvals={'my_tool': ToolApproved()}),
    )

    assert result.output == 'Done!'
    # Verify the metadata is None
    assert len(received_metadata) == 1
    assert received_metadata[0] is None
