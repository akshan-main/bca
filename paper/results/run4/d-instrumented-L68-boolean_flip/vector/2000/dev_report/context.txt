# tests/test_direct.py:216-229
def test_prepare_model():
    with set_instrument_default(False):
        model = _prepare_model('test', None)
        assert isinstance(model, TestModel)

        model = _prepare_model('test', True)
        assert isinstance(model, InstrumentedModel)

    with set_instrument_default(True):
        model = _prepare_model('test', None)
        assert isinstance(model, InstrumentedModel)

        model = _prepare_model('test', False)
        assert isinstance(model, TestModel)

# pydantic_ai_slim/pydantic_ai/models/test.py:92-111
    def __init__(
        self,
        *,
        call_tools: list[str] | Literal['all'] = 'all',
        custom_output_text: str | None = None,
        custom_output_args: Any | None = None,
        seed: int = 0,
        model_name: str = 'test',
        profile: ModelProfileSpec | None = None,
        settings: ModelSettings | None = None,
    ):
        """Initialize TestModel with optional settings and profile."""
        self.call_tools = call_tools
        self.custom_output_text = custom_output_text
        self.custom_output_args = custom_output_args
        self.seed = seed
        self.last_model_request_parameters = None
        self._model_name = model_name
        self._system = 'test'
        super().__init__(settings=settings, profile=profile)

# tests/models/test_model_function.py:26-26
from pydantic_ai.models.test import TestModel

# tests/models/test_model_settings.py:13-13
from pydantic_ai.models.test import TestModel

# tests/models/test_model_test.py:34-34
from pydantic_ai.models.test import TestModel, _chars, _JsonSchemaTestData  # pyright: ignore[reportPrivateUsage]

# tests/test_ag_ui.py:56-56
from pydantic_ai.models.test import TestModel

# tests/test_agent.py:69-69
from pydantic_ai.models.test import TestModel

# tests/test_cli.py:15-15
from pydantic_ai.models.test import TestModel

# tests/test_concurrency.py:14-14
from pydantic_ai.models.test import TestModel

# tests/test_dbos.py:36-36
from pydantic_ai.models.test import TestModel

# tests/test_deps.py:4-4
from pydantic_ai.models.test import TestModel

# tests/test_direct.py:33-33
from pydantic_ai.models.test import TestModel

# tests/test_examples.py:47-47
from pydantic_ai.models.test import TestModel

# tests/test_fastmcp.py:16-16
from pydantic_ai.models.test import TestModel

# tests/test_logfire.py:18-18
from pydantic_ai.models.test import TestModel

# tests/test_mcp.py:43-43
from pydantic_ai.models.test import TestModel

# tests/test_native_output_schema.py:4-4
from pydantic_ai.models.test import TestModel

# tests/test_prefect.py:33-33
from pydantic_ai.models.test import TestModel

# tests/test_streaming.py:52-52
from pydantic_ai.models.test import TestModel, TestStreamedResponse as ModelTestStreamedResponse

# tests/test_temporal.py:52-52
from pydantic_ai.models.test import TestModel

# tests/test_tools.py:36-36
from pydantic_ai.models.test import TestModel

# tests/test_toolsets.py:27-27
from pydantic_ai.models.test import TestModel

# tests/test_ui.py:44-44
from pydantic_ai.models.test import TestModel

# tests/test_ui_web.py:16-16
from pydantic_ai.models.test import TestModel

# tests/test_usage_limits.py:27-27
from pydantic_ai.models.test import TestModel

# tests/test_vercel_ai.py:46-46
from pydantic_ai.models.test import TestModel

# pydantic_ai_slim/pydantic_ai/models/test.py:74-74
    call_tools: list[str] | Literal['all'] = 'all'

# pydantic_ai_slim/pydantic_ai/models/test.py:76-76
    custom_output_text: str | None = None

# pydantic_ai_slim/pydantic_ai/models/test.py:78-78
    custom_output_args: Any | None = None

# pydantic_ai_slim/pydantic_ai/models/test.py:80-80
    seed: int = 0

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# tests/models/test_anthropic.py:60-60
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, TestEnv, raise_if_exception, try_import

# tests/models/test_bedrock.py:54-54
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, try_import

# tests/models/test_cohere.py:33-33
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, raise_if_exception, try_import

# tests/models/test_download_item.py:6-6
from ..conftest import IsInstance, IsStr

# tests/models/test_gemini_vertex.py:23-23
from ..conftest import IsDatetime, IsInstance, IsStr, try_import

# tests/models/test_google.py:79-79
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, try_import

# tests/models/test_groq.py:52-52
from ..conftest import IsDatetime, IsInstance, IsStr, raise_if_exception, try_import

# tests/models/test_huggingface.py:41-41
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, raise_if_exception, try_import

# tests/models/test_outlines.py:42-42
from ..conftest import IsDatetime, IsInstance, IsStr, try_import

# tests/models/test_xai.py:73-73
from ..conftest import IsDatetime, IsInstance, IsNow, IsStr, try_import

# tests/test_cli.py:8-8
from dirty_equals import IsInstance, IsStr

# tests/test_mcp.py:47-47
from .conftest import IsDatetime, IsInstance, IsNow, IsStr, try_import

# pydantic_ai_slim/pydantic_ai/direct.py:270-279
def _prepare_model(
    model: models.Model | models.KnownModelName | str,
    instrument: instrumented_models.InstrumentationSettings | bool | None,
) -> models.Model:
    model_instance = models.infer_model(model)

    if instrument is None:
        instrument = agent.Agent._instrument_default  # pyright: ignore[reportPrivateUsage]

    return instrumented_models.instrument_model(model_instance, instrument)

# pydantic_evals/pydantic_evals/evaluators/common.py:132-147
class IsInstance(Evaluator[object, object, object]):
    """Check if the output is an instance of a type with the given name."""

    type_name: str
    evaluation_name: str | None = field(default=None)

    def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> EvaluationReason:
        output = ctx.output
        for cls in type(output).__mro__:
            if cls.__name__ == self.type_name or cls.__qualname__ == self.type_name:
                return EvaluationReason(value=True)

        reason = f'output is of type {type(output).__name__}'
        if type(output).__qualname__ != type(output).__name__:
            reason += f' (qualname: {type(output).__qualname__})'
        return EvaluationReason(value=False, reason=reason)

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:380-386
    def __init__(
        self,
        wrapped: Model | KnownModelName,
        options: InstrumentationSettings | None = None,
    ) -> None:
        super().__init__(wrapped)
        self.instrumentation_settings = options or InstrumentationSettings()

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:92-92
    event_mode: Literal['attributes', 'logs'] = 'attributes'

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:93-93
    include_binary_content: bool = True

# pydantic_evals/pydantic_evals/reporting/__init__.py:639-672
    def failures_table(
        self,
        *,
        include_input: bool = False,
        include_metadata: bool = False,
        include_expected_output: bool = False,
        include_error_message: bool = True,
        include_error_stacktrace: bool = True,
        input_config: RenderValueConfig | None = None,
        metadata_config: RenderValueConfig | None = None,
    ) -> Table:
        """Return a table containing the failures in this report."""
        renderer = EvaluationRenderer(
            include_input=include_input,
            include_metadata=include_metadata,
            include_expected_output=include_expected_output,
            include_output=False,
            include_durations=False,
            include_total_duration=False,
            include_removed_cases=False,
            include_averages=False,
            input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},
            metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},
            output_config=_DEFAULT_VALUE_CONFIG,
            score_configs={},
            label_configs={},
            metric_configs={},
            duration_config=_DEFAULT_DURATION_CONFIG,
            include_reasons=False,
            include_error_message=include_error_message,
            include_error_stacktrace=include_error_stacktrace,
            include_evaluator_failures=False,  # Not applicable for failures table
        )
        return renderer.build_failures_table(self)

# pydantic_evals/pydantic_evals/otel/span_tree.py:371-414
    def repr_xml(
        self,
        include_children: bool = True,
        include_trace_id: bool = False,
        include_span_id: bool = False,
        include_start_timestamp: bool = False,
        include_duration: bool = False,
    ) -> str:
        """Return an XML-like string representation of the node.

        Optionally includes children, trace_id, span_id, start_timestamp, and duration.
        """
        first_line_parts = [f'<SpanNode name={self.name!r}']
        if include_trace_id:
            first_line_parts.append(f"trace_id='{self.trace_id:032x}'")
        if include_span_id:
            first_line_parts.append(f"span_id='{self.span_id:016x}'")
        if include_start_timestamp:
            first_line_parts.append(f'start_timestamp={self.start_timestamp.isoformat()!r}')
        if include_duration:
            first_line_parts.append(f"duration='{self.duration}'")

        extra_lines: list[str] = []
        if include_children and self.children:
            first_line_parts.append('>')
            for child in self.children:
                extra_lines.append(
                    indent(
                        child.repr_xml(
                            include_children=include_children,
                            include_trace_id=include_trace_id,
                            include_span_id=include_span_id,
                            include_start_timestamp=include_start_timestamp,
                            include_duration=include_duration,
                        ),
                        '  ',
                    )
                )
            extra_lines.append('</SpanNode>')
        else:
            if self.children:
                first_line_parts.append('children=...')
            first_line_parts.append('/>')
        return '\n'.join([' '.join(first_line_parts), *extra_lines])

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:94-94
    include_content: bool = True

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:95-95
    version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION

# pydantic_ai_slim/pydantic_ai/models/test.py:89-89
    _model_name: str = field(default='test', repr=False)