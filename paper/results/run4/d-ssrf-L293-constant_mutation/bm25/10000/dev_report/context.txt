# tests/models/test_mistral.py:2278-2290
async def test_txt_url_input(allow_model_requests: None):
    c = completion_message(MistralAssistantMessage(content='world', role='assistant'))
    mock_client = MockMistralAI.create_mock(c)
    m = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(m)

    with pytest.raises(RuntimeError, match='DocumentUrl other than PDF is not supported in Mistral.'):
        await agent.run(
            [
                'hello',
                DocumentUrl(url='https://examplefiles.org/files/documents/plaintext-example-file-download.txt'),
            ]
        )

# tests/test_examples.py:67-67
    known_local_folder: list[str] = field(default_factory=list[str])

# docs/.hooks/test_snippets.py:403-494
def test_complicated_example():
    """Test extracting multiple overlapping sections."""
    content = """line 1
### [fragment1]
line 2
### [fragment2]
line 3
### [highlight1,highlight2]
line 4
### [/fragment1,/highlight1]
line 5
### [/fragment2]
line 6
### [/highlight2]
"""

    with temp_text_file(content) as temp_path:
        parsed = parse_file_sections(temp_path)

    assert parsed.render([], []) == snapshot(
        RenderedSnippet(
            content="""\
line 1
line 2
line 3
line 4
line 5
line 6\
""",
            highlights=[],
            original_range=LineRange(start_line=0, end_line=11),
        )
    )

    assert parsed.render(['fragment1'], ['highlight1']) == snapshot(
        RenderedSnippet(
            content="""\
line 2
line 3
line 4

...\
""",
            highlights=[LineRange(start_line=2, end_line=3)],
            original_range=LineRange(start_line=2, end_line=7),
        )
    )

    assert parsed.render(['fragment1'], ['highlight2']) == snapshot(
        RenderedSnippet(
            content="""\
line 2
line 3
line 4

...\
""",
            highlights=[LineRange(start_line=2, end_line=5)],
            original_range=LineRange(start_line=2, end_line=7),
        )
    )

    assert parsed.render(['fragment2'], ['highlight2']) == snapshot(
        RenderedSnippet(
            content="""\
...

line 3
line 4
line 5

...\
""",
            highlights=[LineRange(start_line=2, end_line=5)],
            original_range=LineRange(start_line=4, end_line=9),
        )
    )

    assert parsed.render(['fragment1', 'fragment2'], []) == snapshot(
        RenderedSnippet(
            content="""\
line 2
line 3
line 4
line 5

...\
""",
            highlights=[],
            original_range=LineRange(start_line=2, end_line=9),
        )
    )

# tests/test_ssrf.py:290-293
    def test_relative_path(self) -> None:
        """Test that relative paths are resolved against the current URL."""
        result = resolve_redirect_url('https://example.com/old/path', 'new-file.txt')
        assert result == 'https://example.com/old/new-file.txt'

# tests/test_examples.py:120-264
def test_docs_examples(
    example: CodeExample,
    eval_example: EvalExample,
    mocker: MockerFixture,
    client_with_handler: ClientWithHandler,
    allow_model_requests: None,
    env: TestEnv,
    tmp_path_cwd: Path,
    vertex_provider_auth: None,
):
    mocker.patch('pydantic_ai.agent.models.infer_model', side_effect=mock_infer_model)
    mocker.patch('pydantic_ai.embeddings.infer_embedding_model', side_effect=mock_infer_embedding_model)
    mocker.patch('pydantic_ai._utils.group_by_temporal', side_effect=mock_group_by_temporal)
    mocker.patch('pydantic_evals.reporting.render_numbers._render_duration', side_effect=mock_render_duration)

    mocker.patch('httpx.Client.get', side_effect=http_request)
    mocker.patch('httpx.Client.post', side_effect=http_request)
    mocker.patch('httpx.AsyncClient.get', side_effect=async_http_request)
    mocker.patch('httpx.AsyncClient.post', side_effect=async_http_request)
    mocker.patch('random.randint', return_value=4)
    mocker.patch('rich.prompt.Prompt.ask', side_effect=rich_prompt_ask)

    # Avoid filesystem access when examples call ssl.create_default_context(cafile=...) with non-existent paths
    mocker.patch('ssl.create_default_context', return_value=ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT))
    mocker.patch('ssl.SSLContext.load_cert_chain', return_value=None)

    class CustomEvaluationReport(EvaluationReport):
        def print(self, *args: Any, **kwargs: Any) -> None:
            kwargs['width'] = 150
            super().print(*args, **kwargs)

    mocker.patch('pydantic_evals.dataset.EvaluationReport', side_effect=CustomEvaluationReport)

    mocker.patch('pydantic_ai.mcp.MCPServerSSE', return_value=MockMCPServer())
    mocker.patch('pydantic_ai.mcp.MCPServerStreamableHTTP', return_value=MockMCPServer())
    mocker.patch('pydantic_ai.toolsets.fastmcp.FastMCPToolset', return_value=MockMCPServer())
    mocker.patch('mcp.server.fastmcp.FastMCP')

    env.set('OPENAI_API_KEY', 'testing')
    env.set('GEMINI_API_KEY', 'testing')
    env.set('GOOGLE_API_KEY', 'testing')
    env.set('GROQ_API_KEY', 'testing')
    env.set('CO_API_KEY', 'testing')
    env.set('MISTRAL_API_KEY', 'testing')
    env.set('ANTHROPIC_API_KEY', 'testing')
    env.set('HF_TOKEN', 'hf_testing')
    env.set('AWS_ACCESS_KEY_ID', 'testing')
    env.set('AWS_SECRET_ACCESS_KEY', 'testing')
    env.set('AWS_DEFAULT_REGION', 'us-east-1')
    env.set('VERCEL_AI_GATEWAY_API_KEY', 'testing')
    env.set('CEREBRAS_API_KEY', 'testing')
    env.set('NEBIUS_API_KEY', 'testing')
    env.set('HEROKU_INFERENCE_KEY', 'testing')
    env.set('FIREWORKS_API_KEY', 'testing')
    env.set('TOGETHER_API_KEY', 'testing')
    env.set('OLLAMA_API_KEY', 'testing')
    env.set('OLLAMA_BASE_URL', 'http://localhost:11434/v1')
    env.set('AZURE_OPENAI_API_KEY', 'testing')
    env.set('AZURE_OPENAI_ENDPOINT', 'https://your-azure-endpoint.openai.azure.com')
    env.set('OPENAI_API_VERSION', '2024-05-01')
    env.set('OPENROUTER_API_KEY', 'testing')
    env.set('GITHUB_API_KEY', 'testing')
    env.set('GROK_API_KEY', 'testing')
    env.set('MOONSHOTAI_API_KEY', 'testing')
    env.set('DEEPSEEK_API_KEY', 'testing')
    env.set('OVHCLOUD_API_KEY', 'testing')
    env.set('ALIBABA_API_KEY', 'testing')
    env.set('SAMBANOVA_API_KEY', 'testing')
    env.set('PYDANTIC_AI_GATEWAY_API_KEY', 'testing')
    env.set('VOYAGE_API_KEY', 'testing')
    env.set('XAI_API_KEY', 'testing')

    prefix_settings = example.prefix_settings()
    opt_test = prefix_settings.get('test', '')
    opt_lint = prefix_settings.get('lint', '')
    noqa = prefix_settings.get('noqa', '')
    python_version = prefix_settings.get('py')
    dunder_name = prefix_settings.get('dunder_name', '__main__')
    requires = prefix_settings.get('requires')

    ruff_target_version: str = 'py310'
    if python_version:
        python_version_info = tuple(int(v) for v in python_version.split('.'))
        if sys.version_info < python_version_info:
            pytest.skip(f'Python version {python_version} required')  # pragma: lax no cover

        ruff_target_version = f'py{python_version_info[0]}{python_version_info[1]}'

    if opt_test.startswith('skip') and opt_lint.startswith('skip'):
        pytest.skip('both running code and lint skipped')

    known_local_folder: list[str] = []
    if requires:
        for req in requires.split(','):
            known_local_folder.append(Path(req).stem)
            if ex := code_examples.get(req):
                (tmp_path_cwd / req).write_text(ex.source, encoding='utf-8')
            else:  # pragma: no cover
                raise KeyError(f'Example {req} not found, check the `requires` header of this example.')

    ruff_ignore: list[str] = ['D', 'Q001']
    # `from bank_database import DatabaseConn` wrongly sorted in imports
    # waiting for https://github.com/pydantic/pytest-examples/issues/43
    # and https://github.com/pydantic/pytest-examples/issues/46
    if 'import DatabaseConn' in example.source:
        ruff_ignore.append('I001')

    if noqa:
        ruff_ignore.extend(noqa.upper().split())

    line_length = int(prefix_settings.get('line_length', '88'))

    eval_example.config = ExamplesConfig(
        ruff_ignore=ruff_ignore,
        target_version=ruff_target_version,  # type: ignore[reportArgumentType]
        line_length=line_length,
        isort=True,
        upgrade=True,
        quotes='single',
        known_first_party=['pydantic_ai', 'pydantic_evals', 'pydantic_graph'],
        known_local_folder=known_local_folder,
    )
    eval_example.print_callback = print_callback
    eval_example.include_print = custom_include_print

    call_name = prefix_settings.get('call_name', 'main')

    if not opt_lint.startswith('skip'):
        # ruff and seem to black disagree here, not sure if that's easily fixable
        if eval_example.update_examples:  # pragma: lax no cover
            eval_example.format_ruff(example)
        else:
            eval_example.lint_ruff(example)

    if opt_test.startswith('skip'):
        pytest.skip(opt_test[4:].lstrip(' -') or 'running code skipped')
    elif opt_test.startswith('ci_only') and os.getenv('GITHUB_ACTIONS', '').lower() != 'true':
        pytest.skip(opt_test[7:].lstrip(' -') or 'running code skipped in local tests')  # pragma: lax no cover
    else:
        test_globals: dict[str, str] = {'__name__': dunder_name}

        if eval_example.update_examples:  # pragma: lax no cover
            eval_example.run_print_update(example, call=call_name, module_globals=test_globals)
        else:
            eval_example.run_print_check(example, call=call_name, module_globals=test_globals)

# docs/.hooks/main.py:89-95
def sub_example(m: re.Match[str]) -> str:
    example_path = EXAMPLES_DIR / m.group(1)
    content = example_path.read_text(encoding='utf-8').strip()
    # remove leading docstring which duplicates what's in the docs page
    content = re.sub(r'^""".*?"""', '', content, count=1, flags=re.S).strip()

    return content

# tests/test_examples.py:19-19
from pytest_examples import CodeExample, EvalExample, find_examples

# tests/test_examples.py:19-19
from pytest_examples import CodeExample, EvalExample, find_examples

# tests/test_format_as_xml.py:30-32
class ExampleEnum(Enum):
    FOO = 1
    BAR = 2

# docs/.hooks/main.py:82-82
EXAMPLES_DIR = Path(__file__).parent.parent.parent / 'examples'

# examples/pydantic_ai_examples/sql_gen.py:53-70
SQL_EXAMPLES = [
    {
        'request': 'show me records where foobar is false',
        'response': "SELECT * FROM records WHERE attributes->>'foobar' = false",
    },
    {
        'request': 'show me records where attributes include the key "foobar"',
        'response': "SELECT * FROM records WHERE attributes ? 'foobar'",
    },
    {
        'request': 'show me records from yesterday',
        'response': "SELECT * FROM records WHERE start_timestamp::date > CURRENT_TIMESTAMP - INTERVAL '1 day'",
    },
    {
        'request': 'show me error records with the tag "foobar"',
        'response': "SELECT * FROM records WHERE level = 'error' and 'foobar' = ANY(tags)",
    },
]

# tests/test_deps.py:17-18
async def example_tool(ctx: RunContext[MyDeps]) -> str:
    return f'{ctx.deps}'

# tests/evals/test_dataset.py:89-103
def example_cases() -> list[Case[TaskInput, TaskOutput, TaskMetadata]]:
    return [
        Case(
            name='case1',
            inputs=TaskInput(query='What is 2+2?'),
            expected_output=TaskOutput(answer='4'),
            metadata=TaskMetadata(difficulty='easy'),
        ),
        Case(
            name='case2',
            inputs=TaskInput(query='What is the capital of France?'),
            expected_output=TaskOutput(answer='Paris'),
            metadata=TaskMetadata(difficulty='medium', category='geography'),
        ),
    ]

# tests/models/test_gemini.py:547-548
def example_usage() -> _GeminiUsageMetaData:
    return _GeminiUsageMetaData(prompt_token_count=1, candidates_token_count=2, total_token_count=3)

# tests/test_examples.py:19-19
from pytest_examples import CodeExample, EvalExample, find_examples

# tests/test_examples.py:61-61
code_examples: dict[str, CodeExample] = {}

# pydantic_ai_slim/pydantic_ai/models/gemini.py:701-705
class _GeminiFileData(_BasePart):
    """See <https://ai.google.dev/api/caching#FileData>."""

    file_uri: Annotated[str, pydantic.Field(alias='fileUri')]
    mime_type: Annotated[str, pydantic.Field(alias='mimeType')]

# tests/test_examples.py:65-75
class ExamplesConfig(BaseExamplesConfig):
    known_first_party: list[str] = field(default_factory=list[str])
    known_local_folder: list[str] = field(default_factory=list[str])

    def ruff_config(self) -> tuple[str, ...]:
        config = super().ruff_config()
        if self.known_first_party:  # pragma: no branch
            config = (*config, '--config', f'lint.isort.known-first-party = {self.known_first_party}')
        if self.known_local_folder:
            config = (*config, '--config', f'lint.isort.known-local-folder = {self.known_local_folder}')
        return config

# tests/test_format_as_xml.py:35-37
class ExampleStrEnum(str, Enum):
    FOO = 'foo'
    BAR = 'bar'

# tests/providers/test_litellm.py:38-59
def test_model_profile_returns_openai_compatible_profile(mocker: MockerFixture):
    provider = LiteLLMProvider(api_key='test-key')

    # Create a proper mock profile object that can be updated
    from dataclasses import dataclass

    @dataclass
    class MockProfile:
        max_tokens: int = 4096
        supports_streaming: bool = True

    mock_profile = MockProfile()
    mock_openai_profile = mocker.patch('pydantic_ai.providers.litellm.openai_model_profile', return_value=mock_profile)

    profile = provider.model_profile('gpt-3.5-turbo')

    # Verify openai_model_profile was called with the model name
    mock_openai_profile.assert_called_once_with('gpt-3.5-turbo')

    # Verify the returned profile is an OpenAIModelProfile with OpenAIJsonSchemaTransformer
    assert isinstance(profile, OpenAIModelProfile)
    assert profile.json_schema_transformer == OpenAIJsonSchemaTransformer

# docs/.hooks/main.py:85-86
def render_examples(markdown: str) -> str:
    return re.sub(r'^#! *examples/(.+)', sub_example, markdown, flags=re.M)

# tests/evals/test_dataset.py:107-110
def example_dataset(
    example_cases: list[Case[TaskInput, TaskOutput, TaskMetadata]],
) -> Dataset[TaskInput, TaskOutput, TaskMetadata]:
    return Dataset[TaskInput, TaskOutput, TaskMetadata](name='example', cases=example_cases)

# pydantic_graph/pydantic_graph/persistence/file.py:57-63
    async def snapshot_node_if_new(
        self, snapshot_id: str, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]
    ) -> None:
        async with self._lock():
            snapshots = await self.load_all()
            if not any(s.id == snapshot_id for s in snapshots):  # pragma: no branch
                await self._append_save(NodeSnapshot(state=state, node=next_node), lock=False)

# tests/test_format_as_xml.py:20-22
class ExampleDataclass:
    name: str
    age: int

# tests/profiles/test_google.py:136-143
def test_removes_examples_field():
    """examples field should be removed."""
    schema = {'examples': ['foo', 'bar'], 'type': 'string'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert 'examples' not in transformed
    assert transformed == snapshot({'type': 'string'})

# tests/test_examples.py:20-20
from pytest_examples.config import ExamplesConfig as BaseExamplesConfig

# tests/test_examples.py:78-95
def find_filter_examples() -> Iterable[ParameterSet]:
    # Ensure this is run from the package root regardless of where/how the tests are run
    root_dir = Path(__file__).parent.parent
    os.chdir(root_dir)

    for ex in find_examples('docs', 'pydantic_ai_slim', 'pydantic_graph', 'pydantic_evals'):
        if ex.path.name != '_utils.py':
            try:
                path = ex.path.relative_to(root_dir)
            except ValueError:
                path = ex.path
            test_id = f'{path}:{ex.start_line}'
            prefix_settings = ex.prefix_settings()
            if title := prefix_settings.get('title'):
                if title.endswith('.py'):
                    code_examples[title] = ex
                test_id += f':{title}'
            yield pytest.param(ex, id=test_id)

# tests/test_format_as_xml.py:25-27
class ExamplePydanticModel(BaseModel):
    name: str
    age: int

# tests/test_format_as_xml.py:40-50
class ExamplePydanticFields(BaseModel):
    name: str = Field(description="The person's name")
    age: int = Field(description='Years', title='Age', default=18)
    height: float = Field(description="The person's height", exclude=True)
    children: list[Self] | None = Field(title='child', alias='child_list', default=None)

    @computed_field(title='Location')
    def location(self) -> str | None:
        if self.name == 'John':
            return 'Australia'
        return None

# examples/pydantic_ai_examples/question_graph.py:23-23
from pydantic_graph.persistence.file import FileStatePersistence

# docs/.hooks/snippets.py:8-8
PYDANTIC_AI_EXAMPLES_ROOT = REPO_ROOT / 'examples' / 'pydantic_ai_examples'

# pydantic_ai_slim/pydantic_ai/__init__.py:50-102
from .messages import (
    AgentStreamEvent,
    AudioFormat,
    AudioMediaType,
    AudioUrl,
    BaseToolCallPart,
    BaseToolReturnPart,
    BinaryContent,
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentFormat,
    DocumentMediaType,
    DocumentUrl,
    FilePart,
    FileUrl,
    FinalResultEvent,
    FinishReason,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    HandleResponseEvent,
    ImageFormat,
    ImageMediaType,
    ImageUrl,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelRequestPart,
    ModelResponse,
    ModelResponsePart,
    ModelResponsePartDelta,
    ModelResponseStreamEvent,
    MultiModalContent,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturn,
    ToolReturnPart,
    UserContent,
    UserPromptPart,
    VideoFormat,
    VideoMediaType,
    VideoUrl,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:50-102
from .messages import (
    AgentStreamEvent,
    AudioFormat,
    AudioMediaType,
    AudioUrl,
    BaseToolCallPart,
    BaseToolReturnPart,
    BinaryContent,
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentFormat,
    DocumentMediaType,
    DocumentUrl,
    FilePart,
    FileUrl,
    FinalResultEvent,
    FinishReason,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    HandleResponseEvent,
    ImageFormat,
    ImageMediaType,
    ImageUrl,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelRequestPart,
    ModelResponse,
    ModelResponsePart,
    ModelResponsePartDelta,
    ModelResponseStreamEvent,
    MultiModalContent,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturn,
    ToolReturnPart,
    UserContent,
    UserPromptPart,
    VideoFormat,
    VideoMediaType,
    VideoUrl,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:50-102
from .messages import (
    AgentStreamEvent,
    AudioFormat,
    AudioMediaType,
    AudioUrl,
    BaseToolCallPart,
    BaseToolReturnPart,
    BinaryContent,
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentFormat,
    DocumentMediaType,
    DocumentUrl,
    FilePart,
    FileUrl,
    FinalResultEvent,
    FinishReason,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    HandleResponseEvent,
    ImageFormat,
    ImageMediaType,
    ImageUrl,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelRequestPart,
    ModelResponse,
    ModelResponsePart,
    ModelResponsePartDelta,
    ModelResponseStreamEvent,
    MultiModalContent,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturn,
    ToolReturnPart,
    UserContent,
    UserPromptPart,
    VideoFormat,
    VideoMediaType,
    VideoUrl,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:50-102
from .messages import (
    AgentStreamEvent,
    AudioFormat,
    AudioMediaType,
    AudioUrl,
    BaseToolCallPart,
    BaseToolReturnPart,
    BinaryContent,
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentFormat,
    DocumentMediaType,
    DocumentUrl,
    FilePart,
    FileUrl,
    FinalResultEvent,
    FinishReason,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    HandleResponseEvent,
    ImageFormat,
    ImageMediaType,
    ImageUrl,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelRequestPart,
    ModelResponse,
    ModelResponsePart,
    ModelResponsePartDelta,
    ModelResponseStreamEvent,
    MultiModalContent,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturn,
    ToolReturnPart,
    UserContent,
    UserPromptPart,
    VideoFormat,
    VideoMediaType,
    VideoUrl,
)

# docs/.hooks/test_snippets.py:146-163
def test_parse_file_sections_comment_style():
    """Test parsing with /// comment style."""
    content = """line 1
/// [section1]
content 1
/// [/section1]
line 5"""

    with temp_text_file(content) as temp_path:
        result = parse_file_sections(temp_path)

    assert result == snapshot(
        ParsedFile(
            lines=['line 1', 'content 1', 'line 5'],
            sections={'section1': [LineRange(start_line=1, end_line=2)]},
            lines_mapping={0: 0, 1: 2, 2: 4},
        )
    )

# pydantic_ai_slim/pydantic_ai/profiles/harmony.py:7-15
def harmony_model_profile(model_name: str) -> ModelProfile | None:
    """The model profile for the OpenAI Harmony Response format.

    See <https://cookbook.openai.com/articles/openai-harmony> for more details.
    """
    profile = openai_model_profile(model_name)
    return OpenAIModelProfile(
        openai_supports_tool_choice_required=False, ignore_streamed_leading_whitespace=True
    ).update(profile)

# tests/test_format_as_xml.py:31-31
    FOO = 1

# tests/test_format_as_xml.py:32-32
    BAR = 2

# tests/test_a2a.py:241-335
async def test_a2a_file_message_with_file():
    agent = Agent(model=model, output_type=tuple[str, str])
    app = agent.to_a2a()

    async with LifespanManager(app):
        transport = httpx.ASGITransport(app)
        async with httpx.AsyncClient(transport=transport) as http_client:
            a2a_client = A2AClient(http_client=http_client)

            message = Message(
                role='user',
                parts=[
                    FilePart(
                        kind='file',
                        file={'uri': 'https://example.com/file.txt', 'mime_type': 'text/plain'},
                    )
                ],
                kind='message',
                message_id=str(uuid.uuid4()),
            )
            response = await a2a_client.send_message(message=message)
            assert 'error' not in response
            assert 'result' in response
            result = response['result']
            assert result['kind'] == 'task'
            assert result == snapshot(
                {
                    'id': IsStr(),
                    'context_id': IsStr(),
                    'kind': 'task',
                    'status': {'state': 'submitted', 'timestamp': IsDatetime(iso_string=True)},
                    'history': [
                        {
                            'role': 'user',
                            'parts': [
                                {
                                    'kind': 'file',
                                    'file': {'mime_type': 'text/plain', 'uri': 'https://example.com/file.txt'},
                                }
                            ],
                            'kind': 'message',
                            'message_id': IsStr(),
                            'context_id': IsStr(),
                            'task_id': IsStr(),
                        }
                    ],
                }
            )

            task_id = result['id']

            while task := await a2a_client.get_task(task_id):  # pragma: no branch
                if 'result' in task and task['result']['status']['state'] == 'completed':
                    break
                await anyio.sleep(0.1)
            assert task == snapshot(
                {
                    'jsonrpc': '2.0',
                    'id': None,
                    'result': {
                        'id': IsStr(),
                        'context_id': IsStr(),
                        'kind': 'task',
                        'status': {'state': 'completed', 'timestamp': IsDatetime(iso_string=True)},
                        'history': [
                            {
                                'role': 'user',
                                'parts': [
                                    {
                                        'kind': 'file',
                                        'file': {'mime_type': 'text/plain', 'uri': 'https://example.com/file.txt'},
                                    }
                                ],
                                'kind': 'message',
                                'message_id': IsStr(),
                                'context_id': IsStr(),
                                'task_id': IsStr(),
                            }
                        ],
                        'artifacts': [
                            {
                                'artifact_id': IsStr(),
                                'name': 'result',
                                'parts': [
                                    {
                                        'metadata': {'json_schema': {'items': {}, 'type': 'array'}},
                                        'kind': 'data',
                                        'data': {'result': ['foo', 'bar']},
                                    }
                                ],
                            }
                        ],
                    },
                }
            )

# docs/.hooks/test_snippets.py:24-33
def temp_text_file(content: str):
    """Context manager for temporary text file with common params."""
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', encoding='utf-8', delete=False) as f:
        f.write(content)
        temp_name = f.name

    try:
        yield Path(temp_name)
    finally:
        os.unlink(temp_name)

# tests/test_ui_web.py:556-562
async def test_get_ui_html_local_file_not_found(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html raises FileNotFoundError for missing local file paths."""
    # Try to use a non-existent local file path
    nonexistent_path = str(tmp_path / 'nonexistent-ui.html')

    with pytest.raises(FileNotFoundError, match='Local UI file not found'):
        await app_module._get_ui_html(html_source=nonexistent_path)  # pyright: ignore[reportPrivateUsage]

# tests/test_format_as_xml.py:36-36
    FOO = 'foo'

# tests/test_format_as_xml.py:37-37
    BAR = 'bar'

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:86-86
    filename: str | None = None

# docs/.hooks/main.py:10-10
from mkdocs.structure.files import Files

# tests/test_format_as_xml.py:22-22
    age: int

# tests/test_ui_web.py:530-539
async def test_get_ui_html_local_file_path_string(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html supports local file paths as strings."""
    # Create a test HTML file
    test_html = b'<html><body>Local UI Content</body></html>'
    local_file = tmp_path / 'custom-ui.html'
    local_file.write_bytes(test_html)

    result = await app_module._get_ui_html(html_source=str(local_file))  # pyright: ignore[reportPrivateUsage]

    assert result == test_html

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:32-53
from .response_types import (
    BaseChunk,
    DoneChunk,
    ErrorChunk,
    FileChunk,
    FinishChunk,
    FinishReason,
    FinishStepChunk,
    ReasoningDeltaChunk,
    ReasoningEndChunk,
    ReasoningStartChunk,
    StartChunk,
    StartStepChunk,
    TextDeltaChunk,
    TextEndChunk,
    TextStartChunk,
    ToolInputAvailableChunk,
    ToolInputDeltaChunk,
    ToolInputStartChunk,
    ToolOutputAvailableChunk,
    ToolOutputErrorChunk,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:50-102
from .messages import (
    AgentStreamEvent,
    AudioFormat,
    AudioMediaType,
    AudioUrl,
    BaseToolCallPart,
    BaseToolReturnPart,
    BinaryContent,
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentFormat,
    DocumentMediaType,
    DocumentUrl,
    FilePart,
    FileUrl,
    FinalResultEvent,
    FinishReason,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    HandleResponseEvent,
    ImageFormat,
    ImageMediaType,
    ImageUrl,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelRequestPart,
    ModelResponse,
    ModelResponsePart,
    ModelResponsePartDelta,
    ModelResponseStreamEvent,
    MultiModalContent,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturn,
    ToolReturnPart,
    UserContent,
    UserPromptPart,
    VideoFormat,
    VideoMediaType,
    VideoUrl,
)

# pydantic_ai_slim/pydantic_ai/models/gemini.py:704-704
    file_uri: Annotated[str, pydantic.Field(alias='fileUri')]

# tests/test_format_as_xml.py:21-21
    name: str

# pydantic_ai_slim/pydantic_ai/_ssrf.py:67-67
    is_https: bool

# pydantic_ai_slim/pydantic_ai/run.py:156-161
    def new_messages(self) -> list[_messages.ModelMessage]:
        """Return new messages for the run so far.

        Messages from older runs are excluded.
        """
        return self.all_messages()[self.ctx.deps.new_message_index :]

# examples/pydantic_ai_examples/slack_lead_qualifier/agent.py:10-10
from .models import Analysis, Profile

# examples/pydantic_ai_examples/slack_lead_qualifier/agent.py:10-10
from .models import Analysis, Profile

# examples/pydantic_ai_examples/slack_lead_qualifier/agent.py:10-10
from .models import Analysis, Profile

# examples/pydantic_ai_examples/slack_lead_qualifier/agent.py:10-10
from .models import Analysis, Profile

# tests/providers/test_sambanova_provider.py:61-66
def test_qwen_profile():
    provider = SambaNovaProvider(api_key='key')
    # Qwen model -> expect qwen profile wrapped in OpenAI compatibility
    profile = provider.model_profile('Qwen3-32B')
    assert isinstance(profile, OpenAIModelProfile)
    assert profile is not None

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:357-357
    file: dict[str, Any] | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:250-252
    def format(self) -> str:
        """The file format."""
        raise NotImplementedError

# docs/.hooks/test_snippets.py:4-4
import tempfile

# pydantic_ai_slim/pydantic_ai/__init__.py:50-102
from .messages import (
    AgentStreamEvent,
    AudioFormat,
    AudioMediaType,
    AudioUrl,
    BaseToolCallPart,
    BaseToolReturnPart,
    BinaryContent,
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentFormat,
    DocumentMediaType,
    DocumentUrl,
    FilePart,
    FileUrl,
    FinalResultEvent,
    FinishReason,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    HandleResponseEvent,
    ImageFormat,
    ImageMediaType,
    ImageUrl,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelRequestPart,
    ModelResponse,
    ModelResponsePart,
    ModelResponsePartDelta,
    ModelResponseStreamEvent,
    MultiModalContent,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturn,
    ToolReturnPart,
    UserContent,
    UserPromptPart,
    VideoFormat,
    VideoMediaType,
    VideoUrl,
)

# docs/.hooks/test_snippets.py:4-4
import tempfile