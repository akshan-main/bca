# tests/models/test_mistral.py:2278-2290
async def test_txt_url_input(allow_model_requests: None):
    c = completion_message(MistralAssistantMessage(content='world', role='assistant'))
    mock_client = MockMistralAI.create_mock(c)
    m = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(m)

    with pytest.raises(RuntimeError, match='DocumentUrl other than PDF is not supported in Mistral.'):
        await agent.run(
            [
                'hello',
                DocumentUrl(url='https://examplefiles.org/files/documents/plaintext-example-file-download.txt'),
            ]
        )

# tests/test_examples.py:67-67
    known_local_folder: list[str] = field(default_factory=list[str])

# docs/.hooks/test_snippets.py:403-494
def test_complicated_example():
    """Test extracting multiple overlapping sections."""
    content = """line 1
### [fragment1]
line 2
### [fragment2]
line 3
### [highlight1,highlight2]
line 4
### [/fragment1,/highlight1]
line 5
### [/fragment2]
line 6
### [/highlight2]
"""

    with temp_text_file(content) as temp_path:
        parsed = parse_file_sections(temp_path)

    assert parsed.render([], []) == snapshot(
        RenderedSnippet(
            content="""\
line 1
line 2
line 3
line 4
line 5
line 6\
""",
            highlights=[],
            original_range=LineRange(start_line=0, end_line=11),
        )
    )

    assert parsed.render(['fragment1'], ['highlight1']) == snapshot(
        RenderedSnippet(
            content="""\
line 2
line 3
line 4

...\
""",
            highlights=[LineRange(start_line=2, end_line=3)],
            original_range=LineRange(start_line=2, end_line=7),
        )
    )

    assert parsed.render(['fragment1'], ['highlight2']) == snapshot(
        RenderedSnippet(
            content="""\
line 2
line 3
line 4

...\
""",
            highlights=[LineRange(start_line=2, end_line=5)],
            original_range=LineRange(start_line=2, end_line=7),
        )
    )

    assert parsed.render(['fragment2'], ['highlight2']) == snapshot(
        RenderedSnippet(
            content="""\
...

line 3
line 4
line 5

...\
""",
            highlights=[LineRange(start_line=2, end_line=5)],
            original_range=LineRange(start_line=4, end_line=9),
        )
    )

    assert parsed.render(['fragment1', 'fragment2'], []) == snapshot(
        RenderedSnippet(
            content="""\
line 2
line 3
line 4
line 5

...\
""",
            highlights=[],
            original_range=LineRange(start_line=2, end_line=9),
        )
    )

# tests/test_ssrf.py:290-293
    def test_relative_path(self) -> None:
        """Test that relative paths are resolved against the current URL."""
        result = resolve_redirect_url('https://example.com/old/path', 'new-file.txt')
        assert result == 'https://example.com/old/new-file.txt'

# tests/test_examples.py:120-264
def test_docs_examples(
    example: CodeExample,
    eval_example: EvalExample,
    mocker: MockerFixture,
    client_with_handler: ClientWithHandler,
    allow_model_requests: None,
    env: TestEnv,
    tmp_path_cwd: Path,
    vertex_provider_auth: None,
):
    mocker.patch('pydantic_ai.agent.models.infer_model', side_effect=mock_infer_model)
    mocker.patch('pydantic_ai.embeddings.infer_embedding_model', side_effect=mock_infer_embedding_model)
    mocker.patch('pydantic_ai._utils.group_by_temporal', side_effect=mock_group_by_temporal)
    mocker.patch('pydantic_evals.reporting.render_numbers._render_duration', side_effect=mock_render_duration)

    mocker.patch('httpx.Client.get', side_effect=http_request)
    mocker.patch('httpx.Client.post', side_effect=http_request)
    mocker.patch('httpx.AsyncClient.get', side_effect=async_http_request)
    mocker.patch('httpx.AsyncClient.post', side_effect=async_http_request)
    mocker.patch('random.randint', return_value=4)
    mocker.patch('rich.prompt.Prompt.ask', side_effect=rich_prompt_ask)

    # Avoid filesystem access when examples call ssl.create_default_context(cafile=...) with non-existent paths
    mocker.patch('ssl.create_default_context', return_value=ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT))
    mocker.patch('ssl.SSLContext.load_cert_chain', return_value=None)

    class CustomEvaluationReport(EvaluationReport):
        def print(self, *args: Any, **kwargs: Any) -> None:
            kwargs['width'] = 150
            super().print(*args, **kwargs)

    mocker.patch('pydantic_evals.dataset.EvaluationReport', side_effect=CustomEvaluationReport)

    mocker.patch('pydantic_ai.mcp.MCPServerSSE', return_value=MockMCPServer())
    mocker.patch('pydantic_ai.mcp.MCPServerStreamableHTTP', return_value=MockMCPServer())
    mocker.patch('pydantic_ai.toolsets.fastmcp.FastMCPToolset', return_value=MockMCPServer())
    mocker.patch('mcp.server.fastmcp.FastMCP')

    env.set('OPENAI_API_KEY', 'testing')
    env.set('GEMINI_API_KEY', 'testing')
    env.set('GOOGLE_API_KEY', 'testing')
    env.set('GROQ_API_KEY', 'testing')
    env.set('CO_API_KEY', 'testing')
    env.set('MISTRAL_API_KEY', 'testing')
    env.set('ANTHROPIC_API_KEY', 'testing')
    env.set('HF_TOKEN', 'hf_testing')
    env.set('AWS_ACCESS_KEY_ID', 'testing')
    env.set('AWS_SECRET_ACCESS_KEY', 'testing')
    env.set('AWS_DEFAULT_REGION', 'us-east-1')
    env.set('VERCEL_AI_GATEWAY_API_KEY', 'testing')
    env.set('CEREBRAS_API_KEY', 'testing')
    env.set('NEBIUS_API_KEY', 'testing')
    env.set('HEROKU_INFERENCE_KEY', 'testing')
    env.set('FIREWORKS_API_KEY', 'testing')
    env.set('TOGETHER_API_KEY', 'testing')
    env.set('OLLAMA_API_KEY', 'testing')
    env.set('OLLAMA_BASE_URL', 'http://localhost:11434/v1')
    env.set('AZURE_OPENAI_API_KEY', 'testing')
    env.set('AZURE_OPENAI_ENDPOINT', 'https://your-azure-endpoint.openai.azure.com')
    env.set('OPENAI_API_VERSION', '2024-05-01')
    env.set('OPENROUTER_API_KEY', 'testing')
    env.set('GITHUB_API_KEY', 'testing')
    env.set('GROK_API_KEY', 'testing')
    env.set('MOONSHOTAI_API_KEY', 'testing')
    env.set('DEEPSEEK_API_KEY', 'testing')
    env.set('OVHCLOUD_API_KEY', 'testing')
    env.set('ALIBABA_API_KEY', 'testing')
    env.set('SAMBANOVA_API_KEY', 'testing')
    env.set('PYDANTIC_AI_GATEWAY_API_KEY', 'testing')
    env.set('VOYAGE_API_KEY', 'testing')
    env.set('XAI_API_KEY', 'testing')

    prefix_settings = example.prefix_settings()
    opt_test = prefix_settings.get('test', '')
    opt_lint = prefix_settings.get('lint', '')
    noqa = prefix_settings.get('noqa', '')
    python_version = prefix_settings.get('py')
    dunder_name = prefix_settings.get('dunder_name', '__main__')
    requires = prefix_settings.get('requires')

    ruff_target_version: str = 'py310'
    if python_version:
        python_version_info = tuple(int(v) for v in python_version.split('.'))
        if sys.version_info < python_version_info:
            pytest.skip(f'Python version {python_version} required')  # pragma: lax no cover

        ruff_target_version = f'py{python_version_info[0]}{python_version_info[1]}'

    if opt_test.startswith('skip') and opt_lint.startswith('skip'):
        pytest.skip('both running code and lint skipped')

    known_local_folder: list[str] = []
    if requires:
        for req in requires.split(','):
            known_local_folder.append(Path(req).stem)
            if ex := code_examples.get(req):
                (tmp_path_cwd / req).write_text(ex.source, encoding='utf-8')
            else:  # pragma: no cover
                raise KeyError(f'Example {req} not found, check the `requires` header of this example.')

    ruff_ignore: list[str] = ['D', 'Q001']
    # `from bank_database import DatabaseConn` wrongly sorted in imports
    # waiting for https://github.com/pydantic/pytest-examples/issues/43
    # and https://github.com/pydantic/pytest-examples/issues/46
    if 'import DatabaseConn' in example.source:
        ruff_ignore.append('I001')

    if noqa:
        ruff_ignore.extend(noqa.upper().split())

    line_length = int(prefix_settings.get('line_length', '88'))

    eval_example.config = ExamplesConfig(
        ruff_ignore=ruff_ignore,
        target_version=ruff_target_version,  # type: ignore[reportArgumentType]
        line_length=line_length,
        isort=True,
        upgrade=True,
        quotes='single',
        known_first_party=['pydantic_ai', 'pydantic_evals', 'pydantic_graph'],
        known_local_folder=known_local_folder,
    )
    eval_example.print_callback = print_callback
    eval_example.include_print = custom_include_print

    call_name = prefix_settings.get('call_name', 'main')

    if not opt_lint.startswith('skip'):
        # ruff and seem to black disagree here, not sure if that's easily fixable
        if eval_example.update_examples:  # pragma: lax no cover
            eval_example.format_ruff(example)
        else:
            eval_example.lint_ruff(example)

    if opt_test.startswith('skip'):
        pytest.skip(opt_test[4:].lstrip(' -') or 'running code skipped')
    elif opt_test.startswith('ci_only') and os.getenv('GITHUB_ACTIONS', '').lower() != 'true':
        pytest.skip(opt_test[7:].lstrip(' -') or 'running code skipped in local tests')  # pragma: lax no cover
    else:
        test_globals: dict[str, str] = {'__name__': dunder_name}

        if eval_example.update_examples:  # pragma: lax no cover
            eval_example.run_print_update(example, call=call_name, module_globals=test_globals)
        else:
            eval_example.run_print_check(example, call=call_name, module_globals=test_globals)

# docs/.hooks/main.py:89-95
def sub_example(m: re.Match[str]) -> str:
    example_path = EXAMPLES_DIR / m.group(1)
    content = example_path.read_text(encoding='utf-8').strip()
    # remove leading docstring which duplicates what's in the docs page
    content = re.sub(r'^""".*?"""', '', content, count=1, flags=re.S).strip()

    return content

# tests/test_examples.py:19-19
from pytest_examples import CodeExample, EvalExample, find_examples

# tests/test_examples.py:19-19
from pytest_examples import CodeExample, EvalExample, find_examples

# tests/test_format_as_xml.py:30-32
class ExampleEnum(Enum):
    FOO = 1
    BAR = 2

# docs/.hooks/main.py:82-82
EXAMPLES_DIR = Path(__file__).parent.parent.parent / 'examples'

# examples/pydantic_ai_examples/sql_gen.py:53-70
SQL_EXAMPLES = [
    {
        'request': 'show me records where foobar is false',
        'response': "SELECT * FROM records WHERE attributes->>'foobar' = false",
    },
    {
        'request': 'show me records where attributes include the key "foobar"',
        'response': "SELECT * FROM records WHERE attributes ? 'foobar'",
    },
    {
        'request': 'show me records from yesterday',
        'response': "SELECT * FROM records WHERE start_timestamp::date > CURRENT_TIMESTAMP - INTERVAL '1 day'",
    },
    {
        'request': 'show me error records with the tag "foobar"',
        'response': "SELECT * FROM records WHERE level = 'error' and 'foobar' = ANY(tags)",
    },
]

# tests/test_deps.py:17-18
async def example_tool(ctx: RunContext[MyDeps]) -> str:
    return f'{ctx.deps}'

# tests/evals/test_dataset.py:89-103
def example_cases() -> list[Case[TaskInput, TaskOutput, TaskMetadata]]:
    return [
        Case(
            name='case1',
            inputs=TaskInput(query='What is 2+2?'),
            expected_output=TaskOutput(answer='4'),
            metadata=TaskMetadata(difficulty='easy'),
        ),
        Case(
            name='case2',
            inputs=TaskInput(query='What is the capital of France?'),
            expected_output=TaskOutput(answer='Paris'),
            metadata=TaskMetadata(difficulty='medium', category='geography'),
        ),
    ]

# tests/models/test_gemini.py:547-548
def example_usage() -> _GeminiUsageMetaData:
    return _GeminiUsageMetaData(prompt_token_count=1, candidates_token_count=2, total_token_count=3)

# tests/test_examples.py:19-19
from pytest_examples import CodeExample, EvalExample, find_examples

# tests/test_examples.py:61-61
code_examples: dict[str, CodeExample] = {}

# pydantic_ai_slim/pydantic_ai/models/gemini.py:701-705
class _GeminiFileData(_BasePart):
    """See <https://ai.google.dev/api/caching#FileData>."""

    file_uri: Annotated[str, pydantic.Field(alias='fileUri')]
    mime_type: Annotated[str, pydantic.Field(alias='mimeType')]

# tests/test_examples.py:65-75
class ExamplesConfig(BaseExamplesConfig):
    known_first_party: list[str] = field(default_factory=list[str])
    known_local_folder: list[str] = field(default_factory=list[str])

    def ruff_config(self) -> tuple[str, ...]:
        config = super().ruff_config()
        if self.known_first_party:  # pragma: no branch
            config = (*config, '--config', f'lint.isort.known-first-party = {self.known_first_party}')
        if self.known_local_folder:
            config = (*config, '--config', f'lint.isort.known-local-folder = {self.known_local_folder}')
        return config

# tests/test_format_as_xml.py:35-37
class ExampleStrEnum(str, Enum):
    FOO = 'foo'
    BAR = 'bar'

# tests/providers/test_litellm.py:38-59
def test_model_profile_returns_openai_compatible_profile(mocker: MockerFixture):
    provider = LiteLLMProvider(api_key='test-key')

    # Create a proper mock profile object that can be updated
    from dataclasses import dataclass

    @dataclass
    class MockProfile:
        max_tokens: int = 4096
        supports_streaming: bool = True

    mock_profile = MockProfile()
    mock_openai_profile = mocker.patch('pydantic_ai.providers.litellm.openai_model_profile', return_value=mock_profile)

    profile = provider.model_profile('gpt-3.5-turbo')

    # Verify openai_model_profile was called with the model name
    mock_openai_profile.assert_called_once_with('gpt-3.5-turbo')

    # Verify the returned profile is an OpenAIModelProfile with OpenAIJsonSchemaTransformer
    assert isinstance(profile, OpenAIModelProfile)
    assert profile.json_schema_transformer == OpenAIJsonSchemaTransformer

# docs/.hooks/main.py:85-86
def render_examples(markdown: str) -> str:
    return re.sub(r'^#! *examples/(.+)', sub_example, markdown, flags=re.M)

# tests/evals/test_dataset.py:107-110
def example_dataset(
    example_cases: list[Case[TaskInput, TaskOutput, TaskMetadata]],
) -> Dataset[TaskInput, TaskOutput, TaskMetadata]:
    return Dataset[TaskInput, TaskOutput, TaskMetadata](name='example', cases=example_cases)

# pydantic_graph/pydantic_graph/persistence/file.py:57-63
    async def snapshot_node_if_new(
        self, snapshot_id: str, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]
    ) -> None:
        async with self._lock():
            snapshots = await self.load_all()
            if not any(s.id == snapshot_id for s in snapshots):  # pragma: no branch
                await self._append_save(NodeSnapshot(state=state, node=next_node), lock=False)

# tests/test_format_as_xml.py:20-22
class ExampleDataclass:
    name: str
    age: int

# tests/profiles/test_google.py:136-143
def test_removes_examples_field():
    """examples field should be removed."""
    schema = {'examples': ['foo', 'bar'], 'type': 'string'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert 'examples' not in transformed
    assert transformed == snapshot({'type': 'string'})

# tests/test_examples.py:20-20
from pytest_examples.config import ExamplesConfig as BaseExamplesConfig

# tests/test_examples.py:78-95
def find_filter_examples() -> Iterable[ParameterSet]:
    # Ensure this is run from the package root regardless of where/how the tests are run
    root_dir = Path(__file__).parent.parent
    os.chdir(root_dir)

    for ex in find_examples('docs', 'pydantic_ai_slim', 'pydantic_graph', 'pydantic_evals'):
        if ex.path.name != '_utils.py':
            try:
                path = ex.path.relative_to(root_dir)
            except ValueError:
                path = ex.path
            test_id = f'{path}:{ex.start_line}'
            prefix_settings = ex.prefix_settings()
            if title := prefix_settings.get('title'):
                if title.endswith('.py'):
                    code_examples[title] = ex
                test_id += f':{title}'
            yield pytest.param(ex, id=test_id)

# tests/test_format_as_xml.py:25-27
class ExamplePydanticModel(BaseModel):
    name: str
    age: int

# docs/.hooks/snippets.py:8-8
PYDANTIC_AI_EXAMPLES_ROOT = REPO_ROOT / 'examples' / 'pydantic_ai_examples'

# tests/test_format_as_xml.py:31-31
    FOO = 1

# tests/test_format_as_xml.py:32-32
    BAR = 2

# tests/test_format_as_xml.py:36-36
    FOO = 'foo'

# tests/test_format_as_xml.py:37-37
    BAR = 'bar'

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)