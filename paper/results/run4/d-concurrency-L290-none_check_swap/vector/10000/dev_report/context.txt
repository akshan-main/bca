# tests/test_concurrency.py:225-254
    async def test_agent_concurrency_limit(self):
        """Test that agent respects max_concurrency."""
        agent = Agent(TestModel(), max_concurrency=2)
        running = 0
        max_running = 0
        lock = anyio.Lock()

        @agent.tool_plain
        async def slow_tool() -> str:
            nonlocal running, max_running
            async with lock:
                running += 1
                max_running = max(max_running, running)
            await anyio.sleep(0.1)
            async with lock:
                running -= 1
            return 'done'

        results: list[Any] = []

        async def run_agent():
            result = await agent.run('call slow_tool', model=TestModel(call_tools=['slow_tool']))
            results.append(result)

        async with anyio.create_task_group() as tg:
            for _ in range(5):
                tg.start_soon(run_agent)

        assert max_running <= 2
        assert len(results) == 5

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:7-7
from asyncio import Lock

# pydantic_ai_slim/pydantic_ai/mcp.py:8-8
from asyncio import Lock

# pydantic_ai_slim/pydantic_ai/providers/google_vertex.py:4-4
from asyncio import Lock

# pydantic_ai_slim/pydantic_ai/toolsets/combined.py:4-4
from asyncio import Lock

# pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py:4-4
from asyncio import Lock

# pydantic_ai_slim/pydantic_ai/concurrency.py:75-75
    max_running: int

# pydantic_ai_slim/pydantic_ai/concurrency.py:161-163
    def max_running(self) -> int:
        """Maximum concurrent operations allowed."""
        return int(self._limiter.total_tokens)

# tests/test_concurrency.py:222-298
class TestAgentConcurrency:
    """Tests for agent-level concurrency limiting."""

    async def test_agent_concurrency_limit(self):
        """Test that agent respects max_concurrency."""
        agent = Agent(TestModel(), max_concurrency=2)
        running = 0
        max_running = 0
        lock = anyio.Lock()

        @agent.tool_plain
        async def slow_tool() -> str:
            nonlocal running, max_running
            async with lock:
                running += 1
                max_running = max(max_running, running)
            await anyio.sleep(0.1)
            async with lock:
                running -= 1
            return 'done'

        results: list[Any] = []

        async def run_agent():
            result = await agent.run('call slow_tool', model=TestModel(call_tools=['slow_tool']))
            results.append(result)

        async with anyio.create_task_group() as tg:
            for _ in range(5):
                tg.start_soon(run_agent)

        assert max_running <= 2
        assert len(results) == 5

    async def test_agent_concurrency_backpressure(self):
        """Test that agent raises when queue exceeds max_queued."""
        agent = Agent(TestModel(), max_concurrency=ConcurrencyLimit(max_running=1, max_queued=1))
        hold = anyio.Event()

        @agent.tool_plain
        async def hold_tool() -> str:
            await hold.wait()
            return 'done'

        async def run_agent():
            await agent.run('x', model=TestModel(call_tools=['hold_tool']))

        async with anyio.create_task_group() as tg:
            # Start 2 runs (1 running + 1 queued = at limit)
            tg.start_soon(run_agent)
            tg.start_soon(run_agent)
            await anyio.sleep(0.05)

            # Third should raise
            with pytest.raises(ConcurrencyLimitExceeded):
                await agent.run('x', model=TestModel(call_tools=['hold_tool']))

            hold.set()

    async def test_agent_no_limit_by_default(self):
        """Test that agents have no concurrency limit by default."""
        agent = Agent(TestModel())
        assert agent._concurrency_limiter is None

    async def test_agent_with_int_concurrency(self):
        """Test that agent accepts int for max_concurrency."""
        agent = Agent(TestModel(), max_concurrency=5)
        assert agent._concurrency_limiter is not None
        assert agent._concurrency_limiter.max_running == 5
        assert agent._concurrency_limiter._max_queued is None

    async def test_agent_with_limiter_concurrency(self):
        """Test that agent accepts ConcurrencyLimit for max_concurrency."""
        agent = Agent(TestModel(), max_concurrency=ConcurrencyLimit(max_running=5, max_queued=10))
        assert agent._concurrency_limiter is not None
        assert agent._concurrency_limiter.max_running == 5
        assert agent._concurrency_limiter._max_queued == 10

# pydantic_graph/pydantic_graph/persistence/file.py:148-166
    async def _lock(self, *, timeout: float = 1.0) -> AsyncIterator[None]:
        """Lock a file by checking and writing a `.pydantic-graph-persistence-lock` to it.

        Args:
            timeout: how long to wait for the lock

        Returns: an async context manager that holds the lock
        """
        lock_file = self.json_file.parent / f'{self.json_file.name}.pydantic-graph-persistence-lock'
        lock_id = secrets.token_urlsafe().encode()

        with anyio.fail_after(timeout):
            while not await _file_append_check(lock_file, lock_id):
                await anyio.sleep(0.01)

        try:
            yield
        finally:
            await _graph_utils.run_in_executor(lock_file.unlink, missing_ok=True)

# examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py:8-8
from pydantic_ai import Agent

# examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py:11-11
agent = Agent('openai:gpt-5-mini')

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:11-11
from pydantic_ai import Agent

# examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py:53-69
agent = Agent(
    'openai:gpt-5-mini',
    instructions=dedent(
        """
        When planning use tools only, without any other messages.
        IMPORTANT:
        - Use the `create_plan` tool to set the initial state of the steps
        - Use the `update_plan_step` tool to update the status of each step
        - Do NOT repeat the plan or summarise it in a message
        - Do NOT confirm the creation or updates in a message
        - Do NOT ask the user for additional information or next steps

        Only one plan can be active at a time, so do not call the `create_plan` tool
        again until all the steps in current plan are completed.
        """
    ),
)

# examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py:10-10
from pydantic_ai import Agent

# examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py:13-25
agent = Agent(
    'openai:gpt-5-mini',
    instructions=dedent(
        """
        When planning tasks use tools only, without any other messages.
        IMPORTANT:
        - Use the `generate_task_steps` tool to display the suggested steps to the user
        - Never repeat the plan, or send a message detailing steps
        - If accepted, confirm the creation of the plan and the number of selected (enabled) steps only
        - If not accepted, ask the user for more information, DO NOT use the `generate_task_steps` tool again
        """
    ),
)

# examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py:10-10
from pydantic_ai import Agent, RunContext

# examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py:21-21
agent = Agent('openai:gpt-5-mini', deps_type=StateDeps[DocumentState])

# examples/pydantic_ai_examples/ag_ui/api/shared_state.py:11-11
from pydantic_ai import Agent, RunContext

# examples/pydantic_ai_examples/ag_ui/api/shared_state.py:88-88
agent = Agent('openai:gpt-5-mini', deps_type=StateDeps[RecipeSnapshot])

# examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py:8-8
from pydantic_ai import Agent

# examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py:11-11
agent = Agent('openai:gpt-5-mini')

# examples/pydantic_ai_examples/bank_support.py:13-13
from pydantic_ai import Agent, RunContext

# examples/pydantic_ai_examples/chat_app.py:28-37
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelResponse,
    TextPart,
    UnexpectedModelBehavior,
    UserPromptPart,
)

# examples/pydantic_ai_examples/chat_app.py:43-43
agent = Agent('openai:gpt-5.2')

# examples/pydantic_ai_examples/data_analyst.py:7-7
from pydantic_ai import Agent, ModelRetry, RunContext

# examples/pydantic_ai_examples/evals/agent.py:6-6
from pydantic_ai import Agent, RunContext

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/pydantic_model.py:13-13
from pydantic_ai import Agent

# examples/pydantic_ai_examples/pydantic_model.py:27-27
agent = Agent(model, output_type=MyModel)

# examples/pydantic_ai_examples/question_graph.py:16-16
from pydantic_ai import Agent, ModelMessage, format_as_xml

# examples/pydantic_ai_examples/rag.py:38-38
from pydantic_ai import Agent, RunContext

# examples/pydantic_ai_examples/rag.py:52-52
agent = Agent('openai:gpt-5.2', deps_type=Deps)

# examples/pydantic_ai_examples/roulette_wheel.py:13-13
from pydantic_ai import Agent, RunContext

# examples/pydantic_ai_examples/slack_lead_qualifier/agent.py:7-7
from pydantic_ai import Agent, NativeOutput

# examples/pydantic_ai_examples/slack_lead_qualifier/agent.py:13-40
agent = Agent(
    'openai:gpt-5.2',
    instructions=dedent(
        """
        When a new person joins our public Slack, please put together a brief snapshot so we can be most useful to them.

        **What to include**

        1. **Who they are:**  Any details about their professional role or projects (e.g. LinkedIn, GitHub, company bio).
        2. **Where they work:**  Name of the organisation and its domain.
        3. **How we can help:**  On a scale of 1–5, estimate how likely they are to benefit from **Pydantic Logfire**
           (our paid observability tool) based on factors such as company size, product maturity, or AI usage.
           *1 = probably not relevant, 5 = very strong fit.*

        **Our products (for context only)**
        • **Pydantic Validation** – Python data-validation (open source)
        • **Pydantic AI** – Python agent framework (open source)
        • **Pydantic Logfire** – Observability for traces, logs & metrics with first-class AI support (commercial)

        **How to research**

        • Use the provided DuckDuckGo search tool to research the person and the organization they work for, based on the email domain or what you find on e.g. LinkedIn and GitHub.
        • If you can't find enough to form a reasonable view, return **None**.
        """
    ),
    tools=[duckduckgo_search_tool()],
    output_type=NativeOutput([Analysis, NoneType]),
)  ### [/agent]

# examples/pydantic_ai_examples/sql_gen.py:27-27
from pydantic_ai import Agent, ModelRetry, RunContext, format_as_xml

# examples/pydantic_ai_examples/sql_gen.py:94-99
agent = Agent[Deps, Response](
    'google-gla:gemini-3-flash-preview',
    # Type ignore while we wait for PEP-0747, nonetheless unions will work fine everywhere else
    output_type=Response,  # type: ignore
    deps_type=Deps,
)

# examples/pydantic_ai_examples/stream_markdown.py:18-18
from pydantic_ai import Agent

# examples/pydantic_ai_examples/stream_markdown.py:25-25
agent = Agent()

# examples/pydantic_ai_examples/stream_whales.py:20-20
from pydantic_ai import Agent

# examples/pydantic_ai_examples/stream_whales.py:42-42
agent = Agent('openai:gpt-5.2', output_type=list[Whale])

# examples/pydantic_ai_examples/weather_agent.py:22-22
from pydantic_ai import Agent, RunContext

# pydantic_ai_slim/pydantic_ai/__init__.py:3-11
from .agent import (
    Agent,
    CallToolsNode,
    EndStrategy,
    InstrumentationSettings,
    ModelRequestNode,
    UserPromptNode,
    capture_run_messages,
)

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:18-18
from ..agent import AbstractAgent, Agent

# pydantic_ai_slim/pydantic_ai/_cli/web.py:5-5
from pydantic_ai import Agent

# pydantic_ai_slim/pydantic_ai/direct.py:22-22
from . import agent, messages, models, settings

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:13-13
from pydantic_ai import Agent

# pydantic_ai_slim/pydantic_ai/ui/_web/app.py:11-11
from pydantic_ai import Agent

# pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py:10-10
from pydantic_ai import Agent, UserContent, models

# pydantic_evals/pydantic_evals/generation.py:16-16
from pydantic_ai import Agent, models

# tests/conftest.py:27-27
from pydantic_ai import Agent, BinaryContent, BinaryImage, Embedder

# tests/ext/test_langchain.py:8-8
from pydantic_ai import Agent

# tests/models/anthropic/test_output.py:22-22
from pydantic_ai import Agent

# tests/models/test_anthropic.py:18-47
from pydantic_ai import (
    Agent,
    BinaryContent,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentUrl,
    FinalResultEvent,
    ImageUrl,
    ModelAPIError,
    ModelHTTPError,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturnPart,
    UsageLimitExceeded,
    UserPromptPart,
)

# tests/models/test_bedrock.py:39-39
from pydantic_ai.agent import Agent

# tests/models/test_cerebras.py:7-7
from pydantic_ai import Agent, ModelRequest, TextPart

# tests/models/test_cohere.py:12-27
from pydantic_ai import (
    Agent,
    ImageUrl,
    ModelAPIError,
    ModelHTTPError,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)

# tests/models/test_deepseek.py:8-15
from pydantic_ai import (
    Agent,
    ModelRequest,
    ModelResponse,
    TextPart,
    ThinkingPart,
    UserPromptPart,
)

# tests/models/test_fallback.py:15-27
from pydantic_ai import (
    Agent,
    ModelAPIError,
    ModelHTTPError,
    ModelMessage,
    ModelProfile,
    ModelRequest,
    ModelResponse,
    TextPart,
    ToolCallPart,
    ToolDefinition,
    UserPromptPart,
)

# tests/models/test_gemini.py:19-36
from pydantic_ai import (
    Agent,
    BinaryContent,
    DocumentUrl,
    ImageUrl,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    RetryPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UnexpectedModelBehavior,
    UserError,
    UserPromptPart,
    VideoUrl,
)

# tests/models/test_gemini_vertex.py:9-19
from pydantic_ai import (
    Agent,
    AudioUrl,
    DocumentUrl,
    ImageUrl,
    ModelRequest,
    ModelResponse,
    TextPart,
    UserPromptPart,
    VideoUrl,
)

# tests/models/test_google.py:54-54
from pydantic_ai.agent import Agent

# tests/models/test_groq.py:18-43
from pydantic_ai import (
    Agent,
    BinaryContent,
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    FinalResultEvent,
    ImageUrl,
    ModelAPIError,
    ModelHTTPError,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)

# tests/models/test_huggingface.py:15-33
from pydantic_ai import (
    Agent,
    AudioUrl,
    BinaryContent,
    CachePoint,
    DocumentUrl,
    ImageUrl,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
)

# tests/models/test_mcp_sampling.py:10-10
from pydantic_ai.agent import Agent

# tests/models/test_mistral.py:31-31
from pydantic_ai.agent import Agent

# tests/models/test_model_function.py:12-24
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    RunContext,
    SystemPromptPart,
    TextPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)

# tests/models/test_model_settings.py:7-7
from pydantic_ai import Agent

# tests/models/test_model_test.py:17-32
from pydantic_ai import (
    Agent,
    AudioUrl,
    BinaryContent,
    ImageUrl,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    RetryPromptPart,
    RunContext,
    TextPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
)

# tests/models/test_openai.py:19-40
from pydantic_ai import (
    Agent,
    AudioUrl,
    BinaryContent,
    CachePoint,
    DocumentUrl,
    ImageUrl,
    ModelAPIError,
    ModelHTTPError,
    ModelProfile,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    RetryPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UnexpectedModelBehavior,
    UserError,
    UserPromptPart,
)

# tests/models/test_openrouter.py:11-27
from pydantic_ai import (
    Agent,
    BinaryContent,
    DocumentUrl,
    ModelHTTPError,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    PartEndEvent,
    PartStartEvent,
    RunUsage,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolDefinition,
    UnexpectedModelBehavior,
)

# tests/models/test_outlines.py:18-18
from pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior

# tests/models/test_xai.py:29-60
from pydantic_ai import (
    Agent,
    AudioUrl,
    BinaryContent,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CodeExecutionTool,
    DocumentUrl,
    FilePart,
    FinalResultEvent,
    ImageUrl,
    MCPServerTool,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelRetry,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    TextPartDelta,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
    WebSearchTool,
)

# tests/providers/test_azure.py:8-8
from pydantic_ai.agent import Agent

# tests/providers/test_gateway.py:12-12
from pydantic_ai import Agent, UserError

# tests/providers/test_google_vertex.py:15-15
from pydantic_ai.agent import Agent

# tests/providers/test_heroku.py:7-7
from pydantic_ai.agent import Agent

# tests/providers/test_openrouter.py:9-9
from pydantic_ai.agent import Agent

# tests/test_a2a.py:12-23
from pydantic_ai import (
    Agent,
    BinaryContent,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    TextPart as PydanticAITextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)

# tests/test_ag_ui.py:45-45
from pydantic_ai.agent import Agent, AgentRunResult

# tests/test_agent.py:18-52
from pydantic_ai import (
    AbstractToolset,
    Agent,
    AgentStreamEvent,
    AudioUrl,
    BinaryContent,
    BinaryImage,
    CallDeferred,
    CombinedToolset,
    DocumentUrl,
    ExternalToolset,
    FunctionToolset,
    ImageUrl,
    IncompleteToolCall,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelProfile,
    ModelRequest,
    ModelResponse,
    ModelResponsePart,
    ModelRetry,
    PrefixedToolset,
    RetryPromptPart,
    RunContext,
    SystemPromptPart,
    TextPart,
    ToolCallPart,
    ToolReturn,
    ToolReturnPart,
    UnexpectedModelBehavior,
    UserError,
    UserPromptPart,
    VideoUrl,
    capture_run_messages,
)

# tests/test_agent_output_schemas.py:5-14
from pydantic_ai import (
    Agent,
    BinaryImage,
    DeferredToolRequests,
    NativeOutput,
    PromptedOutput,
    StructuredDict,
    TextOutput,
    ToolOutput,
)

# tests/test_builtin_tools.py:6-6
from pydantic_ai.agent import Agent

# tests/test_cli.py:14-14
from pydantic_ai import Agent, ModelMessage, ModelResponse, TextPart, ToolCallPart

# tests/test_concurrency.py:12-12
from pydantic_ai import Agent, ConcurrencyLimit, ConcurrencyLimiter, ConcurrencyLimitExceeded

# tests/test_dbos.py:18-31
from pydantic_ai import (
    Agent,
    AgentStreamEvent,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelSettings,
    RetryPromptPart,
    RunContext,
    TextPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
)

# tests/test_deps.py:3-3
from pydantic_ai import Agent, RunContext

# tests/test_deps.py:13-13
agent = Agent(TestModel(), deps_type=MyDeps)

# tests/test_direct.py:10-10
from pydantic_ai import Agent

# tests/test_history_processor.py:7-17
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRequest,
    ModelRequestPart,
    ModelResponse,
    SystemPromptPart,
    TextPart,
    UserPromptPart,
    capture_run_messages,
)

# tests/test_logfire.py:13-13
from pydantic_ai import Agent, ModelMessage, ModelRequest, ModelResponse, TextPart, ToolCallPart, UserPromptPart

# tests/test_mcp.py:27-27
from pydantic_ai.agent import Agent

# tests/test_native_output_schema.py:3-3
from pydantic_ai import Agent

# tests/test_prefect.py:15-29
from pydantic_ai import (
    Agent,
    AgentRunResult,
    AgentRunResultEvent,
    AgentStreamEvent,
    ExternalToolset,
    FunctionToolset,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelSettings,
    RunContext,
    TextPart,
    UserPromptPart,
)

# tests/test_settings.py:5-5
from pydantic_ai import Agent

# tests/test_streaming.py:18-45
from pydantic_ai import (
    Agent,
    AgentRunResult,
    AgentRunResultEvent,
    AgentStreamEvent,
    ExternalToolset,
    FinalResultEvent,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    ImageUrl,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    RunContext,
    TextPart,
    TextPartDelta,
    ToolCallPart,
    ToolReturnPart,
    UnexpectedModelBehavior,
    UserError,
    UserPromptPart,
    capture_run_messages,
    models,
)

# tests/test_temporal.py:16-47
from pydantic_ai import (
    Agent,
    AgentRunResultEvent,
    AgentStreamEvent,
    BinaryContent,
    BinaryImage,
    DocumentUrl,
    ExternalToolset,
    FinalResultEvent,
    FunctionToolCallEvent,
    FunctionToolResultEvent,
    FunctionToolset,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    ModelSettings,
    MultiModalContent,
    PartDeltaEvent,
    PartEndEvent,
    PartStartEvent,
    RetryPromptPart,
    RunContext,
    RunUsage,
    TextPart,
    TextPartDelta,
    ToolCallPart,
    ToolCallPartDelta,
    ToolReturnPart,
    UserPromptPart,
    WebSearchTool,
    WebSearchUserLocation,
)

# tests/test_tools.py:16-33
from pydantic_ai import (
    Agent,
    ExternalToolset,
    FunctionToolset,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    PrefixedToolset,
    RetryPromptPart,
    RunContext,
    TextPart,
    Tool,
    ToolCallPart,
    ToolReturn,
    ToolReturnPart,
    UserError,
    UserPromptPart,
)

# tests/test_ui.py:12-12
from pydantic_ai import Agent

# tests/test_ui_web.py:14-14
from pydantic_ai import Agent, ModelSettings

# tests/test_usage_limits.py:14-24
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRequest,
    ModelResponse,
    RunContext,
    ToolCallPart,
    ToolReturnPart,
    UsageLimitExceeded,
    UserPromptPart,
)

# tests/test_validation_context.py:7-17
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelResponse,
    NativeOutput,
    PromptedOutput,
    RunContext,
    TextPart,
    ToolCallPart,
    ToolOutput,
)

# tests/test_vercel_ai.py:10-10
from pydantic_ai import Agent

# tests/typed_agent.py:13-13
from pydantic_ai import Agent, ModelRetry, RunContext, Tool

# tests/typed_deps.py:6-6
from pydantic_ai import Agent, RunContext, Tool, ToolDefinition

# tests/typed_deps.py:24-28
agent = Agent(
    instructions='...',
    model='...',
    deps_type=AgentDeps,
)

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:1354-1408
    def to_a2a(
        self,
        *,
        storage: Storage | None = None,
        broker: Broker | None = None,
        # Agent card
        name: str | None = None,
        url: str = 'http://localhost:8000',
        version: str = '1.0.0',
        description: str | None = None,
        provider: AgentProvider | None = None,
        skills: list[Skill] | None = None,
        # Starlette
        debug: bool = False,
        routes: Sequence[Route] | None = None,
        middleware: Sequence[Middleware] | None = None,
        exception_handlers: dict[Any, ExceptionHandler] | None = None,
        lifespan: Lifespan[FastA2A] | None = None,
    ) -> FastA2A:
        """Convert the agent to a FastA2A application.

        Example:
        ```python
        from pydantic_ai import Agent

        agent = Agent('openai:gpt-5.2')
        app = agent.to_a2a()
        ```

        The `app` is an ASGI application that can be used with any ASGI server.

        To run the application, you can use the following command:

        ```bash
        uvicorn app:app --host 0.0.0.0 --port 8000
        ```
        """
        from .._a2a import agent_to_a2a

        return agent_to_a2a(
            self,
            storage=storage,
            broker=broker,
            name=name,
            url=url,
            version=version,
            description=description,
            provider=provider,
            skills=skills,
            debug=debug,
            routes=routes,
            middleware=middleware,
            exception_handlers=exception_handlers,
            lifespan=lifespan,
        )

# tests/test_concurrency.py:286-291
    async def test_agent_with_int_concurrency(self):
        """Test that agent accepts int for max_concurrency."""
        agent = Agent(TestModel(), max_concurrency=5)
        assert agent._concurrency_limiter is not None
        assert agent._concurrency_limiter.max_running == 5
        assert agent._concurrency_limiter._max_queued is None

# tests/test_concurrency.py:293-298
    async def test_agent_with_limiter_concurrency(self):
        """Test that agent accepts ConcurrencyLimit for max_concurrency."""
        agent = Agent(TestModel(), max_concurrency=ConcurrencyLimit(max_running=5, max_queued=10))
        assert agent._concurrency_limiter is not None
        assert agent._concurrency_limiter.max_running == 5
        assert agent._concurrency_limiter._max_queued == 10

# tests/test_mcp.py:88-89
def agent(model: Model, mcp_server: MCPServerStdio) -> Agent:
    return Agent(model, toolsets=[mcp_server])

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:11-11
import anyio

# pydantic_ai_slim/pydantic_ai/concurrency.py:11-11
import anyio

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:11-11
import anyio

# pydantic_ai_slim/pydantic_ai/mcp.py:16-16
import anyio

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:7-7
import anyio

# pydantic_evals/pydantic_evals/_utils.py:12-12
import anyio

# pydantic_evals/pydantic_evals/dataset.py:26-26
import anyio

# pydantic_graph/pydantic_graph/persistence/file.py:11-11
import anyio

# tests/test_a2a.py:5-5
import anyio

# tests/test_concurrency.py:9-9
import anyio

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:117-117
    agent: AbstractAgent[AgentDepsT, OutputDataT]

# pydantic_ai_slim/pydantic_ai/_a2a.py:122-122
    agent: AbstractAgent[AgentDepsT, WorkerOutputT]

# pydantic_ai_slim/pydantic_ai/result.py:434-436
    async def stream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:
        async for output in self.stream_output(debounce_by=debounce_by):
            yield output

# pydantic_ai_slim/pydantic_ai/concurrency.py:23-62
class AbstractConcurrencyLimiter(ABC):
    """Abstract base class for concurrency limiters.

    Subclass this to create custom concurrency limiters
    (e.g., Redis-backed distributed limiters).

    Example:
    ```python
    from pydantic_ai.concurrency import AbstractConcurrencyLimiter


    class RedisConcurrencyLimiter(AbstractConcurrencyLimiter):
        def __init__(self, redis_client, key: str, max_running: int):
            self._redis = redis_client
            self._key = key
            self._max_running = max_running

        async def acquire(self, source: str) -> None:
            # Implement Redis-based distributed locking
            ...

        def release(self) -> None:
            # Release the Redis lock
            ...
    ```
    """

    @abstractmethod
    async def acquire(self, source: str) -> None:
        """Acquire a slot, waiting if necessary.

        Args:
            source: Identifier for observability (e.g., 'model:gpt-4o').
        """
        ...

    @abstractmethod
    def release(self) -> None:
        """Release a slot."""
        ...

# pydantic_ai_slim/pydantic_ai/_a2a.py:75-115
def agent_to_a2a(
    agent: AbstractAgent[AgentDepsT, OutputDataT],
    *,
    storage: Storage | None = None,
    broker: Broker | None = None,
    # Agent card
    name: str | None = None,
    url: str = 'http://localhost:8000',
    version: str = '1.0.0',
    description: str | None = None,
    provider: AgentProvider | None = None,
    skills: list[Skill] | None = None,
    # Starlette
    debug: bool = False,
    routes: Sequence[Route] | None = None,
    middleware: Sequence[Middleware] | None = None,
    exception_handlers: dict[Any, ExceptionHandler] | None = None,
    lifespan: Lifespan[FastA2A] | None = None,
) -> FastA2A:
    """Create a FastA2A server from an agent."""
    storage = storage or InMemoryStorage()
    broker = broker or InMemoryBroker()
    worker = AgentWorker(agent=agent, broker=broker, storage=storage)

    lifespan = lifespan or partial(worker_lifespan, worker=worker, agent=agent)

    return FastA2A(
        storage=storage,
        broker=broker,
        name=name or agent.name,
        url=url,
        version=version,
        description=description,
        provider=provider,
        skills=skills,
        debug=debug,
        routes=routes,
        middleware=middleware,
        exception_handlers=exception_handlers,
        lifespan=lifespan,
    )

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:1707-1724
    async def run_mcp_servers(
        self, model: models.Model | models.KnownModelName | str | None = None
    ) -> AsyncIterator[None]:
        """Run [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] so they can be used by the agent.

        Deprecated: use [`async with agent`][pydantic_ai.agent.Agent.__aenter__] instead.
        If you need to set a sampling model on all MCP servers, use [`agent.set_mcp_sampling_model()`][pydantic_ai.agent.Agent.set_mcp_sampling_model].

        Returns: a context manager to start and shutdown the servers.
        """
        try:
            self.set_mcp_sampling_model(model)
        except exceptions.UserError:
            if model is not None:
                raise

        async with self:
            yield

# pydantic_ai_slim/pydantic_ai/result.py:64-91
    async def stream_output(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:
        """Asynchronously stream the (validated) agent outputs."""
        if self._cached_output is not None:
            yield deepcopy(self._cached_output)
            return

        last_response: _messages.ModelResponse | None = None
        async for response in self.stream_responses(debounce_by=debounce_by):
            if self._raw_stream_response.final_result_event is None or (
                last_response and response.parts == last_response.parts
            ):
                continue
            last_response = response

            try:
                yield await self.validate_response_output(response, allow_partial=True)
            except ValidationError:
                pass

        if self._raw_stream_response.final_result_event is not None:  # pragma: no branch
            response = self.response
            # Final validation with allow_partial=False (the default).
            # We always yield the final result even if the content matches the last partial yield, because:
            # 1. Output validators/functions receive partial_output=False only on this final call,
            #    and may behave differently based on that flag
            # 2. Users can rely on the last yielded item being the fully validated output
            self._cached_output = await self.validate_response_output(response)
            yield deepcopy(self._cached_output)

# tests/test_concurrency.py:256-279
    async def test_agent_concurrency_backpressure(self):
        """Test that agent raises when queue exceeds max_queued."""
        agent = Agent(TestModel(), max_concurrency=ConcurrencyLimit(max_running=1, max_queued=1))
        hold = anyio.Event()

        @agent.tool_plain
        async def hold_tool() -> str:
            await hold.wait()
            return 'done'

        async def run_agent():
            await agent.run('x', model=TestModel(call_tools=['hold_tool']))

        async with anyio.create_task_group() as tg:
            # Start 2 runs (1 running + 1 queued = at limit)
            tg.start_soon(run_agent)
            tg.start_soon(run_agent)
            await anyio.sleep(0.05)

            # Third should raise
            with pytest.raises(ConcurrencyLimitExceeded):
                await agent.run('x', model=TestModel(call_tools=['hold_tool']))

            hold.set()

# pydantic_ai_slim/pydantic_ai/result.py:93-104
    async def stream_responses(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[_messages.ModelResponse]:
        """Asynchronously stream the (unvalidated) model responses for the agent."""
        # if the message currently has any parts with content, yield before streaming
        msg = self.response
        for part in msg.parts:
            if part.has_content():
                yield msg
                break

        async with _utils.group_by_temporal(self, debounce_by) as group_iter:
            async for _items in group_iter:
                yield self.response  # current state of the response

# tests/models/mock_xai.py:723-737
def create_usage(
    prompt_tokens: int = 0,
    completion_tokens: int = 0,
    reasoning_tokens: int = 0,
    cached_prompt_text_tokens: int = 0,
    server_side_tools_used: list[usage_pb2.ServerSideTool] | None = None,
) -> usage_pb2.SamplingUsage:
    """Helper to create xAI SamplingUsage protobuf objects for tests with all required fields."""
    return usage_pb2.SamplingUsage(
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        reasoning_tokens=reasoning_tokens,
        cached_prompt_text_tokens=cached_prompt_text_tokens,
        server_side_tools_used=server_side_tools_used or [],
    )

# tests/test_temporal.py:2011-2012
    def get_status(self) -> Literal['running', 'waiting_for_results', 'done']:
        return self._status

# pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py:4-4
import anyio.to_thread

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:12-12
import anyio.to_thread