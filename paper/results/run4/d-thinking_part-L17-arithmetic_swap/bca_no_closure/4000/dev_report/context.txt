## examples/pydantic_ai_examples/evals/agent.py

    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

## examples/pydantic_ai_examples/rag.py

    def embedding_content(self) -> str:
        return '\n\n'.join((f'path: {self.path}', f'title: {self.title}', self.content))

## examples/pydantic_ai_examples/slack_lead_qualifier/store.py

    async def add(cls, analysis: Analysis):
        await cls._get_store().put.aio(analysis.profile.email, analysis.model_dump())

    async def clear(cls):
        await cls._get_store().clear.aio()

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    model_response: _messages.ModelResponse

## pydantic_ai_slim/pydantic_ai/_cli/__init__.py

    def __init__(self, special_suggestions: list[str] | None = None):
        super().__init__()
        self.special_suggestions = special_suggestions or []

## pydantic_ai_slim/pydantic_ai/_json_schema.py

    def transform(self, schema: JsonSchema) -> JsonSchema:
        """Make changes to the schema."""
        return schema

## pydantic_ai_slim/pydantic_ai/_otel_messages.py

class TextPart(TypedDict):
    type: Literal['text']
    content: NotRequired[str]

    type: Literal['text']

class MediaUrlPart(TypedDict):
    type: Literal['image-url', 'audio-url', 'video-url', 'document-url']
    url: NotRequired[str]

class BinaryDataPart(TypedDict):
    type: Literal['binary']
    media_type: str
    content: NotRequired[str]

class ThinkingPart(TypedDict):
    type: Literal['thinking']
    content: NotRequired[str]

## pydantic_ai_slim/pydantic_ai/_output.py

    def label(self) -> str:
        return "the agent's output tools"

## pydantic_ai_slim/pydantic_ai/_utils.py

def number_to_datetime(x: int | float) -> datetime:
    return _datetime_ta.validate_python(x)

## pydantic_ai_slim/pydantic_ai/messages.py

    def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:
        return [_otel_messages.TextPart(type='text', **{'content': self.content} if settings.include_content else {})]

    def format(self) -> str:
        """The file format."""
        raise NotImplementedError

    def user_text_prompt(cls, user_prompt: str, *, instructions: str | None = None) -> ModelRequest:
        """Create a `ModelRequest` with a single user prompt as text."""
        return cls(parts=[UserPromptPart(user_prompt)], instructions=instructions)

class TextPart:
    """A plain text response from a model."""

    content: str
    """The text content of the response."""

    _: KW_ONLY

    id: str | None = None
    """An optional identifier of the text part.

    When this field is set, `provider_name` is required to identify the provider that generated this data.
    """

    provider_name: str | None = None
    """The name of the provider that generated the response.

    Required to be set when `provider_details` or `id` is set.
    """

    provider_details: dict[str, Any] | None = None
    """Additional data returned by the provider that can't be mapped to standard fields.

    This is used for data that is required to be sent back to APIs, as well as data users may want to access programmatically.
    When this field is set, `provider_name` is required to identify the provider that generated this data.
    """

    part_kind: Literal['text'] = 'text'
    """Part type identifier, this is available on all parts as a discriminator."""

    def has_content(self) -> bool:
        """Return `True` if the text content is non-empty."""
        return bool(self.content)

    __repr__ = _utils.dataclasses_no_defaults_repr

    def has_content(self) -> bool:
        """Return `True` if the text content is non-empty."""
        return bool(self.content)

class ThinkingPart:
    """A thinking response from a model."""

    content: str
    """The thinking content of the response."""

    _: KW_ONLY

    id: str | None = None
    """The identifier of the thinking part.

    When this field is set, `provider_name` is required to identify the provider that generated this data.
    """

    signature: str | None = None
    """The signature of the thinking.

    Supported by:

    * Anthropic (corresponds to the `signature` field)
    * Bedrock (corresponds to the `signature` field)
    * Google (corresponds to the `thought_signature` field)
    * OpenAI (corresponds to the `encrypted_content` field)

    When this field is set, `provider_name` is required to identify the provider that generated this data.
    """

    provider_name: str | None = None
    """The name of the provider that generated the response.

    Signatures are only sent back to the same provider.
    Required to be set when `provider_details`, `id` or `signature` is set.
    """

    provider_details: dict[str, Any] | None = None
    """Additional data returned by the provider that can't be mapped to standard fields.

    This is used for data that is required to be sent back to APIs, as well as data users may want to access programmatically.
    When this field is set, `provider_name` is required to identify the provider that generated this data.
    """

    part_kind: Literal['thinking'] = 'thinking'
    """Part type identifier, this is available on all parts as a discriminator."""

    def has_content(self) -> bool:
        """Return `True` if the thinking content is non-empty."""
        return bool(self.content)

    __repr__ = _utils.dataclasses_no_defaults_repr

    def has_content(self) -> bool:
        """Return `True` if the thinking content is non-empty."""
        return bool(self.content)

    def thinking(self) -> str | None:
        """Get the thinking in the response."""
        thinking_parts = [part.content for part in self.parts if isinstance(part, ThinkingPart)]
        if not thinking_parts:
            return None
        return '\n\n'.join(thinking_parts)

    def price(self) -> genai_types.PriceCalculation:  # pragma: no cover
        return self.cost()

## pydantic_ai_slim/pydantic_ai/models/fallback.py

    def system(self) -> str:
        return f'fallback:{",".join(model.system for model in self.models)}'

## pydantic_ai_slim/pydantic_ai/models/huggingface.py

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ChatCompletionInputTool]:
        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]

## pydantic_ai_slim/pydantic_ai/models/openai.py

    def _map_usage(self, response: chat.ChatCompletion) -> usage.RequestUsage:
        return _map_usage(response, self._provider.name, self._provider.base_url, self.model_name)

## pydantic_ai_slim/pydantic_ai/models/test.py

    def generate(self) -> dict[str, Any]:
        """Generate data for the JSON schema."""
        return self._gen_any(self.schema)

## pydantic_ai_slim/pydantic_ai/profiles/__init__.py

    thinking_tags: tuple[str, str] = ('<think>', '</think>')

## pydantic_ai_slim/pydantic_ai/providers/__init__.py

    def model_profile(self, model_name: str) -> ModelProfile | None:
        """The model profile for the named model, if available."""
        return None  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/retries.py

    sleep: Callable[[int | float], None | Awaitable[None]]

    def close(self) -> None:
        self.wrapped.close()  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/toolsets/abstract.py

    def validate_python(
        self, input: Any, *, allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False, **kwargs: Any
    ) -> Any: ...

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py

    def encode(self, sdk_version: int) -> str:
        return self.model_dump_json(by_alias=True, exclude_none=True)

## pydantic_evals/pydantic_evals/otel/span_tree.py

    def find(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:
        """Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order."""
        return list(self._filter(predicate))

    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## tests/conftest.py

    def set(self, name: str, value: str) -> None:
        self.envars[name] = os.getenv(name)
        os.environ[name] = value

    def remove(self, name: str) -> None:
        self.envars[name] = os.environ.pop(name, None)

## tests/graph/beta/test_broadcast_and_spread.py

class CounterState:
    values: list[int] = field(default_factory=list[int])

    values: list[int] = field(default_factory=list[int])

## tests/graph/beta/test_decisions.py

class DecisionState:
    path_taken: str | None = None
    value: int = 0

## tests/graph/beta/test_edge_cases.py

class EdgeCaseState:
    value: int = 0
    error_raised: bool = False

## tests/graph/beta/test_edge_labels.py

class LabelState:
    value: int = 0

## tests/graph/beta/test_graph_builder.py

class SimpleState:
    counter: int = 0
    result: str | None = None

## tests/graph/beta/test_graph_edge_cases.py

class MyState:
    value: int = 0

## tests/graph/beta/test_graph_execution.py

class ExecutionState:
    log: list[str] = field(default_factory=list[str])
    counter: int = 0

## tests/graph/beta/test_graph_iteration.py

class IterState:
    counter: int = 0

## tests/graph/beta/test_v1_v2_integration.py

class IntegrationState:
    log: list[str] = field(default_factory=list[str])

## tests/graph/test_mermaid.py

class Foo(BaseNode):
    async def run(self, ctx: GraphRunContext) -> Bar:
        return Bar()

class Bar(BaseNode[None, None, None]):
    async def run(self, ctx: GraphRunContext) -> End[None]:
        return End(None)

## tests/models/mock_openai.py

    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(completions=completions))

## tests/models/test_anthropic.py

    def create_mock(cls, messages_: MockAnthropicMessage | Sequence[MockAnthropicMessage]) -> AsyncAnthropic:
        return cast(AsyncAnthropic, cls(messages_=messages_))

## tests/models/test_cohere.py

    def create_mock(cls, completions: MockChatResponse | Sequence[MockChatResponse]) -> AsyncClientV2:
        return cast(AsyncClientV2, cls(completions=completions))

## tests/models/test_groq.py

def text_chunk(text: str, finish_reason: FinishReason | None = None) -> chat.ChatCompletionChunk:
    return chunk([ChoiceDelta(content=text, role='assistant')], finish_reason=finish_reason)

## tests/models/test_huggingface.py

    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncInferenceClient:
        return cast(AsyncInferenceClient, cls(completions=completions))

async def test_thinking_part_in_history(allow_model_requests: None):
    c = completion_message(ChatCompletionOutputMessage(content='response', role='assistant'))  # type: ignore
    mock_client = MockHuggingFace.create_mock(c)
    model = HuggingFaceModel('hf-model', provider=HuggingFaceProvider(hf_client=mock_client, api_key='x'))
    agent = Agent(model)
    messages = [
        ModelRequest(parts=[UserPromptPart(content='request')], timestamp=IsDatetime()),
        ModelResponse(
            parts=[
                TextPart(content='text 1'),
                ThinkingPart(content='let me do some thinking'),
                TextPart(content='text 2'),
            ],
            model_name='hf-model',
            timestamp=datetime.now(timezone.utc),
        ),
    ]

    await agent.run('another request', message_history=messages)

    kwargs = get_mock_chat_completion_kwargs(mock_client)[0]
    sent_messages = kwargs['messages']
    assert [{k: v for k, v in asdict(m).items() if v is not None} for m in sent_messages] == snapshot(
        [
            {'content': 'request', 'role': 'user'},
            {
                'content': """\
text 1

<think>
let me do some thinking
</think>

text 2\
""",
                'role': 'assistant',
            },
            {'content': 'another request', 'role': 'user'},
        ]
    )

## tests/models/test_mistral.py

    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> Mistral:
        return cast(Mistral, cls(completions=completions))

## tests/models/test_model_function.py

async def foo(_: RunContext[None], x: int) -> str:
    return str(x + 1)

def bar(ctx, x: int) -> str:  # pyright: ignore[reportUnknownParameterType,reportMissingParameterType]
    return str(x + 2)

async def baz(x: int) -> str:
    return str(x + 3)

## tests/test_deps.py

class MyDeps:
    foo: int
    bar: int

## tests/test_messages.py

def test_thinking_part_delta_apply_to_thinking_part_delta():
    """Test lines 768-775: Apply ThinkingPartDelta to another ThinkingPartDelta."""
    original_delta = ThinkingPartDelta(
        content_delta='original',
        signature_delta='sig1',
        provider_name='original_provider',
        provider_details={'foo': 'bar', 'baz': 'qux'},
    )

    # Test applying delta with no content or signature - should raise error
    empty_delta = ThinkingPartDelta()
    with pytest.raises(ValueError, match='Cannot apply ThinkingPartDelta with no content or signature'):
        empty_delta.apply(original_delta)

    # Test applying delta with content_delta
    content_delta = ThinkingPartDelta(content_delta=' new_content')
    result = content_delta.apply(original_delta)
    assert isinstance(result, ThinkingPartDelta)
    assert result.content_delta == 'original new_content'

    # Test applying delta with signature_delta
    sig_delta = ThinkingPartDelta(signature_delta='new_sig')
    result = sig_delta.apply(original_delta)
    assert isinstance(result, ThinkingPartDelta)
    assert result.signature_delta == 'new_sig'

    # Test applying delta with provider_name
    content_delta = ThinkingPartDelta(content_delta='', provider_name='new_provider')
    result = content_delta.apply(original_delta)
    assert isinstance(result, ThinkingPartDelta)
    assert result.provider_name == 'new_provider'

    # Test applying delta with provider_details
    provider_details_delta = ThinkingPartDelta(
        content_delta='', provider_details={'finish_reason': 'STOP', 'foo': 'qux'}
    )
    result = provider_details_delta.apply(original_delta)
    assert isinstance(result, ThinkingPartDelta)
    assert result.provider_details == {'foo': 'qux', 'baz': 'qux', 'finish_reason': 'STOP'}

    # Test chaining callable provider_details in delta-to-delta
    delta1 = ThinkingPartDelta(
        content_delta='first',
        provider_details=lambda d: {**(d or {}), 'first': 1},
    )
    delta2 = ThinkingPartDelta(
        content_delta=' second',
        provider_details=lambda d: {**(d or {}), 'second': 2},
    )
    chained = delta2.apply(delta1)
    assert isinstance(chained, ThinkingPartDelta)
    assert callable(chained.provider_details)
    # Apply chained delta to actual ThinkingPart to verify both callables ran
    part = ThinkingPart(content='')
    result_part = chained.apply(part)
    assert result_part.provider_details == {'first': 1, 'second': 2}

    # Test applying dict delta to callable delta (dict should merge with callable result)
    delta_callable = ThinkingPartDelta(
        content_delta='callable',
        provider_details=lambda d: {**(d or {}), 'from_callable': 'yes'},
    )
    delta_dict = ThinkingPartDelta(
        content_delta=' dict',
        provider_details={'from_dict': 'also'},
    )
    chained = delta_dict.apply(delta_callable)
    assert isinstance(chained, ThinkingPartDelta)
    assert callable(chained.provider_details)
    part = ThinkingPart(content='')
    result_part = chained.apply(part)
    assert result_part.provider_details == {'from_callable': 'yes', 'from_dict': 'also'}

## tests/test_thinking_part.py

def test_split_content(thinking_tags: tuple[str, str], content: str, parts: list[ModelResponsePart]):
    assert split_content_into_text_and_thinking(content, thinking_tags) == parts

def test_thinking_part_delta_applies_both_content_and_signature():
    thinking_part = ThinkingPart(content='Initial content', signature='initial_sig')
    delta = ThinkingPartDelta(content_delta=' added', signature_delta='new_sig')

    result = delta.apply(thinking_part)

    # The content is appended, and the signature is updated.
    assert result == snapshot(ThinkingPart(content='Initial content added', signature='new_sig'))

def test_thinking_part_delta_applies_signature_only():
    thinking_part = ThinkingPart(content='Initial content', signature='initial_sig')
    delta_sig_only = ThinkingPartDelta(content_delta=None, signature_delta='sig_only')

    result_sig_only = delta_sig_only.apply(thinking_part)

    # The content is unchanged, and the signature is updated.
    assert result_sig_only == snapshot(ThinkingPart(content='Initial content', signature='sig_only'))

def test_thinking_part_delta_applies_content_only_preserves_signature():
    thinking_part = ThinkingPart(content='Initial content', signature='initial_sig')
    delta_content_only = ThinkingPartDelta(content_delta=' more', signature_delta=None)

    result_content_only = delta_content_only.apply(thinking_part)

    # The content is appended, and the signature is preserved.
    assert result_content_only == snapshot(ThinkingPart(content='Initial content more', signature='initial_sig'))

def test_thinking_part_delta_applies_to_part_with_none_signature():
    thinking_part_no_sig = ThinkingPart(content='No sig content', signature=None)
    delta_to_none_sig = ThinkingPartDelta(content_delta=' extra', signature_delta='added_sig')

    result_none_sig = delta_to_none_sig.apply(thinking_part_no_sig)

    # The content is appended, and the signature is updated.
    assert result_none_sig == snapshot(ThinkingPart(content='No sig content extra', signature='added_sig'))
