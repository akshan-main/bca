# tests/test_builtin_tools.py:78-103
def test_url_context_tool_backward_compatibility():
    """Test that old payloads with 'url_context' kind can be deserialized."""
    adapter = TypeAdapter(AbstractBuiltinTool)

    # Test 1: Old payload with url_context should deserialize to UrlContextTool (which is deprecated)
    old_payload = {'kind': 'url_context', 'max_uses': 5, 'enable_citations': True}
    with pytest.warns(DeprecationWarning, match='Use `WebFetchTool` instead.'):
        result = adapter.validate_python(old_payload)
    assert isinstance(result, UrlContextTool)  # pyright: ignore[reportDeprecated]
    assert isinstance(result, WebFetchTool)  # UrlContextTool is a subclass of WebFetchTool
    assert result.kind == 'url_context'  # Preserves the original kind from payload
    assert result.max_uses == 5
    assert result.enable_citations is True

    # Test 2: Re-serialization should preserve the kind
    serialized = adapter.dump_python(result)
    assert serialized['kind'] == 'url_context'
    assert serialized['max_uses'] == 5
    assert serialized['enable_citations'] is True

    # Test 3: New payload with web_fetch should work normally
    new_payload = {'kind': 'web_fetch', 'max_uses': 10}
    result2 = adapter.validate_python(new_payload)
    assert isinstance(result2, WebFetchTool)
    assert result2.kind == 'web_fetch'
    assert result2.max_uses == 10

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:257-265
class UrlContextTool(WebFetchTool):
    """Deprecated alias for WebFetchTool. Use WebFetchTool instead.

    Overrides kind to 'url_context' so old serialized payloads with {"kind": "url_context", ...}
    can be deserialized to UrlContextTool for backward compatibility.
    """

    kind: str = 'url_context'
    """The kind of tool (deprecated value for backward compatibility)."""

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:6-6
import warnings

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py:3-3
import warnings

# pydantic_ai_slim/pydantic_ai/embeddings/instrumented.py:4-4
import warnings

# pydantic_ai_slim/pydantic_ai/mcp.py:6-6
import warnings

# pydantic_ai_slim/pydantic_ai/models/__init__.py:10-10
import warnings

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:5-5
import warnings

# pydantic_ai_slim/pydantic_ai/models/openai.py:6-6
import warnings

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:4-4
import warnings

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:3-3
import warnings

# pydantic_evals/pydantic_evals/_utils.py:5-5
import warnings

# pydantic_evals/pydantic_evals/dataset.py:17-17
import warnings

# pydantic_graph/pydantic_graph/_utils.py:6-6
import warnings

# tests/models/test_model.py:2-2
import warnings

# tests/models/test_openai.py:5-5
import warnings

# tests/test_prefect.py:4-4
import warnings

# examples/pydantic_ai_examples/flight_booking.py:109-110
class Failed(BaseModel):
    """Unable to extract a seat selection."""

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:264-264
    kind: str = 'url_context'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:251-251
    kind: str = 'web_fetch'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:47-47
    kind: str = 'unknown_builtin_tool'

# tests/models/test_xai.py:4047-4157
async def test_xai_builtin_tool_failed_in_history(allow_model_requests: None):
    """Test that failed BuiltinToolReturnPart in history updates call status.

    This test creates a message history with BOTH BuiltinToolCallPart AND BuiltinToolReturnPart
    with matching tool_call_id, where the return part has status='failed'.
    where the call status is updated to FAILED.
    """
    # Create a response for the second call
    response = create_response(content='I understand the tool failed')
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m, builtin_tools=[CodeExecutionTool()])

    # Manually construct a message history with:
    # 1. BuiltinToolCallPart (populates builtin_calls dict in _map_response_parts)
    # 2. BuiltinToolReturnPart with status='failed'
    message_history: list[ModelMessage] = [
        ModelRequest(parts=[UserPromptPart(content='Run some code')]),
        ModelResponse(
            parts=[
                BuiltinToolCallPart(
                    tool_name='code_execution',
                    args={'code': 'print("test")'},
                    tool_call_id='code_fail_1',
                    provider_name='xai',  # Must match self.system
                ),
                BuiltinToolReturnPart(
                    tool_name='code_execution',
                    content='Error: execution failed',
                    tool_call_id='code_fail_1',  # Same ID as BuiltinToolCallPart
                    provider_name='xai',  # Must match self.system
                    provider_details={'status': 'failed', 'error': 'Execution timeout'},
                ),
            ],
            model_name=XAI_NON_REASONING_MODEL,
        ),
    ]

    result = await agent.run('What happened?', message_history=message_history)

    # Verify kwargs - the call should have the failed builtin tool with FAILED status and error_message
    assert get_mock_chat_create_kwargs(mock_client) == snapshot(
        [
            {
                'model': XAI_NON_REASONING_MODEL,
                'messages': [
                    {'content': [{'text': 'Run some code'}], 'role': 'ROLE_USER'},
                    {
                        'content': [{'text': ''}],
                        'role': 'ROLE_ASSISTANT',
                        'tool_calls': [
                            {
                                'id': 'code_fail_1',
                                'type': 'TOOL_CALL_TYPE_CODE_EXECUTION_TOOL',
                                'status': 'TOOL_CALL_STATUS_COMPLETED',
                                'function': {'name': 'code_execution', 'arguments': '{"code":"print(\\"test\\")"}'},
                            }
                        ],
                    },
                    {'content': [{'text': 'What happened?'}], 'role': 'ROLE_USER'},
                ],
                'tools': [{'code_execution': {}}],
                'tool_choice': 'auto',
                'response_format': None,
                'use_encrypted_content': False,
                'include': [],
            },
        ]
    )

    assert result.all_messages() == snapshot(
        [
            ModelRequest(parts=[UserPromptPart(content='Run some code', timestamp=IsDatetime())]),
            ModelResponse(
                parts=[
                    BuiltinToolCallPart(
                        tool_name='code_execution',
                        args={'code': 'print("test")'},
                        tool_call_id='code_fail_1',
                        provider_name='xai',
                    ),
                    BuiltinToolReturnPart(
                        tool_name='code_execution',
                        content='Error: execution failed',
                        tool_call_id='code_fail_1',
                        timestamp=IsDatetime(),
                        provider_name='xai',
                        provider_details={'status': 'failed', 'error': 'Execution timeout'},
                    ),
                ],
                model_name=XAI_NON_REASONING_MODEL,
                timestamp=IsDatetime(),
            ),
            ModelRequest(
                parts=[UserPromptPart(content='What happened?', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='I understand the tool failed')],
                usage=RequestUsage(),
                model_name=XAI_NON_REASONING_MODEL,
                timestamp=IsDatetime(),
                provider_url='https://api.x.ai/v1',
                provider_name='xai',
                provider_response_id=IsStr(),
                finish_reason='stop',
                run_id=IsStr(),
            ),
        ]
    )

# pydantic_ai_slim/pydantic_ai/messages.py:1318-1318
    kind: Literal['response'] = 'response'

# pydantic_ai_slim/pydantic_ai/messages.py:493-493
    kind: Literal['binary'] = 'binary'

# pydantic_ai_slim/pydantic_ai/tools.py:512-512
    kind: ToolKind = field(default='function')

# pydantic_ai_slim/pydantic_ai/messages.py:1043-1043
    kind: Literal['request'] = 'request'

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:62-94
def default_render_number_diff(old: float | int, new: float | int) -> str | None:
    """Return a string representing the difference between old and new values.

    Rules:
      - If the two values are equal, return None.
      - For integers, return the raw difference (with a leading sign), e.g.:
            _default_format_number_diff(3, 4) -> '+1'
      - For floats (or a mix of float and int):
          * Compute the raw delta = new - old and format it with ABS_SIG_FIGS significant figures.
          * If `old` is nonzero, compute a relative change:
              - If |delta|/|old| ≤ 1, render the relative change as a percentage with
                PERC_DECIMALS decimal places, e.g. '+0.7 / +70.0%'.
              - If |delta|/|old| > 1, render a multiplier (new/old). Use one decimal place
                if the absolute multiplier is less than MULTIPLIER_ONE_DECIMAL_THRESHOLD,
                otherwise no decimals.
          * However, if the percentage rounds to 0.0% (e.g. '+0.0%'), return only the absolute diff.
          * Also, if |old| is below BASE_THRESHOLD and |delta| exceeds MULTIPLIER_DROP_FACTOR×|old|,
            drop the relative change indicator.
    """
    if old == new:
        return None

    if isinstance(old, int) and isinstance(new, int):
        diff_int = new - old
        return f'{diff_int:+d}'

    delta = new - old
    abs_diff_str = _render_signed(delta, ABS_SIG_FIGS)
    rel_diff_str = _render_relative(new, old, BASE_THRESHOLD)
    if rel_diff_str is None:
        return abs_diff_str
    else:
        return f'{abs_diff_str} / {rel_diff_str}'

# pydantic_ai_slim/pydantic_ai/messages.py:372-372
    kind: Literal['image-url'] = 'image-url'

# pydantic_ai_slim/pydantic_ai/messages.py:418-418
    kind: Literal['document-url'] = 'document-url'

# pydantic_ai_slim/pydantic_ai/messages.py:658-658
    kind: Literal['cache-point'] = 'cache-point'

# pydantic_ai_slim/pydantic_ai/messages.py:266-266
    kind: Literal['video-url'] = 'video-url'

# pydantic_ai_slim/pydantic_ai/messages.py:325-325
    kind: Literal['audio-url'] = 'audio-url'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:152-152
    kind: str = 'web_search'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:356-356
    kind: str = 'image_generation'

# pydantic_graph/pydantic_graph/persistence/__init__.py:58-58
    kind: Literal['node'] = 'node'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:433-433
    kind: str = 'mcp_server'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:191-191
    kind: str = 'code_execution'

# pydantic_ai_slim/pydantic_ai/messages.py:701-701
    kind: Literal['tool-return'] = 'tool-return'

# pydantic_graph/pydantic_graph/persistence/__init__.py:79-79
    kind: Literal['end'] = 'end'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:464-464
    kind: str = 'file_search'

# pydantic_ai_slim/pydantic_ai/tools.py:174-174
    kind: Literal['tool-approved'] = 'tool-approved'

# pydantic_ai_slim/pydantic_ai/tools.py:186-186
    kind: Literal['tool-denied'] = 'tool-denied'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:235-235
    enable_citations: bool = False

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:369-369
    kind: str = 'memory'

# pydantic_graph/pydantic_graph/beta/mermaid.py:38-38
    kind: NodeKind

# tests/test_builtin_tools.py:72-75
def test_url_context_tool_is_deprecated():
    """Test that UrlContextTool is deprecated and warns users to use WebFetchTool instead."""
    with pytest.warns(DeprecationWarning, match='Use `WebFetchTool` instead.'):
        UrlContextTool()  # pyright: ignore[reportDeprecated]

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:207-207
    max_uses: int | None = None

# pydantic_ai_slim/pydantic_ai/_output.py:675-675
    kind: str

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:40-40
    kind: Literal['approval_required'] = 'approval_required'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:46-46
    kind: Literal['call_deferred'] = 'call_deferred'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:52-52
    kind: Literal['model_retry'] = 'model_retry'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:58-58
    kind: Literal['tool_return'] = 'tool_return'

# tests/test_agent.py:5965-5980
def test_deprecated_kwargs_still_work():
    """Test that valid deprecated kwargs still work with warnings."""
    import warnings

    try:
        from pydantic_ai.mcp import MCPServerStdio

        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')

            Agent('test', mcp_servers=[MCPServerStdio('python', ['-m', 'tests.mcp_server'])])  # type: ignore[call-arg]
            assert len(w) == 1
            assert issubclass(w[0].category, DeprecationWarning)
            assert '`mcp_servers` is deprecated' in str(w[0].message)
    except ImportError:
        pass

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:39-84
class AbstractBuiltinTool(ABC):
    """A builtin tool that can be used by an agent.

    This class is abstract and cannot be instantiated directly.

    The builtin tools are passed to the model as part of the `ModelRequestParameters`.
    """

    kind: str = 'unknown_builtin_tool'
    """Built-in tool identifier, this should be available on all built-in tools as a discriminator."""

    @property
    def unique_id(self) -> str:
        """A unique identifier for the builtin tool.

        If multiple instances of the same builtin tool can be passed to the model, subclasses should override this property to allow them to be distinguished.
        """
        return self.kind

    @property
    def label(self) -> str:
        """Human-readable label for UI display.

        Subclasses should override this to provide a meaningful label.
        """
        return self.kind.replace('_', ' ').title()

    def __init_subclass__(cls, **kwargs: Any) -> None:
        super().__init_subclass__(**kwargs)
        BUILTIN_TOOL_TYPES[cls.kind] = cls

    @classmethod
    def __get_pydantic_core_schema__(
        cls, _source_type: Any, handler: pydantic.GetCoreSchemaHandler
    ) -> core_schema.CoreSchema:
        if cls is not AbstractBuiltinTool:
            return handler(cls)

        tools = BUILTIN_TOOL_TYPES.values()
        if len(tools) != 1:  # pragma: no cover
            tools_type = next(iter(tools))
        else:
            tools_annotated = [Annotated[tool, pydantic.Tag(tool.kind)] for tool in tools]
            tools_type = Annotated[Union[tuple(tools_annotated)], pydantic.Discriminator(_tool_discriminator)]  # noqa: UP007

        return handler(tools_type)

# pydantic_evals/pydantic_evals/dataset.py:16-16
import traceback

# pydantic_evals/pydantic_evals/evaluators/_run_evaluator.py:3-3
import traceback

# pydantic_graph/pydantic_graph/beta/paths.py:32-57
class TransformFunction(Protocol[StateT, DepsT, InputT, OutputT]):
    """Protocol for step functions that can be executed in the graph.

    Transform functions are sync callables that receive a step context and return
    a result. This protocol enables serialization and deserialization of step
    calls similar to how evaluators work.

    This is very similar to a StepFunction, but must be sync instead of async.

    Type Parameters:
        StateT: The type of the graph state
        DepsT: The type of the dependencies
        InputT: The type of the input data
        OutputT: The type of the output data
    """

    def __call__(self, ctx: StepContext[StateT, DepsT, InputT]) -> OutputT:
        """Execute the step function with the given context.

        Args:
            ctx: The step context containing state, dependencies, and inputs

        Returns:
            An awaitable that resolves to the step's output
        """
        raise NotImplementedError

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:144-144
    max_uses: int | None = None

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:20-20
    type: Literal['tool_call']

# tests/models/test_instrumented.py:10-10
from inline_snapshot.extra import warns

# tests/test_usage_limits.py:11-11
from inline_snapshot.extra import warns

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:15-15
    type: Literal['text']

# pydantic_graph/pydantic_graph/persistence/__init__.py:215-225
    def set_types(self, state_type: type[StateT], run_end_type: type[RunEndT]) -> None:
        """Set the types of the state and run end.

        This can be used to create [type adapters][pydantic.TypeAdapter] for serializing and deserializing snapshots,
        e.g. with [`build_snapshot_list_type_adapter`][pydantic_graph.persistence.build_snapshot_list_type_adapter].

        Args:
            state_type: The state type.
            run_end_type: The run end type.
        """
        pass

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:47-47
    type: Literal['thinking']

# pydantic_ai_slim/pydantic_ai/__init__.py:12-22
from .builtin_tools import (
    CodeExecutionTool,
    FileSearchTool,
    ImageGenerationTool,
    MCPServerTool,
    MemoryTool,
    UrlContextTool,  # pyright: ignore[reportDeprecated]
    WebFetchTool,
    WebSearchTool,
    WebSearchUserLocation,
)

# tests/models/test_google.py:55-62
from pydantic_ai.builtin_tools import (
    CodeExecutionTool,
    FileSearchTool,
    ImageGenerationTool,
    UrlContextTool,  # pyright: ignore[reportDeprecated]
    WebFetchTool,
    WebSearchTool,
)

# tests/test_builtin_tools.py:7-14
from pydantic_ai.builtin_tools import (
    AbstractBuiltinTool,
    CodeExecutionTool,
    FileSearchTool,
    UrlContextTool,  # pyright: ignore[reportDeprecated]
    WebFetchTool,
    WebSearchTool,
)

# tests/test_messages.py:801-835
def test_tool_return_content_with_explicit_image_url():
    """Test that ImageUrl with explicit 'kind' discriminator is correctly deserialized."""
    from pydantic_ai.messages import ToolReturnPart

    serialized_history = r"""[
      {
        "parts": [{"content": "Hello", "timestamp": "2026-02-03T22:25:50Z", "part_kind": "user-prompt"}],
        "kind": "request"
      },
      {
        "parts": [
          {
            "tool_name": "image_tool",
            "content": {
              "url": "https://example.com/image.png",
              "kind": "image-url"
            },
            "tool_call_id": "call_1",
            "timestamp": "2026-02-03T22:27:32Z",
            "part_kind": "tool-return"
          }
        ],
        "kind": "request"
      }
    ]
    """

    deserialized = ModelMessagesTypeAdapter.validate_json(serialized_history)

    tool_return_part = deserialized[1].parts[0]
    assert isinstance(tool_return_part, ToolReturnPart)

    # Content with explicit kind: "image-url" should become ImageUrl
    assert isinstance(tool_return_part.content, ImageUrl)
    assert tool_return_part.content.url == 'https://example.com/image.png'

# tests/models/mock_xai.py:493-524
def create_failed_builtin_tool_response(
    tool_name: str,
    tool_type: chat_pb2.ToolCallType,
    *,
    tool_call_id: str = 'failed_tool_001',
    error_message: str = 'tool failed',
    content: ToolCallOutputType | None = None,
) -> chat_types.Response:
    """Create a Response representing a failed builtin tool call."""
    output = chat_pb2.CompletionOutput(
        index=0,
        finish_reason=sample_pb2.FinishReason.REASON_STOP,
        message=chat_pb2.CompletionMessage(
            role=chat_pb2.MessageRole.ROLE_ASSISTANT,
            content=_serialize_content(content or ''),
            tool_calls=[
                create_server_tool_call(
                    tool_name,
                    {},
                    tool_call_id=tool_call_id,
                    tool_type=tool_type,
                    status=chat_pb2.ToolCallStatus.TOOL_CALL_STATUS_FAILED,
                    error_message=error_message,
                )
            ],
        ),
    )

    return _build_response_with_outputs(
        response_id=f'grok-{tool_call_id}',
        outputs=[output],
    )

# pydantic_evals/pydantic_evals/reporting/__init__.py:106-130
class ReportCaseFailure(Generic[InputsT, OutputT, MetadataT]):
    """A single case in an evaluation report that failed due to an error during task execution."""

    name: str
    """The name of the [case][pydantic_evals.Case]."""
    inputs: InputsT
    """The inputs to the task, from [`Case.inputs`][pydantic_evals.dataset.Case.inputs]."""
    metadata: MetadataT | None
    """Any metadata associated with the case, from [`Case.metadata`][pydantic_evals.dataset.Case.metadata]."""
    expected_output: OutputT | None
    """The expected output of the task, from [`Case.expected_output`][pydantic_evals.dataset.Case.expected_output]."""

    error_message: str
    """The message of the exception that caused the failure."""
    error_stacktrace: str
    """The stacktrace of the exception that caused the failure."""

    source_case_name: str | None = None
    """The original case name before run-indexing. Serves as the aggregation key
    for multi-run experiments. None when repeat == 1."""

    trace_id: str | None = None
    """The trace ID of the case span."""
    span_id: str | None = None
    """The span ID of the case span."""

# tests/test_builtin_tools.py:126-148
def test_url_context_discriminated_union():
    """Test that the discriminated union correctly handles both url_context and web_fetch."""
    adapter = TypeAdapter(list[AbstractBuiltinTool])

    # Mix of old and new payloads
    payloads = [
        {'kind': 'url_context', 'max_uses': 1},
        {'kind': 'web_fetch', 'max_uses': 2},
        {'kind': 'web_search'},
        {'kind': 'code_execution'},
    ]

    # Old url_context payloads will trigger deprecation warnings
    with pytest.warns(DeprecationWarning, match='Use `WebFetchTool` instead.'):
        results = adapter.validate_python(payloads)
    assert len(results) == 4
    assert isinstance(results[0], UrlContextTool)  # pyright: ignore[reportDeprecated]
    assert isinstance(results[0], WebFetchTool)  # UrlContextTool is a subclass
    assert results[0].kind == 'url_context'
    assert results[0].max_uses == 1
    assert isinstance(results[1], WebFetchTool)
    assert results[1].kind == 'web_fetch'
    assert results[1].max_uses == 2

# tests/test_json_body_serializer.py:6-6
from .json_body_serializer import deserialize, serialize

# pydantic_graph/pydantic_graph/beta/paths.py:295-421
class EdgePathBuilder(Generic[StateT, DepsT, OutputT]):
    """A builder for constructing complete edge paths with method chaining.

    EdgePathBuilder combines source nodes with path building capabilities
    to create complete edge definitions. It cannot use dataclass due to
    type variance issues.

    Type Parameters:
        StateT: The type of the graph state
        DepsT: The type of the dependencies
        OutputT: The type of the current data in the path
    """

    def __init__(
        self, sources: Sequence[SourceNode[StateT, DepsT, Any]], path_builder: PathBuilder[StateT, DepsT, OutputT]
    ):
        """Initialize an edge path builder.

        Args:
            sources: The source nodes for this edge path
            path_builder: The path builder for defining the data flow
        """
        self.sources = sources
        self._path_builder = path_builder

    def to(
        self,
        destination: DestinationNode[StateT, DepsT, OutputT] | type[BaseNode[StateT, DepsT, Any]],
        /,
        *extra_destinations: DestinationNode[StateT, DepsT, OutputT] | type[BaseNode[StateT, DepsT, Any]],
        fork_id: str | None = None,
    ) -> EdgePath[StateT, DepsT]:
        """Complete the edge path by routing to destination nodes.

        Args:
            destination: Either a destination node or a function that generates edge paths
            *extra_destinations: Additional destination nodes (creates a broadcast)
            fork_id: Optional ID for the fork created when multiple destinations are specified

        Returns:
            A complete EdgePath connecting sources to destinations
        """
        # `type[BaseNode[StateT, DepsT, Any]]` could actually be a `typing._GenericAlias` like `pydantic_ai._agent_graph.UserPromptNode[~DepsT, ~OutputT]`,
        # so we get the origin to get to the actual class
        destination = get_origin(destination) or destination
        extra_destinations = tuple(get_origin(d) or d for d in extra_destinations)
        destinations = [(NodeStep(d) if inspect.isclass(d) else d) for d in (destination, *extra_destinations)]
        return EdgePath(
            sources=self.sources,
            path=self._path_builder.to(destinations[0], *destinations[1:], fork_id=fork_id),
            destinations=destinations,
        )

    def broadcast(
        self, get_forks: Callable[[Self], Sequence[EdgePath[StateT, DepsT]]], /, *, fork_id: str | None = None
    ) -> EdgePath[StateT, DepsT]:
        """Broadcast this EdgePathBuilder into multiple destinations.

        Args:
            get_forks: The callback that will return a sequence of EdgePaths to broadcast to.
            fork_id: Optional node ID to use for the resulting broadcast fork.

        Returns:
            A completed EdgePath with the specified destinations.
        """
        new_edge_paths = get_forks(self)
        new_paths = [Path(x.path.items) for x in new_edge_paths]
        if not new_paths:
            raise GraphBuildingError(f'The call to {get_forks} returned no branches, but must return at least one.')
        path = self._path_builder.broadcast(new_paths, fork_id=fork_id)
        destinations = [d for ep in new_edge_paths for d in ep.destinations]
        return EdgePath(
            sources=self.sources,
            path=path,
            destinations=destinations,
        )

    def map(
        self: EdgePathBuilder[StateT, DepsT, Iterable[T]] | EdgePathBuilder[StateT, DepsT, AsyncIterable[T]],
        *,
        fork_id: str | None = None,
        downstream_join_id: JoinID | None = None,
    ) -> EdgePathBuilder[StateT, DepsT, T]:
        """Spread iterable data across parallel execution paths.

        Args:
            fork_id: Optional ID for the fork, defaults to a generated value
            downstream_join_id: Optional ID of a downstream join node which is involved when mapping empty iterables

        Returns:
            A new EdgePathBuilder that operates on individual items from the iterable
        """
        if len(self.sources) > 1:
            # The current implementation mishandles this because you get one copy of each edge
            # from the MapMarker to its destination for each source, resulting in unintentional multiple execution.
            # I suspect this is fixable without a major refactor, though it's not clear to me what the ideal behavior
            # would be. But for now, it's definitely easiest to just raise an error for this.
            raise NotImplementedError(
                'Map is not currently supported with multiple source nodes.'
                ' You can work around this by just creating a separate edge for each source.'
            )
        return EdgePathBuilder(
            sources=self.sources,
            path_builder=self._path_builder.map(fork_id=fork_id, downstream_join_id=downstream_join_id),
        )

    def transform(self, func: TransformFunction[StateT, DepsT, OutputT, T], /) -> EdgePathBuilder[StateT, DepsT, T]:
        """Add a transformation step to the edge path.

        Args:
            func: The step function that will transform the data

        Returns:
            A new EdgePathBuilder with the transformation added
        """
        return EdgePathBuilder(sources=self.sources, path_builder=self._path_builder.transform(func))

    def label(self, label: str) -> EdgePathBuilder[StateT, DepsT, OutputT]:
        """Add a human-readable label to this point in the edge path.

        Args:
            label: The label to add for documentation/debugging purposes

        Returns:
            A new EdgePathBuilder with the label added
        """
        return EdgePathBuilder(sources=self.sources, path_builder=self._path_builder.label(label))

# tests/models/test_xai.py:5235-5296
async def test_xai_builtin_tool_failed_without_error_in_history(allow_model_requests: None):
    """Test failed BuiltinToolReturnPart without error message in history."""
    response = create_response(content='I see it failed')
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m, builtin_tools=[CodeExecutionTool()])

    # Construct history with failed builtin tool but NO 'error' key in provider_details
    # This triggers the branch where error_msg is falsy
    message_history: list[ModelMessage] = [
        ModelRequest(parts=[UserPromptPart(content='Run code')]),
        ModelResponse(
            parts=[
                BuiltinToolCallPart(
                    tool_name='code_execution',
                    args={},
                    tool_call_id='fail_no_error_1',
                    provider_name='xai',
                ),
                BuiltinToolReturnPart(
                    tool_name='code_execution',
                    content='Failed',
                    tool_call_id='fail_no_error_1',
                    provider_name='xai',
                    provider_details={'status': 'failed'},  # No 'error' key!
                ),
            ],
            model_name=XAI_NON_REASONING_MODEL,
        ),
    ]

    await agent.run('What happened?', message_history=message_history)

    # Verify kwargs - status is FAILED but no error_message since 'error' key was missing
    assert get_mock_chat_create_kwargs(mock_client) == snapshot(
        [
            {
                'model': XAI_NON_REASONING_MODEL,
                'messages': [
                    {'content': [{'text': 'Run code'}], 'role': 'ROLE_USER'},
                    {
                        'content': [{'text': ''}],
                        'role': 'ROLE_ASSISTANT',
                        'tool_calls': [
                            {
                                'id': 'fail_no_error_1',
                                'type': 'TOOL_CALL_TYPE_CODE_EXECUTION_TOOL',
                                'status': 'TOOL_CALL_STATUS_COMPLETED',
                                'function': {'name': 'code_execution', 'arguments': '{}'},
                            }
                        ],
                    },
                    {'content': [{'text': 'What happened?'}], 'role': 'ROLE_USER'},
                ],
                'tools': [{'code_execution': {}}],
                'tool_choice': 'auto',
                'response_format': None,
                'use_encrypted_content': False,
                'include': [],
            }
        ]
    )

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:93-105
class StateDeps(Generic[StateT]):
    """Dependency type that holds state.

    This class is used to manage the state of an agent run. It allows setting
    the state of the agent run with a specific type of state model, which must
    be a subclass of `BaseModel`.

    The state is set using the `state` setter by the `Adapter` when the run starts.

    Implements the `StateHandler` protocol.
    """

    state: StateT

# tests/test_usage_limits.py:363-379
async def test_failed_tool_calls_not_counted() -> None:
    """Test that failed tool calls (raising ModelRetry) are not counted in usage or against limits."""
    test_agent = Agent(TestModel())

    call_count = 0

    @test_agent.tool_plain
    async def flaky_tool(x: str) -> str:
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            raise ModelRetry('Temporary failure, please retry')
        return f'{x}-success'

    result = await test_agent.run('test', usage_limits=UsageLimits(tool_calls_limit=1))
    assert call_count == 2
    assert result.usage() == snapshot(RunUsage(requests=3, input_tokens=176, output_tokens=29, tool_calls=1))

# pydantic_evals/pydantic_evals/evaluators/evaluator.py:146-162
    def get_default_evaluation_name(self) -> str:
        """Return the default name to use in reports for the output of this evaluator.

        By default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used.
        Otherwise, the serialization name of the evaluator (which is usually the class name) will be used.

        This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.

        Note that evaluators that return a mapping of results will always use the keys of that mapping as the names
        of the associated evaluation results.
        """
        evaluation_name = getattr(self, 'evaluation_name', None)
        if isinstance(evaluation_name, str):
            # If the evaluator has an attribute `name` of type string, use that
            return evaluation_name

        return self.get_serialization_name()

# pydantic_graph/pydantic_graph/beta/decision.py:88-124
class DecisionBranch(Generic[SourceT]):
    """Represents a single branch within a decision node.

    Each branch defines the conditions under which it should be taken
    and the path to follow when those conditions are met.

    Note: with the current design, it is actually _critical_ that this class is invariant in SourceT for the sake
    of type-checking that inputs to a Decision are actually handled. See the `# type: ignore` comment in
    `tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch` for an example of how this works.
    """

    source: TypeOrTypeExpression[SourceT]
    """The expected type of data for this branch.

    This is necessary for exhaustiveness-checking when handling the inputs to a decision node."""

    matches: Callable[[Any], bool] | None
    """An optional predicate function used to determine whether input data matches this branch.

    If `None`, default logic is used which attempts to check the value for type-compatibility with the `source` type:
    * If `source` is `Any` or `object`, the branch will always match
    * If `source` is a `Literal` type, this branch will match if the value is one of the parametrizing literal values
    * If `source` is any other type, the value will be checked for matching using `isinstance`

    Inputs are tested against each branch of a decision node in order, and the path of the first matching branch is
    used to handle the input value.
    """

    path: Path
    """The execution path to follow when an input value matches this branch of a decision node.

    This can include transforming, mapping, and broadcasting the output before sending to the next node or nodes.

    The path can also include position-aware labels which are used when generating mermaid diagrams."""

    destinations: list[AnyDestinationNode]
    """The destination nodes that can be referenced by DestinationMarker in the path."""

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:152-152
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_graph/pydantic_graph/beta/step.py:90-112
class StreamFunction(Protocol[StateT, DepsT, InputT, OutputT]):
    """Protocol for stream functions that can be executed in the graph.

    Stream functions are async callables that receive a step context and return an async iterator.

    Type Parameters:
        StateT: The type of the graph state
        DepsT: The type of the dependencies
        InputT: The type of the input data
        OutputT: The type of the output data
    """

    def __call__(self, ctx: StepContext[StateT, DepsT, InputT]) -> AsyncIterator[OutputT]:
        """Execute the stream function with the given context.

        Args:
            ctx: The step context containing state, dependencies, and inputs

        Returns:
            An async iterator yielding the streamed output
        """
        raise NotImplementedError
        yield

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:139-139
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:192-192
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:90-90
    type: Literal['tool-input-start'] = 'tool-input-start'

# pydantic_evals/pydantic_evals/reporting/analyses.py:21-21
    type: Literal['confusion_matrix'] = 'confusion_matrix'

# pydantic_evals/pydantic_evals/reporting/analyses.py:52-52
    type: Literal['precision_recall'] = 'precision_recall'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:78-78
    type: Literal['file'] = 'file'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:128-128
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:205-205
    type: Literal['dynamic-tool'] = 'dynamic-tool'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:124-124
    type: Literal['tool-input-available'] = 'tool-input-available'

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:136-136
    type: Literal['tool-input-error'] = 'tool-input-error'

# pydantic_evals/pydantic_evals/reporting/analyses.py:62-62
    type: Literal['scalar'] = 'scalar'

# pydantic_evals/pydantic_evals/reporting/analyses.py:73-73
    type: Literal['table'] = 'table'

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:16-23
from ..builtin_tools import (
    AbstractBuiltinTool,
    CodeExecutionTool,
    MCPServerTool,
    MemoryTool,
    WebFetchTool,
    WebSearchTool,
)

# tests/models/test_anthropic.py:48-48
from pydantic_ai.builtin_tools import CodeExecutionTool, MCPServerTool, MemoryTool, WebFetchTool, WebSearchTool

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:27-27
    type: Literal['text'] = 'text'