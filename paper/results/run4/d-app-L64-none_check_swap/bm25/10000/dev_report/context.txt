# pydantic_graph/pydantic_graph/mermaid.py:268-268
    httpx_client: httpx.Client

# tests/conftest.py:345-370
async def close_cached_httpx_client(anyio_backend: str, monkeypatch: pytest.MonkeyPatch) -> AsyncIterator[None]:
    """Track and close cached httpx clients created during each test.

    Prevents reusing AsyncClient instances across tests (and event loops),
    which can cause 'Event loop is closed' errors, without touching prod code.
    """
    created_clients: set[httpx.AsyncClient] = set()

    # Patch the cached factory to record returned clients while preserving caching.
    original_cached_func = pydantic_ai.models._cached_async_http_client  # type: ignore[reportPrivateUsage]

    def tracked_cached_async_http_client(*args: Any, **kwargs: Any):
        client = original_cached_func(*args, **kwargs)
        created_clients.add(client)
        return client

    monkeypatch.setattr(pydantic_ai.models, '_cached_async_http_client', tracked_cached_async_http_client)

    yield

    # Close only the clients that were actually created/accessed in this test
    for client in created_clients:
        await client.aclose()

    # Ensure no stale cached clients persist between tests (new event loop per test)
    original_cached_func.cache_clear()

# tests/conftest.py:345-370
async def close_cached_httpx_client(anyio_backend: str, monkeypatch: pytest.MonkeyPatch) -> AsyncIterator[None]:
    """Track and close cached httpx clients created during each test.

    Prevents reusing AsyncClient instances across tests (and event loops),
    which can cause 'Event loop is closed' errors, without touching prod code.
    """
    created_clients: set[httpx.AsyncClient] = set()

    # Patch the cached factory to record returned clients while preserving caching.
    original_cached_func = pydantic_ai.models._cached_async_http_client  # type: ignore[reportPrivateUsage]

    def tracked_cached_async_http_client(*args: Any, **kwargs: Any):
        client = original_cached_func(*args, **kwargs)
        created_clients.add(client)
        return client

    monkeypatch.setattr(pydantic_ai.models, '_cached_async_http_client', tracked_cached_async_http_client)

    yield

    # Close only the clients that were actually created/accessed in this test
    for client in created_clients:
        await client.aclose()

    # Ensure no stale cached clients persist between tests (new event loop per test)
    original_cached_func.cache_clear()

# tests/conftest.py:345-370
async def close_cached_httpx_client(anyio_backend: str, monkeypatch: pytest.MonkeyPatch) -> AsyncIterator[None]:
    """Track and close cached httpx clients created during each test.

    Prevents reusing AsyncClient instances across tests (and event loops),
    which can cause 'Event loop is closed' errors, without touching prod code.
    """
    created_clients: set[httpx.AsyncClient] = set()

    # Patch the cached factory to record returned clients while preserving caching.
    original_cached_func = pydantic_ai.models._cached_async_http_client  # type: ignore[reportPrivateUsage]

    def tracked_cached_async_http_client(*args: Any, **kwargs: Any):
        client = original_cached_func(*args, **kwargs)
        created_clients.add(client)
        return client

    monkeypatch.setattr(pydantic_ai.models, '_cached_async_http_client', tracked_cached_async_http_client)

    yield

    # Close only the clients that were actually created/accessed in this test
    for client in created_clients:
        await client.aclose()

    # Ensure no stale cached clients persist between tests (new event loop per test)
    original_cached_func.cache_clear()

# tests/models/test_google.py:3952-3960
async def test_google_httpx_client_is_not_closed(allow_model_requests: None, gemini_api_key: str):
    # This should not raise any errors, see https://github.com/pydantic/pydantic-ai/issues/3242.
    agent = Agent(GoogleModel('gemini-2.5-flash-lite', provider=GoogleProvider(api_key=gemini_api_key)))
    result = await agent.run('What is the capital of France?')
    assert result.output == snapshot('The capital of France is **Paris**.')

    agent = Agent(GoogleModel('gemini-2.5-flash-lite', provider=GoogleProvider(api_key=gemini_api_key)))
    result = await agent.run('What is the capital of Mexico?')
    assert result.output == snapshot('The capital of Mexico is **Mexico City**.')

# tests/providers/test_alibaba_provider.py:83-86
def test_alibaba_provider_with_http_client():
    http_client = httpx.AsyncClient()
    provider = AlibabaProvider(api_key='foo', http_client=http_client)
    assert provider.client.api_key == 'foo'

# tests/providers/test_alibaba_provider.py:77-80
def test_alibaba_provider_with_openai_client():
    client = openai.AsyncOpenAI(api_key='foo')
    provider = AlibabaProvider(openai_client=client)
    assert provider.client is client

# tests/models/test_gemini.py:501-537
async def get_gemini_client(
    client_with_handler: ClientWithHandler, env: TestEnv, allow_model_requests: None
) -> GetGeminiClient:
    env.set('GEMINI_API_KEY', 'via-env-var')

    def create_client(response_or_list: ResOrList) -> httpx.AsyncClient:
        index = 0

        def handler(request: httpx.Request) -> httpx.Response:
            nonlocal index

            ua = request.headers.get('User-Agent')
            assert isinstance(ua, str) and ua.startswith('pydantic-ai')

            if isinstance(response_or_list, Sequence):
                response = response_or_list[index]
                index += 1
            else:
                response = response_or_list

            if isinstance(response, httpx.AsyncByteStream):
                content: bytes | None = None
                stream: httpx.AsyncByteStream | None = response
            else:
                content = _gemini_response_ta.dump_json(response, by_alias=True)
                stream = None

            return httpx.Response(
                200,
                content=content,
                stream=stream,
                headers={'Content-Type': 'application/json'},
            )

        return client_with_handler(handler)

    return create_client

# tests/providers/test_together.py:57-60
def test_together_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = TogetherProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/models/test_huggingface.py:788-801
async def test_model_client_response_error(allow_model_requests: None) -> None:
    request_info = Mock(spec=aiohttp.RequestInfo)
    request_info.url = 'http://test.com'
    request_info.method = 'POST'
    request_info.headers = {}
    request_info.real_url = 'http://test.com'
    error = aiohttp.ClientResponseError(request_info, history=(), status=400, message='Bad Request')

    mock_client = MockHuggingFace.create_mock(error)
    m = HuggingFaceModel('not_a_model', provider=HuggingFaceProvider(hf_client=mock_client, api_key='x'))
    agent = Agent(m)
    with pytest.raises(ModelHTTPError) as exc_info:
        await agent.run('hello')
    assert str(exc_info.value) == snapshot('status_code: 400, model_name: not_a_model, body: Bad Request')

# tests/test_ui_web.py:193-204
def test_chat_app_index_endpoint():
    """Test that the index endpoint serves HTML with proper caching headers."""
    agent = Agent('test')
    app = create_web_app(agent)

    with TestClient(app) as client:
        response = client.get('/')
        assert response.status_code == 200
        assert response.headers['content-type'] == 'text/html; charset=utf-8'
        assert 'cache-control' in response.headers
        assert response.headers['cache-control'] == 'public, max-age=3600'
        assert len(response.content) > 0

# tests/providers/test_together.py:51-54
def test_together_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = TogetherProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/conftest.py:345-370
async def close_cached_httpx_client(anyio_backend: str, monkeypatch: pytest.MonkeyPatch) -> AsyncIterator[None]:
    """Track and close cached httpx clients created during each test.

    Prevents reusing AsyncClient instances across tests (and event loops),
    which can cause 'Event loop is closed' errors, without touching prod code.
    """
    created_clients: set[httpx.AsyncClient] = set()

    # Patch the cached factory to record returned clients while preserving caching.
    original_cached_func = pydantic_ai.models._cached_async_http_client  # type: ignore[reportPrivateUsage]

    def tracked_cached_async_http_client(*args: Any, **kwargs: Any):
        client = original_cached_func(*args, **kwargs)
        created_clients.add(client)
        return client

    monkeypatch.setattr(pydantic_ai.models, '_cached_async_http_client', tracked_cached_async_http_client)

    yield

    # Close only the clients that were actually created/accessed in this test
    for client in created_clients:
        await client.aclose()

    # Ensure no stale cached clients persist between tests (new event loop per test)
    original_cached_func.cache_clear()

# tests/models/test_google.py:15-15
from httpx import AsyncClient as HttpxAsyncClient, Timeout

# tests/models/test_cohere.py:75-75
    _client_wrapper: MockClientWrapper = None  # type: ignore

# tests/providers/test_gateway.py:65-68
async def test_init_with_http_client():
    async with httpx.AsyncClient() as http_client:
        provider = gateway_provider('openai', http_client=http_client, api_key='foobar')
        assert provider.client._client == http_client  # type: ignore

# tests/providers/test_gateway.py:65-68
async def test_init_with_http_client():
    async with httpx.AsyncClient() as http_client:
        provider = gateway_provider('openai', http_client=http_client, api_key='foobar')
        assert provider.client._client == http_client  # type: ignore

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# tests/providers/test_litellm.py:31-35
def test_init_with_openai_client():
    openai_client = AsyncOpenAI(api_key='custom-key', base_url='https://custom.openai.com/v1')
    provider = LiteLLMProvider(openai_client=openai_client)
    assert provider.client == openai_client
    assert provider.base_url == 'https://custom.openai.com/v1/'

# tests/providers/test_vercel.py:124-128
def test_vercel_with_http_client():
    http_client = httpx.AsyncClient()
    provider = VercelProvider(api_key='test-key', http_client=http_client)
    assert provider.client.api_key == 'test-key'
    assert str(provider.client.base_url) == 'https://ai-gateway.vercel.sh/v1/'

# tests/graph/test_mermaid.py:266-266
HttpxWithHandler = Callable[[Callable[[httpx.Request], httpx.Response]], httpx.Client]

# tests/test_mcp.py:1580-1593
async def test_client_sampling(run_context: RunContext[int]):
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'])
    server.sampling_model = TestModel(custom_output_text='sampling model response')
    async with server:
        result = await server.direct_call_tool('use_sampling', {'foo': 'bar'})
        assert result == snapshot(
            {
                '_meta': None,
                'role': 'assistant',
                'content': {'type': 'text', 'text': 'sampling model response', 'annotations': None, '_meta': None},
                'model': 'test',
                'stopReason': None,
            }
        )

# tests/graph/test_mermaid.py:250-263
def httpx_with_handler() -> Iterator[HttpxWithHandler]:
    client: httpx.Client | None = None

    def create_client(handler: Callable[[httpx.Request], httpx.Response]) -> httpx.Client:
        nonlocal client
        assert client is None, 'client_with_handler can only be called once'
        client = httpx.Client(mounts={'all://': httpx.MockTransport(handler)})
        return client

    try:
        yield create_client
    finally:
        if client:  # pragma: no branch
            client.close()

# pydantic_ai_slim/pydantic_ai/models/gemini.py:21-21
from httpx import USE_CLIENT_DEFAULT, Response as HTTPResponse

# tests/providers/test_xai.py:36-39
def test_xai_pass_xai_client() -> None:
    xai_client = AsyncClient(api_key='api-key')
    provider = XaiProvider(xai_client=xai_client)
    assert provider.client == xai_client

# tests/providers/test_google_vertex.py:34-43
def http_client():
    async def handler(request: httpx.Request):
        if (
            request.url.path
            == '/v1/projects/my-project-id/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent'
        ):
            return httpx.Response(200, json={'content': 'success'})
        raise NotImplementedError(f'Unexpected request: {request.url!r}')  # pragma: no cover

    return httpx.AsyncClient(transport=httpx.MockTransport(handler=handler))

# tests/providers/test_google_vertex.py:34-43
def http_client():
    async def handler(request: httpx.Request):
        if (
            request.url.path
            == '/v1/projects/my-project-id/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent'
        ):
            return httpx.Response(200, json={'content': 'success'})
        raise NotImplementedError(f'Unexpected request: {request.url!r}')  # pragma: no cover

    return httpx.AsyncClient(transport=httpx.MockTransport(handler=handler))

# tests/providers/test_google_vertex.py:34-43
def http_client():
    async def handler(request: httpx.Request):
        if (
            request.url.path
            == '/v1/projects/my-project-id/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent'
        ):
            return httpx.Response(200, json={'content': 'success'})
        raise NotImplementedError(f'Unexpected request: {request.url!r}')  # pragma: no cover

    return httpx.AsyncClient(transport=httpx.MockTransport(handler=handler))

# tests/providers/test_google_vertex.py:34-43
def http_client():
    async def handler(request: httpx.Request):
        if (
            request.url.path
            == '/v1/projects/my-project-id/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent'
        ):
            return httpx.Response(200, json={'content': 'success'})
        raise NotImplementedError(f'Unexpected request: {request.url!r}')  # pragma: no cover

    return httpx.AsyncClient(transport=httpx.MockTransport(handler=handler))

# pydantic_ai_slim/pydantic_ai/providers/__init__.py:26-26
    _client: InterfaceClient

# tests/test_mcp.py:2258-2266
async def test_http_client_mutually_exclusive_with_headers():
    server = MCPServerStreamableHTTP(
        url='https://example.com/mcp',
        http_client=cached_async_http_client(),
        headers={'Authorization': 'Bearer token'},
    )
    with pytest.raises(ValueError, match='`http_client` is mutually exclusive with `headers`'):
        async with server:
            pass

# pydantic_ai_slim/pydantic_ai/mcp.py:363-363
    _client: ClientSession

# tests/mcp_server.py:227-242
async def get_client_info(ctx: Context[ServerSession, None]) -> dict[str, Any] | None:
    """Get information about the connected MCP client.

    Returns:
        Dictionary with client info (name, version, etc.) or None if not available.
    """
    client_params = ctx.session.client_params
    if client_params is None:
        return None
    client_info = client_params.clientInfo
    return {
        'name': client_info.name,
        'version': client_info.version,
        'title': getattr(client_info, 'title', None),
        'websiteUrl': getattr(client_info, 'websiteUrl', None),
    }

# tests/providers/test_grok.py:53-56
def test_grok_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = GrokProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_sambanova_provider.py:98-101
def test_sambanova_provider_with_http_client():
    http_client = httpx.AsyncClient()
    provider = SambaNovaProvider(api_key='foo', http_client=http_client)
    assert provider.client.api_key == 'foo'

# tests/providers/test_litellm.py:138-151
async def test_cached_http_client_usage(mocker: MockerFixture):
    # Create a real AsyncClient for the mock
    async with httpx.AsyncClient() as mock_cached_client:
        mock_cached_http_client_func = mocker.patch(
            'pydantic_ai.providers.litellm.cached_async_http_client', return_value=mock_cached_client
        )

        provider = LiteLLMProvider(api_key='test-key')

        # Verify cached_async_http_client was called with 'litellm' provider
        mock_cached_http_client_func.assert_called_once_with(provider='litellm')

        # Verify the client was created
        assert isinstance(provider.client, AsyncOpenAI)

# tests/test_mcp.py:1596-1601
async def test_client_sampling_disabled(run_context: RunContext[int]):
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'], allow_sampling=False)
    server.sampling_model = TestModel(custom_output_text='sampling model response')
    async with server:
        with pytest.raises(ModelRetry, match='Error executing tool use_sampling: Sampling not supported'):
            await server.direct_call_tool('use_sampling', {'foo': 'bar'})

# tests/providers/test_github.py:52-55
def test_github_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='ghp_test_token')
    provider = GitHubProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_heroku.py:52-55
def test_heroku_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = HerokuProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_nebius.py:53-56
def test_nebius_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = NebiusProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_ovhcloud.py:57-61
def test_ovhcloud_pass_http_client():
    http_client = httpx.AsyncClient()
    provider = OVHcloudProvider(api_key='your-api-key', http_client=http_client)
    assert isinstance(provider.client, openai.AsyncOpenAI)
    assert provider.client.api_key == 'your-api-key'

# tests/providers/test_vercel.py:54-57
def test_vercel_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = VercelProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_sambanova_provider.py:92-95
def test_sambanova_provider_with_openai_client():
    client = openai.AsyncOpenAI(api_key='foo', base_url='https://api.sambanova.ai/v1')
    provider = SambaNovaProvider(openai_client=client)
    assert provider.client is client

# pydantic_ai_slim/pydantic_ai/providers/cohere.py:39-40
    def v1_client(self) -> AsyncClient | None:
        return self._v1_client

# tests/providers/test_litellm.py:154-160
async def test_init_with_http_client_overrides_cached():
    async with httpx.AsyncClient() as custom_client:
        provider = LiteLLMProvider(api_key='test-key', http_client=custom_client)

        # Verify the provider was created successfully with custom client
        assert isinstance(provider.client, AsyncOpenAI)
        assert provider.client.api_key == 'test-key'

# tests/providers/test_ovhcloud.py:51-54
def test_ovhcloud_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='your-api-key')
    provider = OVHcloudProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_azure.py:57-64
def test_azure_provider_with_azure_openai_client():
    client = AsyncAzureOpenAI(
        api_version='2024-12-01-preview',
        azure_endpoint='https://project-id.openai.azure.com/',
        api_key='1234567890',
    )
    provider = AzureProvider(openai_client=client)
    assert isinstance(provider.client, AsyncAzureOpenAI)

# tests/models/xai_proto_cassettes.py:250-250
    _client: XaiProtoCassetteClient

# tests/providers/test_deepseek.py:46-49
def test_deep_seek_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = DeepSeekProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_fireworks.py:57-60
def test_fireworks_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = FireworksProvider(openai_client=openai_client)
    assert provider.client == openai_client

# pydantic_ai_slim/pydantic_ai/mcp.py:1001-1001
    http_client: httpx.AsyncClient | None

# tests/providers/test_huggingface.py:114-117
def test_huggingface_provider_init_with_hf_client():
    mock_client = Mock(spec=AsyncInferenceClient)
    provider = HuggingFaceProvider(hf_client=mock_client, api_key='key')
    assert provider.client is mock_client

# tests/providers/test_openrouter.py:78-81
def test_openrouter_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = OpenRouterProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/test_mcp.py:2243-2255
async def test_custom_http_client_not_closed():
    custom_http_client = cached_async_http_client()

    assert not custom_http_client.is_closed

    my_mcp_server = MCPServerStreamableHTTP(
        url='https://mcp.deepwiki.com/mcp', http_client=custom_http_client, timeout=30
    )

    tools = await my_mcp_server.list_tools()
    assert len(tools) > 0

    assert not custom_http_client.is_closed

# tests/providers/test_grok.py:47-50
def test_grok_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = GrokProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_groq.py:51-54
def test_groq_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = GroqProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_groq.py:57-60
def test_groq_provider_pass_groq_client() -> None:
    groq_client = AsyncGroq(api_key='api-key')
    provider = GroqProvider(groq_client=groq_client)
    assert provider.client == groq_client

# tests/test_fastmcp.py:159-164
    async def test_init_with_client(self, fastmcp_client: Client[FastMCPTransport]):
        """Test initialization with a FastMCP client."""
        toolset = FastMCPToolset(fastmcp_client)

        # Test that the client is accessible via the property
        assert toolset.id is None

# tests/test_embeddings.py:1094-1111
    async def test_client_error_with_status_code(self, bedrock_provider: BedrockProvider):
        """Test error handling when ClientError is raised with HTTP status code."""
        model = BedrockEmbeddingModel('amazon.titan-embed-text-v2:0', provider=bedrock_provider)

        error_response = {
            'Error': {'Code': 'ValidationException', 'Message': 'Invalid input'},
            'ResponseMetadata': {'HTTPStatusCode': 400},
        }
        with patch.object(
            model.client,
            'invoke_model',
            side_effect=ClientError(error_response, 'InvokeModel'),  # pyright: ignore[reportArgumentType]
        ):
            with pytest.raises(ExceptionGroup) as exc_info:
                await model.embed(['test'], input_type='query')
            assert len(exc_info.value.exceptions) == 1
            assert isinstance(exc_info.value.exceptions[0], ModelHTTPError)
            assert exc_info.value.exceptions[0].status_code == 400

# tests/providers/test_huggingface.py:121-124
def test_huggingface_provider_init_without_hf_client(MockAsyncInferenceClient: MagicMock):
    provider = HuggingFaceProvider(api_key='key')
    assert provider.client is MockAsyncInferenceClient.return_value
    MockAsyncInferenceClient.assert_called_with(api_key='key', provider=None, base_url=None)

# tests/providers/test_cohere.py:34-39
def test_cohere_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = CohereProvider(http_client=http_client, api_key='api-key')
    # The AsyncClientV2 wraps our httpx client in an AsyncHttpClient
    # So we just check that the httpx_client is an instance of AsyncHttpClient
    assert isinstance(provider.client._client_wrapper.httpx_client, AsyncHttpClient)  # type: ignore[reportPrivateUsage]

# tests/providers/test_github.py:46-49
def test_github_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = GitHubProvider(http_client=http_client, api_key='ghp_test_token')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_heroku.py:46-49
def test_heroku_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = HerokuProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_nebius.py:59-62
def test_nebius_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = NebiusProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_ollama.py:52-55
def test_ollama_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = OllamaProvider(http_client=http_client, base_url='http://localhost:11434/v1/')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_openai.py:23-25
def test_init_with_no_api_key_will_still_setup_client():
    provider = OpenAIProvider(base_url='http://localhost:19434/v1')
    assert provider.base_url == 'http://localhost:19434/v1/'

# tests/test_mcp.py:2218-2227
async def test_client_info_not_set() -> None:
    """Test that when client_info is not set, the default MCP client info is used."""
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'])

    async with server:
        result = await server.direct_call_tool('get_client_info', {})
        # When client_info is not set, the MCP library provides default client info
        assert result is not None
        assert isinstance(result, dict)
        assert result['name'] == 'mcp'

# tests/providers/test_mistral.py:41-44
def test_mistral_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = MistralProvider(http_client=http_client, api_key='api-key')
    assert provider.client.sdk_configuration.async_client == http_client

# tests/providers/test_mistral.py:47-50
def test_mistral_provider_pass_groq_client() -> None:
    mistral_client = Mistral(api_key='api-key')
    provider = MistralProvider(mistral_client=mistral_client)
    assert provider.client == mistral_client

# tests/providers/test_huggingface.py:104-106
def test_huggingface_provider_init_http_client_error():
    with pytest.raises(ValueError, match='`http_client` is ignored'):
        HuggingFaceProvider(api_key='key', http_client=Mock())  # type: ignore[call-overload]

# tests/test_embeddings.py:1113-1129
    async def test_client_error_without_status_code(self, bedrock_provider: BedrockProvider):
        """Test error handling when ClientError is raised without HTTP status code."""
        model = BedrockEmbeddingModel('amazon.titan-embed-text-v2:0', provider=bedrock_provider)

        error_response = {
            'Error': {'Code': 'UnknownError', 'Message': 'Something went wrong'},
            'ResponseMetadata': {},  # No HTTPStatusCode
        }
        with patch.object(
            model.client,
            'invoke_model',
            side_effect=ClientError(error_response, 'InvokeModel'),  # pyright: ignore[reportArgumentType]
        ):
            with pytest.raises(ExceptionGroup) as exc_info:
                await model.embed(['test'], input_type='query')
            assert len(exc_info.value.exceptions) == 1
            assert isinstance(exc_info.value.exceptions[0], ModelAPIError)

# tests/providers/test_cerebras.py:49-52
def test_cerebras_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = CerebrasProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_cohere.py:42-45
def test_cohere_provider_pass_cohere_client() -> None:
    cohere_client = AsyncClientV2(api_key='test-api-key')
    provider = CohereProvider(cohere_client=cohere_client)
    assert provider.client == cohere_client

# tests/providers/test_ollama.py:58-61
def test_ollama_provider_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(base_url='http://localhost:11434/v1/', api_key='test')
    provider = OllamaProvider(openai_client=openai_client)
    assert provider.client == openai_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# tests/providers/test_deepseek.py:40-43
def test_deep_seek_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = DeepSeekProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_fireworks.py:51-54
def test_fireworks_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = FireworksProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/providers/test_huggingface.py:58-61
def test_huggingface_provider_pass_hf_client() -> None:
    hf_client = AsyncInferenceClient(api_key='api-key')
    provider = HuggingFaceProvider(hf_client=hf_client, api_key='api-key')
    assert provider.client == hf_client

# pydantic_ai_slim/pydantic_ai/models/__init__.py:1233-1239
def _cached_async_http_client(
    provider: str | None, timeout: int = DEFAULT_HTTP_TIMEOUT, connect: int = 5
) -> httpx.AsyncClient:
    return httpx.AsyncClient(
        timeout=httpx.Timeout(timeout=timeout, connect=connect),
        headers={'User-Agent': get_user_agent()},
    )

# tests/providers/test_cerebras.py:55-58
def test_cerebras_provider_pass_openai_client() -> None:
    openai_client = AsyncOpenAI(api_key='api-key')
    provider = CerebrasProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_openrouter.py:72-75
def test_openrouter_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = OpenRouterProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/test_fastmcp.py:138-140
async def fastmcp_client(fastmcp_server: FastMCP) -> Client[FastMCPTransport]:
    """Create a real FastMCP client connected to the test server."""
    return Client(transport=fastmcp_server)

# tests/providers/test_huggingface.py:49-55
def test_huggingface_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    with pytest.raises(
        ValueError,
        match=re.escape('`http_client` is ignored for HuggingFace provider, please use `hf_client` instead'),
    ):
        HuggingFaceProvider(http_client=http_client, api_key='api-key')  # type: ignore

# tests/providers/test_voyageai.py:31-34
def test_voyageai_provider_pass_voyageai_client() -> None:
    voyageai_client = AsyncClient(api_key='test-api-key')
    provider = VoyageAIProvider(voyageai_client=voyageai_client)
    assert provider.client == voyageai_client

# tests/providers/test_anthropic.py:24-36
def test_anthropic_provider_pass_anthropic_client() -> None:
    anthropic_client = AsyncAnthropic(api_key='api-key')
    provider = AnthropicProvider(anthropic_client=anthropic_client)
    assert provider.client == anthropic_client
    bedrock_client = AsyncAnthropicBedrock(
        aws_secret_key='aws-secret-key',
        aws_access_key='aws-access-key',
        aws_region='us-west-2',
        aws_profile='default',
        aws_session_token='aws-session-token',
    )
    provider = AnthropicProvider(anthropic_client=bedrock_client)
    assert provider.client == bedrock_client

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_logfire.py:44-61
    async def connect_service_client(
        self, config: ConnectConfig, next: Callable[[ConnectConfig], Awaitable[ServiceClient]]
    ) -> ServiceClient:
        logfire = self.setup_logfire()

        if self.metrics:
            logfire_config = logfire.config
            token = logfire_config.token
            if logfire_config.send_to_logfire and token is not None and logfire_config.metrics is not False:
                base_url = logfire_config.advanced.generate_base_url(token)
                metrics_url = base_url + '/v1/metrics'
                headers = {'Authorization': f'Bearer {token}'}

                config.runtime = Runtime(
                    telemetry=TelemetryConfig(metrics=OpenTelemetryConfig(url=metrics_url, headers=headers))
                )

        return await next(config)

# pydantic_ai_slim/pydantic_ai/result.py:817-835
def _get_deferred_tool_requests(
    tool_calls: Iterable[_messages.ToolCallPart], tool_manager: ToolManager[AgentDepsT]
) -> DeferredToolRequests | None:
    """Get the deferred tool requests from the model response tool calls."""
    approvals: list[_messages.ToolCallPart] = []
    calls: list[_messages.ToolCallPart] = []

    for tool_call in tool_calls:
        tool_def = tool_manager.get_tool_def(tool_call.tool_name)
        if tool_def is not None:  # pragma: no branch
            if tool_def.kind == 'unapproved':
                approvals.append(tool_call)
            elif tool_def.kind == 'external':
                calls.append(tool_call)

    if not calls and not approvals:
        return None

    return DeferredToolRequests(calls=calls, approvals=approvals)

# tests/test_temporal.py:2015-2016
    def get_deferred_tool_requests(self) -> DeferredToolRequests | None:
        return self._deferred_tool_requests

# tests/test_tools.py:1713-1715
def test_output_type_deferred_tool_requests_by_itself():
    with pytest.raises(UserError, match='At least one output type must be provided other than `DeferredToolRequests`.'):
        Agent(TestModel(), output_type=DeferredToolRequests)

# tests/test_mcp.py:2198-2215
async def test_client_info_passed_to_session() -> None:
    """Test that provided client_info is passed unchanged to ClientSession."""
    implementation = Implementation(
        name='MyCustomClient',
        version='2.5.3',
        title='Custom MCP client',
        websiteUrl='https://example.com/client',
    )
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'], client_info=implementation)

    async with server:
        result = await server.direct_call_tool('get_client_info', {})
        assert result == {
            'name': 'MyCustomClient',
            'version': '2.5.3',
            'title': 'Custom MCP client',
            'websiteUrl': 'https://example.com/client',
        }

# tests/providers/test_moonshotai.py:49-53
def test_moonshotai_pass_openai_client() -> None:
    """Test passing a custom OpenAI client to MoonshotAI provider."""
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = MoonshotAIProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/models/test_bedrock.py:117-122
def _bedrock_model_with_client_error(error: ClientError) -> BedrockConverseModel:
    """Instantiate a BedrockConverseModel wired to always raise the given error."""
    return BedrockConverseModel(
        'us.amazon.nova-micro-v1:0',
        provider=_StubBedrockProvider(_StubBedrockClient(error)),
    )

# tests/providers/test_moonshotai.py:56-61
def test_moonshotai_provider_with_cached_http_client() -> None:
    """Test MoonshotAI provider using cached HTTP client (covers line 76)."""
    # This should use the else branch with cached_async_http_client
    provider = MoonshotAIProvider(api_key='api-key')
    assert isinstance(provider.client, openai.AsyncOpenAI)
    assert provider.client.api_key == 'api-key'

# tests/providers/test_moonshotai.py:42-46
def test_moonshotai_provider_pass_http_client() -> None:
    """Test passing a custom HTTP client to MoonshotAI provider."""
    http_client = httpx.AsyncClient()
    provider = MoonshotAIProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# pydantic_ai_slim/pydantic_ai/_ssrf.py:18-18
from .models import cached_async_http_client

# pydantic_ai_slim/pydantic_ai/providers/cerebras.py:87-121
    def __init__(
        self,
        *,
        api_key: str | None = None,
        openai_client: AsyncOpenAI | None = None,
        http_client: httpx.AsyncClient | None = None,
    ) -> None:
        """Create a new Cerebras provider.

        Args:
            api_key: The API key to use for authentication, if not provided, the `CEREBRAS_API_KEY` environment variable
                will be used if available.
            openai_client: An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`.
            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.
        """
        api_key = api_key or os.getenv('CEREBRAS_API_KEY')
        if not api_key and openai_client is None:
            raise UserError(
                'Set the `CEREBRAS_API_KEY` environment variable or pass it via `CerebrasProvider(api_key=...)` '
                'to use the Cerebras provider.'
            )

        default_headers = {'X-Cerebras-3rd-Party-Integration': 'pydantic-ai'}

        if openai_client is not None:
            self._client = openai_client
        elif http_client is not None:
            self._client = AsyncOpenAI(
                base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=default_headers
            )
        else:
            http_client = cached_async_http_client(provider='cerebras')
            self._client = AsyncOpenAI(
                base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=default_headers
            )

# tests/test_direct.py:193-202
def test_model_request_stream_sync_intermediate_get():
    """Test getting properties of StreamedResponse before consuming all events."""
    messages: list[ModelMessage] = [ModelRequest.user_text_prompt('x')]

    with model_request_stream_sync('test', messages) as stream:
        response = stream.get()
        assert response is not None

        usage = stream.usage()
        assert usage is not None

# pydantic_ai_slim/pydantic_ai/providers/github.py:84-112
    def __init__(
        self,
        *,
        api_key: str | None = None,
        openai_client: AsyncOpenAI | None = None,
        http_client: httpx.AsyncClient | None = None,
    ) -> None:
        """Create a new GitHub Models provider.

        Args:
            api_key: The GitHub token to use for authentication. If not provided, the `GITHUB_API_KEY`
                environment variable will be used if available.
            openai_client: An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`.
            http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.
        """
        api_key = api_key or os.getenv('GITHUB_API_KEY')
        if not api_key and openai_client is None:
            raise UserError(
                'Set the `GITHUB_API_KEY` environment variable or pass it via `GitHubProvider(api_key=...)`'
                ' to use the GitHub Models provider.'
            )

        if openai_client is not None:
            self._client = openai_client
        elif http_client is not None:
            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)
        else:
            http_client = cached_async_http_client(provider='github')
            self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)

# tests/evals/test_utils.py:104-113
def test_get_unwrapped_function_name_error():
    """Test get_unwrapped_function_name with invalid input."""

    class InvalidCallable:
        pass

    with pytest.raises(AttributeError) as exc_info:
        get_unwrapped_function_name(InvalidCallable())  # type: ignore

    assert str(exc_info.value) == "'InvalidCallable' object has no attribute '__name__'"

# tests/models/test_anthropic.py:210-276
async def test_sync_request_text_response(allow_model_requests: None):
    c = completion_message([BetaTextBlock(text='world', type='text')], BetaUsage(input_tokens=5, output_tokens=10))
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(m)

    result = await agent.run('hello')
    assert result.output == 'world'
    assert result.usage() == snapshot(
        RunUsage(
            requests=1,
            input_tokens=5,
            output_tokens=10,
            details={'input_tokens': 5, 'output_tokens': 10},
        )
    )
    # reset the index so we get the same response again
    mock_client.index = 0  # type: ignore

    result = await agent.run('hello', message_history=result.new_messages())
    assert result.output == 'world'
    assert result.usage() == snapshot(
        RunUsage(
            requests=1,
            input_tokens=5,
            output_tokens=10,
            details={'input_tokens': 5, 'output_tokens': 10},
        )
    )
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='hello', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='world')],
                usage=RequestUsage(input_tokens=5, output_tokens=10, details={'input_tokens': 5, 'output_tokens': 10}),
                model_name='claude-3-5-haiku-123',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='anthropic',
                provider_url='https://api.anthropic.com',
                provider_details={'finish_reason': 'end_turn'},
                provider_response_id='123',
                finish_reason='stop',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[UserPromptPart(content='hello', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='world')],
                usage=RequestUsage(input_tokens=5, output_tokens=10, details={'input_tokens': 5, 'output_tokens': 10}),
                model_name='claude-3-5-haiku-123',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='anthropic',
                provider_url='https://api.anthropic.com',
                provider_details={'finish_reason': 'end_turn'},
                provider_response_id='123',
                finish_reason='stop',
                run_id=IsStr(),
            ),
        ]
    )

# tests/models/test_mistral.py:509-580
async def test_request_native_with_arguments_str_response(allow_model_requests: None):
    class CityLocation(BaseModel):
        city: str
        country: str

    completion = completion_message(
        MistralAssistantMessage(
            content=None,
            role='assistant',
            tool_calls=[
                MistralToolCall(
                    id='123',
                    function=MistralFunctionCall(
                        arguments='{"city": "paris", "country": "france"}', name='final_result'
                    ),
                    type='function',
                )
            ],
        )
    )
    mock_client = MockMistralAI.create_mock(completion)
    model = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(model=model, output_type=CityLocation)

    result = await agent.run('User prompt value')

    assert result.output == CityLocation(city='paris', country='france')
    assert result.usage().input_tokens == 1
    assert result.usage().output_tokens == 1
    assert result.usage().details == {}
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='User prompt value', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ToolCallPart(
                        tool_name='final_result',
                        args='{"city": "paris", "country": "france"}',
                        tool_call_id='123',
                    )
                ],
                usage=RequestUsage(input_tokens=1, output_tokens=1),
                model_name='mistral-large-123',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='mistral',
                provider_url='https://api.mistral.ai',
                provider_details={
                    'finish_reason': 'stop',
                    'timestamp': datetime(2024, 1, 1, 0, 0, tzinfo=timezone.utc),
                },
                provider_response_id='123',
                finish_reason='stop',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='final_result',
                        content='Final result processed.',
                        tool_call_id='123',
                        timestamp=IsNow(tz=timezone.utc),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

# tests/models/test_anthropic.py:1081-1099
async def test_async_request_text_response(allow_model_requests: None):
    c = completion_message(
        [BetaTextBlock(text='world', type='text')],
        usage=BetaUsage(input_tokens=3, output_tokens=5),
    )
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(m)

    result = await agent.run('hello')
    assert result.output == 'world'
    assert result.usage() == snapshot(
        RunUsage(
            requests=1,
            input_tokens=3,
            output_tokens=5,
            details={'input_tokens': 3, 'output_tokens': 5},
        )
    )

# tests/models/test_anthropic.py:1102-1151
async def test_request_structured_response(allow_model_requests: None):
    c = completion_message(
        [BetaToolUseBlock(id='123', input={'response': [1, 2, 3]}, name='final_result', type='tool_use')],
        usage=BetaUsage(input_tokens=3, output_tokens=5),
    )
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(m, output_type=list[int])

    result = await agent.run('hello')
    assert result.output == [1, 2, 3]
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[UserPromptPart(content='hello', timestamp=IsNow(tz=timezone.utc))],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ToolCallPart(
                        tool_name='final_result',
                        args={'response': [1, 2, 3]},
                        tool_call_id='123',
                    )
                ],
                usage=RequestUsage(input_tokens=3, output_tokens=5, details={'input_tokens': 3, 'output_tokens': 5}),
                model_name='claude-3-5-haiku-123',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='anthropic',
                provider_url='https://api.anthropic.com',
                provider_details={'finish_reason': 'end_turn'},
                provider_response_id='123',
                finish_reason='stop',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='final_result',
                        content='Final result processed.',
                        tool_call_id='123',
                        timestamp=IsNow(tz=timezone.utc),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

# pydantic_ai_slim/pydantic_ai/tools.py:161-161
    approvals: list[ToolCallPart] = field(default_factory=list[ToolCallPart])