# pydantic_graph/pydantic_graph/mermaid.py:268-268
    httpx_client: httpx.Client

# tests/conftest.py:345-370
async def close_cached_httpx_client(anyio_backend: str, monkeypatch: pytest.MonkeyPatch) -> AsyncIterator[None]:
    """Track and close cached httpx clients created during each test.

    Prevents reusing AsyncClient instances across tests (and event loops),
    which can cause 'Event loop is closed' errors, without touching prod code.
    """
    created_clients: set[httpx.AsyncClient] = set()

    # Patch the cached factory to record returned clients while preserving caching.
    original_cached_func = pydantic_ai.models._cached_async_http_client  # type: ignore[reportPrivateUsage]

    def tracked_cached_async_http_client(*args: Any, **kwargs: Any):
        client = original_cached_func(*args, **kwargs)
        created_clients.add(client)
        return client

    monkeypatch.setattr(pydantic_ai.models, '_cached_async_http_client', tracked_cached_async_http_client)

    yield

    # Close only the clients that were actually created/accessed in this test
    for client in created_clients:
        await client.aclose()

    # Ensure no stale cached clients persist between tests (new event loop per test)
    original_cached_func.cache_clear()

# tests/conftest.py:345-370
async def close_cached_httpx_client(anyio_backend: str, monkeypatch: pytest.MonkeyPatch) -> AsyncIterator[None]:
    """Track and close cached httpx clients created during each test.

    Prevents reusing AsyncClient instances across tests (and event loops),
    which can cause 'Event loop is closed' errors, without touching prod code.
    """
    created_clients: set[httpx.AsyncClient] = set()

    # Patch the cached factory to record returned clients while preserving caching.
    original_cached_func = pydantic_ai.models._cached_async_http_client  # type: ignore[reportPrivateUsage]

    def tracked_cached_async_http_client(*args: Any, **kwargs: Any):
        client = original_cached_func(*args, **kwargs)
        created_clients.add(client)
        return client

    monkeypatch.setattr(pydantic_ai.models, '_cached_async_http_client', tracked_cached_async_http_client)

    yield

    # Close only the clients that were actually created/accessed in this test
    for client in created_clients:
        await client.aclose()

    # Ensure no stale cached clients persist between tests (new event loop per test)
    original_cached_func.cache_clear()

# tests/conftest.py:345-370
async def close_cached_httpx_client(anyio_backend: str, monkeypatch: pytest.MonkeyPatch) -> AsyncIterator[None]:
    """Track and close cached httpx clients created during each test.

    Prevents reusing AsyncClient instances across tests (and event loops),
    which can cause 'Event loop is closed' errors, without touching prod code.
    """
    created_clients: set[httpx.AsyncClient] = set()

    # Patch the cached factory to record returned clients while preserving caching.
    original_cached_func = pydantic_ai.models._cached_async_http_client  # type: ignore[reportPrivateUsage]

    def tracked_cached_async_http_client(*args: Any, **kwargs: Any):
        client = original_cached_func(*args, **kwargs)
        created_clients.add(client)
        return client

    monkeypatch.setattr(pydantic_ai.models, '_cached_async_http_client', tracked_cached_async_http_client)

    yield

    # Close only the clients that were actually created/accessed in this test
    for client in created_clients:
        await client.aclose()

    # Ensure no stale cached clients persist between tests (new event loop per test)
    original_cached_func.cache_clear()

# tests/models/test_google.py:3952-3960
async def test_google_httpx_client_is_not_closed(allow_model_requests: None, gemini_api_key: str):
    # This should not raise any errors, see https://github.com/pydantic/pydantic-ai/issues/3242.
    agent = Agent(GoogleModel('gemini-2.5-flash-lite', provider=GoogleProvider(api_key=gemini_api_key)))
    result = await agent.run('What is the capital of France?')
    assert result.output == snapshot('The capital of France is **Paris**.')

    agent = Agent(GoogleModel('gemini-2.5-flash-lite', provider=GoogleProvider(api_key=gemini_api_key)))
    result = await agent.run('What is the capital of Mexico?')
    assert result.output == snapshot('The capital of Mexico is **Mexico City**.')

# tests/providers/test_alibaba_provider.py:83-86
def test_alibaba_provider_with_http_client():
    http_client = httpx.AsyncClient()
    provider = AlibabaProvider(api_key='foo', http_client=http_client)
    assert provider.client.api_key == 'foo'

# tests/providers/test_alibaba_provider.py:77-80
def test_alibaba_provider_with_openai_client():
    client = openai.AsyncOpenAI(api_key='foo')
    provider = AlibabaProvider(openai_client=client)
    assert provider.client is client

# tests/models/test_gemini.py:501-537
async def get_gemini_client(
    client_with_handler: ClientWithHandler, env: TestEnv, allow_model_requests: None
) -> GetGeminiClient:
    env.set('GEMINI_API_KEY', 'via-env-var')

    def create_client(response_or_list: ResOrList) -> httpx.AsyncClient:
        index = 0

        def handler(request: httpx.Request) -> httpx.Response:
            nonlocal index

            ua = request.headers.get('User-Agent')
            assert isinstance(ua, str) and ua.startswith('pydantic-ai')

            if isinstance(response_or_list, Sequence):
                response = response_or_list[index]
                index += 1
            else:
                response = response_or_list

            if isinstance(response, httpx.AsyncByteStream):
                content: bytes | None = None
                stream: httpx.AsyncByteStream | None = response
            else:
                content = _gemini_response_ta.dump_json(response, by_alias=True)
                stream = None

            return httpx.Response(
                200,
                content=content,
                stream=stream,
                headers={'Content-Type': 'application/json'},
            )

        return client_with_handler(handler)

    return create_client

# tests/providers/test_together.py:57-60
def test_together_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = TogetherProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/models/test_huggingface.py:788-801
async def test_model_client_response_error(allow_model_requests: None) -> None:
    request_info = Mock(spec=aiohttp.RequestInfo)
    request_info.url = 'http://test.com'
    request_info.method = 'POST'
    request_info.headers = {}
    request_info.real_url = 'http://test.com'
    error = aiohttp.ClientResponseError(request_info, history=(), status=400, message='Bad Request')

    mock_client = MockHuggingFace.create_mock(error)
    m = HuggingFaceModel('not_a_model', provider=HuggingFaceProvider(hf_client=mock_client, api_key='x'))
    agent = Agent(m)
    with pytest.raises(ModelHTTPError) as exc_info:
        await agent.run('hello')
    assert str(exc_info.value) == snapshot('status_code: 400, model_name: not_a_model, body: Bad Request')

# tests/test_ui_web.py:193-204
def test_chat_app_index_endpoint():
    """Test that the index endpoint serves HTML with proper caching headers."""
    agent = Agent('test')
    app = create_web_app(agent)

    with TestClient(app) as client:
        response = client.get('/')
        assert response.status_code == 200
        assert response.headers['content-type'] == 'text/html; charset=utf-8'
        assert 'cache-control' in response.headers
        assert response.headers['cache-control'] == 'public, max-age=3600'
        assert len(response.content) > 0

# tests/providers/test_together.py:51-54
def test_together_provider_pass_http_client() -> None:
    http_client = httpx.AsyncClient()
    provider = TogetherProvider(http_client=http_client, api_key='api-key')
    assert provider.client._client == http_client  # type: ignore[reportPrivateUsage]

# tests/conftest.py:345-370
async def close_cached_httpx_client(anyio_backend: str, monkeypatch: pytest.MonkeyPatch) -> AsyncIterator[None]:
    """Track and close cached httpx clients created during each test.

    Prevents reusing AsyncClient instances across tests (and event loops),
    which can cause 'Event loop is closed' errors, without touching prod code.
    """
    created_clients: set[httpx.AsyncClient] = set()

    # Patch the cached factory to record returned clients while preserving caching.
    original_cached_func = pydantic_ai.models._cached_async_http_client  # type: ignore[reportPrivateUsage]

    def tracked_cached_async_http_client(*args: Any, **kwargs: Any):
        client = original_cached_func(*args, **kwargs)
        created_clients.add(client)
        return client

    monkeypatch.setattr(pydantic_ai.models, '_cached_async_http_client', tracked_cached_async_http_client)

    yield

    # Close only the clients that were actually created/accessed in this test
    for client in created_clients:
        await client.aclose()

    # Ensure no stale cached clients persist between tests (new event loop per test)
    original_cached_func.cache_clear()

# tests/models/test_google.py:15-15
from httpx import AsyncClient as HttpxAsyncClient, Timeout

# tests/models/test_cohere.py:75-75
    _client_wrapper: MockClientWrapper = None  # type: ignore

# tests/providers/test_gateway.py:65-68
async def test_init_with_http_client():
    async with httpx.AsyncClient() as http_client:
        provider = gateway_provider('openai', http_client=http_client, api_key='foobar')
        assert provider.client._client == http_client  # type: ignore

# tests/providers/test_gateway.py:65-68
async def test_init_with_http_client():
    async with httpx.AsyncClient() as http_client:
        provider = gateway_provider('openai', http_client=http_client, api_key='foobar')
        assert provider.client._client == http_client  # type: ignore

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_main_preview_url.py:6-6
import httpx

# tests/providers/test_litellm.py:31-35
def test_init_with_openai_client():
    openai_client = AsyncOpenAI(api_key='custom-key', base_url='https://custom.openai.com/v1')
    provider = LiteLLMProvider(openai_client=openai_client)
    assert provider.client == openai_client
    assert provider.base_url == 'https://custom.openai.com/v1/'

# tests/providers/test_vercel.py:124-128
def test_vercel_with_http_client():
    http_client = httpx.AsyncClient()
    provider = VercelProvider(api_key='test-key', http_client=http_client)
    assert provider.client.api_key == 'test-key'
    assert str(provider.client.base_url) == 'https://ai-gateway.vercel.sh/v1/'

# tests/graph/test_mermaid.py:266-266
HttpxWithHandler = Callable[[Callable[[httpx.Request], httpx.Response]], httpx.Client]

# tests/test_mcp.py:1580-1593
async def test_client_sampling(run_context: RunContext[int]):
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'])
    server.sampling_model = TestModel(custom_output_text='sampling model response')
    async with server:
        result = await server.direct_call_tool('use_sampling', {'foo': 'bar'})
        assert result == snapshot(
            {
                '_meta': None,
                'role': 'assistant',
                'content': {'type': 'text', 'text': 'sampling model response', 'annotations': None, '_meta': None},
                'model': 'test',
                'stopReason': None,
            }
        )

# tests/graph/test_mermaid.py:250-263
def httpx_with_handler() -> Iterator[HttpxWithHandler]:
    client: httpx.Client | None = None

    def create_client(handler: Callable[[httpx.Request], httpx.Response]) -> httpx.Client:
        nonlocal client
        assert client is None, 'client_with_handler can only be called once'
        client = httpx.Client(mounts={'all://': httpx.MockTransport(handler)})
        return client

    try:
        yield create_client
    finally:
        if client:  # pragma: no branch
            client.close()

# pydantic_ai_slim/pydantic_ai/models/gemini.py:21-21
from httpx import USE_CLIENT_DEFAULT, Response as HTTPResponse

# tests/providers/test_xai.py:36-39
def test_xai_pass_xai_client() -> None:
    xai_client = AsyncClient(api_key='api-key')
    provider = XaiProvider(xai_client=xai_client)
    assert provider.client == xai_client

# tests/providers/test_google_vertex.py:34-43
def http_client():
    async def handler(request: httpx.Request):
        if (
            request.url.path
            == '/v1/projects/my-project-id/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent'
        ):
            return httpx.Response(200, json={'content': 'success'})
        raise NotImplementedError(f'Unexpected request: {request.url!r}')  # pragma: no cover

    return httpx.AsyncClient(transport=httpx.MockTransport(handler=handler))

# tests/providers/test_google_vertex.py:34-43
def http_client():
    async def handler(request: httpx.Request):
        if (
            request.url.path
            == '/v1/projects/my-project-id/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent'
        ):
            return httpx.Response(200, json={'content': 'success'})
        raise NotImplementedError(f'Unexpected request: {request.url!r}')  # pragma: no cover

    return httpx.AsyncClient(transport=httpx.MockTransport(handler=handler))

# tests/providers/test_google_vertex.py:34-43
def http_client():
    async def handler(request: httpx.Request):
        if (
            request.url.path
            == '/v1/projects/my-project-id/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent'
        ):
            return httpx.Response(200, json={'content': 'success'})
        raise NotImplementedError(f'Unexpected request: {request.url!r}')  # pragma: no cover

    return httpx.AsyncClient(transport=httpx.MockTransport(handler=handler))

# tests/providers/test_google_vertex.py:34-43
def http_client():
    async def handler(request: httpx.Request):
        if (
            request.url.path
            == '/v1/projects/my-project-id/locations/us-central1/publishers/google/models/gemini-1.0-pro:generateContent'
        ):
            return httpx.Response(200, json={'content': 'success'})
        raise NotImplementedError(f'Unexpected request: {request.url!r}')  # pragma: no cover

    return httpx.AsyncClient(transport=httpx.MockTransport(handler=handler))

# pydantic_ai_slim/pydantic_ai/providers/__init__.py:26-26
    _client: InterfaceClient

# tests/test_mcp.py:2258-2266
async def test_http_client_mutually_exclusive_with_headers():
    server = MCPServerStreamableHTTP(
        url='https://example.com/mcp',
        http_client=cached_async_http_client(),
        headers={'Authorization': 'Bearer token'},
    )
    with pytest.raises(ValueError, match='`http_client` is mutually exclusive with `headers`'):
        async with server:
            pass

# pydantic_ai_slim/pydantic_ai/mcp.py:363-363
    _client: ClientSession

# tests/mcp_server.py:227-242
async def get_client_info(ctx: Context[ServerSession, None]) -> dict[str, Any] | None:
    """Get information about the connected MCP client.

    Returns:
        Dictionary with client info (name, version, etc.) or None if not available.
    """
    client_params = ctx.session.client_params
    if client_params is None:
        return None
    client_info = client_params.clientInfo
    return {
        'name': client_info.name,
        'version': client_info.version,
        'title': getattr(client_info, 'title', None),
        'websiteUrl': getattr(client_info, 'websiteUrl', None),
    }

# tests/providers/test_grok.py:53-56
def test_grok_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = GrokProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_sambanova_provider.py:98-101
def test_sambanova_provider_with_http_client():
    http_client = httpx.AsyncClient()
    provider = SambaNovaProvider(api_key='foo', http_client=http_client)
    assert provider.client.api_key == 'foo'

# tests/providers/test_litellm.py:138-151
async def test_cached_http_client_usage(mocker: MockerFixture):
    # Create a real AsyncClient for the mock
    async with httpx.AsyncClient() as mock_cached_client:
        mock_cached_http_client_func = mocker.patch(
            'pydantic_ai.providers.litellm.cached_async_http_client', return_value=mock_cached_client
        )

        provider = LiteLLMProvider(api_key='test-key')

        # Verify cached_async_http_client was called with 'litellm' provider
        mock_cached_http_client_func.assert_called_once_with(provider='litellm')

        # Verify the client was created
        assert isinstance(provider.client, AsyncOpenAI)

# tests/test_mcp.py:1596-1601
async def test_client_sampling_disabled(run_context: RunContext[int]):
    server = MCPServerStdio('python', ['-m', 'tests.mcp_server'], allow_sampling=False)
    server.sampling_model = TestModel(custom_output_text='sampling model response')
    async with server:
        with pytest.raises(ModelRetry, match='Error executing tool use_sampling: Sampling not supported'):
            await server.direct_call_tool('use_sampling', {'foo': 'bar'})

# tests/providers/test_github.py:52-55
def test_github_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='ghp_test_token')
    provider = GitHubProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_heroku.py:52-55
def test_heroku_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = HerokuProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_nebius.py:53-56
def test_nebius_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = NebiusProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_ovhcloud.py:57-61
def test_ovhcloud_pass_http_client():
    http_client = httpx.AsyncClient()
    provider = OVHcloudProvider(api_key='your-api-key', http_client=http_client)
    assert isinstance(provider.client, openai.AsyncOpenAI)
    assert provider.client.api_key == 'your-api-key'

# tests/providers/test_vercel.py:54-57
def test_vercel_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = VercelProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_sambanova_provider.py:92-95
def test_sambanova_provider_with_openai_client():
    client = openai.AsyncOpenAI(api_key='foo', base_url='https://api.sambanova.ai/v1')
    provider = SambaNovaProvider(openai_client=client)
    assert provider.client is client

# pydantic_ai_slim/pydantic_ai/providers/cohere.py:39-40
    def v1_client(self) -> AsyncClient | None:
        return self._v1_client

# tests/providers/test_litellm.py:154-160
async def test_init_with_http_client_overrides_cached():
    async with httpx.AsyncClient() as custom_client:
        provider = LiteLLMProvider(api_key='test-key', http_client=custom_client)

        # Verify the provider was created successfully with custom client
        assert isinstance(provider.client, AsyncOpenAI)
        assert provider.client.api_key == 'test-key'

# tests/providers/test_ovhcloud.py:51-54
def test_ovhcloud_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='your-api-key')
    provider = OVHcloudProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_azure.py:57-64
def test_azure_provider_with_azure_openai_client():
    client = AsyncAzureOpenAI(
        api_version='2024-12-01-preview',
        azure_endpoint='https://project-id.openai.azure.com/',
        api_key='1234567890',
    )
    provider = AzureProvider(openai_client=client)
    assert isinstance(provider.client, AsyncAzureOpenAI)

# tests/models/xai_proto_cassettes.py:250-250
    _client: XaiProtoCassetteClient

# tests/providers/test_deepseek.py:46-49
def test_deep_seek_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = DeepSeekProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/providers/test_fireworks.py:57-60
def test_fireworks_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = FireworksProvider(openai_client=openai_client)
    assert provider.client == openai_client

# pydantic_ai_slim/pydantic_ai/mcp.py:1001-1001
    http_client: httpx.AsyncClient | None

# tests/providers/test_huggingface.py:114-117
def test_huggingface_provider_init_with_hf_client():
    mock_client = Mock(spec=AsyncInferenceClient)
    provider = HuggingFaceProvider(hf_client=mock_client, api_key='key')
    assert provider.client is mock_client

# tests/providers/test_openrouter.py:78-81
def test_openrouter_pass_openai_client() -> None:
    openai_client = openai.AsyncOpenAI(api_key='api-key')
    provider = OpenRouterProvider(openai_client=openai_client)
    assert provider.client == openai_client

# tests/test_mcp.py:2243-2255
async def test_custom_http_client_not_closed():
    custom_http_client = cached_async_http_client()

    assert not custom_http_client.is_closed

    my_mcp_server = MCPServerStreamableHTTP(
        url='https://mcp.deepwiki.com/mcp', http_client=custom_http_client, timeout=30
    )

    tools = await my_mcp_server.list_tools()
    assert len(tools) > 0

    assert not custom_http_client.is_closed