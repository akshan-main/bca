# tests/test_direct.py:85-111
def test_model_request_stream_sync():
    with model_request_stream_sync('test', [ModelRequest.user_text_prompt('x')]) as stream:
        chunks = list(stream)
        assert chunks == snapshot(
            [
                PartStartEvent(index=0, part=TextPart(content='')),
                FinalResultEvent(tool_name=None, tool_call_id=None),
                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='success ')),
                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='(no ')),
                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='tool ')),
                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='calls)')),
                PartEndEvent(index=0, part=TextPart(content='success (no tool calls)')),
            ]
        )
        assert stream.response == snapshot(
            ModelResponse(
                parts=[TextPart(content='success (no tool calls)')],
                usage=RequestUsage(input_tokens=51, output_tokens=4),
                model_name='test',
                timestamp=IsDatetime(),
                provider_name='test',
            )
        )

        repr_str = repr(stream)
        assert 'TestStreamedResponse' in repr_str
        assert 'test' in repr_str

# tests/test_direct.py:164-173
def test_model_request_stream_sync_exception_in_stream():
    """Test handling of exceptions raised during streaming."""
    async_stream_mock = AsyncMock()
    async_stream_mock.__aenter__ = AsyncMock(side_effect=ValueError('Stream error'))

    stream_sync = StreamedResponseSync(_async_stream_cm=async_stream_mock)

    with stream_sync:
        with pytest.raises(ValueError, match='Stream error'):
            list(stream_sync)

# tests/test_direct.py:176-190
def test_model_request_stream_sync_timeout():
    """Test timeout when stream fails to initialize."""
    async_stream_mock = AsyncMock()

    async def slow_init():
        await asyncio.sleep(0.1)

    async_stream_mock.__aenter__ = AsyncMock(side_effect=slow_init)

    stream_sync = StreamedResponseSync(_async_stream_cm=async_stream_mock)

    with patch('pydantic_ai.direct.STREAM_INITIALIZATION_TIMEOUT', 0.01):
        with stream_sync:
            with pytest.raises(RuntimeError, match='Stream failed to initialize within timeout'):
                stream_sync.get()

# tests/test_direct.py:130-161
def test_model_request_stream_sync_without_context_manager():
    """Test that accessing properties or iterating without context manager raises RuntimeError."""
    messages: list[ModelMessage] = [ModelRequest.user_text_prompt('x')]

    expected_error_msg = re.escape(
        'StreamedResponseSync must be used as a context manager. Use: `with model_request_stream_sync(...) as stream:`'
    )

    stream_cm = model_request_stream_sync('test', messages)

    stream_repr = repr(stream_cm)
    assert 'StreamedResponseSync' in stream_repr
    assert 'context_entered=False' in stream_repr

    with pytest.raises(RuntimeError, match=expected_error_msg):
        _ = stream_cm.model_name

    with pytest.raises(RuntimeError, match=expected_error_msg):
        _ = stream_cm.timestamp

    with pytest.raises(RuntimeError, match=expected_error_msg):
        stream_cm.get()

    with pytest.raises(RuntimeError, match=expected_error_msg):
        stream_cm.usage()

    with pytest.raises(RuntimeError, match=expected_error_msg):
        list(stream_cm)

    with pytest.raises(RuntimeError, match=expected_error_msg):
        for _ in stream_cm:
            break  # pragma: no cover

# tests/models/test_model_settings.py:171-204
def test_direct_model_request_merges_model_settings():
    """Ensure direct requests merge model defaults with provided run settings."""

    captured_settings = None

    async def capture(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('ok')])

    model = FunctionModel(
        capture,
        settings=ModelSettings(max_tokens=50, temperature=0.3),
    )

    messages: list[ModelMessage] = [ModelRequest.user_text_prompt('hi')]
    run_settings = ModelSettings(temperature=0.9, top_p=0.2)

    async def _run() -> ModelResponse:
        return await direct_model_request(
            model,
            messages,
            model_settings=run_settings,
            model_request_parameters=ModelRequestParameters(),
        )

    response = asyncio.run(_run())

    assert response.parts == [TextPart('ok')]
    assert captured_settings == {
        'max_tokens': 50,
        'temperature': 0.9,
        'top_p': 0.2,
    }

# tests/test_direct.py:193-202
def test_model_request_stream_sync_intermediate_get():
    """Test getting properties of StreamedResponse before consuming all events."""
    messages: list[ModelMessage] = [ModelRequest.user_text_prompt('x')]

    with model_request_stream_sync('test', messages) as stream:
        response = stream.get()
        assert response is not None

        usage = stream.usage()
        assert usage is not None

# tests/test_streaming.py:2452-2454
def test_streamed_run_result_metadata_none_without_run_or_stream() -> None:
    streamed = StreamedRunResult(all_messages=[], new_message_index=0, stream_response=None, on_complete=None)
    assert streamed.metadata is None

# tests/test_streaming.py:2446-2449
def test_streamed_run_result_metadata_none_without_sources() -> None:
    run_result = _make_run_result(metadata=None)
    streamed = StreamedRunResult(all_messages=[], new_message_index=0, run_result=run_result)
    assert streamed.metadata is None

# tests/test_tools.py:2665-2675
def test_tool_timeout_default_none():
    """Test that timeout defaults to None when not specified."""
    agent = Agent(TestModel())

    @agent.tool_plain
    def tool_without_timeout() -> str:
        return 'done'  # pragma: no cover

    tool = agent._function_toolset.tools['tool_without_timeout']
    assert tool.timeout is None
    assert tool.tool_def.timeout is None

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# examples/pydantic_ai_examples/evals/example_01_generate_dataset.py:3-3
from types import NoneType

# tests/graph/beta/test_edge_cases.py:34-57
async def test_step_returning_none():
    """Test steps that return None."""
    g = GraphBuilder(state_type=EdgeCaseState)

    @g.step
    async def do_nothing(ctx: StepContext[EdgeCaseState, None, None]) -> None:
        ctx.state.value = 99
        return None

    @g.step
    async def return_none(ctx: StepContext[EdgeCaseState, None, None]) -> None:
        return None

    g.add(
        g.edge_from(g.start_node).to(do_nothing),
        g.edge_from(do_nothing).to(return_none),
        g.edge_from(return_none).to(g.end_node),
    )

    graph = g.build()
    state = EdgeCaseState()
    result = await graph.run(state=state)
    assert result is None
    assert state.value == 99

# tests/test_tools.py:1713-1715
def test_output_type_deferred_tool_requests_by_itself():
    with pytest.raises(UserError, match='At least one output type must be provided other than `DeferredToolRequests`.'):
        Agent(TestModel(), output_type=DeferredToolRequests)

# tests/models/test_model_function.py:279-296
def test_deps_none():
    agent = Agent(FunctionModel(call_tool))

    @agent.tool
    async def get_none(ctx: RunContext[None]):
        nonlocal called

        called = True
        assert ctx.deps is None
        return ''

    called = False
    agent.run_sync('Hello')
    assert called

    called = False
    agent.run_sync('Hello')
    assert called