# tests/test_builtin_tools.py:78-103
def test_url_context_tool_backward_compatibility():
    """Test that old payloads with 'url_context' kind can be deserialized."""
    adapter = TypeAdapter(AbstractBuiltinTool)

    # Test 1: Old payload with url_context should deserialize to UrlContextTool (which is deprecated)
    old_payload = {'kind': 'url_context', 'max_uses': 5, 'enable_citations': True}
    with pytest.warns(DeprecationWarning, match='Use `WebFetchTool` instead.'):
        result = adapter.validate_python(old_payload)
    assert isinstance(result, UrlContextTool)  # pyright: ignore[reportDeprecated]
    assert isinstance(result, WebFetchTool)  # UrlContextTool is a subclass of WebFetchTool
    assert result.kind == 'url_context'  # Preserves the original kind from payload
    assert result.max_uses == 5
    assert result.enable_citations is True

    # Test 2: Re-serialization should preserve the kind
    serialized = adapter.dump_python(result)
    assert serialized['kind'] == 'url_context'
    assert serialized['max_uses'] == 5
    assert serialized['enable_citations'] is True

    # Test 3: New payload with web_fetch should work normally
    new_payload = {'kind': 'web_fetch', 'max_uses': 10}
    result2 = adapter.validate_python(new_payload)
    assert isinstance(result2, WebFetchTool)
    assert result2.kind == 'web_fetch'
    assert result2.max_uses == 10

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:47-47
    kind: str = 'unknown_builtin_tool'

# pydantic_ai_slim/pydantic_ai/tools.py:512-512
    kind: ToolKind = field(default='function')

# pydantic_ai_slim/pydantic_ai/tools.py:186-186
    kind: Literal['tool-denied'] = 'tool-denied'

# pydantic_ai_slim/pydantic_ai/__init__.py:12-22
from .builtin_tools import (
    CodeExecutionTool,
    FileSearchTool,
    ImageGenerationTool,
    MCPServerTool,
    MemoryTool,
    UrlContextTool,  # pyright: ignore[reportDeprecated]
    WebFetchTool,
    WebSearchTool,
    WebSearchUserLocation,
)

# pydantic_ai_slim/pydantic_ai/messages.py:701-701
    kind: Literal['tool-return'] = 'tool-return'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:58-58
    kind: Literal['tool_return'] = 'tool_return'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:46-46
    kind: Literal['call_deferred'] = 'call_deferred'

# pydantic_ai_slim/pydantic_ai/_griffe.py:10-10
from griffe import Docstring, DocstringSectionKind, GoogleOptions, Object as GriffeObject

# pydantic_ai_slim/pydantic_ai/messages.py:901-901
    part_kind: Literal['tool-return'] = 'tool-return'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:264-264
    kind: str = 'url_context'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:251-251
    kind: str = 'web_fetch'

# pydantic_ai_slim/pydantic_ai/messages.py:923-923
    part_kind: Literal['builtin-tool-return'] = 'builtin-tool-return'

# pydantic_ai_slim/pydantic_ai/_output.py:675-675
    kind: str

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:33-43
from .tools import (
    BuiltinToolFunc,
    DeferredToolCallResult,
    DeferredToolResult,
    DeferredToolResults,
    RunContext,
    ToolApproved,
    ToolDefinition,
    ToolDenied,
    ToolKind,
)

# pydantic_ai_slim/pydantic_ai/models/instrumented.py:20-20
from opentelemetry.trace import Span, SpanKind, Tracer, TracerProvider, get_tracer_provider

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:33-43
from .tools import (
    BuiltinToolFunc,
    DeferredToolCallResult,
    DeferredToolResult,
    DeferredToolResults,
    RunContext,
    ToolApproved,
    ToolDefinition,
    ToolDenied,
    ToolKind,
)

# pydantic_graph/pydantic_graph/beta/mermaid.py:30-30
NodeKind = Literal['broadcast', 'map', 'join', 'start', 'end', 'step', 'decision']

# pydantic_ai_slim/pydantic_ai/messages.py:266-266
    kind: Literal['video-url'] = 'video-url'

# pydantic_ai_slim/pydantic_ai/messages.py:325-325
    kind: Literal['audio-url'] = 'audio-url'

# pydantic_ai_slim/pydantic_ai/messages.py:372-372
    kind: Literal['image-url'] = 'image-url'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:369-369
    kind: str = 'memory'

# pydantic_ai_slim/pydantic_ai/messages.py:658-658
    kind: Literal['cache-point'] = 'cache-point'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:52-52
    kind: Literal['model_retry'] = 'model_retry'

# pydantic_ai_slim/pydantic_ai/messages.py:418-418
    kind: Literal['document-url'] = 'document-url'

# pydantic_graph/pydantic_graph/beta/mermaid.py:38-38
    kind: NodeKind

# pydantic_graph/pydantic_graph/persistence/__init__.py:79-79
    kind: Literal['end'] = 'end'

# pydantic_ai_slim/pydantic_ai/messages.py:1043-1043
    kind: Literal['request'] = 'request'

# pydantic_ai_slim/pydantic_ai/tools.py:174-174
    kind: Literal['tool-approved'] = 'tool-approved'

# pydantic_graph/pydantic_graph/persistence/__init__.py:58-58
    kind: Literal['node'] = 'node'

# pydantic_ai_slim/pydantic_ai/messages.py:759-759
    part_kind: Literal['user-prompt'] = 'user-prompt'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:152-152
    kind: str = 'web_search'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:433-433
    kind: str = 'mcp_server'

# pydantic_ai_slim/pydantic_ai/messages.py:493-493
    kind: Literal['binary'] = 'binary'

# pydantic_ai_slim/pydantic_ai/messages.py:1318-1318
    kind: Literal['response'] = 'response'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:464-464
    kind: str = 'file_search'

# pydantic_ai_slim/pydantic_ai/messages.py:1963-1963
    event_kind: Literal['final_result'] = 'final_result'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:191-191
    kind: str = 'code_execution'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset.py:40-40
    kind: Literal['approval_required'] = 'approval_required'

# pydantic_ai_slim/pydantic_ai/messages.py:1088-1088
    part_kind: Literal['text'] = 'text'

# pydantic_ai_slim/pydantic_ai/messages.py:1178-1178
    part_kind: Literal['file'] = 'file'

# pydantic_ai_slim/pydantic_ai/run.py:452-452
    event_kind: Literal['agent_run_result'] = 'agent_run_result'

# pydantic_ai_slim/pydantic_ai/builtin_tools.py:356-356
    kind: str = 'image_generation'

# pydantic_ai_slim/pydantic_ai/messages.py:2052-2052
    event_kind: Literal['builtin_tool_result'] = 'builtin_tool_result'

# pydantic_ai_slim/pydantic_ai/messages.py:1140-1140
    part_kind: Literal['thinking'] = 'thinking'

# pydantic_ai_slim/pydantic_ai/messages.py:1272-1272
    part_kind: Literal['tool-call'] = 'tool-call'

# pydantic_ai_slim/pydantic_ai/messages.py:2013-2013
    event_kind: Literal['function_tool_result'] = 'function_tool_result'

# pydantic_ai_slim/pydantic_ai/messages.py:1949-1949
    event_kind: Literal['part_end'] = 'part_end'

# pydantic_ai_slim/pydantic_ai/messages.py:967-967
    part_kind: Literal['retry-prompt'] = 'retry-prompt'

# pydantic_ai_slim/pydantic_ai/messages.py:148-148
    part_kind: Literal['system-prompt'] = 'system-prompt'

# pydantic_ai_slim/pydantic_ai/messages.py:1909-1909
    event_kind: Literal['part_start'] = 'part_start'

# pydantic_ai_slim/pydantic_ai/messages.py:1925-1925
    event_kind: Literal['part_delta'] = 'part_delta'

# pydantic_ai_slim/pydantic_ai/messages.py:1282-1282
    part_kind: Literal['builtin-tool-call'] = 'builtin-tool-call'

# pydantic_ai_slim/pydantic_ai/messages.py:1941-1943
    next_part_kind: (
        Literal['text', 'thinking', 'tool-call', 'builtin-tool-call', 'builtin-tool-return', 'file'] | None
    ) = None

# pydantic_ai_slim/pydantic_ai/messages.py:2036-2036
    event_kind: Literal['builtin_tool_call'] = 'builtin_tool_call'

# tests/test_mcp.py:1166-1247
async def test_tool_returning_unstructured_dict(allow_model_requests: None, agent: Agent):
    async with agent:
        result = await agent.run('Get me an unstructured dict, respond on one line')
        assert result.output == snapshot('{"foo":"bar","baz":123}')
        assert result.all_messages() == snapshot(
            [
                ModelRequest(
                    parts=[
                        UserPromptPart(
                            content='Get me an unstructured dict, respond on one line',
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[
                        ToolCallPart(
                            tool_name='get_unstructured_dict', args='{}', tool_call_id='call_R0n2R7S9vL2aZOX25T9jahTd'
                        )
                    ],
                    usage=RequestUsage(
                        input_tokens=343,
                        output_tokens=12,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={
                        'finish_reason': 'tool_calls',
                        'timestamp': IsDatetime(),
                    },
                    provider_response_id='chatcmpl-CLbP82ODQMEznhobUKdq6Rjn9Aa12',
                    finish_reason='tool_call',
                    run_id=IsStr(),
                ),
                ModelRequest(
                    parts=[
                        ToolReturnPart(
                            tool_name='get_unstructured_dict',
                            content={'foo': 'bar', 'baz': 123},
                            tool_call_id='call_R0n2R7S9vL2aZOX25T9jahTd',
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[TextPart(content='{"foo":"bar","baz":123}')],
                    usage=RequestUsage(
                        input_tokens=374,
                        output_tokens=10,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={
                        'finish_reason': 'stop',
                        'timestamp': IsDatetime(),
                    },
                    provider_response_id='chatcmpl-CLbPAOYN3jPYdvYeD8JNOOXF5N554',
                    finish_reason='stop',
                    run_id=IsStr(),
                ),
            ]
        )

# pydantic_ai_slim/pydantic_ai/messages.py:1984-1984
    event_kind: Literal['function_tool_call'] = 'function_tool_call'

# pydantic_ai_slim/pydantic_ai/messages.py:1581-1581
    part_delta_kind: Literal['text'] = 'text'

# tests/test_builtin_tools.py:72-75
def test_url_context_tool_is_deprecated():
    """Test that UrlContextTool is deprecated and warns users to use WebFetchTool instead."""
    with pytest.warns(DeprecationWarning, match='Use `WebFetchTool` instead.'):
        UrlContextTool()  # pyright: ignore[reportDeprecated]

# pydantic_ai_slim/pydantic_ai/messages.py:1638-1638
    part_delta_kind: Literal['thinking'] = 'thinking'

# pydantic_ai_slim/pydantic_ai/messages.py:1751-1751
    part_delta_kind: Literal['tool_call'] = 'tool_call'

# pydantic_ai_slim/pydantic_ai/messages.py:1901-1903
    previous_part_kind: (
        Literal['text', 'thinking', 'tool-call', 'builtin-tool-call', 'builtin-tool-return', 'file'] | None
    ) = None

# tests/test_messages.py:801-835
def test_tool_return_content_with_explicit_image_url():
    """Test that ImageUrl with explicit 'kind' discriminator is correctly deserialized."""
    from pydantic_ai.messages import ToolReturnPart

    serialized_history = r"""[
      {
        "parts": [{"content": "Hello", "timestamp": "2026-02-03T22:25:50Z", "part_kind": "user-prompt"}],
        "kind": "request"
      },
      {
        "parts": [
          {
            "tool_name": "image_tool",
            "content": {
              "url": "https://example.com/image.png",
              "kind": "image-url"
            },
            "tool_call_id": "call_1",
            "timestamp": "2026-02-03T22:27:32Z",
            "part_kind": "tool-return"
          }
        ],
        "kind": "request"
      }
    ]
    """

    deserialized = ModelMessagesTypeAdapter.validate_json(serialized_history)

    tool_return_part = deserialized[1].parts[0]
    assert isinstance(tool_return_part, ToolReturnPart)

    # Content with explicit kind: "image-url" should become ImageUrl
    assert isinstance(tool_return_part.content, ImageUrl)
    assert tool_return_part.content.url == 'https://example.com/image.png'

# tests/models/test_mistral.py:760-843
async def test_stream_result_type_primitif_dict(allow_model_requests: None):
    """This test tests the primitif result with the pydantic ai format model response"""

    class MyTypedDict(TypedDict, total=False):
        first: str
        second: str

    stream = [
        text_chunk('{'),
        text_chunk('"'),
        text_chunk('f'),
        text_chunk('i'),
        text_chunk('r'),
        text_chunk('s'),
        text_chunk('t'),
        text_chunk('"'),
        text_chunk(':'),
        text_chunk(' '),
        text_chunk('"'),
        text_chunk('O'),
        text_chunk('n'),
        text_chunk('e'),
        text_chunk('"'),
        text_chunk(','),
        text_chunk(' '),
        text_chunk('"'),
        text_chunk('s'),
        text_chunk('e'),
        text_chunk('c'),
        text_chunk('o'),
        text_chunk('n'),
        text_chunk('d'),
        text_chunk('"'),
        text_chunk(':'),
        text_chunk(' '),
        text_chunk('"'),
        text_chunk('T'),
        text_chunk('w'),
        text_chunk('o'),
        text_chunk('"'),
        text_chunk('}'),
        chunk([]),
    ]

    mock_client = MockMistralAI.create_stream_mock(stream)
    model = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(model=model, output_type=MyTypedDict)

    async with agent.run_stream('User prompt value') as result:
        assert not result.is_complete
        v = [c async for c in result.stream_output(debounce_by=None)]
        assert v == snapshot(
            [
                {'first': 'O'},
                {'first': 'On'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One'},
                {'first': 'One', 'second': ''},
                {'first': 'One', 'second': 'T'},
                {'first': 'One', 'second': 'Tw'},
                {'first': 'One', 'second': 'Two'},
                {'first': 'One', 'second': 'Two'},
                {'first': 'One', 'second': 'Two'},
                {'first': 'One', 'second': 'Two'},
            ]
        )
        assert result.is_complete
        assert result.usage().input_tokens == 34
        assert result.usage().output_tokens == 34

        # double check usage matches stream count
        assert result.usage().output_tokens == len(stream)

# tests/test_mcp.py:1086-1163
async def test_tool_returning_dict(allow_model_requests: None, agent: Agent):
    async with agent:
        result = await agent.run('Get me a dict, respond on one line')
        assert result.output == snapshot('{"foo":"bar","baz":123}')
        assert result.all_messages() == snapshot(
            [
                ModelRequest(
                    parts=[
                        UserPromptPart(
                            content='Get me a dict, respond on one line',
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[ToolCallPart(tool_name='get_dict', args='{}', tool_call_id='call_oqKviITBj8PwpQjGyUu4Zu5x')],
                    usage=RequestUsage(
                        input_tokens=195,
                        output_tokens=11,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={
                        'finish_reason': 'tool_calls',
                        'timestamp': IsDatetime(),
                    },
                    provider_response_id='chatcmpl-BRloOs7Bb2tq8wJyy9Rv7SQ7L65a7',
                    finish_reason='tool_call',
                    run_id=IsStr(),
                ),
                ModelRequest(
                    parts=[
                        ToolReturnPart(
                            tool_name='get_dict',
                            content={'foo': 'bar', 'baz': 123},
                            tool_call_id='call_oqKviITBj8PwpQjGyUu4Zu5x',
                            timestamp=IsDatetime(),
                        )
                    ],
                    timestamp=IsDatetime(),
                    run_id=IsStr(),
                ),
                ModelResponse(
                    parts=[TextPart(content='{"foo":"bar","baz":123}')],
                    usage=RequestUsage(
                        input_tokens=222,
                        output_tokens=11,
                        details={
                            'accepted_prediction_tokens': 0,
                            'audio_tokens': 0,
                            'reasoning_tokens': 0,
                            'rejected_prediction_tokens': 0,
                        },
                    ),
                    model_name='gpt-4o-2024-08-06',
                    timestamp=IsDatetime(),
                    provider_name='openai',
                    provider_url='https://api.openai.com/v1/',
                    provider_details={
                        'finish_reason': 'stop',
                        'timestamp': IsDatetime(),
                    },
                    provider_response_id='chatcmpl-BRloPczU1HSCWnreyo21DdNtdOM7L',
                    finish_reason='stop',
                    run_id=IsStr(),
                ),
            ]
        )

# pydantic_ai_slim/pydantic_ai/tools.py:491-491
    outer_typed_dict_key: str | None = None

# tests/mcp_server.py:151-152
async def get_unstructured_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}

# tests/test_mcp.py:2093-2109
def test_load_mcp_servers_with_default_values_in_env_dict(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test ${VAR:-default} syntax in env dictionary."""
    config = tmp_path / 'mcp.json'

    monkeypatch.delenv('API_KEY', raising=False)
    monkeypatch.setenv('CUSTOM_VAR', 'custom_value')
    config.write_text(
        '{"mcpServers": {"server": {"command": "python", "args": [], '
        '"env": {"API_KEY": "${API_KEY:-default_key}", "CUSTOM": "${CUSTOM_VAR:-fallback}"}}}}',
        encoding='utf-8',
    )

    servers = load_mcp_servers(config)
    assert len(servers) == 1
    server = servers[0]
    assert isinstance(server, MCPServerStdio)
    assert server.env == {'API_KEY': 'default_key', 'CUSTOM': 'custom_value'}

# tests/test_agent.py:1672-1742
def test_output_type_structured_dict():
    PersonDict = StructuredDict(
        {
            'type': 'object',
            'properties': {
                'name': {'type': 'string'},
                'age': {'type': 'integer'},
            },
            'required': ['name', 'age'],
        },
        name='Person',
        description='A person',
    )
    AnimalDict = StructuredDict(
        {
            'type': 'object',
            'properties': {
                'name': {'type': 'string'},
                'species': {'type': 'string'},
            },
            'required': ['name', 'species'],
        },
        name='Animal',
        description='An animal',
    )

    output_tools = None

    def call_tool(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        assert info.output_tools is not None

        nonlocal output_tools
        output_tools = info.output_tools

        args_json = '{"name": "John Doe", "age": 30}'
        return ModelResponse(parts=[ToolCallPart(info.output_tools[0].name, args_json)])

    agent = Agent(
        FunctionModel(call_tool),
        output_type=[PersonDict, AnimalDict],
    )

    result = agent.run_sync('Generate a person')

    assert result.output == snapshot({'name': 'John Doe', 'age': 30})
    assert output_tools == snapshot(
        [
            ToolDefinition(
                name='final_result_Person',
                parameters_json_schema={
                    'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}},
                    'required': ['name', 'age'],
                    'title': 'Person',
                    'type': 'object',
                },
                description='A person',
                kind='output',
            ),
            ToolDefinition(
                name='final_result_Animal',
                parameters_json_schema={
                    'properties': {'name': {'type': 'string'}, 'species': {'type': 'string'}},
                    'required': ['name', 'species'],
                    'title': 'Animal',
                    'type': 'object',
                },
                description='An animal',
                kind='output',
            ),
        ]
    )

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:8-8
from collections import defaultdict, deque

# tests/test_agent.py:1745-1810
def test_output_type_structured_dict_nested():
    """Test StructuredDict with nested JSON schemas using $ref - Issue #2466."""
    # Schema with nested $ref that pydantic's generator can't resolve
    CarDict = StructuredDict(
        {
            '$defs': {
                'Tire': {
                    'type': 'object',
                    'properties': {'brand': {'type': 'string'}, 'size': {'type': 'integer'}},
                    'required': ['brand', 'size'],
                }
            },
            'type': 'object',
            'properties': {
                'make': {'type': 'string'},
                'model': {'type': 'string'},
                'tires': {'type': 'array', 'items': {'$ref': '#/$defs/Tire'}},
            },
            'required': ['make', 'model', 'tires'],
        },
        name='Car',
        description='A car with tires',
    )

    def call_tool(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        assert info.output_tools is not None

        # Verify the output tool schema has been properly transformed
        # The $refs should be inlined by InlineDefsJsonSchemaTransformer
        output_tool = info.output_tools[0]
        schema = output_tool.parameters_json_schema
        assert schema is not None

        assert schema == snapshot(
            {
                'properties': {
                    'make': {'type': 'string'},
                    'model': {'type': 'string'},
                    'tires': {
                        'items': {
                            'properties': {'brand': {'type': 'string'}, 'size': {'type': 'integer'}},
                            'required': ['brand', 'size'],
                            'type': 'object',
                        },
                        'type': 'array',
                    },
                },
                'required': ['make', 'model', 'tires'],
                'title': 'Car',
                'type': 'object',
            }
        )

        return ModelResponse(
            parts=[
                ToolCallPart(
                    output_tool.name, {'make': 'Toyota', 'model': 'Camry', 'tires': [{'brand': 'Michelin', 'size': 17}]}
                )
            ]
        )

    agent = Agent(FunctionModel(call_tool), output_type=CarDict)

    result = agent.run_sync('Generate a car')

    assert result.output == snapshot({'make': 'Toyota', 'model': 'Camry', 'tires': [{'brand': 'Michelin', 'size': 17}]})

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# pydantic_ai_slim/pydantic_ai/__init__.py:104-104
from .output import NativeOutput, PromptedOutput, StructuredDict, TextOutput, ToolOutput

# tests/test_agent.py:1813-1845
def test_structured_dict_recursive_refs():
    class Node(BaseModel):
        nodes: list['Node'] | dict[str, 'Node']

    schema = Node.model_json_schema()
    assert schema == snapshot(
        {
            '$defs': {
                'Node': {
                    'properties': {
                        'nodes': {
                            'anyOf': [
                                {'items': {'$ref': '#/$defs/Node'}, 'type': 'array'},
                                {'additionalProperties': {'$ref': '#/$defs/Node'}, 'type': 'object'},
                            ],
                            'title': 'Nodes',
                        }
                    },
                    'required': ['nodes'],
                    'title': 'Node',
                    'type': 'object',
                }
            },
            '$ref': '#/$defs/Node',
        }
    )
    with pytest.raises(
        UserError,
        match=re.escape(
            '`StructuredDict` does not currently support recursive `$ref`s and `$defs`. See https://github.com/pydantic/pydantic/issues/12145 for more information.'
        ),
    ):
        StructuredDict(schema)

# tests/typed_agent.py:176-182
structured_dict = StructuredDict(
    {
        'type': 'object',
        'properties': {'name': {'type': 'string'}, 'age': {'type': 'integer'}},
        'required': ['name', 'age'],
    }
)

# tests/evals/test_evaluator_common.py:93-117
async def test_contains_dict():
    """Test Contains evaluator with dictionaries."""
    evaluator = Contains(value={'key': 'value'})

    # Test dictionary containment
    assert evaluator.evaluate(MockContext(output={'key': 'value', 'extra': 'data'})) == snapshot(
        EvaluationReason(value=True)
    )

    # Test dictionary key missing
    assert evaluator.evaluate(MockContext(output={'different': 'value'})) == snapshot(
        EvaluationReason(value=False, reason="Output dictionary does not contain expected key 'key'")
    )

    # Test dictionary value mismatch
    assert evaluator.evaluate(MockContext(output={'key': 'different'})) == snapshot(
        EvaluationReason(
            value=False,
            reason="Output dictionary has different value for key 'key': 'different' != 'value'",
        )
    )

    # Test non-dict value in dict
    evaluator_single = Contains(value='key')
    assert evaluator_single.evaluate(MockContext(output={'key': 'value'})) == snapshot(EvaluationReason(value=True))

# tests/test_vercel_ai.py:3010-3045
async def test_adapter_dump_messages_tool_call_without_return():
    """Test dumping messages with a tool call that has no corresponding result."""
    messages = [
        ModelResponse(
            parts=[
                ToolCallPart(
                    tool_name='get_weather',
                    args={'city': 'New York'},
                    tool_call_id='tool_abc',
                ),
            ]
        ),
    ]

    ui_messages = VercelAIAdapter.dump_messages(messages)
    ui_message_dicts = [msg.model_dump() for msg in ui_messages]

    assert ui_message_dicts == snapshot(
        [
            {
                'id': IsStr(),
                'role': 'assistant',
                'metadata': None,
                'parts': [
                    {
                        'type': 'dynamic-tool',
                        'tool_name': 'get_weather',
                        'tool_call_id': 'tool_abc',
                        'state': 'input-available',
                        'input': '{"city":"New York"}',
                        'call_provider_metadata': None,
                    }
                ],
            }
        ]
    )

# tests/models/test_mistral.py:1473-1606
async def test_stream_tool_call_with_return_type(allow_model_requests: None):
    class MyTypedDict(TypedDict, total=False):
        won: bool

    completion = [
        [
            chunk(
                delta=[MistralDeltaMessage(role=MistralUnset(), content='', tool_calls=MistralUnset())],
                finish_reason='tool_calls',
            ),
            func_chunk(
                tool_calls=[
                    MistralToolCall(
                        id='1',
                        function=MistralFunctionCall(arguments='{"loc_name": "San Fransisco"}', name='get_location'),
                        type='function',
                    )
                ],
                finish_reason='tool_calls',
            ),
        ],
        [
            chunk(
                delta=[MistralDeltaMessage(role=MistralUnset(), content='', tool_calls=MistralUnset())],
                finish_reason='tool_calls',
            ),
            func_chunk(
                tool_calls=[
                    MistralToolCall(
                        id='1',
                        function=MistralFunctionCall(arguments='{"won": true}', name='final_result'),
                        type=None,
                    )
                ],
                finish_reason='tool_calls',
            ),
        ],
    ]

    mock_client = MockMistralAI.create_stream_mock(completion)
    model = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(model, instructions='this is the system prompt', output_type=MyTypedDict)

    @agent.tool_plain
    async def get_location(loc_name: str) -> str:
        return json.dumps({'lat': 51, 'lng': 0})

    async with agent.run_stream('User prompt value') as result:
        assert not result.is_complete
        v = [c async for c in result.stream_output(debounce_by=None)]
        assert v == snapshot([{'won': True}, {'won': True}])
        assert result.is_complete
        assert result.timestamp() == IsNow(tz=timezone.utc)
        assert result.usage().input_tokens == 4
        assert result.usage().output_tokens == 4

        # double check usage matches stream count
        assert result.usage().output_tokens == 4

    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(content='User prompt value', timestamp=IsNow(tz=timezone.utc)),
                ],
                instructions='this is the system prompt',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ToolCallPart(
                        tool_name='get_location',
                        args='{"loc_name": "San Fransisco"}',
                        tool_call_id='1',
                    )
                ],
                usage=RequestUsage(input_tokens=2, output_tokens=2),
                model_name='gpt-4',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='mistral',
                provider_url='https://api.mistral.ai',
                provider_details={
                    'finish_reason': 'tool_calls',
                    'timestamp': datetime(2024, 1, 1, 0, 0, tzinfo=timezone.utc),
                },
                provider_response_id='x',
                finish_reason='tool_call',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='get_location',
                        content='{"lat": 51, "lng": 0}',
                        tool_call_id='1',
                        timestamp=IsNow(tz=timezone.utc),
                    )
                ],
                instructions='this is the system prompt',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='final_result', args='{"won": true}', tool_call_id='1')],
                usage=RequestUsage(input_tokens=2, output_tokens=2),
                model_name='gpt-4',
                timestamp=IsNow(tz=timezone.utc),
                provider_name='mistral',
                provider_url='https://api.mistral.ai',
                provider_details={
                    'finish_reason': 'tool_calls',
                    'timestamp': datetime(2024, 1, 1, 0, 0, tzinfo=timezone.utc),
                },
                provider_response_id='x',
                finish_reason='tool_call',
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='final_result',
                        content='Final result processed.',
                        tool_call_id='1',
                        timestamp=IsNow(tz=timezone.utc),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

    assert await result.get_output() == {'won': True}

# tests/models/test_bedrock.py:2668-2676
async def test_get_last_user_message_content_non_dict_block(
    allow_model_requests: None, bedrock_provider: BedrockProvider
):
    """Test _get_last_user_message_content returns None when last block is not a dict."""
    model = BedrockConverseModel('us.anthropic.claude-3-5-sonnet-20240620-v1:0', provider=bedrock_provider)
    # Directly test the helper with a message that has non-dict content
    messages: list[MessageUnionTypeDef] = [{'role': 'user', 'content': ['string content']}]  # type: ignore[list-item]
    result = model._get_last_user_message_content(messages)  # pyright: ignore[reportPrivateUsage]
    assert result is None

# tests/typed_agent.py:183-183
structured_dict_agent = Agent(output_type=structured_dict)

# pydantic_ai_slim/pydantic_ai/tools.py:497-497
    strict: bool | None = None

# tests/test_vercel_ai.py:2499-2544
async def test_adapter_dump_messages_with_builtin_tool_without_return():
    """Test dumping messages with a builtin tool call that has no return in the same message."""
    messages = [
        ModelRequest(parts=[UserPromptPart(content='Search for something')]),
        ModelResponse(
            parts=[
                BuiltinToolCallPart(
                    tool_name='web_search',
                    args={'query': 'orphan query'},
                    tool_call_id='orphan_tool_id',
                    provider_name='openai',
                ),
            ]
        ),
    ]

    ui_messages = VercelAIAdapter.dump_messages(messages)
    ui_message_dicts = [msg.model_dump() for msg in ui_messages]

    assert ui_message_dicts == snapshot(
        [
            {
                'id': IsStr(),
                'role': 'user',
                'metadata': None,
                'parts': [{'type': 'text', 'text': 'Search for something', 'state': 'done', 'provider_metadata': None}],
            },
            {
                'id': IsStr(),
                'role': 'assistant',
                'metadata': None,
                'parts': [
                    {
                        'type': 'tool-web_search',
                        'tool_call_id': 'orphan_tool_id',
                        'state': 'input-available',
                        'input': '{"query":"orphan query"}',
                        'provider_executed': True,
                        'call_provider_metadata': {
                            'pydantic_ai': {'provider_name': 'openai'}
                        },  # No return part, so defaults to normal call provider name
                    }
                ],
            },
        ]
    )

# tests/test_builtin_tools.py:106-123
def test_url_context_tool_instance_behavior():
    """Test that UrlContextTool instances work correctly with deprecation warning."""
    adapter = TypeAdapter(AbstractBuiltinTool)

    # Create instance with deprecation warning
    with pytest.warns(DeprecationWarning, match='Use `WebFetchTool` instead.'):
        tool = UrlContextTool(max_uses=3, enable_citations=True)  # pyright: ignore[reportDeprecated]

    # UrlContextTool inherits from WebFetchTool and overrides kind to 'url_context'
    assert isinstance(tool, WebFetchTool)
    assert tool.kind == 'url_context'
    assert tool.max_uses == 3
    assert tool.enable_citations is True

    # Serialization should use 'url_context'
    serialized = adapter.dump_python(tool)
    assert serialized['kind'] == 'url_context'
    assert serialized['max_uses'] == 3

# tests/test_agent.py:2932-2938
async def test_agent_metadata_override_with_dict() -> None:
    agent = Agent(TestModel(custom_output_text='override dict base'), metadata={'env': 'base'})

    with agent.override(metadata={'env': 'override'}):
        result = await agent.run('override dict prompt')

    assert result.metadata == {'env': 'override'}

# tests/test_tools.py:1114-1148
def test_tool_parameters_with_attribute_docstrings():
    agent = Agent(FunctionModel(get_json_schema))

    class Data(TypedDict):
        a: int
        """The first parameter"""
        b: int
        """The second parameter"""

    @agent.tool_plain
    def get_score(data: Data) -> int: ...  # pragma: no branch

    result = agent.run_sync('Hello')
    json_schema = json.loads(result.output)
    assert json_schema == snapshot(
        {
            'name': 'get_score',
            'description': None,
            'parameters_json_schema': {
                'properties': {
                    'a': {'description': 'The first parameter', 'type': 'integer'},
                    'b': {'description': 'The second parameter', 'type': 'integer'},
                },
                'required': ['a', 'b'],
                'title': 'Data',
                'type': 'object',
            },
            'outer_typed_dict_key': None,
            'strict': None,
            'kind': 'function',
            'sequential': False,
            'metadata': None,
            'timeout': None,
        }
    )

# tests/graph/test_persistence.py:346-348
def test_snapshot_type_adapter_error():
    with pytest.raises(RuntimeError, match='Unable to build a Pydantic schema for `BaseNode` without setting'):
        build_snapshot_list_type_adapter(int, int)

# tests/test_builtin_tools.py:28-33
async def test_builtin_tools_not_supported_web_search_stream(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[WebSearchTool()])

    with pytest.raises(UserError):
        async with agent.run_stream('What day is tomorrow?'):
            ...  # pragma: no cover

# tests/test_builtin_tools.py:64-69
async def test_builtin_tools_not_supported_file_search_stream(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[FileSearchTool(file_store_ids=['test-id'])])

    with pytest.raises(UserError):
        async with agent.run_stream('Search my files'):
            ...  # pragma: no cover

# tests/evals/test_report_evaluators.py:612-629
def test_confusion_matrix_evaluator_metadata_non_dict():
    """ConfusionMatrixEvaluator with metadata_from but non-dict metadata returns str(metadata)."""
    cases = [
        _make_report_case('c1', expected_output='A', metadata='some_string'),
    ]
    report = _make_report(cases)

    evaluator = ConfusionMatrixEvaluator(
        predicted_from='metadata',
        predicted_key=None,
        expected_from='expected_output',
    )
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)
    result = evaluator.evaluate(ctx)

    assert isinstance(result, ConfusionMatrix)
    assert result.class_labels == ['A', 'some_string']
    assert result.matrix == [[0, 1], [0, 0]]

# tests/test_builtin_tools.py:126-148
def test_url_context_discriminated_union():
    """Test that the discriminated union correctly handles both url_context and web_fetch."""
    adapter = TypeAdapter(list[AbstractBuiltinTool])

    # Mix of old and new payloads
    payloads = [
        {'kind': 'url_context', 'max_uses': 1},
        {'kind': 'web_fetch', 'max_uses': 2},
        {'kind': 'web_search'},
        {'kind': 'code_execution'},
    ]

    # Old url_context payloads will trigger deprecation warnings
    with pytest.warns(DeprecationWarning, match='Use `WebFetchTool` instead.'):
        results = adapter.validate_python(payloads)
    assert len(results) == 4
    assert isinstance(results[0], UrlContextTool)  # pyright: ignore[reportDeprecated]
    assert isinstance(results[0], WebFetchTool)  # UrlContextTool is a subclass
    assert results[0].kind == 'url_context'
    assert results[0].max_uses == 1
    assert isinstance(results[1], WebFetchTool)
    assert results[1].kind == 'web_fetch'
    assert results[1].max_uses == 2

# tests/models/anthropic/test_output.py:331-349
def test_strict_true_tool_no_output(
    allow_model_requests: None,
    anthropic_model: ANTHROPIC_MODEL_FIXTURE,
) -> None:
    """Tool with strict=True, no output_type → beta header, tool has strict field."""
    model = anthropic_model('claude-sonnet-4-5')
    hook = create_header_verification_hook(expect_beta=True, test_name='test_strict_true_tool_no_output')
    model.client._client.event_hooks['request'].append(hook)  # pyright: ignore[reportPrivateUsage]

    agent = Agent(model)

    @agent.tool_plain(strict=True)
    def get_weather(city: str) -> str:
        return f'Weather in {city}: Sunny, 22°C'

    agent.run_sync("What's the weather in San Francisco?")

    if errors := hook.errors:  # type: ignore[attr-defined]
        assert False, '\n'.join(sorted(errors))

# tests/models/test_openai.py:1753-1754
def tool_with_typed_dict(x: MyNormalTypedDict) -> str:
    return f'{x}'  # pragma: no cover

# tests/mcp_server.py:146-147
async def get_dict() -> dict[str, Any]:
    return {'foo': 'bar', 'baz': 123}