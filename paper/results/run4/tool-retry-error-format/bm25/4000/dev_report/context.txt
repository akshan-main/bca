# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# tests/test_agent.py:241-295
def test_result_pydantic_model_validation_error():
    def return_model(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        assert info.output_tools is not None
        if len(messages) == 1:
            args_json = '{"a": 1, "b": "foo"}'
        else:
            args_json = '{"a": 1, "b": "bar"}'
        return ModelResponse(parts=[ToolCallPart(info.output_tools[0].name, args_json)])

    class Bar(BaseModel):
        a: int
        b: str

        @field_validator('b')
        def check_b(cls, v: str) -> str:
            if v == 'foo':
                raise ValueError('must not be foo')
            return v

    agent = Agent(FunctionModel(return_model), output_type=Bar)

    result = agent.run_sync('Hello')
    assert isinstance(result.output, Bar)
    assert result.output.model_dump() == snapshot({'a': 1, 'b': 'bar'})
    messages_part_kinds = [(m.kind, [p.part_kind for p in m.parts]) for m in result.all_messages()]
    assert messages_part_kinds == snapshot(
        [
            ('request', ['user-prompt']),
            ('response', ['tool-call']),
            ('request', ['retry-prompt']),
            ('response', ['tool-call']),
            ('request', ['tool-return']),
        ]
    )

    user_retry = result.all_messages()[2]
    assert isinstance(user_retry, ModelRequest)
    retry_prompt = user_retry.parts[0]
    assert isinstance(retry_prompt, RetryPromptPart)
    assert retry_prompt.model_response() == snapshot("""\
1 validation error:
```json
[
  {
    "type": "value_error",
    "loc": [
      "b"
    ],
    "msg": "Value error, must not be foo",
    "input": "foo"
  }
]
```

Fix the errors and try again.""")

# pydantic_graph/pydantic_graph/beta/graph_builder.py:49-49
from pydantic_graph.exceptions import GraphBuildingError, GraphValidationError

# pydantic_graph/pydantic_graph/beta/graph_builder.py:49-49
from pydantic_graph.exceptions import GraphBuildingError, GraphValidationError

# pydantic_graph/pydantic_graph/beta/graph_builder.py:49-49
from pydantic_graph.exceptions import GraphBuildingError, GraphValidationError

# tests/test_exceptions.py:77-87
def test_tool_retry_error_str_with_error_details():
    """Test that ToolRetryError formats ErrorDetails automatically."""
    validation_error = ValidationError.from_exception_data(
        'Test', [{'type': 'string_type', 'loc': ('name',), 'input': 123}]
    )
    part = RetryPromptPart(content=validation_error.errors(include_url=False), tool_name='my_tool')
    error = ToolRetryError(part)

    assert str(error) == (
        "1 validation error for 'my_tool'\nname\n  Input should be a valid string [type=string_type, input_value=123]"
    )

# pydantic_graph/pydantic_graph/exceptions.py:32-32
    message: str

# pydantic_graph/pydantic_graph/exceptions.py:35-37
    def __init__(self, message: str):
        self.message = message
        super().__init__(message)

# tests/test_agent.py:5647-5691
def test_tool_call_with_validation_value_error_serializable():
    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(parts=[ToolCallPart('foo_tool', {'bar': 0})])
        elif len(messages) == 3:
            return ModelResponse(parts=[ToolCallPart('foo_tool', {'bar': 1})])
        else:
            return ModelResponse(parts=[TextPart('Tool returned 1')])

    agent = Agent(FunctionModel(llm))

    class Foo(BaseModel):
        bar: int

        @field_validator('bar')
        def validate_bar(cls, v: int) -> int:
            if v == 0:
                raise ValueError('bar cannot be 0')
            return v

    @agent.tool_plain
    def foo_tool(foo: Foo) -> int:
        return foo.bar

    result = agent.run_sync('Hello')
    assert json.loads(result.all_messages_json())[2] == snapshot(
        {
            'parts': [
                {
                    'content': [
                        {'type': 'value_error', 'loc': ['bar'], 'msg': 'Value error, bar cannot be 0', 'input': 0}
                    ],
                    'tool_name': 'foo_tool',
                    'tool_call_id': IsStr(),
                    'timestamp': IsStr(),
                    'part_kind': 'retry-prompt',
                }
            ],
            'timestamp': IsStr(),
            'instructions': None,
            'kind': 'request',
            'run_id': IsStr(),
            'metadata': None,
        }
    )

# tests/graph/beta/test_graph_builder.py:431-444
async def test_validation_can_be_disabled():
    """Test that validation can be disabled with validate_graph_structure=False."""
    g = GraphBuilder(output_type=int)

    @g.step
    async def orphan_step(ctx: StepContext[None, None, None]) -> int:
        return 42  # pragma: no cover

    # Add the step to the graph but don't connect it to start
    # This would normally fail validation
    g.add(g.edge_from(orphan_step).to(g.end_node))

    # Should not raise an error when validation is disabled
    g.build(validate_graph_structure=False)

# tests/test_validation_context.py:5-5
from pydantic import BaseModel, ValidationInfo, field_validator

# tests/test_streaming.py:2603-2651
async def test_output_tool_validation_failure_events():
    """Test that output tools that fail validation emit events during streaming."""

    def call_final_result_with_bad_data(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        """Mock function that calls final_result tool with invalid data."""
        assert info.output_tools is not None
        return ModelResponse(
            parts=[
                ToolCallPart('final_result', {'bad_value': 'invalid'}),  # Invalid field name
                ToolCallPart('final_result', {'value': 'valid'}),  # Valid field name
            ]
        )

    agent = Agent(FunctionModel(call_final_result_with_bad_data), output_type=OutputType)

    events: list[Any] = []
    async with agent.iter('test') as agent_run:
        async for node in agent_run:
            if Agent.is_call_tools_node(node):
                async with node.stream(agent_run.ctx) as event_stream:
                    async for event in event_stream:
                        events.append(event)

    assert events == snapshot(
        [
            FunctionToolCallEvent(
                part=ToolCallPart(
                    tool_name='final_result',
                    args={'bad_value': 'invalid'},
                    tool_call_id=IsStr(),
                ),
            ),
            FunctionToolResultEvent(
                result=RetryPromptPart(
                    content=[
                        ErrorDetails(
                            type='missing',
                            loc=('value',),
                            msg='Field required',
                            input={'bad_value': 'invalid'},
                        ),
                    ],
                    tool_name='final_result',
                    tool_call_id=IsStr(),
                    timestamp=IsNow(tz=timezone.utc),
                )
            ),
        ]
    )

# tests/test_agent.py:5953-5962
def test_deprecated_kwargs_validation_agent_init():
    """Test that invalid kwargs raise UserError in Agent constructor."""
    with pytest.raises(UserError, match='Unknown keyword arguments: `usage_limits`'):
        Agent('test', usage_limits='invalid')  # type: ignore[call-arg]

    with pytest.raises(UserError, match='Unknown keyword arguments: `invalid_kwarg`'):
        Agent('test', invalid_kwarg='value')  # type: ignore[call-arg]

    with pytest.raises(UserError, match='Unknown keyword arguments: `foo`, `bar`'):
        Agent('test', foo='value1', bar='value2')  # type: ignore[call-arg]

# tests/test_streaming.py:3230-3248
def test_structured_response_sync_validation():
    async def text_stream(_messages: list[ModelMessage], agent_info: AgentInfo) -> AsyncIterator[DeltaToolCalls]:
        assert agent_info.output_tools is not None
        assert len(agent_info.output_tools) == 1
        name = agent_info.output_tools[0].name
        json_data = json.dumps({'response': [1, 2, 3, 4]})
        yield {0: DeltaToolCall(name=name)}
        yield {0: DeltaToolCall(json_args=json_data[:15])}
        yield {0: DeltaToolCall(json_args=json_data[15:])}

    agent = Agent(FunctionModel(stream_function=text_stream), output_type=list[int])

    chunks: list[list[int]] = []
    result = agent.run_stream_sync('')
    for structured_response, last in result.stream_responses(debounce_by=None):
        response_data = result.validate_response_output(structured_response, allow_partial=not last)
        chunks.append(response_data)

    assert chunks == snapshot([[1], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])

# tests/test_ssrf.py:626-632
    async def test_protocol_validation(self) -> None:
        """Test that non-http(s) protocols are rejected."""
        with pytest.raises(ValueError, match='URL protocol "file" is not allowed'):
            await safe_download('file:///etc/passwd')

        with pytest.raises(ValueError, match='URL protocol "ftp" is not allowed'):
            await safe_download('ftp://ftp.example.com/file.txt')

# tests/graph/beta/test_graph_builder.py:347-359
async def test_validation_no_edges_to_end():
    """Test that validation catches graphs with no edges to end node."""
    g = GraphBuilder(output_type=int)

    @g.step
    async def dead_end_step(ctx: StepContext[None, None, None]) -> int:
        return 42  # pragma: no cover

    # Connect start to step but don't connect step to end
    g.add(g.edge_from(g.start_node).to(dead_end_step))

    with pytest.raises(GraphValidationError, match='The graph has no edges to the end node'):
        g.build()

# tests/test_exceptions.py:70-74
def test_tool_retry_error_str_with_string_content():
    """Test that ToolRetryError uses string content as message automatically."""
    part = RetryPromptPart(content='error from tool', tool_name='my_tool')
    error = ToolRetryError(part)
    assert str(error) == 'error from tool'

# tests/graph/beta/test_graph_builder.py:408-428
async def test_validation_unreachable_nodes():
    """Test that validation catches nodes that are not reachable from start."""
    g = GraphBuilder(output_type=int)

    @g.step
    async def reachable_step(ctx: StepContext[None, None, None]) -> int:
        return 10  # pragma: no cover

    @g.step
    async def unreachable_step(ctx: StepContext[None, None, int]) -> int:
        return ctx.inputs * 2  # pragma: no cover

    # unreachable_step is in the graph but not connected from start
    g.add(
        g.edge_from(g.start_node).to(reachable_step),
        g.edge_from(reachable_step).to(g.end_node),
        g.edge_from(unreachable_step).to(g.end_node),
    )

    with pytest.raises(GraphValidationError, match='The following nodes are not reachable from the start node'):
        g.build()

# pydantic_ai_slim/pydantic_ai/exceptions.py:211-227
    def _format_error_details(errors: list[pydantic_core.ErrorDetails], tool_name: str | None) -> str:
        """Format ErrorDetails as a human-readable message.

        We format manually rather than using ValidationError.from_exception_data because
        some error types (value_error, assertion_error, etc.) require an 'error' key in ctx,
        but when ErrorDetails are serialized, exception objects are stripped from ctx.
        The 'msg' field already contains the human-readable message, so we use that directly.
        """
        error_count = len(errors)
        lines = [
            f'{error_count} validation error{"s" if error_count == 1 else ""}{f" for {tool_name!r}" if tool_name else ""}'
        ]
        for e in errors:
            loc = '.'.join(str(x) for x in e['loc']) if e['loc'] else '__root__'
            lines.append(loc)
            lines.append(f'  {e["msg"]} [type={e["type"]}, input_value={e["input"]!r}]')
        return '\n'.join(lines)

# tests/models/test_openrouter.py:599-625
async def test_openrouter_file_annotation_validation(openrouter_api_key: str) -> None:
    """Test that file annotations from OpenRouter are correctly validated.

    This unit test verifies that responses containing type="file" annotations
    are parsed without validation errors, which was failing before the fix.
    """
    from openai.types.chat.chat_completion_message import ChatCompletionMessage

    provider = OpenRouterProvider(api_key=openrouter_api_key)
    model = OpenRouterModel('openai/gpt-4.1-mini', provider=provider)

    message = ChatCompletionMessage.model_construct(
        role='assistant',
        content='Here is the summary of your file.',
        annotations=[
            {'type': 'file', 'file': {'filename': 'test.pdf', 'file_id': 'file-123'}},
        ],
    )
    choice = Choice.model_construct(index=0, message=message, finish_reason='stop', native_finish_reason='stop')
    response = ChatCompletion.model_construct(
        id='test', choices=[choice], created=0, object='chat.completion', model='test', provider='test'
    )

    # This should not raise a validation error
    result = model._process_response(response)  # type: ignore[reportPrivateUsage]
    text_part = cast(TextPart, result.parts[0])
    assert text_part.content == 'Here is the summary of your file.'

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:167-167
    _validation_context: Any | Callable[[RunContext[AgentDepsT]], Any] = dataclasses.field(repr=False)

# tests/test_validation_context.py:50-68
def test_agent_output_with_validation_context(output_type: OutputSpec[Value]):
    """Test that the output is validated using the validation context"""

    def mock_llm(_: list[ModelMessage], _info: AgentInfo) -> ModelResponse:
        if isinstance(output_type, ToolOutput):
            return ModelResponse(parts=[ToolCallPart(tool_name='final_result', args={'x': 0})])
        else:
            text = Value(x=0).model_dump_json()
            return ModelResponse(parts=[TextPart(content=text)])

    agent = Agent(
        FunctionModel(mock_llm),
        output_type=output_type,
        deps_type=Deps,
        validation_context=lambda ctx: ctx.deps.increment,
    )

    result = agent.run_sync('', deps=Deps(increment=10))
    assert result.output.x == snapshot(10)

# tests/graph/beta/test_graph_builder.py:332-344
async def test_validation_no_edges_from_start():
    """Test that validation catches graphs with no edges from start node."""
    g = GraphBuilder(output_type=int)

    @g.step
    async def orphan_step(ctx: StepContext[None, None, None]) -> int:
        return 42  # pragma: no cover

    # Add the step to the graph but don't connect it to start
    g.add(g.edge_from(orphan_step).to(g.end_node))

    with pytest.raises(GraphValidationError, match='The graph has no edges from the start node'):
        g.build()

# pydantic_ai_slim/pydantic_ai/_run_context.py:43-43
    validation_context: Any = None

# tests/graph/beta/test_graph_builder.py:385-405
async def test_validation_end_node_unreachable():
    """Test that validation catches when end node is unreachable from start."""
    g = GraphBuilder(input_type=int, output_type=int)

    @g.step
    async def first_step(ctx: StepContext[None, None, int]) -> int:
        return 42  # pragma: no cover

    @g.step
    async def second_step(ctx: StepContext[None, None, int]) -> int:
        return ctx.inputs  # pragma: no cover

    # Create a cycle that doesn't reach the end node
    g.add(
        g.edge_from(g.start_node).to(first_step),
        g.edge_from(first_step).to(second_step),
        g.edge_from(second_step).to(first_step),
    )

    with pytest.raises(GraphValidationError, match='The graph has no edges to the end node'):
        g.build()

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:145-145
    validation_context: Any | Callable[[RunContext[DepsT]], Any]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:136-136
    type: Literal['tool-input-error'] = 'tool-input-error'

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:138-138
    tool_name: str

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:155-155
    input: Any | None = None

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:152-152
    type: Annotated[str, Field(pattern=r'^tool-')]

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:209-209
    input: Any

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:30-30
from .exceptions import ToolRetryError

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:30-30
from .exceptions import ToolRetryError

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:30-30
from .exceptions import ToolRetryError

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:30-30
from .exceptions import ToolRetryError