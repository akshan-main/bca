## pydantic_ai_slim/pydantic_ai/agent/__init__.py

    def name(self, value: str | None) -> None:
        """Set the name of the agent, used for logging."""
        self._name = value

## pydantic_ai_slim/pydantic_ai/agent/abstract.py

    def name(self, value: str | None) -> None:
        """Set the name of the agent, used for logging."""
        raise NotImplementedError

## pydantic_ai_slim/pydantic_ai/agent/wrapper.py

    def name(self, value: str | None) -> None:
        self.wrapped.name = value

## pydantic_ai_slim/pydantic_ai/concurrency.py

    def name(self) -> str | None:
        """Name of the limiter for observability."""
        return self._name

## pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py

    def name(self, value: str | None) -> None:  # pragma: no cover
        raise UserError(
            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'
        )

## pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py

    def name(self, value: str | None) -> None:  # pragma: no cover
        raise UserError(
            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'
        )

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py

    def name(self, value: str | None) -> None:  # pragma: no cover
        raise UserError(
            'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'
        )

## pydantic_ai_slim/pydantic_ai/exceptions.py

class ToolRetryError(Exception):
    """Exception used to signal a `ToolRetry` message should be returned to the LLM."""

    def __init__(self, tool_retry: RetryPromptPart):
        self.tool_retry = tool_retry
        message = (
            tool_retry.content
            if isinstance(tool_retry.content, str)
            else self._format_error_details(tool_retry.content, tool_retry.tool_name)
        )
        super().__init__(message)

    @staticmethod
    def _format_error_details(errors: list[pydantic_core.ErrorDetails], tool_name: str | None) -> str:
        """Format ErrorDetails as a human-readable message.

        We format manually rather than using ValidationError.from_exception_data because
        some error types (value_error, assertion_error, etc.) require an 'error' key in ctx,
        but when ErrorDetails are serialized, exception objects are stripped from ctx.
        The 'msg' field already contains the human-readable message, so we use that directly.
        """
        error_count = len(errors)
        lines = [
            f'{error_count} validation error{"s" if error_count == 1 else ""}{f" for {tool_name!r}" if tool_name else ""}'
        ]
        for e in errors:
            loc = '.'.join(str(x) for x in e['loc']) if e['loc'] else '__root__'
            lines.append(loc)
            lines.append(f'  {e["msg"]} [type={e["type"]}, input_value={e["input"]!r}]')
        return '\n'.join(lines)

## pydantic_ai_slim/pydantic_ai/ext/langchain.py

    def name(self) -> str: ...

## pydantic_ai_slim/pydantic_ai/messages.py

class RetryPromptPart:
    """A message back to a model asking it to try again.

    This can be sent for a number of reasons:

    * Pydantic validation of tool arguments failed, here content is derived from a Pydantic
      [`ValidationError`][pydantic_core.ValidationError]
    * a tool raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception
    * no tool was found for the tool name
    * the model returned plain text when a structured response was expected
    * Pydantic validation of a structured response failed, here content is derived from a Pydantic
      [`ValidationError`][pydantic_core.ValidationError]
    * an output validator raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception
    """

    content: list[pydantic_core.ErrorDetails] | str
    """Details of why and how the model should retry.

    If the retry was triggered by a [`ValidationError`][pydantic_core.ValidationError], this will be a list of
    error details.
    """

    _: KW_ONLY

    tool_name: str | None = None
    """The name of the tool that was called, if any."""

    tool_call_id: str = field(default_factory=_generate_tool_call_id)
    """The tool call identifier, this is used by some models including OpenAI.

    In case the tool call id is not provided by the model, Pydantic AI will generate a random one.
    """

    timestamp: datetime = field(default_factory=_now_utc)
    """The timestamp, when the retry was triggered."""

    part_kind: Literal['retry-prompt'] = 'retry-prompt'
    """Part type identifier, this is available on all parts as a discriminator."""

    def model_response(self) -> str:
        """Return a string message describing why the retry is requested."""
        if isinstance(self.content, str):
            if self.tool_name is None:
                description = f'Validation feedback:\n{self.content}'
            else:
                description = self.content
        else:
            json_errors = error_details_ta.dump_json(self.content, exclude={'__all__': {'ctx'}}, indent=2)
            plural = isinstance(self.content, list) and len(self.content) != 1
            description = (
                f'{len(self.content)} validation error{"s" if plural else ""}:\n```json\n{json_errors.decode()}\n```'
            )
        return f'{description}\n\nFix the errors and try again.'

    def otel_event(self, settings: InstrumentationSettings) -> LogRecord:
        if self.tool_name is None:
            return LogRecord(
                attributes={'event.name': 'gen_ai.user.message'},
                body={'content': self.model_response(), 'role': 'user'},
            )
        else:
            return LogRecord(
                attributes={'event.name': 'gen_ai.tool.message'},
                body={
                    **({'content': self.model_response()} if settings.include_content else {}),
                    'role': 'tool',
                    'id': self.tool_call_id,
                    'name': self.tool_name,
                },
            )

    def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:
        if self.tool_name is None:
            return [_otel_messages.TextPart(type='text', content=self.model_response())]
        else:
            part = _otel_messages.ToolCallResponsePart(
                type='tool_call_response',
                id=self.tool_call_id,
                name=self.tool_name,
            )

            if settings.include_content:
                part['result'] = self.model_response()

            return [part]

    __repr__ = _utils.dataclasses_no_defaults_repr

## pydantic_ai_slim/pydantic_ai/models/test.py

class TestStreamedResponse(StreamedResponse):
    """A structured response that streams test data."""

    _model_name: str
    _structured_response: ModelResponse
    _messages: InitVar[Iterable[ModelMessage]]
    _provider_name: str
    _provider_url: str | None = None
    _timestamp: datetime = field(default_factory=_utils.now_utc, init=False)

    def __post_init__(self, _messages: Iterable[ModelMessage]):
        self._usage = _estimate_usage(_messages)

    async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:
        for i, part in enumerate(self._structured_response.parts):
            if isinstance(part, TextPart):
                text = part.content
                *words, last_word = text.split(' ')
                words = [f'{word} ' for word in words]
                words.append(last_word)
                if len(words) == 1 and len(text) > 2:
                    mid = len(text) // 2
                    words = [text[:mid], text[mid:]]
                self._usage += _get_string_usage('')
                for event in self._parts_manager.handle_text_delta(vendor_part_id=i, content=''):
                    yield event
                for word in words:
                    self._usage += _get_string_usage(word)
                    for event in self._parts_manager.handle_text_delta(vendor_part_id=i, content=word):
                        yield event
            elif isinstance(part, ToolCallPart):
                yield self._parts_manager.handle_tool_call_part(
                    vendor_part_id=i, tool_name=part.tool_name, args=part.args, tool_call_id=part.tool_call_id
                )
            elif isinstance(part, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover
                # NOTE: These parts are not generated by TestModel, but we need to handle them for type checking
                assert False, f'Unexpected part type in TestModel: {type(part).__name__}'
            elif isinstance(part, ThinkingPart):  # pragma: no cover
                # NOTE: There's no way to reach this part of the code, since we don't generate ThinkingPart on TestModel.
                assert False, "This should be unreachable — we don't generate ThinkingPart on TestModel."
            elif isinstance(part, FilePart):  # pragma: no cover
                # NOTE: There's no way to reach this part of the code, since we don't generate FilePart on TestModel.
                assert False, "This should be unreachable — we don't generate FilePart on TestModel."
            else:
                assert_never(part)

    @property
    def model_name(self) -> str:
        """Get the model name of the response."""
        return self._model_name

    @property
    def provider_name(self) -> str:
        """Get the provider name."""
        return self._provider_name

    @property
    def provider_url(self) -> str | None:
        """Get the provider base URL."""
        return self._provider_url

    @property
    def timestamp(self) -> datetime:
        """Get the timestamp of the response."""
        return self._timestamp

## pydantic_ai_slim/pydantic_ai/providers/__init__.py

    def name(self) -> str:
        """The provider name."""
        raise NotImplementedError()

## pydantic_ai_slim/pydantic_ai/providers/alibaba.py

    def name(self) -> str:
        return 'alibaba'

## pydantic_ai_slim/pydantic_ai/providers/anthropic.py

    def name(self) -> str:
        return 'anthropic'

## pydantic_ai_slim/pydantic_ai/providers/azure.py

    def name(self) -> str:
        return 'azure'

## pydantic_ai_slim/pydantic_ai/providers/bedrock.py

    def name(self) -> str:
        return 'bedrock'

## pydantic_ai_slim/pydantic_ai/providers/cerebras.py

    def name(self) -> str:
        return 'cerebras'

## pydantic_ai_slim/pydantic_ai/providers/cohere.py

    def name(self) -> str:
        return 'cohere'

## pydantic_ai_slim/pydantic_ai/providers/deepseek.py

    def name(self) -> str:
        return 'deepseek'

## pydantic_ai_slim/pydantic_ai/providers/fireworks.py

    def name(self) -> str:
        return 'fireworks'

## pydantic_ai_slim/pydantic_ai/providers/github.py

    def name(self) -> str:
        return 'github'

## pydantic_ai_slim/pydantic_ai/providers/google.py

    def name(self) -> str:
        return 'google-vertex' if self._client._api_client.vertexai else 'google-gla'  # type: ignore[reportPrivateUsage]

## pydantic_ai_slim/pydantic_ai/providers/google_gla.py

    def name(self):
        return 'google-gla'

## pydantic_ai_slim/pydantic_ai/providers/google_vertex.py

    def name(self) -> str:
        return 'google-vertex'

## pydantic_ai_slim/pydantic_ai/providers/grok.py

    def name(self) -> str:
        return 'grok'

## pydantic_ai_slim/pydantic_ai/providers/groq.py

    def name(self) -> str:
        return 'groq'

## pydantic_ai_slim/pydantic_ai/providers/heroku.py

    def name(self) -> str:
        return 'heroku'

## pydantic_ai_slim/pydantic_ai/providers/huggingface.py

    def name(self) -> str:
        return 'huggingface'

## pydantic_ai_slim/pydantic_ai/providers/litellm.py

    def name(self) -> str:
        return 'litellm'

## pydantic_ai_slim/pydantic_ai/providers/mistral.py

    def name(self) -> str:
        return 'mistral'

## pydantic_ai_slim/pydantic_ai/providers/moonshotai.py

    def name(self) -> str:
        return 'moonshotai'

## pydantic_ai_slim/pydantic_ai/providers/nebius.py

    def name(self) -> str:
        return 'nebius'

## pydantic_ai_slim/pydantic_ai/providers/ollama.py

    def name(self) -> str:
        return 'ollama'

## pydantic_ai_slim/pydantic_ai/providers/openai.py

    def name(self) -> str:
        return 'openai'

## pydantic_ai_slim/pydantic_ai/providers/openrouter.py

    def name(self) -> str:
        return 'openrouter'

## pydantic_ai_slim/pydantic_ai/providers/outlines.py

    def name(self) -> str:
        """The provider name."""
        return 'outlines'

## pydantic_ai_slim/pydantic_ai/providers/ovhcloud.py

    def name(self) -> str:
        return 'ovhcloud'

## pydantic_ai_slim/pydantic_ai/providers/sambanova.py

    def name(self) -> str:
        """Return the provider name."""
        return 'sambanova'

## pydantic_ai_slim/pydantic_ai/providers/sentence_transformers.py

    def name(self) -> str:
        """The provider name."""
        return 'sentence-transformers'  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/providers/together.py

    def name(self) -> str:
        return 'together'

## pydantic_ai_slim/pydantic_ai/providers/vercel.py

    def name(self) -> str:
        return 'vercel'

## pydantic_ai_slim/pydantic_ai/providers/voyageai.py

    def name(self) -> str:
        return 'voyageai'

## pydantic_ai_slim/pydantic_ai/providers/xai.py

    def name(self) -> str:
        return 'xai'

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py

    type: Annotated[str, Field(pattern=r'^data-')]

## pydantic_evals/pydantic_evals/evaluators/evaluator.py

    def name(cls) -> str:
        """`name` has been renamed, use `get_serialization_name` instead."""
        return cls.get_serialization_name()

## tests/evals/test_report_evaluators.py

def test_report_evaluator_build_serialization_arguments_excludes_defaults():
    """ConfusionMatrixEvaluator with all defaults returns empty dict."""
    evaluator = ConfusionMatrixEvaluator()
    args = evaluator.build_serialization_arguments()
    assert args == {}

## tests/graph/beta/test_edge_cases.py

async def test_step_that_modifies_deps():
    """Test that deps modifications don't persist (deps should be immutable)."""

    @dataclass
    class MutableDeps:
        value: int

    g = GraphBuilder(state_type=EdgeCaseState, deps_type=MutableDeps, output_type=int)

    @g.step
    async def try_modify_deps(ctx: StepContext[EdgeCaseState, MutableDeps, None]) -> int:
        original = ctx.deps.value
        # Attempt to modify (this DOES mutate the object, but that's user error)
        ctx.deps.value = 999
        return original

    @g.step
    async def check_deps(ctx: StepContext[EdgeCaseState, MutableDeps, int]) -> int:
        # Deps will show the mutation since it's the same object
        return ctx.deps.value

    g.add(
        g.edge_from(g.start_node).to(try_modify_deps),
        g.edge_from(try_modify_deps).to(check_deps),
        g.edge_from(check_deps).to(g.end_node),
    )

    graph = g.build()
    deps = MutableDeps(value=42)
    result = await graph.run(state=EdgeCaseState(), deps=deps)
    # The deps object was mutated (user responsibility to avoid this)
    assert result == 999
    assert deps.value == 999

## tests/models/test_bedrock.py

    def name(self) -> str:
        return 'bedrock-stub'

async def test_bedrock_anthropic_tool_with_thinking(allow_model_requests: None, bedrock_provider: BedrockProvider):
    """When using thinking with tool calls in Anthropic, we need to send the thinking part back to the provider.

    This tests the issue raised in https://github.com/pydantic/pydantic-ai/issues/2453.
    """
    m = BedrockConverseModel('us.anthropic.claude-3-7-sonnet-20250219-v1:0', provider=bedrock_provider)
    settings = BedrockModelSettings(
        bedrock_additional_model_requests_fields={'thinking': {'type': 'enabled', 'budget_tokens': 1024}},
    )
    agent = Agent(m, model_settings=settings)

    @agent.tool_plain
    async def get_user_country() -> str:
        return 'Mexico'

    result = await agent.run('What is the largest city in the user country?')
    assert result.output == snapshot(
        """\
Based on your location in Mexico, the largest city is Mexico City (Ciudad de México). It's not only the capital but also the most populous city in Mexico with a metropolitan area population of over 21 million people, making it one of the largest urban agglomerations in the world.

Mexico City is an important cultural, financial, and political center for the country and has a rich history dating back to the Aztec empire when it was known as Tenochtitlán.\
"""
    )

async def test_bedrock_cache_point_multiple_markers_with_documents_no_back_to_back(
    allow_model_requests: None, bedrock_provider: BedrockProvider
):
    """Multiple CachePoints with trailing documents should not create back-to-back cache points.

    When processing ['text', doc1, CachePoint(), doc2, CachePoint()], both documents form
    a single trailing group. The first CachePoint is placed before the group, and the second
    CachePoint is skipped to avoid back-to-back cachePoints.
    """
    model = BedrockConverseModel('us.anthropic.claude-haiku-4-5-20251001-v1:0', provider=bedrock_provider)
    messages: list[ModelMessage] = [
        ModelRequest(
            parts=[
                UserPromptPart(
                    content=[
                        'Analyze these:',
                        BinaryContent(data=b'Doc 1', media_type='text/plain'),
                        CachePoint(),
                        BinaryContent(data=b'Doc 2', media_type='text/plain'),
                        CachePoint(),
                    ]
                )
            ]
        )
    ]
    _, bedrock_messages = await model._map_messages(messages, ModelRequestParameters(), BedrockModelSettings())  # pyright: ignore[reportPrivateUsage]
    # Both docs are trailing, so first CachePoint goes before both.
    # Second CachePoint is skipped to avoid back-to-back cachePoints.
    assert bedrock_messages[0]['content'] == snapshot(
        [
            {'text': 'Analyze these:'},
            {'cachePoint': {'type': 'default'}},
            {'document': {'name': 'Document 1', 'format': 'txt', 'source': {'bytes': b'Doc 1'}}},
            {'document': {'name': 'Document 2', 'format': 'txt', 'source': {'bytes': b'Doc 2'}}},
        ]
    )

## tests/models/test_fallback.py

def test_first_successful() -> None:
    fallback_model = FallbackModel(success_model, failure_model)
    agent = Agent(model=fallback_model)
    result = agent.run_sync('hello')
    assert result.output == snapshot('success')
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(content='hello', timestamp=IsNow(tz=timezone.utc)),
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='success')],
                usage=RequestUsage(input_tokens=51, output_tokens=1),
                model_name='function:success_response:',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

## tests/models/test_google.py

def test_google_provider_respects_custom_http_client_timeout(gemini_api_key: str):
    """Test that GoogleProvider respects a custom timeout from a user-provided http_client.

    See https://github.com/pydantic/pydantic-ai/pull/4032#discussion_r2709797127
    """
    custom_timeout = 120
    custom_http_client = HttpxAsyncClient(timeout=Timeout(custom_timeout))
    provider = GoogleProvider(api_key=gemini_api_key, http_client=custom_http_client)

    http_options = provider._client._api_client._http_options  # pyright: ignore[reportPrivateUsage]
    assert http_options.timeout == custom_timeout * 1000

## tests/models/test_huggingface.py

async def test_stream_text(allow_model_requests: None):
    stream = [text_chunk('hello '), text_chunk('world'), chunk([])]
    mock_client = MockHuggingFace.create_stream_mock(stream)
    m = HuggingFaceModel('hf-model', provider=HuggingFaceProvider(hf_client=mock_client, api_key='x'))
    agent = Agent(m)

    async with agent.run_stream('') as result:
        assert not result.is_complete
        assert [c async for c in result.stream_text(debounce_by=None)] == snapshot(['hello ', 'hello world'])
        assert result.is_complete
        assert result.usage() == snapshot(RunUsage(requests=1, input_tokens=6, output_tokens=3))

## tests/models/test_model_function.py

def test_call_all():
    result = agent_all.run_sync('Hello', model=TestModel())
    assert result.output == snapshot('{"foo":"1","bar":"2","baz":"3","qux":"4","quz":"a"}')
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    SystemPromptPart(content='foobar', timestamp=IsNow(tz=timezone.utc)),
                    UserPromptPart(content='Hello', timestamp=IsNow(tz=timezone.utc)),
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[
                    ToolCallPart(tool_name='foo', args={'x': 0}, tool_call_id=IsStr()),
                    ToolCallPart(tool_name='bar', args={'x': 0}, tool_call_id=IsStr()),
                    ToolCallPart(tool_name='baz', args={'x': 0}, tool_call_id=IsStr()),
                    ToolCallPart(tool_name='qux', args={'x': 0}, tool_call_id=IsStr()),
                    ToolCallPart(tool_name='quz', args={'x': 'a'}, tool_call_id=IsStr()),
                ],
                usage=RequestUsage(input_tokens=52, output_tokens=21),
                model_name='test',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='foo', content='1', timestamp=IsNow(tz=timezone.utc), tool_call_id=IsStr()
                    ),
                    ToolReturnPart(
                        tool_name='bar', content='2', timestamp=IsNow(tz=timezone.utc), tool_call_id=IsStr()
                    ),
                    ToolReturnPart(
                        tool_name='baz', content='3', timestamp=IsNow(tz=timezone.utc), tool_call_id=IsStr()
                    ),
                    ToolReturnPart(
                        tool_name='qux', content='4', timestamp=IsNow(tz=timezone.utc), tool_call_id=IsStr()
                    ),
                    ToolReturnPart(
                        tool_name='quz', content='a', timestamp=IsNow(tz=timezone.utc), tool_call_id=IsStr()
                    ),
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='{"foo":"1","bar":"2","baz":"3","qux":"4","quz":"a"}')],
                usage=RequestUsage(input_tokens=57, output_tokens=33),
                model_name='test',
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
        ]
    )

## tests/models/test_model_settings.py

def test_settings_merge_hierarchy():
    """Test the complete settings merge hierarchy: model -> agent -> run."""
    # Create a function that captures the merged settings
    captured_settings = None

    def capture_settings(messages: list[ModelMessage], agent_info: AgentInfo) -> ModelResponse:
        nonlocal captured_settings
        captured_settings = agent_info.model_settings
        return ModelResponse(parts=[TextPart('captured')])

    # Model settings (lowest priority)
    model_settings = ModelSettings(max_tokens=100, temperature=0.5, top_p=0.8, seed=123)
    model = FunctionModel(capture_settings, settings=model_settings)

    # Agent settings (medium priority)
    agent_settings = ModelSettings(
        max_tokens=200,  # overrides model
        temperature=0.6,  # overrides model
        frequency_penalty=0.1,  # new setting
    )
    agent = Agent(model=model, model_settings=agent_settings)

    # Run settings (highest priority)
    run_settings = ModelSettings(
        temperature=0.7,  # overrides agent and model
        presence_penalty=0.2,  # new setting
        seed=456,  # overrides model
    )

    # Run the agent
    result = agent.run_sync('test', model_settings=run_settings)
    assert result.output == 'captured'

    # Verify the merged settings follow the correct precedence
    assert captured_settings is not None
    assert captured_settings['temperature'] == 0.7  # from run_settings
    assert captured_settings['max_tokens'] == 200  # from agent_settings
    assert captured_settings['top_p'] == 0.8  # from model_settings
    assert captured_settings['seed'] == 456  # from run_settings
    assert captured_settings['frequency_penalty'] == 0.1  # from agent_settings
    assert captured_settings['presence_penalty'] == 0.2  # from run_settings

## tests/models/test_openrouter.py

async def test_openrouter_url_citation_annotation_validation(openrouter_api_key: str) -> None:
    """Test that url_citation annotations from OpenRouter are correctly validated."""
    from openai.types.chat.chat_completion_message import ChatCompletionMessage

    provider = OpenRouterProvider(api_key=openrouter_api_key)
    model = OpenRouterModel('openai/gpt-4.1-mini', provider=provider)

    message = ChatCompletionMessage.model_construct(
        role='assistant',
        content='According to the source, this is the answer.',
        annotations=[
            {
                'type': 'url_citation',
                'url_citation': {'url': 'https://example.com', 'title': 'Example', 'start_index': 0, 'end_index': 10},
            },
        ],
    )
    choice = Choice.model_construct(index=0, message=message, finish_reason='stop', native_finish_reason='stop')
    response = ChatCompletion.model_construct(
        id='test', choices=[choice], created=0, object='chat.completion', model='test', provider='test'
    )

    # This should not raise a validation error
    result = model._process_response(response)  # type: ignore[reportPrivateUsage]
    text_part = cast(TextPart, result.parts[0])
    assert text_part.content == 'According to the source, this is the answer.'

## tests/providers/test_fireworks.py

def test_fireworks_provider():
    provider = FireworksProvider(api_key='api-key')
    assert provider.name == 'fireworks'
    assert provider.base_url == 'https://api.fireworks.ai/inference/v1'
    assert isinstance(provider.client, openai.AsyncOpenAI)
    assert provider.client.api_key == 'api-key'

## tests/test_dbos.py

async def test_multiple_agents(allow_model_requests: None, dbos: DBOS):
    """Test that multiple agents can run in a DBOS workflow."""
    # This is just a smoke test to ensure that multiple agents can run in a DBOS workflow.
    # We don't need to check the output as it's already tested in the individual agent tests.
    result = await simple_dbos_agent.run('What is the capital of Mexico?')
    assert result.output == snapshot('The capital of Mexico is Mexico City.')

    result = await complex_dbos_agent.run(
        'Tell me: the capital of the country; the weather there; the product name', deps=Deps(country='Mexico')
    )
    assert result.output == snapshot(
        Response(
            answers=[
                Answer(label='Capital of the Country', answer='Mexico City'),
                Answer(label='Weather in Mexico City', answer='Sunny'),
                Answer(label='Product Name', answer='Pydantic AI'),
            ]
        )
    )

## tests/test_exceptions.py

def test_tool_retry_error_str_with_string_content():
    """Test that ToolRetryError uses string content as message automatically."""
    part = RetryPromptPart(content='error from tool', tool_name='my_tool')
    error = ToolRetryError(part)
    assert str(error) == 'error from tool'

def test_tool_retry_error_str_with_error_details():
    """Test that ToolRetryError formats ErrorDetails automatically."""
    validation_error = ValidationError.from_exception_data(
        'Test', [{'type': 'string_type', 'loc': ('name',), 'input': 123}]
    )
    part = RetryPromptPart(content=validation_error.errors(include_url=False), tool_name='my_tool')
    error = ToolRetryError(part)

    assert str(error) == (
        "1 validation error for 'my_tool'\nname\n  Input should be a valid string [type=string_type, input_value=123]"
    )

def test_tool_retry_error_str_with_value_error_type():
    """Test that ToolRetryError handles value_error type without ctx.error.

    When ErrorDetails are serialized, the exception object in ctx is stripped.
    This test ensures we handle error types that normally require ctx.error.
    """
    # Simulate serialized ErrorDetails where ctx.error has been stripped
    error_details: list[ErrorDetails] = [
        {
            'type': 'value_error',
            'loc': ('field',),
            'msg': 'Value error, must not be foo',
            'input': 'foo',
        }
    ]
    part = RetryPromptPart(content=error_details, tool_name='my_tool')
    error = ToolRetryError(part)

    assert str(error) == (
        "1 validation error for 'my_tool'\nfield\n  Value error, must not be foo [type=value_error, input_value='foo']"
    )

## tests/test_function_schema.py

def test_function_wrong_annotation_type():
    """Test function with wrong annotation type for first parameter."""

    def func_wrong_annotation(ctx: str, x: int) -> str: ...  # pragma: no cover

    assert _takes_ctx(func_wrong_annotation) is False

## tests/test_messages.py

def test_binary_content_base64():
    bc = BinaryContent(data=b'Hello, world!', media_type='image/png')
    assert bc.base64 == 'SGVsbG8sIHdvcmxkIQ=='
    assert not bc.base64.startswith('data:')
    assert bc.data_uri == 'data:image/png;base64,SGVsbG8sIHdvcmxkIQ=='

## tests/test_prefect.py

async def test_toolset_without_id():
    """Test that agents can be created with toolsets without IDs."""
    # This is allowed in Prefect
    PrefectAgent(Agent(model=model, name='test_agent', toolsets=[FunctionToolset()]))

## tests/test_streaming.py

async def test_iter_stream_responses():
    m = TestModel(custom_output_text='The cat sat on the mat.')

    agent = Agent(m)

    @agent.output_validator
    def output_validator_simple(data: str) -> str:
        # Make a substitution in the validated results
        return re.sub('cat sat', 'bat sat', data)

    run: AgentRun
    stream: AgentStream
    messages: list[ModelResponse] = []
    async with agent.iter('Hello') as run:
        assert isinstance(run.run_id, str)
        async for node in run:
            if agent.is_model_request_node(node):
                async with node.stream(run.ctx) as stream:
                    async for chunk in stream.stream_responses(debounce_by=None):
                        messages.append(chunk)

    assert messages == [
        ModelResponse(
            parts=[TextPart(content=text)],
            usage=RequestUsage(input_tokens=IsInt(), output_tokens=IsInt()),
            model_name='test',
            timestamp=IsNow(tz=timezone.utc),
            provider_name='test',
        )
        for text in [
            '',
            '',
            'The ',
            'The cat ',
            'The cat sat ',
            'The cat sat on ',
            'The cat sat on the ',
            'The cat sat on the mat.',
            'The cat sat on the mat.',
        ]
    ]

    # Note: as you can see above, the output validator is not applied to the streamed responses, just the final result:
    assert run.result is not None
    assert run.result.output == 'The bat sat on the mat.'

async def test_tool_raises_call_deferred():
    agent = Agent(TestModel(), output_type=[str, DeferredToolRequests])

    @agent.tool_plain()
    def my_tool(x: int) -> int:
        raise CallDeferred

    async with agent.run_stream('Hello') as result:
        assert not result.is_complete
        assert isinstance(result.run_id, str)
        assert [c async for c in result.stream_output(debounce_by=None)] == snapshot(
            [DeferredToolRequests(calls=[ToolCallPart(tool_name='my_tool', args={'x': 0}, tool_call_id=IsStr())])]
        )
        assert await result.get_output() == snapshot(
            DeferredToolRequests(calls=[ToolCallPart(tool_name='my_tool', args={'x': 0}, tool_call_id=IsStr())])
        )
        responses = [c async for c, _is_last in result.stream_responses(debounce_by=None)]
        assert responses == snapshot(
            [
                ModelResponse(
                    parts=[ToolCallPart(tool_name='my_tool', args={'x': 0}, tool_call_id=IsStr())],
                    usage=RequestUsage(input_tokens=51),
                    model_name='test',
                    timestamp=IsDatetime(),
                    provider_name='test',
                    run_id=IsStr(),
                )
            ]
        )
        assert await result.validate_response_output(responses[0]) == snapshot(
            DeferredToolRequests(calls=[ToolCallPart(tool_name='my_tool', args={'x': 0}, tool_call_id=IsStr())])
        )
        assert result.usage() == snapshot(RunUsage(requests=1, input_tokens=51, output_tokens=0))
        assert result.timestamp() == IsNow(tz=timezone.utc)
        assert result.is_complete

## tests/test_temporal.py

def test_pydantic_ai_plugin_with_default_payload_converter_replaced() -> None:
    """When converter uses DefaultPayloadConverter, replace payload_converter_class without warning."""
    plugin = PydanticAIPlugin()
    converter = DataConverter(payload_converter_class=DefaultPayloadConverter)
    config: dict[str, Any] = {'data_converter': converter}
    result = plugin.configure_client(config)  # type: ignore[arg-type]
    assert result['data_converter'] is not converter
    assert result['data_converter'].payload_converter_class is PydanticPayloadConverter

## tests/test_validation_context.py

def test_agent_output_validator_with_intermediary_deps_change_and_validation_context():
    """Test that the validation context is updated as run dependencies are mutated."""

    agent = Agent(
        'test',
        output_type=Value,
        deps_type=Deps,
        validation_context=lambda ctx: ctx.deps.increment,
    )

    @agent.tool
    def bump_increment(ctx: RunContext[Deps]):
        assert ctx.validation_context == snapshot(10)  # validation ctx was first computed using the original deps
        ctx.deps.increment += 5  # update the deps

    @agent.output_validator
    def identity(ctx: RunContext[Deps], v: Value) -> Value:
        assert ctx.validation_context == snapshot(15)  # validation ctx was re-computed after deps update from tool call

        return v

    result = agent.run_sync('', deps=Deps(increment=10))
    assert result.output.x == snapshot(15)
