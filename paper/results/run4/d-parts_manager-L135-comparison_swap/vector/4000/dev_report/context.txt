# pydantic_ai_slim/pydantic_ai/profiles/__init__.py:59-59
    thinking_tags: tuple[str, str] = ('<think>', '</think>')

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:16-16
    content: NotRequired[str]

# pydantic_ai_slim/pydantic_ai/messages.py:1064-1064
    content: str

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:48-48
    content: NotRequired[str]

# pydantic_ai_slim/pydantic_ai/messages.py:1102-1102
    content: str

# pydantic_ai_slim/pydantic_ai/messages.py:751-751
    content: str | Sequence[UserContent]

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:87-173
    def handle_text_delta(
        self,
        *,
        vendor_part_id: VendorId | None,
        content: str,
        id: str | None = None,
        provider_name: str | None = None,
        provider_details: dict[str, Any] | None = None,
        thinking_tags: tuple[str, str] | None = None,
        ignore_leading_whitespace: bool = False,
    ) -> Iterator[ModelResponseStreamEvent]:
        """Handle incoming text content, creating or updating a TextPart in the manager as appropriate.

        When `vendor_part_id` is None, the latest part is updated if it exists and is a TextPart;
        otherwise, a new TextPart is created. When a non-None ID is specified, the TextPart corresponding
        to that vendor ID is either created or updated.

        Args:
            vendor_part_id: The ID the vendor uses to identify this piece
                of text. If None, a new part will be created unless the latest part is already
                a TextPart.
            content: The text content to append to the appropriate TextPart.
            id: An optional id for the text part.
            provider_name: An optional provider name for the text part.
            provider_details: An optional dictionary of provider-specific details for the text part.
            thinking_tags: If provided, will handle content between the thinking tags as thinking parts.
            ignore_leading_whitespace: If True, will ignore leading whitespace in the content.

        Yields:
            A `PartStartEvent` if a new part was created, or a `PartDeltaEvent` if an existing part was updated.
            Yields nothing if no event should be emitted (e.g., the first text part was all whitespace).

        Raises:
            UnexpectedModelBehavior: If attempting to apply text content to a part that is not a TextPart.
        """
        existing_text_part_and_index: tuple[TextPart, int] | None = None

        if vendor_part_id is None:
            # If the vendor_part_id is None, check if the latest part is a TextPart to update
            existing_text_part_and_index = self._latest_part_if_of_type(TextPart)
        else:
            # Otherwise, attempt to look up an existing TextPart by vendor_part_id
            part_index = self._vendor_id_to_part_index.get(vendor_part_id)
            if part_index is not None:
                existing_part = self._parts[part_index]

                if thinking_tags and isinstance(existing_part, ThinkingPart):
                    # We may be building a thinking part instead of a text part if we had previously seen a thinking tag
                    if content != thinking_tags[1]:
                        # When we see the thinking end tag, we're done with the thinking part and the next text delta will need a new part
                        self._handle_embedded_thinking_end(vendor_part_id)
                        return
                    yield from self._handle_embedded_thinking_content(
                        existing_part, part_index, content, provider_name, provider_details
                    )
                    return
                elif isinstance(existing_part, TextPart):
                    existing_text_part_and_index = existing_part, part_index
                else:
                    raise UnexpectedModelBehavior(f'Cannot apply a text delta to {existing_part=}')

        if thinking_tags and content == thinking_tags[0]:
            # When we see a thinking start tag (which is a single token), we'll build a new thinking part instead
            yield from self._handle_embedded_thinking_start(vendor_part_id, provider_name, provider_details)
            return

        if existing_text_part_and_index is None:
            # This is a workaround for models that emit `<think>\n</think>\n\n` or an empty text part ahead of tool calls (e.g. Ollama + Qwen3),
            # which we don't want to end up treating as a final result when using `run_stream` with `str` a valid `output_type`.
            if ignore_leading_whitespace and (len(content) == 0 or content.isspace()):
                return

            # There is no existing text part that should be updated, so create a new one
            part = TextPart(content=content, id=id, provider_name=provider_name, provider_details=provider_details)
            new_part_index = self._append_part(part, vendor_part_id)
            yield PartStartEvent(index=new_part_index, part=part)
        else:
            # Update the existing TextPart with the new content delta
            existing_text_part, part_index = existing_text_part_and_index

            part_delta = TextPartDelta(
                content_delta=content,
                provider_name=self._resolve_provider_name(existing_text_part, provider_name),
                provider_details=provider_details,
            )
            self._parts[part_index] = part_delta.apply(existing_text_part)
            yield PartDeltaEvent(index=part_index, delta=part_delta)

# pydantic_ai_slim/pydantic_ai/messages.py:946-946
    content: list[pydantic_core.ErrorDetails] | str

# pydantic_ai_slim/pydantic_ai/messages.py:134-134
    content: str

# pydantic_ai_slim/pydantic_ai/messages.py:1154-1154
    content: Annotated[BinaryContent, pydantic.AfterValidator(BinaryImage.narrow_type)]

# pydantic_ai_slim/pydantic_ai/messages.py:2010-2010
    content: str | Sequence[UserContent] | None = None

# pydantic_ai_slim/pydantic_ai/_thinking_part.py:6-31
def split_content_into_text_and_thinking(content: str, thinking_tags: tuple[str, str]) -> list[ThinkingPart | TextPart]:
    """Split a string into text and thinking parts.

    Some models don't return the thinking part as a separate part, but rather as a tag in the content.
    This function splits the content into text and thinking parts.
    """
    start_tag, end_tag = thinking_tags
    parts: list[ThinkingPart | TextPart] = []

    start_index = content.find(start_tag)
    while start_index >= 0:
        before_think, content = content[:start_index], content[start_index + len(start_tag) :]
        if before_think:
            parts.append(TextPart(content=before_think))
        end_index = content.find(end_tag)
        if end_index >= 0:
            think_content, content = content[:end_index], content[end_index + len(end_tag) :]
            parts.append(ThinkingPart(content=think_content))
        else:
            # We lose the `<think>` tag, but it shouldn't matter.
            parts.append(TextPart(content=content))
            content = ''
        start_index = content.find(start_tag)
    if content:
        parts.append(TextPart(content=content))
    return parts

# pydantic_ai_slim/pydantic_ai/messages.py:695-695
    content: str | Sequence[UserContent] | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:827-827
    content: ToolReturnContent

# tests/test_dbos.py:199-199
    content: str

# tests/test_prefect.py:186-186
    content: str

# tests/test_temporal.py:324-324
    content: str

# examples/pydantic_ai_examples/rag.py:174-174
    content: str

# examples/pydantic_ai_examples/chat_app.py:86-86
    content: str

# docs/.hooks/algolia.py:19-19
    content: str

# pydantic_ai_slim/pydantic_ai/models/gemini.py:867-867
    content: NotRequired[_GeminiContent]

# docs/.hooks/snippets.py:56-56
    content: str

# pydantic_ai_slim/pydantic_ai/common_tools/tavily.py:31-31
    content: str

# pydantic_ai_slim/pydantic_ai/models/function.py:263-263
    content: str | None = None

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:43-43
    content: NotRequired[str]

# tests/models/test_model_test.py:13-13
from anyio import Event

# pydantic_ai_slim/pydantic_ai/messages.py:1088-1088
    part_kind: Literal['text'] = 'text'

# docs/.hooks/test_snippets.py:9-9
from inline_snapshot import snapshot

# pydantic_graph/pydantic_graph/__init__.py:4-4
from .persistence import EndSnapshot, NodeSnapshot, Snapshot

# pydantic_graph/pydantic_graph/persistence/__init__.py:98-98
Snapshot = NodeSnapshot[StateT, RunEndT] | EndSnapshot[StateT, RunEndT]

# pydantic_graph/pydantic_graph/persistence/file.py:16-26
from . import (
    BaseStatePersistence,
    EndSnapshot,
    NodeSnapshot,
    RunEndT,
    Snapshot,
    SnapshotStatus,
    StateT,
    _utils,
    build_snapshot_list_type_adapter,
)

# pydantic_graph/pydantic_graph/persistence/in_mem.py:19-28
from . import (
    BaseStatePersistence,
    EndSnapshot,
    NodeSnapshot,
    RunEndT,
    Snapshot,
    StateT,
    _utils,
    build_snapshot_list_type_adapter,
)

# tests/evals/test_dataset.py:12-12
from inline_snapshot import snapshot

# tests/evals/test_evaluator_base.py:9-9
from inline_snapshot import snapshot

# tests/evals/test_evaluator_common.py:7-7
from inline_snapshot import snapshot

# tests/evals/test_evaluators.py:7-7
from inline_snapshot import snapshot

# tests/evals/test_llm_as_a_judge.py:4-4
from inline_snapshot import snapshot

# tests/evals/test_multi_run.py:8-8
from inline_snapshot import snapshot

# tests/evals/test_otel.py:6-6
from inline_snapshot import snapshot

# tests/evals/test_render_numbers.py:4-4
from inline_snapshot import snapshot

# tests/evals/test_reporting.py:6-6
from inline_snapshot import snapshot

# tests/evals/test_reports.py:6-6
from inline_snapshot import snapshot

# tests/ext/test_langchain.py:5-5
from inline_snapshot import snapshot

# tests/graph/beta/test_graph_edge_cases.py:10-10
from inline_snapshot import snapshot

# tests/graph/beta/test_parent_forks.py:4-4
from inline_snapshot import snapshot

# tests/graph/beta/test_v1_v2_integration.py:9-9
from inline_snapshot import snapshot

# tests/graph/test_file_persistence.py:9-9
from inline_snapshot import snapshot

# tests/graph/test_graph.py:11-11
from inline_snapshot import snapshot

# tests/graph/test_mermaid.py:12-12
from inline_snapshot import snapshot

# tests/graph/test_persistence.py:11-11
from inline_snapshot import snapshot

# tests/graph/test_state.py:7-7
from inline_snapshot import snapshot

# tests/models/anthropic/test_output.py:19-19
from inline_snapshot import snapshot

# tests/models/test_anthropic.py:15-15
from inline_snapshot import snapshot

# tests/models/test_bedrock.py:8-8
from inline_snapshot import snapshot

# tests/models/test_cohere.py:10-10
from inline_snapshot import snapshot

# tests/models/test_deepseek.py:6-6
from inline_snapshot import snapshot

# tests/models/test_fallback.py:11-11
from inline_snapshot import snapshot

# tests/models/test_gemini.py:16-16
from inline_snapshot import snapshot

# tests/models/test_gemini_vertex.py:6-6
from inline_snapshot import Is, snapshot

# tests/models/test_google.py:16-16
from inline_snapshot import Is, snapshot

# tests/models/test_groq.py:14-14
from inline_snapshot import snapshot

# tests/models/test_huggingface.py:12-12
from inline_snapshot import snapshot

# tests/models/test_instrumented.py:9-9
from inline_snapshot import snapshot

# tests/models/test_mcp_sampling.py:7-7
from inline_snapshot import snapshot

# tests/models/test_mistral.py:12-12
from inline_snapshot import snapshot

# tests/models/test_model_function.py:9-9
from inline_snapshot import snapshot

# tests/models/test_model_request_parameters.py:1-1
from inline_snapshot import snapshot

# tests/models/test_model_test.py:14-14
from inline_snapshot import snapshot

# tests/models/test_openai.py:15-15
from inline_snapshot import snapshot

# tests/models/test_openrouter.py:7-7
from inline_snapshot import snapshot

# tests/models/test_outlines.py:15-15
from inline_snapshot import snapshot

# tests/models/test_xai.py:25-25
from inline_snapshot import snapshot

# tests/profiles/test_anthropic.py:22-22
from inline_snapshot import snapshot

# tests/profiles/test_google.py:13-13
from inline_snapshot import snapshot

# tests/providers/test_azure.py:4-4
from inline_snapshot import snapshot

# tests/providers/test_gateway.py:9-9
from inline_snapshot import snapshot

# tests/providers/test_google_vertex.py:12-12
from inline_snapshot import snapshot

# tests/providers/test_heroku.py:5-5
from inline_snapshot import snapshot

# tests/providers/test_openrouter.py:5-5
from inline_snapshot import snapshot

# tests/test_a2a.py:9-9
from inline_snapshot import snapshot

# tests/test_ag_ui.py:16-16
from inline_snapshot import snapshot

# tests/test_agent.py:13-13
from inline_snapshot import snapshot

# tests/test_agent_output_schemas.py:2-2
from inline_snapshot import snapshot

# tests/test_cli.py:9-9
from inline_snapshot import snapshot

# tests/test_dbos.py:72-72
from inline_snapshot import snapshot

# tests/test_direct.py:8-8
from inline_snapshot import snapshot

# tests/test_embeddings.py:11-11
from inline_snapshot import snapshot

# tests/test_fastmcp.py:11-11
from inline_snapshot import snapshot

# tests/test_format_as_xml.py:11-11
from inline_snapshot import snapshot

# tests/test_history_processor.py:5-5
from inline_snapshot import snapshot

# tests/test_logfire.py:9-9
from inline_snapshot import snapshot

# tests/test_mcp.py:13-13
from inline_snapshot import snapshot

# tests/test_messages.py:6-6
from inline_snapshot import snapshot

# tests/test_parts_manager.py:7-7
from inline_snapshot import snapshot

# tests/test_prefect.py:69-69
from inline_snapshot import snapshot

# tests/test_streaming.py:14-14
from inline_snapshot import snapshot

# tests/test_temporal.py:13-13
from inline_snapshot import snapshot

# tests/test_thinking_part.py:4-4
from inline_snapshot import snapshot

# tests/test_tools.py:9-9
from inline_snapshot import snapshot

# tests/test_toolsets.py:10-10
from inline_snapshot import snapshot

# tests/test_ui.py:9-9
from inline_snapshot import snapshot

# tests/test_ui_web.py:12-12
from inline_snapshot import snapshot

# tests/test_usage_limits.py:10-10
from inline_snapshot import snapshot

# tests/test_utils.py:11-11
from inline_snapshot import snapshot

# tests/test_validation_context.py:4-4
from inline_snapshot import snapshot

# tests/test_vercel_ai.py:8-8
from inline_snapshot import snapshot

# pydantic_ai_slim/pydantic_ai/models/xai.py:1085-1099
def _get_tool_result_content(content: str) -> dict[str, Any] | str | None:
    """Extract tool result content from a content string.

    Args:
        content: The content string (may be JSON or plain text)

    Returns:
        Tool result content as dict (if JSON), string, or None if no content
    """
    if content:
        try:
            return json.loads(content)
        except (json.JSONDecodeError, TypeError):
            return content
    return None

# pydantic_ai_slim/pydantic_ai/messages.py:1140-1140
    part_kind: Literal['thinking'] = 'thinking'

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:175-256
    def handle_thinking_delta(
        self,
        *,
        vendor_part_id: Hashable | None,
        content: str | None = None,
        id: str | None = None,
        signature: str | None = None,
        provider_name: str | None = None,
        provider_details: ProviderDetailsDelta = None,
    ) -> Iterator[ModelResponseStreamEvent]:
        """Handle incoming thinking content, creating or updating a ThinkingPart in the manager as appropriate.

        When `vendor_part_id` is None, the latest part is updated if it exists and is a ThinkingPart;
        otherwise, a new ThinkingPart is created. When a non-None ID is specified, the ThinkingPart corresponding
        to that vendor ID is either created or updated.

        Args:
            vendor_part_id: The ID the vendor uses to identify this piece
                of thinking. If None, a new part will be created unless the latest part is already
                a ThinkingPart.
            content: The thinking content to append to the appropriate ThinkingPart.
            id: An optional id for the thinking part.
            signature: An optional signature for the thinking content.
            provider_name: An optional provider name for the thinking part.
            provider_details: Either a dict of provider-specific details, or a callable that takes
                the existing part's `provider_details` and returns the updated details. Callables
                allow provider-specific update logic without the parts manager knowing the details.

        Yields:
            A `PartStartEvent` if a new part was created, or a `PartDeltaEvent` if an existing part was updated.

        Raises:
            UnexpectedModelBehavior: If attempting to apply a thinking delta to a part that is not a ThinkingPart.
        """
        existing_thinking_part_and_index: tuple[ThinkingPart, int] | None = None

        if vendor_part_id is None:
            # If the vendor_part_id is None, check if the latest part is a ThinkingPart to update
            existing_thinking_part_and_index = self._latest_part_if_of_type(ThinkingPart)
        else:
            # Otherwise, attempt to look up an existing ThinkingPart by vendor_part_id
            part_index = self._vendor_id_to_part_index.get(vendor_part_id)
            if part_index is not None:
                existing_part = self._parts[part_index]
                if not isinstance(existing_part, ThinkingPart):
                    raise UnexpectedModelBehavior(f'Cannot apply a thinking delta to {existing_part=}')
                existing_thinking_part_and_index = existing_part, part_index

        if existing_thinking_part_and_index is None:
            if content is not None or signature is not None or provider_details is not None:
                # There is no existing thinking part that should be updated, so create a new one
                # Resolve provider_details if it's a callback (with None since there's no existing part)
                resolved_details: dict[str, Any] | None
                resolved_details = provider_details(None) if callable(provider_details) else provider_details
                part = ThinkingPart(
                    content=content or '',
                    id=id,
                    signature=signature,
                    provider_name=provider_name,
                    provider_details=resolved_details,
                )
                new_part_index = self._append_part(part, vendor_part_id)
                yield PartStartEvent(index=new_part_index, part=part)
            else:
                raise UnexpectedModelBehavior(
                    'Cannot create a ThinkingPart with no content, signature, or provider_details'
                )
        else:
            existing_thinking_part, part_index = existing_thinking_part_and_index

            # Skip if nothing to update
            if content is None and signature is None and provider_name is None and provider_details is None:
                return

            part_delta = ThinkingPartDelta(
                content_delta=content,
                signature_delta=signature,
                provider_name=self._resolve_provider_name(existing_thinking_part, provider_name),
                provider_details=provider_details,
            )
            self._parts[part_index] = part_delta.apply(existing_thinking_part)
            yield PartDeltaEvent(index=part_index, delta=part_delta)

# pydantic_ai_slim/pydantic_ai/messages.py:1272-1272
    part_kind: Literal['tool-call'] = 'tool-call'

# pydantic_ai_slim/pydantic_ai/messages.py:759-759
    part_kind: Literal['user-prompt'] = 'user-prompt'

# pydantic_ai_slim/pydantic_ai/messages.py:1282-1282
    part_kind: Literal['builtin-tool-call'] = 'builtin-tool-call'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py:51-51
    event: _messages.AgentStreamEvent

# pydantic_ai_slim/pydantic_ai/messages.py:923-923
    part_kind: Literal['builtin-tool-return'] = 'builtin-tool-return'

# pydantic_ai_slim/pydantic_ai/messages.py:967-967
    part_kind: Literal['retry-prompt'] = 'retry-prompt'

# pydantic_ai_slim/pydantic_ai/messages.py:901-901
    part_kind: Literal['tool-return'] = 'tool-return'

# pydantic_ai_slim/pydantic_ai/messages.py:148-148
    part_kind: Literal['system-prompt'] = 'system-prompt'

# pydantic_ai_slim/pydantic_ai/messages.py:1178-1178
    part_kind: Literal['file'] = 'file'

# pydantic_ai_slim/pydantic_ai/messages.py:1909-1909
    event_kind: Literal['part_start'] = 'part_start'

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:444-451
    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[EventT]:
        """Handle a `TextPartDelta`.

        Args:
            delta: The text part delta.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:463-471
    async def handle_thinking_start(self, part: ThinkingPart, follows_thinking: bool = False) -> AsyncIterator[EventT]:
        """Handle the start of a `ThinkingPart`.

        Args:
            part: The thinking part.
            follows_thinking: Whether the part is directly preceded by another thinking part. In this case, you may want to yield a "thinking-delta" event instead of a "thinking-start" event.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

# pydantic_ai_slim/pydantic_ai/messages.py:1370-1375
    def thinking(self) -> str | None:
        """Get the thinking in the response."""
        thinking_parts = [part.content for part in self.parts if isinstance(part, ThinkingPart)]
        if not thinking_parts:
            return None
        return '\n\n'.join(thinking_parts)

# tests/test_ui.py:145-146
    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[str]:
        yield delta.content_delta

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:465-480
    def _handle_embedded_thinking_content(
        self,
        existing_part: ThinkingPart,
        part_index: int,
        content: str,
        provider_name: str | None,
        provider_details: dict[str, Any] | None,
    ) -> Iterator[ModelResponseStreamEvent]:
        """Handle content inside <think>...</think>."""
        part_delta = ThinkingPartDelta(
            content_delta=content,
            provider_name=self._resolve_provider_name(existing_part, provider_name),
            provider_details=provider_details,
        )
        self._parts[part_index] = part_delta.apply(existing_part)
        yield PartDeltaEvent(index=part_index, delta=part_delta)

# pydantic_ai_slim/pydantic_ai/messages.py:1925-1925
    event_kind: Literal['part_delta'] = 'part_delta'

# pydantic_ai_slim/pydantic_ai/messages.py:1895-1895
    index: int

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:133-138
    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[BaseChunk]:
        if delta.content_delta:  # pragma: no branch
            provider_metadata = dump_provider_metadata(
                provider_name=delta.provider_name, provider_details=delta.provider_details
            )
            yield TextDeltaChunk(id=self.message_id, delta=delta.content_delta, provider_metadata=provider_metadata)

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/_event_stream.py:137-139
    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[BaseEvent]:
        if delta.content_delta:  # pragma: no branch
            yield TextMessageContentEvent(message_id=self.message_id, delta=delta.content_delta)

# tests/test_thinking_part.py:75-76
def test_split_content(thinking_tags: tuple[str, str], content: str, parts: list[ModelResponsePart]):
    assert split_content_into_text_and_thinking(content, thinking_tags) == parts

# examples/pydantic_ai_examples/stream_markdown.py:16-16
from rich.text import Text

# pydantic_evals/pydantic_evals/reporting/__init__.py:13-13
from rich.text import Text