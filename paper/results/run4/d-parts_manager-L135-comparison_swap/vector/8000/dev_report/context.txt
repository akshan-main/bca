# pydantic_ai_slim/pydantic_ai/profiles/__init__.py:59-59
    thinking_tags: tuple[str, str] = ('<think>', '</think>')

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:16-16
    content: NotRequired[str]

# pydantic_ai_slim/pydantic_ai/messages.py:1064-1064
    content: str

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:48-48
    content: NotRequired[str]

# pydantic_ai_slim/pydantic_ai/messages.py:1102-1102
    content: str

# pydantic_ai_slim/pydantic_ai/messages.py:751-751
    content: str | Sequence[UserContent]

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:87-173
    def handle_text_delta(
        self,
        *,
        vendor_part_id: VendorId | None,
        content: str,
        id: str | None = None,
        provider_name: str | None = None,
        provider_details: dict[str, Any] | None = None,
        thinking_tags: tuple[str, str] | None = None,
        ignore_leading_whitespace: bool = False,
    ) -> Iterator[ModelResponseStreamEvent]:
        """Handle incoming text content, creating or updating a TextPart in the manager as appropriate.

        When `vendor_part_id` is None, the latest part is updated if it exists and is a TextPart;
        otherwise, a new TextPart is created. When a non-None ID is specified, the TextPart corresponding
        to that vendor ID is either created or updated.

        Args:
            vendor_part_id: The ID the vendor uses to identify this piece
                of text. If None, a new part will be created unless the latest part is already
                a TextPart.
            content: The text content to append to the appropriate TextPart.
            id: An optional id for the text part.
            provider_name: An optional provider name for the text part.
            provider_details: An optional dictionary of provider-specific details for the text part.
            thinking_tags: If provided, will handle content between the thinking tags as thinking parts.
            ignore_leading_whitespace: If True, will ignore leading whitespace in the content.

        Yields:
            A `PartStartEvent` if a new part was created, or a `PartDeltaEvent` if an existing part was updated.
            Yields nothing if no event should be emitted (e.g., the first text part was all whitespace).

        Raises:
            UnexpectedModelBehavior: If attempting to apply text content to a part that is not a TextPart.
        """
        existing_text_part_and_index: tuple[TextPart, int] | None = None

        if vendor_part_id is None:
            # If the vendor_part_id is None, check if the latest part is a TextPart to update
            existing_text_part_and_index = self._latest_part_if_of_type(TextPart)
        else:
            # Otherwise, attempt to look up an existing TextPart by vendor_part_id
            part_index = self._vendor_id_to_part_index.get(vendor_part_id)
            if part_index is not None:
                existing_part = self._parts[part_index]

                if thinking_tags and isinstance(existing_part, ThinkingPart):
                    # We may be building a thinking part instead of a text part if we had previously seen a thinking tag
                    if content != thinking_tags[1]:
                        # When we see the thinking end tag, we're done with the thinking part and the next text delta will need a new part
                        self._handle_embedded_thinking_end(vendor_part_id)
                        return
                    yield from self._handle_embedded_thinking_content(
                        existing_part, part_index, content, provider_name, provider_details
                    )
                    return
                elif isinstance(existing_part, TextPart):
                    existing_text_part_and_index = existing_part, part_index
                else:
                    raise UnexpectedModelBehavior(f'Cannot apply a text delta to {existing_part=}')

        if thinking_tags and content == thinking_tags[0]:
            # When we see a thinking start tag (which is a single token), we'll build a new thinking part instead
            yield from self._handle_embedded_thinking_start(vendor_part_id, provider_name, provider_details)
            return

        if existing_text_part_and_index is None:
            # This is a workaround for models that emit `<think>\n</think>\n\n` or an empty text part ahead of tool calls (e.g. Ollama + Qwen3),
            # which we don't want to end up treating as a final result when using `run_stream` with `str` a valid `output_type`.
            if ignore_leading_whitespace and (len(content) == 0 or content.isspace()):
                return

            # There is no existing text part that should be updated, so create a new one
            part = TextPart(content=content, id=id, provider_name=provider_name, provider_details=provider_details)
            new_part_index = self._append_part(part, vendor_part_id)
            yield PartStartEvent(index=new_part_index, part=part)
        else:
            # Update the existing TextPart with the new content delta
            existing_text_part, part_index = existing_text_part_and_index

            part_delta = TextPartDelta(
                content_delta=content,
                provider_name=self._resolve_provider_name(existing_text_part, provider_name),
                provider_details=provider_details,
            )
            self._parts[part_index] = part_delta.apply(existing_text_part)
            yield PartDeltaEvent(index=part_index, delta=part_delta)

# pydantic_ai_slim/pydantic_ai/messages.py:946-946
    content: list[pydantic_core.ErrorDetails] | str

# pydantic_ai_slim/pydantic_ai/messages.py:134-134
    content: str

# pydantic_ai_slim/pydantic_ai/messages.py:1154-1154
    content: Annotated[BinaryContent, pydantic.AfterValidator(BinaryImage.narrow_type)]

# pydantic_ai_slim/pydantic_ai/messages.py:2010-2010
    content: str | Sequence[UserContent] | None = None

# pydantic_ai_slim/pydantic_ai/_thinking_part.py:6-31
def split_content_into_text_and_thinking(content: str, thinking_tags: tuple[str, str]) -> list[ThinkingPart | TextPart]:
    """Split a string into text and thinking parts.

    Some models don't return the thinking part as a separate part, but rather as a tag in the content.
    This function splits the content into text and thinking parts.
    """
    start_tag, end_tag = thinking_tags
    parts: list[ThinkingPart | TextPart] = []

    start_index = content.find(start_tag)
    while start_index >= 0:
        before_think, content = content[:start_index], content[start_index + len(start_tag) :]
        if before_think:
            parts.append(TextPart(content=before_think))
        end_index = content.find(end_tag)
        if end_index >= 0:
            think_content, content = content[:end_index], content[end_index + len(end_tag) :]
            parts.append(ThinkingPart(content=think_content))
        else:
            # We lose the `<think>` tag, but it shouldn't matter.
            parts.append(TextPart(content=content))
            content = ''
        start_index = content.find(start_tag)
    if content:
        parts.append(TextPart(content=content))
    return parts

# pydantic_ai_slim/pydantic_ai/messages.py:695-695
    content: str | Sequence[UserContent] | None = None

# pydantic_ai_slim/pydantic_ai/messages.py:827-827
    content: ToolReturnContent

# tests/test_dbos.py:199-199
    content: str

# tests/test_prefect.py:186-186
    content: str

# tests/test_temporal.py:324-324
    content: str

# examples/pydantic_ai_examples/rag.py:174-174
    content: str

# examples/pydantic_ai_examples/chat_app.py:86-86
    content: str

# docs/.hooks/algolia.py:19-19
    content: str

# pydantic_ai_slim/pydantic_ai/models/gemini.py:867-867
    content: NotRequired[_GeminiContent]

# docs/.hooks/snippets.py:56-56
    content: str

# pydantic_ai_slim/pydantic_ai/common_tools/tavily.py:31-31
    content: str

# pydantic_ai_slim/pydantic_ai/models/function.py:263-263
    content: str | None = None

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:43-43
    content: NotRequired[str]

# tests/models/test_model_test.py:13-13
from anyio import Event

# pydantic_ai_slim/pydantic_ai/messages.py:1088-1088
    part_kind: Literal['text'] = 'text'

# docs/.hooks/test_snippets.py:9-9
from inline_snapshot import snapshot

# pydantic_graph/pydantic_graph/__init__.py:4-4
from .persistence import EndSnapshot, NodeSnapshot, Snapshot

# pydantic_graph/pydantic_graph/persistence/__init__.py:98-98
Snapshot = NodeSnapshot[StateT, RunEndT] | EndSnapshot[StateT, RunEndT]

# pydantic_graph/pydantic_graph/persistence/file.py:16-26
from . import (
    BaseStatePersistence,
    EndSnapshot,
    NodeSnapshot,
    RunEndT,
    Snapshot,
    SnapshotStatus,
    StateT,
    _utils,
    build_snapshot_list_type_adapter,
)

# pydantic_graph/pydantic_graph/persistence/in_mem.py:19-28
from . import (
    BaseStatePersistence,
    EndSnapshot,
    NodeSnapshot,
    RunEndT,
    Snapshot,
    StateT,
    _utils,
    build_snapshot_list_type_adapter,
)

# tests/evals/test_dataset.py:12-12
from inline_snapshot import snapshot

# tests/evals/test_evaluator_base.py:9-9
from inline_snapshot import snapshot

# tests/evals/test_evaluator_common.py:7-7
from inline_snapshot import snapshot

# tests/evals/test_evaluators.py:7-7
from inline_snapshot import snapshot

# tests/evals/test_llm_as_a_judge.py:4-4
from inline_snapshot import snapshot

# tests/evals/test_multi_run.py:8-8
from inline_snapshot import snapshot

# tests/evals/test_otel.py:6-6
from inline_snapshot import snapshot

# tests/evals/test_render_numbers.py:4-4
from inline_snapshot import snapshot

# tests/evals/test_reporting.py:6-6
from inline_snapshot import snapshot

# tests/evals/test_reports.py:6-6
from inline_snapshot import snapshot

# tests/ext/test_langchain.py:5-5
from inline_snapshot import snapshot

# tests/graph/beta/test_graph_edge_cases.py:10-10
from inline_snapshot import snapshot

# tests/graph/beta/test_parent_forks.py:4-4
from inline_snapshot import snapshot

# tests/graph/beta/test_v1_v2_integration.py:9-9
from inline_snapshot import snapshot

# tests/graph/test_file_persistence.py:9-9
from inline_snapshot import snapshot

# tests/graph/test_graph.py:11-11
from inline_snapshot import snapshot

# tests/graph/test_mermaid.py:12-12
from inline_snapshot import snapshot

# tests/graph/test_persistence.py:11-11
from inline_snapshot import snapshot

# tests/graph/test_state.py:7-7
from inline_snapshot import snapshot

# tests/models/anthropic/test_output.py:19-19
from inline_snapshot import snapshot

# tests/models/test_anthropic.py:15-15
from inline_snapshot import snapshot

# tests/models/test_bedrock.py:8-8
from inline_snapshot import snapshot

# tests/models/test_cohere.py:10-10
from inline_snapshot import snapshot

# tests/models/test_deepseek.py:6-6
from inline_snapshot import snapshot

# tests/models/test_fallback.py:11-11
from inline_snapshot import snapshot

# tests/models/test_gemini.py:16-16
from inline_snapshot import snapshot

# tests/models/test_gemini_vertex.py:6-6
from inline_snapshot import Is, snapshot

# tests/models/test_google.py:16-16
from inline_snapshot import Is, snapshot

# tests/models/test_groq.py:14-14
from inline_snapshot import snapshot

# tests/models/test_huggingface.py:12-12
from inline_snapshot import snapshot

# tests/models/test_instrumented.py:9-9
from inline_snapshot import snapshot

# tests/models/test_mcp_sampling.py:7-7
from inline_snapshot import snapshot

# tests/models/test_mistral.py:12-12
from inline_snapshot import snapshot

# tests/models/test_model_function.py:9-9
from inline_snapshot import snapshot

# tests/models/test_model_request_parameters.py:1-1
from inline_snapshot import snapshot

# tests/models/test_model_test.py:14-14
from inline_snapshot import snapshot

# tests/models/test_openai.py:15-15
from inline_snapshot import snapshot

# tests/models/test_openrouter.py:7-7
from inline_snapshot import snapshot

# tests/models/test_outlines.py:15-15
from inline_snapshot import snapshot

# tests/models/test_xai.py:25-25
from inline_snapshot import snapshot

# tests/profiles/test_anthropic.py:22-22
from inline_snapshot import snapshot

# tests/profiles/test_google.py:13-13
from inline_snapshot import snapshot

# tests/providers/test_azure.py:4-4
from inline_snapshot import snapshot

# tests/providers/test_gateway.py:9-9
from inline_snapshot import snapshot

# tests/providers/test_google_vertex.py:12-12
from inline_snapshot import snapshot

# tests/providers/test_heroku.py:5-5
from inline_snapshot import snapshot

# tests/providers/test_openrouter.py:5-5
from inline_snapshot import snapshot

# tests/test_a2a.py:9-9
from inline_snapshot import snapshot

# tests/test_ag_ui.py:16-16
from inline_snapshot import snapshot

# tests/test_agent.py:13-13
from inline_snapshot import snapshot

# tests/test_agent_output_schemas.py:2-2
from inline_snapshot import snapshot

# tests/test_cli.py:9-9
from inline_snapshot import snapshot

# tests/test_dbos.py:72-72
from inline_snapshot import snapshot

# tests/test_direct.py:8-8
from inline_snapshot import snapshot

# tests/test_embeddings.py:11-11
from inline_snapshot import snapshot

# tests/test_fastmcp.py:11-11
from inline_snapshot import snapshot

# tests/test_format_as_xml.py:11-11
from inline_snapshot import snapshot

# tests/test_history_processor.py:5-5
from inline_snapshot import snapshot

# tests/test_logfire.py:9-9
from inline_snapshot import snapshot

# tests/test_mcp.py:13-13
from inline_snapshot import snapshot

# tests/test_messages.py:6-6
from inline_snapshot import snapshot

# tests/test_parts_manager.py:7-7
from inline_snapshot import snapshot

# tests/test_prefect.py:69-69
from inline_snapshot import snapshot

# tests/test_streaming.py:14-14
from inline_snapshot import snapshot

# tests/test_temporal.py:13-13
from inline_snapshot import snapshot

# tests/test_thinking_part.py:4-4
from inline_snapshot import snapshot

# tests/test_tools.py:9-9
from inline_snapshot import snapshot

# tests/test_toolsets.py:10-10
from inline_snapshot import snapshot

# tests/test_ui.py:9-9
from inline_snapshot import snapshot

# tests/test_ui_web.py:12-12
from inline_snapshot import snapshot

# tests/test_usage_limits.py:10-10
from inline_snapshot import snapshot

# tests/test_utils.py:11-11
from inline_snapshot import snapshot

# tests/test_validation_context.py:4-4
from inline_snapshot import snapshot

# tests/test_vercel_ai.py:8-8
from inline_snapshot import snapshot

# pydantic_ai_slim/pydantic_ai/models/xai.py:1085-1099
def _get_tool_result_content(content: str) -> dict[str, Any] | str | None:
    """Extract tool result content from a content string.

    Args:
        content: The content string (may be JSON or plain text)

    Returns:
        Tool result content as dict (if JSON), string, or None if no content
    """
    if content:
        try:
            return json.loads(content)
        except (json.JSONDecodeError, TypeError):
            return content
    return None

# pydantic_ai_slim/pydantic_ai/messages.py:1140-1140
    part_kind: Literal['thinking'] = 'thinking'

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:175-256
    def handle_thinking_delta(
        self,
        *,
        vendor_part_id: Hashable | None,
        content: str | None = None,
        id: str | None = None,
        signature: str | None = None,
        provider_name: str | None = None,
        provider_details: ProviderDetailsDelta = None,
    ) -> Iterator[ModelResponseStreamEvent]:
        """Handle incoming thinking content, creating or updating a ThinkingPart in the manager as appropriate.

        When `vendor_part_id` is None, the latest part is updated if it exists and is a ThinkingPart;
        otherwise, a new ThinkingPart is created. When a non-None ID is specified, the ThinkingPart corresponding
        to that vendor ID is either created or updated.

        Args:
            vendor_part_id: The ID the vendor uses to identify this piece
                of thinking. If None, a new part will be created unless the latest part is already
                a ThinkingPart.
            content: The thinking content to append to the appropriate ThinkingPart.
            id: An optional id for the thinking part.
            signature: An optional signature for the thinking content.
            provider_name: An optional provider name for the thinking part.
            provider_details: Either a dict of provider-specific details, or a callable that takes
                the existing part's `provider_details` and returns the updated details. Callables
                allow provider-specific update logic without the parts manager knowing the details.

        Yields:
            A `PartStartEvent` if a new part was created, or a `PartDeltaEvent` if an existing part was updated.

        Raises:
            UnexpectedModelBehavior: If attempting to apply a thinking delta to a part that is not a ThinkingPart.
        """
        existing_thinking_part_and_index: tuple[ThinkingPart, int] | None = None

        if vendor_part_id is None:
            # If the vendor_part_id is None, check if the latest part is a ThinkingPart to update
            existing_thinking_part_and_index = self._latest_part_if_of_type(ThinkingPart)
        else:
            # Otherwise, attempt to look up an existing ThinkingPart by vendor_part_id
            part_index = self._vendor_id_to_part_index.get(vendor_part_id)
            if part_index is not None:
                existing_part = self._parts[part_index]
                if not isinstance(existing_part, ThinkingPart):
                    raise UnexpectedModelBehavior(f'Cannot apply a thinking delta to {existing_part=}')
                existing_thinking_part_and_index = existing_part, part_index

        if existing_thinking_part_and_index is None:
            if content is not None or signature is not None or provider_details is not None:
                # There is no existing thinking part that should be updated, so create a new one
                # Resolve provider_details if it's a callback (with None since there's no existing part)
                resolved_details: dict[str, Any] | None
                resolved_details = provider_details(None) if callable(provider_details) else provider_details
                part = ThinkingPart(
                    content=content or '',
                    id=id,
                    signature=signature,
                    provider_name=provider_name,
                    provider_details=resolved_details,
                )
                new_part_index = self._append_part(part, vendor_part_id)
                yield PartStartEvent(index=new_part_index, part=part)
            else:
                raise UnexpectedModelBehavior(
                    'Cannot create a ThinkingPart with no content, signature, or provider_details'
                )
        else:
            existing_thinking_part, part_index = existing_thinking_part_and_index

            # Skip if nothing to update
            if content is None and signature is None and provider_name is None and provider_details is None:
                return

            part_delta = ThinkingPartDelta(
                content_delta=content,
                signature_delta=signature,
                provider_name=self._resolve_provider_name(existing_thinking_part, provider_name),
                provider_details=provider_details,
            )
            self._parts[part_index] = part_delta.apply(existing_thinking_part)
            yield PartDeltaEvent(index=part_index, delta=part_delta)

# pydantic_ai_slim/pydantic_ai/messages.py:1272-1272
    part_kind: Literal['tool-call'] = 'tool-call'

# pydantic_ai_slim/pydantic_ai/messages.py:759-759
    part_kind: Literal['user-prompt'] = 'user-prompt'

# pydantic_ai_slim/pydantic_ai/messages.py:1282-1282
    part_kind: Literal['builtin-tool-call'] = 'builtin-tool-call'

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py:51-51
    event: _messages.AgentStreamEvent

# pydantic_ai_slim/pydantic_ai/messages.py:923-923
    part_kind: Literal['builtin-tool-return'] = 'builtin-tool-return'

# pydantic_ai_slim/pydantic_ai/messages.py:967-967
    part_kind: Literal['retry-prompt'] = 'retry-prompt'

# pydantic_ai_slim/pydantic_ai/messages.py:901-901
    part_kind: Literal['tool-return'] = 'tool-return'

# pydantic_ai_slim/pydantic_ai/messages.py:148-148
    part_kind: Literal['system-prompt'] = 'system-prompt'

# pydantic_ai_slim/pydantic_ai/messages.py:1178-1178
    part_kind: Literal['file'] = 'file'

# tests/test_parts_manager.py:84-162
def test_handle_text_deltas_with_think_tags():
    manager = ModelResponsePartsManager()
    thinking_tags = ('<think>', '</think>')

    event = next(manager.handle_text_delta(vendor_part_id='content', content='pre-', thinking_tags=thinking_tags))
    assert event == snapshot(
        PartStartEvent(index=0, part=TextPart(content='pre-', part_kind='text'), event_kind='part_start')
    )
    assert manager.get_parts() == snapshot([TextPart(content='pre-', part_kind='text')])

    event = next(manager.handle_text_delta(vendor_part_id='content', content='thinking', thinking_tags=thinking_tags))
    assert event == snapshot(
        PartDeltaEvent(
            index=0, delta=TextPartDelta(content_delta='thinking', part_delta_kind='text'), event_kind='part_delta'
        )
    )
    assert manager.get_parts() == snapshot([TextPart(content='pre-thinking', part_kind='text')])

    event = next(manager.handle_text_delta(vendor_part_id='content', content='<think>', thinking_tags=thinking_tags))
    assert event == snapshot(
        PartStartEvent(index=1, part=ThinkingPart(content='', part_kind='thinking'), event_kind='part_start')
    )
    assert manager.get_parts() == snapshot(
        [TextPart(content='pre-thinking', part_kind='text'), ThinkingPart(content='', part_kind='thinking')]
    )

    event = next(manager.handle_text_delta(vendor_part_id='content', content='thinking', thinking_tags=thinking_tags))
    assert event == snapshot(
        PartDeltaEvent(
            index=1,
            delta=ThinkingPartDelta(content_delta='thinking', part_delta_kind='thinking'),
            event_kind='part_delta',
        )
    )
    assert manager.get_parts() == snapshot(
        [TextPart(content='pre-thinking', part_kind='text'), ThinkingPart(content='thinking', part_kind='thinking')]
    )

    event = next(manager.handle_text_delta(vendor_part_id='content', content=' more', thinking_tags=thinking_tags))
    assert event == snapshot(
        PartDeltaEvent(
            index=1, delta=ThinkingPartDelta(content_delta=' more', part_delta_kind='thinking'), event_kind='part_delta'
        )
    )
    assert manager.get_parts() == snapshot(
        [
            TextPart(content='pre-thinking', part_kind='text'),
            ThinkingPart(content='thinking more', part_kind='thinking'),
        ]
    )

    events = list(manager.handle_text_delta(vendor_part_id='content', content='</think>', thinking_tags=thinking_tags))
    assert events == []

    event = next(manager.handle_text_delta(vendor_part_id='content', content='post-', thinking_tags=thinking_tags))
    assert event == snapshot(
        PartStartEvent(index=2, part=TextPart(content='post-', part_kind='text'), event_kind='part_start')
    )
    assert manager.get_parts() == snapshot(
        [
            TextPart(content='pre-thinking', part_kind='text'),
            ThinkingPart(content='thinking more', part_kind='thinking'),
            TextPart(content='post-', part_kind='text'),
        ]
    )

    event = next(manager.handle_text_delta(vendor_part_id='content', content='thinking', thinking_tags=thinking_tags))
    assert event == snapshot(
        PartDeltaEvent(
            index=2, delta=TextPartDelta(content_delta='thinking', part_delta_kind='text'), event_kind='part_delta'
        )
    )
    assert manager.get_parts() == snapshot(
        [
            TextPart(content='pre-thinking', part_kind='text'),
            ThinkingPart(content='thinking more', part_kind='thinking'),
            TextPart(content='post-thinking', part_kind='text'),
        ]
    )

# pydantic_ai_slim/pydantic_ai/messages.py:1909-1909
    event_kind: Literal['part_start'] = 'part_start'

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:444-451
    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[EventT]:
        """Handle a `TextPartDelta`.

        Args:
            delta: The text part delta.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:463-471
    async def handle_thinking_start(self, part: ThinkingPart, follows_thinking: bool = False) -> AsyncIterator[EventT]:
        """Handle the start of a `ThinkingPart`.

        Args:
            part: The thinking part.
            follows_thinking: Whether the part is directly preceded by another thinking part. In this case, you may want to yield a "thinking-delta" event instead of a "thinking-start" event.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

# pydantic_ai_slim/pydantic_ai/messages.py:1370-1375
    def thinking(self) -> str | None:
        """Get the thinking in the response."""
        thinking_parts = [part.content for part in self.parts if isinstance(part, ThinkingPart)]
        if not thinking_parts:
            return None
        return '\n\n'.join(thinking_parts)

# tests/test_ui.py:145-146
    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[str]:
        yield delta.content_delta

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:465-480
    def _handle_embedded_thinking_content(
        self,
        existing_part: ThinkingPart,
        part_index: int,
        content: str,
        provider_name: str | None,
        provider_details: dict[str, Any] | None,
    ) -> Iterator[ModelResponseStreamEvent]:
        """Handle content inside <think>...</think>."""
        part_delta = ThinkingPartDelta(
            content_delta=content,
            provider_name=self._resolve_provider_name(existing_part, provider_name),
            provider_details=provider_details,
        )
        self._parts[part_index] = part_delta.apply(existing_part)
        yield PartDeltaEvent(index=part_index, delta=part_delta)

# pydantic_ai_slim/pydantic_ai/messages.py:1925-1925
    event_kind: Literal['part_delta'] = 'part_delta'

# pydantic_ai_slim/pydantic_ai/messages.py:1895-1895
    index: int

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py:133-138
    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[BaseChunk]:
        if delta.content_delta:  # pragma: no branch
            provider_metadata = dump_provider_metadata(
                provider_name=delta.provider_name, provider_details=delta.provider_details
            )
            yield TextDeltaChunk(id=self.message_id, delta=delta.content_delta, provider_metadata=provider_metadata)

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:434-442
    async def handle_text_start(self, part: TextPart, follows_text: bool = False) -> AsyncIterator[EventT]:
        """Handle the start of a `TextPart`.

        Args:
            part: The text part.
            follows_text: Whether the part is directly preceded by another text part. In this case, you may want to yield a "text-delta" event instead of a "text-start" event.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

# pydantic_ai_slim/pydantic_ai/ui/ag_ui/_event_stream.py:137-139
    async def handle_text_delta(self, delta: TextPartDelta) -> AsyncIterator[BaseEvent]:
        if delta.content_delta:  # pragma: no branch
            yield TextMessageContentEvent(message_id=self.message_id, delta=delta.content_delta)

# pydantic_ai_slim/pydantic_ai/messages.py:1584-1603
    def apply(self, part: ModelResponsePart) -> TextPart:
        """Apply this text delta to an existing `TextPart`.

        Args:
            part: The existing model response part, which must be a `TextPart`.

        Returns:
            A new `TextPart` with updated text content.

        Raises:
            ValueError: If `part` is not a `TextPart`.
        """
        if not isinstance(part, TextPart):
            raise ValueError('Cannot apply TextPartDeltas to non-TextParts')  # pragma: no cover
        return replace(
            part,
            content=part.content + self.content_delta,
            provider_name=self.provider_name or part.provider_name,
            provider_details={**(part.provider_details or {}), **(self.provider_details or {})} or None,
        )

# pydantic_ai_slim/pydantic_ai/messages.py:1559-1605
class TextPartDelta:
    """A partial update (delta) for a `TextPart` to append new text content."""

    content_delta: str
    """The incremental text content to add to the existing `TextPart` content."""

    _: KW_ONLY

    provider_name: str | None = None
    """The name of the provider that generated the response.

    This is required to be set when `provider_details` is set and the initial TextPart does not have a `provider_name` or it has changed.
    """

    provider_details: dict[str, Any] | None = None
    """Additional data returned by the provider that can't be mapped to standard fields.

    This is used for data that is required to be sent back to APIs, as well as data users may want to access programmatically.

    When this field is set, `provider_name` is required to identify the provider that generated this data.
    """

    part_delta_kind: Literal['text'] = 'text'
    """Part delta type identifier, used as a discriminator."""

    def apply(self, part: ModelResponsePart) -> TextPart:
        """Apply this text delta to an existing `TextPart`.

        Args:
            part: The existing model response part, which must be a `TextPart`.

        Returns:
            A new `TextPart` with updated text content.

        Raises:
            ValueError: If `part` is not a `TextPart`.
        """
        if not isinstance(part, TextPart):
            raise ValueError('Cannot apply TextPartDeltas to non-TextParts')  # pragma: no cover
        return replace(
            part,
            content=part.content + self.content_delta,
            provider_name=self.provider_name or part.provider_name,
            provider_details={**(part.provider_details or {}), **(self.provider_details or {})} or None,
        )

    __repr__ = _utils.dataclasses_no_defaults_repr

# tests/test_thinking_part.py:75-76
def test_split_content(thinking_tags: tuple[str, str], content: str, parts: list[ModelResponsePart]):
    assert split_content_into_text_and_thinking(content, thinking_tags) == parts

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:92-98
    def content_type(self) -> str:
        """Get the content type for the event stream, compatible with the `Accept` header value.

        By default, this returns the Server-Sent Events content type (`text/event-stream`).
        If a subclass supports other types as well, it should consider `self.accept` in [`encode_event()`][pydantic_ai.ui.UIEventStream.encode_event] and return the resulting content type.
        """
        return SSE_CONTENT_TYPE

# examples/pydantic_ai_examples/stream_markdown.py:16-16
from rich.text import Text

# pydantic_evals/pydantic_evals/reporting/__init__.py:13-13
from rich.text import Text

# pydantic_ai_slim/pydantic_ai/messages.py:1949-1949
    event_kind: Literal['part_end'] = 'part_end'

# pydantic_ai_slim/pydantic_ai/messages.py:1963-1963
    event_kind: Literal['final_result'] = 'final_result'

# pydantic_ai_slim/pydantic_ai/run.py:452-452
    event_kind: Literal['agent_run_result'] = 'agent_run_result'

# pydantic_ai_slim/pydantic_ai/messages.py:2013-2013
    event_kind: Literal['function_tool_result'] = 'function_tool_result'

# pydantic_ai_slim/pydantic_ai/messages.py:1984-1984
    event_kind: Literal['function_tool_call'] = 'function_tool_call'

# pydantic_ai_slim/pydantic_ai/messages.py:1919-1919
    index: int

# pydantic_ai_slim/pydantic_ai/messages.py:2036-2036
    event_kind: Literal['builtin_tool_call'] = 'builtin_tool_call'

# pydantic_ai_slim/pydantic_ai/messages.py:2052-2052
    event_kind: Literal['builtin_tool_result'] = 'builtin_tool_result'

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:482-492
    async def handle_thinking_end(
        self, part: ThinkingPart, followed_by_thinking: bool = False
    ) -> AsyncIterator[EventT]:
        """Handle the end of a `ThinkingPart`.

        Args:
            part: The thinking part.
            followed_by_thinking: Whether the part is directly followed by another thinking part. In this case, you may not want to yield a "thinking-end" event yet.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

# docs/.hooks/test_snippets.py:24-33
def temp_text_file(content: str):
    """Context manager for temporary text file with common params."""
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', encoding='utf-8', delete=False) as f:
        f.write(content)
        temp_name = f.name

    try:
        yield Path(temp_name)
    finally:
        os.unlink(temp_name)

# pydantic_ai_slim/pydantic_ai/messages.py:1351-1367
    def text(self) -> str | None:
        """Get the text in the response."""
        texts: list[str] = []
        last_part: ModelResponsePart | None = None
        for part in self.parts:
            if isinstance(part, TextPart):
                # Adjacent text parts should be joined together, but if there are parts in between
                # (like built-in tool calls) they should have newlines between them
                if isinstance(last_part, TextPart):
                    texts[-1] += part.content
                else:
                    texts.append(part.content)
            last_part = part
        if not texts:
            return None

        return '\n\n'.join(texts)

# tests/evals/test_report_evaluators.py:45-45
    text: str

# pydantic_ai_slim/pydantic_ai/ui/_event_stream.py:453-461
    async def handle_text_end(self, part: TextPart, followed_by_text: bool = False) -> AsyncIterator[EventT]:
        """Handle the end of a `TextPart`.

        Args:
            part: The text part.
            followed_by_text: Whether the part is directly followed by another text part. In this case, you may not want to yield a "text-end" event yet.
        """
        return  # pragma: no cover
        yield  # Make this an async generator

# pydantic_ai_slim/pydantic_ai/models/xai.py:591-659
    def _process_response(self, response: chat_types.Response) -> ModelResponse:
        """Convert xAI SDK response to pydantic_ai ModelResponse.

        Processes response.proto.outputs to extract (in order):
        - ThinkingPart: For reasoning/thinking content
        - TextPart: For text content
        - ToolCallPart: For client-side tool calls
        - BuiltinToolCallPart + BuiltinToolReturnPart: For server-side (builtin) tool calls
        """
        parts: list[ModelResponsePart] = []
        outputs = response.proto.outputs

        for output in outputs:
            message = output.message

            # Add reasoning/thinking content if present
            if message.reasoning_content or message.encrypted_content:
                signature = message.encrypted_content or None
                parts.append(
                    ThinkingPart(
                        content=message.reasoning_content or '',
                        signature=signature,
                        provider_name=self.system if signature else None,
                    )
                )

            # Add text content from assistant messages
            if message.content and message.role == chat_types.chat_pb2.MessageRole.ROLE_ASSISTANT:
                part_provider_details: dict[str, Any] | None = None
                if output.logprobs and output.logprobs.content:
                    part_provider_details = {'logprobs': _map_logprobs(output.logprobs)}
                parts.append(TextPart(content=message.content, provider_details=part_provider_details))

            # Process tool calls in this output
            for tool_call in message.tool_calls:
                tool_result_content = _get_tool_result_content(message.content)
                _, part = _create_tool_call_part(
                    tool_call,
                    tool_result_content,
                    self.system,
                    message_role=message.role,
                )
                parts.append(part)

        # Convert usage with detailed token information
        usage = _extract_usage(response, self._model_name, self._provider.name, self._provider.base_url)

        # Map finish reason.
        #
        # The xAI SDK exposes `response.finish_reason` as a *string* for the overall response, but in
        # multi-output responses (e.g. server-side tools) it can reflect an intermediate TOOL_CALLS
        # output rather than the final STOP output. We derive the finish reason from the final output
        # when available.
        if outputs:
            last_reason = outputs[-1].finish_reason
            finish_reason = _FINISH_REASON_PROTO_MAP.get(last_reason, 'stop')
        else:  # pragma: no cover
            finish_reason = _FINISH_REASON_MAP.get(response.finish_reason, 'stop')

        return ModelResponse(
            parts=parts,
            usage=usage,
            model_name=self._model_name,
            timestamp=response.created,
            provider_name=self.system,
            provider_url=self._provider.base_url,
            provider_response_id=response.id,
            finish_reason=finish_reason,
        )

# examples/pydantic_ai_examples/chat_app.py:58-59
async def index() -> FileResponse:
    return FileResponse((THIS_DIR / 'chat_app.html'), media_type='text/html')

# pydantic_ai_slim/pydantic_ai/direct.py:144-203
def model_request_stream(
    model: models.Model | models.KnownModelName | str,
    messages: Sequence[messages.ModelMessage],
    *,
    model_settings: settings.ModelSettings | None = None,
    model_request_parameters: models.ModelRequestParameters | None = None,
    instrument: instrumented_models.InstrumentationSettings | bool | None = None,
) -> AbstractAsyncContextManager[models.StreamedResponse]:
    """Make a streamed async request to a model.

    ```py {title="model_request_stream_example.py"}

    from pydantic_ai import ModelRequest
    from pydantic_ai.direct import model_request_stream


    async def main():
        messages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]  # (1)!
        async with model_request_stream('openai:gpt-5-mini', messages) as stream:
            chunks = []
            async for chunk in stream:
                chunks.append(chunk)
            print(chunks)
            '''
            [
                PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),
                FinalResultEvent(tool_name=None, tool_call_id=None),
                PartDeltaEvent(
                    index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')
                ),
                PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),
                PartEndEvent(
                    index=0,
                    part=TextPart(
                        content='Albert Einstein was a German-born theoretical physicist.'
                    ),
                ),
            ]
            '''
    ```

    1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.

    Args:
        model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.
        messages: Messages to send to the model
        model_settings: optional model settings
        model_request_parameters: optional model request parameters
        instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from
            [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.

    Returns:
        A [stream response][pydantic_ai.models.StreamedResponse] async context manager.
    """
    model_instance = _prepare_model(model, instrument)
    return model_instance.request_stream(
        list(messages),
        model_settings,
        model_request_parameters or models.ModelRequestParameters(),
    )

# pydantic_ai_slim/pydantic_ai/_mcp.py:129-137
def map_from_sampling_content(
    content: mcp_types.TextContent | mcp_types.ImageContent | mcp_types.AudioContent,
) -> messages.TextPart:
    """Convert from sampling content to a pydantic-ai text part."""
    if isinstance(content, mcp_types.TextContent):  # pragma: no branch
        return messages.TextPart(content=content.text)
    else:
        # TODO: Add support for Image/Audio using FilePart.
        raise NotImplementedError('Image and Audio responses in sampling are not yet supported')

# pydantic_ai_slim/pydantic_ai/messages.py:1061-1095
class TextPart:
    """A plain text response from a model."""

    content: str
    """The text content of the response."""

    _: KW_ONLY

    id: str | None = None
    """An optional identifier of the text part.

    When this field is set, `provider_name` is required to identify the provider that generated this data.
    """

    provider_name: str | None = None
    """The name of the provider that generated the response.

    Required to be set when `provider_details` or `id` is set.
    """

    provider_details: dict[str, Any] | None = None
    """Additional data returned by the provider that can't be mapped to standard fields.

    This is used for data that is required to be sent back to APIs, as well as data users may want to access programmatically.
    When this field is set, `provider_name` is required to identify the provider that generated this data.
    """

    part_kind: Literal['text'] = 'text'
    """Part type identifier, this is available on all parts as a discriminator."""

    def has_content(self) -> bool:
        """Return `True` if the text content is non-empty."""
        return bool(self.content)

    __repr__ = _utils.dataclasses_no_defaults_repr

# pydantic_ai_slim/pydantic_ai/messages.py:1935-1935
    index: int

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:48-48
    text: str

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:29-29
    text: str

# pydantic_ai_slim/pydantic_ai/_parts_manager.py:482-484
    def _handle_embedded_thinking_end(self, vendor_part_id: VendorId) -> None:
        """Handle </think> tag - stop tracking so next delta creates new part."""
        self._stop_tracking_vendor_id(vendor_part_id)

# pydantic_ai_slim/pydantic_ai/common_tools/exa.py:76-76
    text: str