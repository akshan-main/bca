# tests/test_usage_limits.py:220-223
def test_request_usage_basics():
    usage = RequestUsage()
    assert usage.output_audio_tokens == 0
    assert usage.requests == 1

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_ai_slim/pydantic_ai/_a2a.py:12-12
from typing_extensions import assert_never

# pydantic_evals/pydantic_evals/evaluators/common.py:199-199
    assertion: OutputConfig | Literal[False] = field(default_factory=lambda: OutputConfig(include_reason=True))

# pydantic_evals/pydantic_evals/reporting/__init__.py:88-88
    assertions: dict[str, EvaluationResult[bool]]

# tests/evals/test_reporting.py:46-52
def sample_assertion(mock_evaluator: Evaluator[TaskInput, TaskOutput, TaskMetadata]) -> EvaluationResult[bool]:
    return EvaluationResult(
        name='MockEvaluator',
        value=True,
        reason=None,
        source=mock_evaluator.as_spec(),
    )

# pydantic_evals/pydantic_evals/reporting/__init__.py:171-171
    assertions: float | None

# tests/conftest.py:21-21
from _pytest.assertion.rewrite import AssertionRewritingHook

# pydantic_evals/pydantic_evals/reporting/__init__.py:946-946
    include_assertions: bool

# pydantic_evals/pydantic_evals/reporting/__init__.py:1277-1290
    def _render_assertions(
        self,
        assertions: list[EvaluationResult[bool]],
    ) -> str:
        if not assertions:
            return EMPTY_CELL_STR
        lines: list[str] = []
        for a in assertions:
            line = '[green]✔[/]' if a.value else '[red]✗[/]'
            if self.include_reasons:
                line = f'{a.name}: {line}\n'
                line = f'{line}  Reason: {a.reason}\n\n' if a.reason else line
            lines.append(line)
        return ''.join(lines)

# pydantic_evals/pydantic_evals/reporting/__init__.py:1392-1393
    def include_assertions(self, report: EvaluationReport, baseline: EvaluationReport | None = None):
        return any(case.assertions for case in self._all_cases(report, baseline))

# pydantic_ai_slim/pydantic_ai/usage.py:113-114
    def requests(self):
        return 0

# pydantic_evals/pydantic_evals/reporting/__init__.py:1303-1311
    def _render_assertions_diff(
        assertions: list[EvaluationResult[bool]], new_assertions: list[EvaluationResult[bool]]
    ) -> str:
        if not assertions and not new_assertions:  # pragma: no cover
            return EMPTY_CELL_STR

        old = ''.join(['[green]✔[/]' if a.value else '[red]✗[/]' for a in assertions])
        new = ''.join(['[green]✔[/]' if a.value else '[red]✗[/]' for a in new_assertions])
        return old if old == new else f'{old} → {new}'

# tests/evals/test_report_evaluators.py:311-323
def test_precision_recall_assertions_requires_key():
    evaluator = PrecisionRecallEvaluator(
        score_from='scores',
        score_key='s',
        positive_from='assertions',
        positive_key=None,
    )
    cases = [_make_report_case('c1', scores={'s': 0.5})]
    report = _make_report(cases)
    ctx = ReportEvaluatorContext(name='test', report=report, experiment_metadata=None)

    with pytest.raises(ValueError, match="'positive_key' is required"):
        evaluator.evaluate(ctx)

# pydantic_evals/pydantic_evals/reporting/__init__.py:1293-1300
    def _render_aggregate_assertions(
        assertions: float | None,
    ) -> str:
        return (
            default_render_percentage(assertions) + ' [green]✔[/]'
            if assertions is not None
            else EMPTY_AGGREGATE_CELL_STR
        )

# pydantic_evals/pydantic_evals/reporting/__init__.py:1314-1324
    def _render_aggregate_assertions_diff(
        baseline: float | None,
        new: float | None,
    ) -> str:
        if baseline is None and new is None:  # pragma: no cover
            return EMPTY_AGGREGATE_CELL_STR
        rendered_baseline = (
            default_render_percentage(baseline) + ' [green]✔[/]' if baseline is not None else EMPTY_CELL_STR
        )
        rendered_new = default_render_percentage(new) + ' [green]✔[/]' if new is not None else EMPTY_CELL_STR
        return rendered_new if rendered_baseline == rendered_new else f'{rendered_baseline} → {rendered_new}'

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/usage.py:176-176
    requests: int = 0

# tests/test_usage_limits.py:462-486
def test_usage_limits_preserves_explicit_zero():
    """Test that explicit 0 token limits are preserved and not replaced by deprecated fallbacks."""
    # When input_tokens_limit=0 and deprecated request_tokens_limit is also set,
    # the explicit 0 should be preserved (not overwritten by the deprecated fallback).
    # We ignore type errors below because overloads don't allow mixing current and deprecated args.
    limits = UsageLimits(input_tokens_limit=0, request_tokens_limit=123)  # pyright: ignore[reportCallIssue]
    assert limits.input_tokens_limit == 0

    limits = UsageLimits(output_tokens_limit=0, response_tokens_limit=456)  # pyright: ignore[reportCallIssue]
    assert limits.output_tokens_limit == 0

    # When only deprecated arg is passed, should use it as fallback
    limits = UsageLimits(request_tokens_limit=123)  # pyright: ignore[reportDeprecated]
    assert limits.input_tokens_limit == 123

    limits = UsageLimits(response_tokens_limit=456)  # pyright: ignore[reportDeprecated]
    assert limits.output_tokens_limit == 456

    # When neither is passed, should be None
    limits = UsageLimits()
    assert limits.input_tokens_limit is None

    # When only current arg is set, should use it
    limits = UsageLimits(input_tokens_limit=100)
    assert limits.input_tokens_limit == 100

# tests/test_prefect.py:111-116
def flow_raises(exc_type: type[Exception], exc_message: str) -> Iterator[None]:
    """Helper for asserting that a Prefect flow fails with the expected error."""
    with pytest.raises(Exception) as exc_info:
        yield
    assert isinstance(exc_info.value, Exception)
    assert str(exc_info.value) == exc_message

# tests/test_dbos.py:104-109
def workflow_raises(exc_type: type[Exception], exc_message: str) -> Iterator[None]:
    """Helper for asserting that a DBOS workflow fails with the expected error."""
    with pytest.raises(Exception) as exc_info:
        yield
    assert isinstance(exc_info.value, Exception)
    assert str(exc_info.value) == exc_message

# tests/test_dbos.py:104-109
def workflow_raises(exc_type: type[Exception], exc_message: str) -> Iterator[None]:
    """Helper for asserting that a DBOS workflow fails with the expected error."""
    with pytest.raises(Exception) as exc_info:
        yield
    assert isinstance(exc_info.value, Exception)
    assert str(exc_info.value) == exc_message

# pydantic_ai_slim/pydantic_ai/run.py:284-286
    def usage(self) -> _usage.RunUsage:
        """Get usage statistics for the run so far, including token usage, model requests, and so on."""
        return self._graph_run.state.usage

# tests/models/mock_xai.py:723-737
def create_usage(
    prompt_tokens: int = 0,
    completion_tokens: int = 0,
    reasoning_tokens: int = 0,
    cached_prompt_text_tokens: int = 0,
    server_side_tools_used: list[usage_pb2.ServerSideTool] | None = None,
) -> usage_pb2.SamplingUsage:
    """Helper to create xAI SamplingUsage protobuf objects for tests with all required fields."""
    return usage_pb2.SamplingUsage(
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        reasoning_tokens=reasoning_tokens,
        cached_prompt_text_tokens=cached_prompt_text_tokens,
        server_side_tools_used=server_side_tools_used or [],
    )

# pydantic_ai_slim/pydantic_ai/usage.py:116-122
    def incr(self, incr_usage: RequestUsage) -> None:
        """Increment the usage in place.

        Args:
            incr_usage: The usage to increment by.
        """
        return _incr_usage_tokens(self, incr_usage)

# tests/models/test_anthropic.py:2964-2967
def test_usage(
    message_callback: Callable[[], BetaMessage | BetaRawMessageStartEvent | BetaRawMessageDeltaEvent], usage: RunUsage
):
    assert _map_usage(message_callback(), 'anthropic', '', 'unknown') == usage

# pydantic_ai_slim/pydantic_ai/usage.py:43-43
    output_audio_tokens: int = 0

# pydantic_evals/pydantic_evals/evaluators/__init__.py:1-10
from .common import (
    Contains,
    Equals,
    EqualsExpected,
    HasMatchingSpan,
    IsInstance,
    LLMJudge,
    MaxDuration,
    OutputConfig,
)

# tests/models/test_gemini.py:2183-2216
def test_map_usage():
    response = gemini_response(_content_model_response(ModelResponse(parts=[TextPart('Hello world')])))
    assert 'usage_metadata' in response
    response['usage_metadata']['cached_content_token_count'] = 9100
    response['usage_metadata']['prompt_tokens_details'] = [
        _GeminiModalityTokenCount(modality='AUDIO', token_count=9200)
    ]
    response['usage_metadata']['cache_tokens_details'] = [
        _GeminiModalityTokenCount(modality='AUDIO', token_count=9300),
    ]
    response['usage_metadata']['candidates_tokens_details'] = [
        _GeminiModalityTokenCount(modality='AUDIO', token_count=9400)
    ]
    response['usage_metadata']['thoughts_token_count'] = 9500
    response['usage_metadata']['tool_use_prompt_token_count'] = 9600

    assert _metadata_as_usage(response) == snapshot(
        RequestUsage(
            input_tokens=1,
            cache_read_tokens=9100,
            output_tokens=9502,
            input_audio_tokens=9200,
            cache_audio_read_tokens=9300,
            output_audio_tokens=9400,
            details={
                'cached_content_tokens': 9100,
                'audio_prompt_tokens': 9200,
                'audio_cache_tokens': 9300,
                'audio_candidates_tokens': 9400,
                'thoughts_tokens': 9500,
                'tool_use_prompt_tokens': 9600,
            },
        )
    )

# tests/models/test_gemini.py:2183-2216
def test_map_usage():
    response = gemini_response(_content_model_response(ModelResponse(parts=[TextPart('Hello world')])))
    assert 'usage_metadata' in response
    response['usage_metadata']['cached_content_token_count'] = 9100
    response['usage_metadata']['prompt_tokens_details'] = [
        _GeminiModalityTokenCount(modality='AUDIO', token_count=9200)
    ]
    response['usage_metadata']['cache_tokens_details'] = [
        _GeminiModalityTokenCount(modality='AUDIO', token_count=9300),
    ]
    response['usage_metadata']['candidates_tokens_details'] = [
        _GeminiModalityTokenCount(modality='AUDIO', token_count=9400)
    ]
    response['usage_metadata']['thoughts_token_count'] = 9500
    response['usage_metadata']['tool_use_prompt_token_count'] = 9600

    assert _metadata_as_usage(response) == snapshot(
        RequestUsage(
            input_tokens=1,
            cache_read_tokens=9100,
            output_tokens=9502,
            input_audio_tokens=9200,
            cache_audio_read_tokens=9300,
            output_audio_tokens=9400,
            details={
                'cached_content_tokens': 9100,
                'audio_prompt_tokens': 9200,
                'audio_cache_tokens': 9300,
                'audio_candidates_tokens': 9400,
                'thoughts_tokens': 9500,
                'tool_use_prompt_tokens': 9600,
            },
        )
    )

# tests/test_usage_limits.py:226-255
def test_add_usages():
    usage = RunUsage(
        requests=2,
        input_tokens=10,
        output_tokens=20,
        cache_read_tokens=30,
        cache_write_tokens=40,
        input_audio_tokens=50,
        cache_audio_read_tokens=60,
        tool_calls=3,
        details={
            'custom1': 10,
            'custom2': 20,
        },
    )
    assert usage + usage == snapshot(
        RunUsage(
            requests=4,
            input_tokens=20,
            output_tokens=40,
            cache_write_tokens=80,
            cache_read_tokens=60,
            input_audio_tokens=100,
            cache_audio_read_tokens=120,
            tool_calls=6,
            details={'custom1': 20, 'custom2': 40},
        )
    )
    assert usage + RunUsage() == usage
    assert RunUsage() + RunUsage() == RunUsage()

# tests/test_usage_limits.py:147-157
def test_usage_so_far() -> None:
    test_agent = Agent(TestModel())

    with pytest.raises(
        UsageLimitExceeded, match=re.escape('Exceeded the total_tokens_limit of 105 (total_tokens=163)')
    ):
        test_agent.run_sync(
            'Hello, this prompt exceeds the request tokens limit.',
            usage_limits=UsageLimits(total_tokens_limit=105),
            usage=RunUsage(input_tokens=50, output_tokens=50),
        )

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# tests/models/test_anthropic.py:2970-2977
def test_streaming_usage():
    start = BetaRawMessageStartEvent(message=anth_msg(BetaUsage(input_tokens=1, output_tokens=1)), type='message_start')
    initial_usage = _map_usage(start, 'anthropic', '', 'unknown')
    delta = BetaRawMessageDeltaEvent(delta=Delta(), usage=BetaMessageDeltaUsage(output_tokens=5), type='message_delta')
    final_usage = _map_usage(delta, 'anthropic', '', 'unknown', existing_usage=initial_usage)
    assert final_usage == snapshot(
        RequestUsage(input_tokens=1, output_tokens=5, details={'input_tokens': 1, 'output_tokens': 5})
    )

# tests/models/test_gemini.py:2219-2224
def test_map_empty_usage():
    response = gemini_response(_content_model_response(ModelResponse(parts=[TextPart('Hello world')])))
    assert 'usage_metadata' in response
    del response['usage_metadata']

    assert _metadata_as_usage(response) == RequestUsage()

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:28-28
from . import _output, _system_prompt, exceptions, messages as _messages, models, result, usage as _usage

# tests/models/test_openrouter.py:283-312
async def test_openrouter_usage(allow_model_requests: None, openrouter_api_key: str) -> None:
    provider = OpenRouterProvider(api_key=openrouter_api_key)
    model = OpenRouterModel('openai/gpt-5-mini', provider=provider)
    agent = Agent(model, instructions='Be helpful.', retries=1)

    result = await agent.run('Tell me about Venus')

    assert result.usage() == snapshot(
        RunUsage(input_tokens=17, output_tokens=1515, details={'reasoning_tokens': 704}, requests=1)
    )

    settings = OpenRouterModelSettings(openrouter_usage={'include': True})

    result = await agent.run('Tell me about Mars', model_settings=settings)

    assert result.usage() == snapshot(
        RunUsage(
            input_tokens=17,
            output_tokens=2177,
            details={'is_byok': 0, 'reasoning_tokens': 960, 'image_tokens': 0},
            requests=1,
        )
    )

    last_message = result.all_messages()[-1]

    assert isinstance(last_message, ModelResponse)
    assert last_message.provider_details is not None
    for key in ['cost', 'upstream_inference_cost', 'is_byok']:
        assert key in last_message.provider_details

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# pydantic_ai_slim/pydantic_ai/__init__.py:114-114
from .tools import DeferredToolRequests, DeferredToolResults, RunContext, Tool, ToolApproved, ToolDefinition, ToolDenied

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# pydantic_ai_slim/pydantic_ai/usage.py:124-133
    def __add__(self, other: RequestUsage) -> RequestUsage:
        """Add two RequestUsages together.

        This is provided so it's trivial to sum usage information from multiple parts of a response.

        **WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.
        """
        new_usage = copy(self)
        new_usage.incr(other)
        return new_usage

# pydantic_ai_slim/pydantic_ai/usage.py:19-101
class UsageBase:
    input_tokens: Annotated[
        int,
        # `request_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('input_tokens', 'request_tokens')),
    ] = 0
    """Number of input/prompt tokens."""

    cache_write_tokens: int = 0
    """Number of tokens written to the cache."""
    cache_read_tokens: int = 0
    """Number of tokens read from the cache."""

    output_tokens: Annotated[
        int,
        # `response_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('output_tokens', 'response_tokens')),
    ] = 0
    """Number of output/completion tokens."""

    input_audio_tokens: int = 0
    """Number of audio input tokens."""
    cache_audio_read_tokens: int = 0
    """Number of audio tokens read from the cache."""
    output_audio_tokens: int = 0
    """Number of audio output tokens."""

    details: Annotated[
        dict[str, int],
        # `details` can not be `None` any longer, but we still want to support deserializing model responses stored in a DB before this was changed
        BeforeValidator(lambda d: d or {}),
    ] = dataclasses.field(default_factory=dict[str, int])
    """Any extra details returned by the model."""

    @property
    @deprecated('`request_tokens` is deprecated, use `input_tokens` instead')
    def request_tokens(self) -> int:
        return self.input_tokens

    @property
    @deprecated('`response_tokens` is deprecated, use `output_tokens` instead')
    def response_tokens(self) -> int:
        return self.output_tokens

    @property
    def total_tokens(self) -> int:
        """Sum of `input_tokens + output_tokens`."""
        return self.input_tokens + self.output_tokens

    def opentelemetry_attributes(self) -> dict[str, int]:
        """Get the token usage values as OpenTelemetry attributes."""
        result: dict[str, int] = {}
        if self.input_tokens:
            result['gen_ai.usage.input_tokens'] = self.input_tokens
        if self.output_tokens:
            result['gen_ai.usage.output_tokens'] = self.output_tokens

        details = self.details.copy()
        if self.cache_write_tokens:
            details['cache_write_tokens'] = self.cache_write_tokens
        if self.cache_read_tokens:
            details['cache_read_tokens'] = self.cache_read_tokens
        if self.input_audio_tokens:
            details['input_audio_tokens'] = self.input_audio_tokens
        if self.cache_audio_read_tokens:
            details['cache_audio_read_tokens'] = self.cache_audio_read_tokens
        if self.output_audio_tokens:
            details['output_audio_tokens'] = self.output_audio_tokens
        if details:
            prefix = 'gen_ai.usage.details.'
            for key, value in details.items():
                # Skipping check for value since spec implies all detail values are relevant
                if value:
                    result[prefix + key] = value
        return result

    def __repr__(self):
        kv_pairs = (f'{f.name}={value!r}' for f in fields(self) if (value := getattr(self, f.name)))
        return f'{self.__class__.__qualname__}({", ".join(kv_pairs)})'

    def has_values(self) -> bool:
        """Whether any values are set and non-zero."""
        return any(dataclasses.asdict(self).values())

# tests/models/test_cohere.py:163-190
async def test_request_simple_usage(allow_model_requests: None):
    c = completion_message(
        AssistantMessageResponse(
            content=[TextAssistantMessageResponseContentItem(text='world')],
            role='assistant',
        ),
        usage=cohere.Usage(
            tokens=cohere.UsageTokens(input_tokens=1, output_tokens=1),
            billed_units=cohere.UsageBilledUnits(input_tokens=1, output_tokens=1),
        ),
    )
    mock_client = MockAsyncClientV2.create_mock(c)
    m = CohereModel('command-r7b-12-2024', provider=CohereProvider(cohere_client=mock_client))
    agent = Agent(m)

    result = await agent.run('Hello')
    assert result.output == 'world'
    assert result.usage() == snapshot(
        RunUsage(
            requests=1,
            input_tokens=1,
            output_tokens=1,
            details={
                'input_tokens': 1,
                'output_tokens': 1,
            },
        )
    )

# tests/models/test_cohere.py:163-190
async def test_request_simple_usage(allow_model_requests: None):
    c = completion_message(
        AssistantMessageResponse(
            content=[TextAssistantMessageResponseContentItem(text='world')],
            role='assistant',
        ),
        usage=cohere.Usage(
            tokens=cohere.UsageTokens(input_tokens=1, output_tokens=1),
            billed_units=cohere.UsageBilledUnits(input_tokens=1, output_tokens=1),
        ),
    )
    mock_client = MockAsyncClientV2.create_mock(c)
    m = CohereModel('command-r7b-12-2024', provider=CohereProvider(cohere_client=mock_client))
    agent = Agent(m)

    result = await agent.run('Hello')
    assert result.output == 'world'
    assert result.usage() == snapshot(
        RunUsage(
            requests=1,
            input_tokens=1,
            output_tokens=1,
            details={
                'input_tokens': 1,
                'output_tokens': 1,
            },
        )
    )

# tests/models/test_cohere.py:163-190
async def test_request_simple_usage(allow_model_requests: None):
    c = completion_message(
        AssistantMessageResponse(
            content=[TextAssistantMessageResponseContentItem(text='world')],
            role='assistant',
        ),
        usage=cohere.Usage(
            tokens=cohere.UsageTokens(input_tokens=1, output_tokens=1),
            billed_units=cohere.UsageBilledUnits(input_tokens=1, output_tokens=1),
        ),
    )
    mock_client = MockAsyncClientV2.create_mock(c)
    m = CohereModel('command-r7b-12-2024', provider=CohereProvider(cohere_client=mock_client))
    agent = Agent(m)

    result = await agent.run('Hello')
    assert result.output == 'world'
    assert result.usage() == snapshot(
        RunUsage(
            requests=1,
            input_tokens=1,
            output_tokens=1,
            details={
                'input_tokens': 1,
                'output_tokens': 1,
            },
        )
    )

# tests/models/test_cohere.py:163-190
async def test_request_simple_usage(allow_model_requests: None):
    c = completion_message(
        AssistantMessageResponse(
            content=[TextAssistantMessageResponseContentItem(text='world')],
            role='assistant',
        ),
        usage=cohere.Usage(
            tokens=cohere.UsageTokens(input_tokens=1, output_tokens=1),
            billed_units=cohere.UsageBilledUnits(input_tokens=1, output_tokens=1),
        ),
    )
    mock_client = MockAsyncClientV2.create_mock(c)
    m = CohereModel('command-r7b-12-2024', provider=CohereProvider(cohere_client=mock_client))
    agent = Agent(m)

    result = await agent.run('Hello')
    assert result.output == 'world'
    assert result.usage() == snapshot(
        RunUsage(
            requests=1,
            input_tokens=1,
            output_tokens=1,
            details={
                'input_tokens': 1,
                'output_tokens': 1,
            },
        )
    )

# pydantic_ai_slim/pydantic_ai/usage.py:246-247
class Usage(RunUsage):
    """Deprecated alias for `RunUsage`."""

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:224-243
def _map_usage(response: EmbedByTypeResponse, provider: str, provider_url: str, model: str) -> RequestUsage:
    u = response.meta
    if u is None or u.billed_units is None:
        return RequestUsage()  # pragma: no cover
    usage_data = {
        k: int(v)
        for k, v in u.billed_units.model_dump(exclude_none=True).items()
        if isinstance(v, int | float) and v > 0
    }
    details = {k: int(v) for k, v in usage_data.items() if k != 'input_tokens' and isinstance(v, int | float) and v > 0}
    response_data = dict(model=model, meta=dict(billed_units=usage_data))

    return RequestUsage.extract(
        response_data,
        provider=provider,
        provider_url=provider_url,
        provider_fallback='cohere',
        api_flavor='embeddings',
        details=details,
    )

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:224-243
def _map_usage(response: EmbedByTypeResponse, provider: str, provider_url: str, model: str) -> RequestUsage:
    u = response.meta
    if u is None or u.billed_units is None:
        return RequestUsage()  # pragma: no cover
    usage_data = {
        k: int(v)
        for k, v in u.billed_units.model_dump(exclude_none=True).items()
        if isinstance(v, int | float) and v > 0
    }
    details = {k: int(v) for k, v in usage_data.items() if k != 'input_tokens' and isinstance(v, int | float) and v > 0}
    response_data = dict(model=model, meta=dict(billed_units=usage_data))

    return RequestUsage.extract(
        response_data,
        provider=provider,
        provider_url=provider_url,
        provider_fallback='cohere',
        api_flavor='embeddings',
        details=details,
    )

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:224-243
def _map_usage(response: EmbedByTypeResponse, provider: str, provider_url: str, model: str) -> RequestUsage:
    u = response.meta
    if u is None or u.billed_units is None:
        return RequestUsage()  # pragma: no cover
    usage_data = {
        k: int(v)
        for k, v in u.billed_units.model_dump(exclude_none=True).items()
        if isinstance(v, int | float) and v > 0
    }
    details = {k: int(v) for k, v in usage_data.items() if k != 'input_tokens' and isinstance(v, int | float) and v > 0}
    response_data = dict(model=model, meta=dict(billed_units=usage_data))

    return RequestUsage.extract(
        response_data,
        provider=provider,
        provider_url=provider_url,
        provider_fallback='cohere',
        api_flavor='embeddings',
        details=details,
    )

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:224-243
def _map_usage(response: EmbedByTypeResponse, provider: str, provider_url: str, model: str) -> RequestUsage:
    u = response.meta
    if u is None or u.billed_units is None:
        return RequestUsage()  # pragma: no cover
    usage_data = {
        k: int(v)
        for k, v in u.billed_units.model_dump(exclude_none=True).items()
        if isinstance(v, int | float) and v > 0
    }
    details = {k: int(v) for k, v in usage_data.items() if k != 'input_tokens' and isinstance(v, int | float) and v > 0}
    response_data = dict(model=model, meta=dict(billed_units=usage_data))

    return RequestUsage.extract(
        response_data,
        provider=provider,
        provider_url=provider_url,
        provider_fallback='cohere',
        api_flavor='embeddings',
        details=details,
    )

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:224-243
def _map_usage(response: EmbedByTypeResponse, provider: str, provider_url: str, model: str) -> RequestUsage:
    u = response.meta
    if u is None or u.billed_units is None:
        return RequestUsage()  # pragma: no cover
    usage_data = {
        k: int(v)
        for k, v in u.billed_units.model_dump(exclude_none=True).items()
        if isinstance(v, int | float) and v > 0
    }
    details = {k: int(v) for k, v in usage_data.items() if k != 'input_tokens' and isinstance(v, int | float) and v > 0}
    response_data = dict(model=model, meta=dict(billed_units=usage_data))

    return RequestUsage.extract(
        response_data,
        provider=provider,
        provider_url=provider_url,
        provider_fallback='cohere',
        api_flavor='embeddings',
        details=details,
    )

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:224-243
def _map_usage(response: EmbedByTypeResponse, provider: str, provider_url: str, model: str) -> RequestUsage:
    u = response.meta
    if u is None or u.billed_units is None:
        return RequestUsage()  # pragma: no cover
    usage_data = {
        k: int(v)
        for k, v in u.billed_units.model_dump(exclude_none=True).items()
        if isinstance(v, int | float) and v > 0
    }
    details = {k: int(v) for k, v in usage_data.items() if k != 'input_tokens' and isinstance(v, int | float) and v > 0}
    response_data = dict(model=model, meta=dict(billed_units=usage_data))

    return RequestUsage.extract(
        response_data,
        provider=provider,
        provider_url=provider_url,
        provider_fallback='cohere',
        api_flavor='embeddings',
        details=details,
    )

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:224-243
def _map_usage(response: EmbedByTypeResponse, provider: str, provider_url: str, model: str) -> RequestUsage:
    u = response.meta
    if u is None or u.billed_units is None:
        return RequestUsage()  # pragma: no cover
    usage_data = {
        k: int(v)
        for k, v in u.billed_units.model_dump(exclude_none=True).items()
        if isinstance(v, int | float) and v > 0
    }
    details = {k: int(v) for k, v in usage_data.items() if k != 'input_tokens' and isinstance(v, int | float) and v > 0}
    response_data = dict(model=model, meta=dict(billed_units=usage_data))

    return RequestUsage.extract(
        response_data,
        provider=provider,
        provider_url=provider_url,
        provider_fallback='cohere',
        api_flavor='embeddings',
        details=details,
    )

# pydantic_ai_slim/pydantic_ai/_run_context.py:37-37
    usage: RunUsage

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# tests/test_usage_limits.py:458-459
def test_usage_unknown_provider():
    assert RequestUsage.extract({}, provider='unknown', provider_url='', provider_fallback='') == RequestUsage()

# pydantic_ai_slim/pydantic_ai/__init__.py:35-48
from .exceptions import (
    AgentRunError,
    ApprovalRequired,
    CallDeferred,
    ConcurrencyLimitExceeded,
    FallbackExceptionGroup,
    IncompleteToolCall,
    ModelAPIError,
    ModelHTTPError,
    ModelRetry,
    UnexpectedModelBehavior,
    UsageLimitExceeded,
    UserError,
)

# examples/pydantic_ai_examples/flight_booking.py:179-179
usage_limits = UsageLimits(request_limit=15)

# pydantic_ai_slim/pydantic_ai/messages.py:1302-1302
    usage: RequestUsage = field(default_factory=RequestUsage)

# tests/test_usage_limits.py:382-391
def test_deprecated_usage_limits():
    with warns(
        snapshot(['DeprecationWarning: `request_tokens_limit` is deprecated, use `input_tokens_limit` instead'])
    ):
        assert UsageLimits(input_tokens_limit=100).request_tokens_limit == 100  # type: ignore

    with warns(
        snapshot(['DeprecationWarning: `response_tokens_limit` is deprecated, use `output_tokens_limit` instead'])
    ):
        assert UsageLimits(output_tokens_limit=100).response_tokens_limit == 100  # type: ignore