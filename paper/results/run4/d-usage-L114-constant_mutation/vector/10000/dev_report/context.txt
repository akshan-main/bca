# tests/test_usage_limits.py:220-223
def test_request_usage_basics():
    usage = RequestUsage()
    assert usage.output_audio_tokens == 0
    assert usage.requests == 1

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:38-39
    def usage(self) -> RequestUsage:
        return self.response.usage  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:47-48
    def usage(self) -> RequestUsage:
        return self.response.usage  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:56-57
    def usage(self) -> RequestUsage:
        return self.response.usage  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:136-384
class Embedder:
    """High-level interface for generating text embeddings.

    The `Embedder` class provides a convenient way to generate vector embeddings from text
    using various embedding model providers. It handles model inference, settings management,
    and optional OpenTelemetry instrumentation.

    Example:
    ```python
    from pydantic_ai import Embedder

    embedder = Embedder('openai:text-embedding-3-small')


    async def main():
        result = await embedder.embed_query('What is machine learning?')
        print(result.embeddings[0][:5])  # First 5 dimensions
        #> [1.0, 1.0, 1.0, 1.0, 1.0]
    ```
    """

    instrument: InstrumentationSettings | bool | None
    """Options to automatically instrument with OpenTelemetry.

    Set to `True` to use default instrumentation settings, which will use Logfire if it's configured.
    Set to an instance of [`InstrumentationSettings`][pydantic_ai.models.instrumented.InstrumentationSettings] to customize.
    If this isn't set, then the last value set by
    [`Embedder.instrument_all()`][pydantic_ai.embeddings.Embedder.instrument_all]
    will be used, which defaults to False.
    See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.
    """

    _instrument_default: ClassVar[InstrumentationSettings | bool] = False

    def __init__(
        self,
        model: EmbeddingModel | KnownEmbeddingModelName | str,
        *,
        settings: EmbeddingSettings | None = None,
        defer_model_check: bool = True,
        instrument: InstrumentationSettings | bool | None = None,
    ) -> None:
        """Initialize an Embedder.

        Args:
            model: The embedding model to use. Can be specified as:

                - A model name string in the format `'provider:model-name'`
                  (e.g., `'openai:text-embedding-3-small'`)
                - An [`EmbeddingModel`][pydantic_ai.embeddings.EmbeddingModel] instance
            settings: Optional [`EmbeddingSettings`][pydantic_ai.embeddings.EmbeddingSettings]
                to use as defaults for all embed calls.
            defer_model_check: Whether to defer model validation until first use.
                Set to `False` to validate the model immediately on construction.
            instrument: OpenTelemetry instrumentation settings. Set to `True` to enable with defaults,
                or pass an [`InstrumentationSettings`][pydantic_ai.models.instrumented.InstrumentationSettings]
                instance to customize. If `None`, uses the value from
                [`Embedder.instrument_all()`][pydantic_ai.embeddings.Embedder.instrument_all].
        """
        self._model = model if defer_model_check else infer_embedding_model(model)
        self._settings = settings
        self.instrument = instrument

        self._override_model: ContextVar[EmbeddingModel | None] = ContextVar('_override_model', default=None)

    @staticmethod
    def instrument_all(instrument: InstrumentationSettings | bool = True) -> None:
        """Set the default instrumentation options for all embedders where `instrument` is not explicitly set.

        This is useful for enabling instrumentation globally without modifying each embedder individually.

        Args:
            instrument: Instrumentation settings to use as the default. Set to `True` for default settings,
                `False` to disable, or pass an
                [`InstrumentationSettings`][pydantic_ai.models.instrumented.InstrumentationSettings]
                instance to customize.
        """
        Embedder._instrument_default = instrument

    @property
    def model(self) -> EmbeddingModel | KnownEmbeddingModelName | str:
        """The embedding model used by this embedder."""
        return self._model

    @contextmanager
    def override(
        self,
        *,
        model: EmbeddingModel | KnownEmbeddingModelName | str | _utils.Unset = _utils.UNSET,
    ) -> Iterator[None]:
        """Context manager to temporarily override the embedding model.

        Useful for testing or dynamically switching models.

        Args:
            model: The embedding model to use within this context.

        Example:
        ```python
        from pydantic_ai import Embedder

        embedder = Embedder('openai:text-embedding-3-small')


        async def main():
            # Temporarily use a different model
            with embedder.override(model='openai:text-embedding-3-large'):
                result = await embedder.embed_query('test')
                print(len(result.embeddings[0]))  # 3072 dimensions for large model
                #> 3072
        ```
        """
        if _utils.is_set(model):
            model_token = self._override_model.set(infer_embedding_model(model))
        else:
            model_token = None

        try:
            yield
        finally:
            if model_token is not None:
                self._override_model.reset(model_token)

    async def embed_query(
        self, query: str | Sequence[str], *, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Embed one or more query texts.

        Use this method when embedding search queries that will be compared against document embeddings.
        Some models optimize embeddings differently based on whether the input is a query or document.

        Args:
            query: A single query string or sequence of query strings to embed.
            settings: Optional settings to override the embedder's default settings for this call.

        Returns:
            An [`EmbeddingResult`][pydantic_ai.embeddings.EmbeddingResult] containing the embeddings
            and metadata about the operation.
        """
        return await self.embed(query, input_type='query', settings=settings)

    async def embed_documents(
        self, documents: str | Sequence[str], *, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Embed one or more document texts.

        Use this method when embedding documents that will be stored and later searched against.
        Some models optimize embeddings differently based on whether the input is a query or document.

        Args:
            documents: A single document string or sequence of document strings to embed.
            settings: Optional settings to override the embedder's default settings for this call.

        Returns:
            An [`EmbeddingResult`][pydantic_ai.embeddings.EmbeddingResult] containing the embeddings
            and metadata about the operation.
        """
        return await self.embed(documents, input_type='document', settings=settings)

    async def embed(
        self, inputs: str | Sequence[str], *, input_type: EmbedInputType, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Embed text inputs with explicit input type specification.

        This is the low-level embedding method. For most use cases, prefer
        [`embed_query()`][pydantic_ai.embeddings.Embedder.embed_query] or
        [`embed_documents()`][pydantic_ai.embeddings.Embedder.embed_documents].

        Args:
            inputs: A single string or sequence of strings to embed.
            input_type: The type of input, either `'query'` or `'document'`.
            settings: Optional settings to override the embedder's default settings for this call.

        Returns:
            An [`EmbeddingResult`][pydantic_ai.embeddings.EmbeddingResult] containing the embeddings
            and metadata about the operation.
        """
        model = self._get_model()
        settings = merge_embedding_settings(self._settings, settings)
        return await model.embed(inputs, input_type=input_type, settings=settings)

    async def max_input_tokens(self) -> int | None:
        """Get the maximum number of tokens the model can accept as input.

        Returns:
            The maximum token count, or `None` if the limit is unknown for this model.
        """
        model = self._get_model()
        return await model.max_input_tokens()

    async def count_tokens(self, text: str) -> int:
        """Count the number of tokens in the given text.

        Args:
            text: The text to tokenize and count.

        Returns:
            The number of tokens in the text.

        Raises:
            NotImplementedError: If the model doesn't support token counting.
            UserError: If the model or tokenizer is not supported.
        """
        model = self._get_model()
        return await model.count_tokens(text)

    def embed_query_sync(
        self, query: str | Sequence[str], *, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Synchronous version of [`embed_query()`][pydantic_ai.embeddings.Embedder.embed_query]."""
        return _utils.get_event_loop().run_until_complete(self.embed_query(query, settings=settings))

    def embed_documents_sync(
        self, documents: str | Sequence[str], *, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Synchronous version of [`embed_documents()`][pydantic_ai.embeddings.Embedder.embed_documents]."""
        return _utils.get_event_loop().run_until_complete(self.embed_documents(documents, settings=settings))

    def embed_sync(
        self, inputs: str | Sequence[str], *, input_type: EmbedInputType, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        """Synchronous version of [`embed()`][pydantic_ai.embeddings.Embedder.embed]."""
        return _utils.get_event_loop().run_until_complete(self.embed(inputs, input_type=input_type, settings=settings))

    def max_input_tokens_sync(self) -> int | None:
        """Synchronous version of [`max_input_tokens()`][pydantic_ai.embeddings.Embedder.max_input_tokens]."""
        return _utils.get_event_loop().run_until_complete(self.max_input_tokens())

    def count_tokens_sync(self, text: str) -> int:
        """Synchronous version of [`count_tokens()`][pydantic_ai.embeddings.Embedder.count_tokens]."""
        return _utils.get_event_loop().run_until_complete(self.count_tokens(text))

    def _get_model(self) -> EmbeddingModel:
        """Create a model configured for this embedder.

        Returns:
            The embedding model to use, with instrumentation applied if configured.
        """
        model_: EmbeddingModel
        if some_model := self._override_model.get():
            model_ = some_model
        else:
            model_ = self._model = infer_embedding_model(self.model)

        instrument = self.instrument
        if instrument is None:
            instrument = self._instrument_default

        return instrument_embedding_model(model_, instrument)

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/bedrock.py:17-42
from pydantic_ai import (
    AudioUrl,
    BinaryContent,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    CachePoint,
    DocumentUrl,
    FinishReason,
    ImageUrl,
    ModelMessage,
    ModelProfileSpec,
    ModelRequest,
    ModelResponse,
    ModelResponsePart,
    ModelResponseStreamEvent,
    RetryPromptPart,
    SystemPromptPart,
    TextPart,
    ThinkingPart,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
    _utils,
    usage,
)

# pydantic_ai_slim/pydantic_ai/models/cohere.py:11-11
from .. import ModelHTTPError, usage

# pydantic_ai_slim/pydantic_ai/models/function.py:14-14
from .. import _utils, usage

# pydantic_ai_slim/pydantic_ai/models/gemini.py:24-24
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/google.py:14-14
from .. import UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/groq.py:13-13
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/huggingface.py:11-11
from .. import ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/models/openai.py:18-18
from .. import ModelAPIError, ModelHTTPError, UnexpectedModelBehavior, _utils, usage

# pydantic_ai_slim/pydantic_ai/usage.py:113-114
    def requests(self):
        return 0

# pydantic_ai_slim/pydantic_ai/direct.py:390-392
    def usage(self) -> RequestUsage:
        """Get the usage of the response so far."""
        return self._ensure_stream_ready().usage()

# pydantic_ai_slim/pydantic_ai/_run_context.py:37-37
    usage: RunUsage

# pydantic_ai_slim/pydantic_ai/messages.py:1302-1302
    usage: RequestUsage = field(default_factory=RequestUsage)

# pydantic_ai_slim/pydantic_ai/run.py:284-286
    def usage(self) -> _usage.RunUsage:
        """Get usage statistics for the run so far, including token usage, model requests, and so on."""
        return self._graph_run.state.usage

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:76-76
    usage: RequestUsage = field(default_factory=RequestUsage)

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:90-90
    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

# pydantic_ai_slim/pydantic_ai/models/__init__.py:1029-1031
    def usage(self) -> RequestUsage:
        """Get the usage of the response so far. This will not be the final usage until the stream is exhausted."""
        return self._usage

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:464-464
    usage: _OpenRouterUsage | None = None  # type: ignore[reportIncompatibleVariableOverride]

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:645-645
    usage: _OpenRouterUsage | None = None  # type: ignore[reportIncompatibleVariableOverride]

# pydantic_ai_slim/pydantic_ai/run.py:423-425
    def usage(self) -> _usage.RunUsage:
        """Return the usage of the whole run."""
        return self._state.usage

# pydantic_ai_slim/pydantic_ai/usage.py:246-247
class Usage(RunUsage):
    """Deprecated alias for `RunUsage`."""

# tests/models/mock_xai.py:723-737
def create_usage(
    prompt_tokens: int = 0,
    completion_tokens: int = 0,
    reasoning_tokens: int = 0,
    cached_prompt_text_tokens: int = 0,
    server_side_tools_used: list[usage_pb2.ServerSideTool] | None = None,
) -> usage_pb2.SamplingUsage:
    """Helper to create xAI SamplingUsage protobuf objects for tests with all required fields."""
    return usage_pb2.SamplingUsage(
        prompt_tokens=prompt_tokens,
        completion_tokens=completion_tokens,
        reasoning_tokens=reasoning_tokens,
        cached_prompt_text_tokens=cached_prompt_text_tokens,
        server_side_tools_used=server_side_tools_used or [],
    )

# pydantic_ai_slim/pydantic_ai/__init__.py:129-129
from .usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/direct.py:19-19
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py:20-20
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py:21-21
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py:24-24
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:17-17
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:7-7
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/embeddings/google.py:7-7
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/embeddings/openai.py:8-8
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/embeddings/result.py:9-9
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/embeddings/test.py:6-6
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py:9-9
from pydantic_ai.usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/messages.py:28-28
from .usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/models/__init__.py:53-53
from ..usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/models/concurrency.py:21-21
from ..usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/models/mistral.py:43-43
from ..usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/models/test.py:36-36
from ..usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/models/wrapper.py:13-13
from ..usage import RequestUsage

# pydantic_ai_slim/pydantic_ai/models/xai.py:54-54
from ..usage import RequestUsage

# tests/models/test_anthropic.py:58-58
from pydantic_ai.usage import RequestUsage, UsageLimits

# tests/models/test_bedrock.py:52-52
from pydantic_ai.usage import RequestUsage, RunUsage, UsageLimits

# tests/models/test_cohere.py:31-31
from pydantic_ai.usage import RequestUsage, RunUsage

# tests/models/test_deepseek.py:17-17
from pydantic_ai.usage import RequestUsage

# tests/models/test_fallback.py:33-33
from pydantic_ai.usage import RequestUsage

# tests/models/test_gemini.py:63-63
from pydantic_ai.usage import RequestUsage

# tests/models/test_gemini_vertex.py:21-21
from pydantic_ai.usage import RequestUsage

# tests/models/test_google.py:77-77
from pydantic_ai.usage import RequestUsage, RunUsage, UsageLimits

# tests/models/test_groq.py:50-50
from pydantic_ai.usage import RequestUsage, RunUsage

# tests/models/test_huggingface.py:39-39
from pydantic_ai.usage import RequestUsage

# tests/models/test_instrumented.py:45-45
from pydantic_ai.usage import RequestUsage

# tests/models/test_mistral.py:34-34
from pydantic_ai.usage import RequestUsage

# tests/models/test_model_function.py:28-28
from pydantic_ai.usage import RequestUsage

# tests/models/test_model_test.py:35-35
from pydantic_ai.usage import RequestUsage, RunUsage

# tests/models/test_openai.py:51-51
from pydantic_ai.usage import RequestUsage

# tests/models/test_xai.py:71-71
from pydantic_ai.usage import RequestUsage, RunUsage

# tests/test_a2a.py:25-25
from pydantic_ai.usage import RequestUsage

# tests/test_agent.py:74-74
from pydantic_ai.usage import RequestUsage

# tests/test_dbos.py:38-38
from pydantic_ai.usage import RequestUsage

# tests/test_direct.py:35-35
from pydantic_ai.usage import RequestUsage

# tests/test_embeddings.py:29-29
from pydantic_ai.usage import RequestUsage

# tests/test_history_processor.py:21-21
from pydantic_ai.usage import RequestUsage

# tests/test_mcp.py:45-45
from pydantic_ai.usage import RequestUsage, RunUsage

# tests/test_messages.py:9-30
from pydantic_ai import (
    AudioUrl,
    BinaryContent,
    BinaryImage,
    BuiltinToolCallPart,
    BuiltinToolReturnPart,
    DocumentUrl,
    FilePart,
    ImageUrl,
    ModelMessage,
    ModelMessagesTypeAdapter,
    ModelRequest,
    ModelResponse,
    RequestUsage,
    TextPart,
    ThinkingPart,
    ThinkingPartDelta,
    ToolCallPart,
    ToolReturnPart,
    UserPromptPart,
    VideoUrl,
)

# tests/test_prefect.py:35-35
from pydantic_ai.usage import RequestUsage

# tests/test_streaming.py:56-56
from pydantic_ai.usage import RequestUsage

# tests/test_temporal.py:55-55
from pydantic_ai.usage import RequestUsage

# tests/test_tools.py:39-39
from pydantic_ai.usage import RequestUsage

# tests/test_usage_limits.py:29-29
from pydantic_ai.usage import RequestUsage, RunUsage, UsageLimits

# pydantic_ai_slim/pydantic_ai/agent/abstract.py:1354-1408
    def to_a2a(
        self,
        *,
        storage: Storage | None = None,
        broker: Broker | None = None,
        # Agent card
        name: str | None = None,
        url: str = 'http://localhost:8000',
        version: str = '1.0.0',
        description: str | None = None,
        provider: AgentProvider | None = None,
        skills: list[Skill] | None = None,
        # Starlette
        debug: bool = False,
        routes: Sequence[Route] | None = None,
        middleware: Sequence[Middleware] | None = None,
        exception_handlers: dict[Any, ExceptionHandler] | None = None,
        lifespan: Lifespan[FastA2A] | None = None,
    ) -> FastA2A:
        """Convert the agent to a FastA2A application.

        Example:
        ```python
        from pydantic_ai import Agent

        agent = Agent('openai:gpt-5.2')
        app = agent.to_a2a()
        ```

        The `app` is an ASGI application that can be used with any ASGI server.

        To run the application, you can use the following command:

        ```bash
        uvicorn app:app --host 0.0.0.0 --port 8000
        ```
        """
        from .._a2a import agent_to_a2a

        return agent_to_a2a(
            self,
            storage=storage,
            broker=broker,
            name=name,
            url=url,
            version=version,
            description=description,
            provider=provider,
            skills=skills,
            debug=debug,
            routes=routes,
            middleware=middleware,
            exception_handlers=exception_handlers,
            lifespan=lifespan,
        )

# pydantic_ai_slim/pydantic_ai/result.py:434-436
    async def stream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:
        async for output in self.stream_output(debounce_by=debounce_by):
            yield output

# pydantic_ai_slim/pydantic_ai/usage.py:176-176
    requests: int = 0

# pydantic_ai_slim/pydantic_ai/result.py:560-571
    def usage(self) -> RunUsage:
        """Return the usage of the whole run.

        !!! note
            This won't return the full usage until the stream is finished.
        """
        if self._run_result is not None:
            return self._run_result.usage()
        elif self._stream_response is not None:
            return self._stream_response.usage()
        else:
            raise ValueError('No stream response or run result provided')  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/result.py:162-168
    def usage(self) -> RunUsage:
        """Return the usage of the whole run.

        !!! note
            This won't return the full usage until the stream is finished.
        """
        return self._initial_run_ctx_usage + self._raw_stream_response.usage()

# pydantic_ai_slim/pydantic_ai/result.py:743-749
    def usage(self) -> RunUsage:
        """Return the usage of the whole run.

        !!! note
            This won't return the full usage until the stream is finished.
        """
        return self._streamed_run_result.usage()

# pydantic_evals/pydantic_evals/reporting/render_numbers.py:62-94
def default_render_number_diff(old: float | int, new: float | int) -> str | None:
    """Return a string representing the difference between old and new values.

    Rules:
      - If the two values are equal, return None.
      - For integers, return the raw difference (with a leading sign), e.g.:
            _default_format_number_diff(3, 4) -> '+1'
      - For floats (or a mix of float and int):
          * Compute the raw delta = new - old and format it with ABS_SIG_FIGS significant figures.
          * If `old` is nonzero, compute a relative change:
              - If |delta|/|old| ≤ 1, render the relative change as a percentage with
                PERC_DECIMALS decimal places, e.g. '+0.7 / +70.0%'.
              - If |delta|/|old| > 1, render a multiplier (new/old). Use one decimal place
                if the absolute multiplier is less than MULTIPLIER_ONE_DECIMAL_THRESHOLD,
                otherwise no decimals.
          * However, if the percentage rounds to 0.0% (e.g. '+0.0%'), return only the absolute diff.
          * Also, if |old| is below BASE_THRESHOLD and |delta| exceeds MULTIPLIER_DROP_FACTOR×|old|,
            drop the relative change indicator.
    """
    if old == new:
        return None

    if isinstance(old, int) and isinstance(new, int):
        diff_int = new - old
        return f'{diff_int:+d}'

    delta = new - old
    abs_diff_str = _render_signed(delta, ABS_SIG_FIGS)
    rel_diff_str = _render_relative(new, old, BASE_THRESHOLD)
    if rel_diff_str is None:
        return abs_diff_str
    else:
        return f'{abs_diff_str} / {rel_diff_str}'

# pydantic_ai_slim/pydantic_ai/usage.py:170-221
class RunUsage(UsageBase):
    """LLM usage associated with an agent run.

    Responsibility for calculating request usage is on the model; Pydantic AI simply sums the usage information across requests.
    """

    requests: int = 0
    """Number of requests made to the LLM API."""

    tool_calls: int = 0
    """Number of successful tool calls executed during the run."""

    input_tokens: int = 0
    """Total number of input/prompt tokens."""

    cache_write_tokens: int = 0
    """Total number of tokens written to the cache."""

    cache_read_tokens: int = 0
    """Total number of tokens read from the cache."""

    input_audio_tokens: int = 0
    """Total number of audio input tokens."""

    cache_audio_read_tokens: int = 0
    """Total number of audio tokens read from the cache."""

    output_tokens: int = 0
    """Total number of output/completion tokens."""

    details: dict[str, int] = dataclasses.field(default_factory=dict[str, int])
    """Any extra details returned by the model."""

    def incr(self, incr_usage: RunUsage | RequestUsage) -> None:
        """Increment the usage in place.

        Args:
            incr_usage: The usage to increment by.
        """
        if isinstance(incr_usage, RunUsage):
            self.requests += incr_usage.requests
            self.tool_calls += incr_usage.tool_calls
        return _incr_usage_tokens(self, incr_usage)

    def __add__(self, other: RunUsage | RequestUsage) -> RunUsage:
        """Add two RunUsages together.

        This is provided so it's trivial to sum usage information from multiple runs.
        """
        new_usage = copy(self)
        new_usage.incr(other)
        return new_usage

# pydantic_ai_slim/pydantic_ai/models/anthropic.py:1172-1202
def _map_usage(
    message: BetaMessage | BetaRawMessageStartEvent | BetaRawMessageDeltaEvent,
    provider: str,
    provider_url: str,
    model: str,
    existing_usage: usage.RequestUsage | None = None,
) -> usage.RequestUsage:
    if isinstance(message, BetaMessage):
        response_usage = message.usage
    elif isinstance(message, BetaRawMessageStartEvent):
        response_usage = message.message.usage
    elif isinstance(message, BetaRawMessageDeltaEvent):
        response_usage = message.usage
    else:
        assert_never(message)

    # In streaming, usage appears in different events.
    # The values are cumulative, meaning new values should replace existing ones entirely.
    details: dict[str, int] = (existing_usage.details if existing_usage else {}) | {
        key: value for key, value in response_usage.model_dump().items() if isinstance(value, int)
    }

    # Note: genai-prices already extracts cache_creation_input_tokens and cache_read_input_tokens
    # from the Anthropic response and maps them to cache_write_tokens and cache_read_tokens
    return usage.RequestUsage.extract(
        dict(model=model, usage=details),
        provider=provider,
        provider_url=provider_url,
        provider_fallback='anthropic',
        details=details,
    )

# pydantic_evals/pydantic_evals/dataset.py:16-16
import traceback

# pydantic_evals/pydantic_evals/evaluators/_run_evaluator.py:3-3
import traceback

# pydantic_ai_slim/pydantic_ai/usage.py:43-43
    output_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/embeddings/openai.py:171-192
def _map_usage(
    usage: Usage | None,
    provider: str,
    provider_url: str,
    model: str,
) -> RequestUsage:
    # OpenAI SDK types say CreateEmbeddingResponse.usage will always be set, in reality some OpenAI-compatible APIs omit it.
    if usage is None:
        return RequestUsage()

    usage_data = usage.model_dump(exclude_none=True)
    details = {k: v for k, v in usage_data.items() if k not in {'prompt_tokens', 'total_tokens'} if isinstance(v, int)}
    response_data = dict(model=model, usage=usage_data)

    return RequestUsage.extract(
        response_data,
        provider=provider,
        provider_url=provider_url,
        provider_fallback='openai',
        api_flavor='embeddings',
        details=details,
    )

# tests/test_ssrf.py:394-399
    async def test_cgnat_range_blocked(self) -> None:
        """Test that CGNAT range (100.64.0.0/10) is blocked."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            mock_executor.return_value = [(2, 1, 6, '', ('100.64.0.1', 0))]
            with pytest.raises(ValueError, match='Access to private/internal IP address'):
                await validate_and_resolve_url('http://cgnat-host.internal/path', allow_local=False)

# tests/models/test_anthropic.py:8219-8257
async def test_anthropic_cache_messages_real_api(allow_model_requests: None, anthropic_api_key: str):
    """Test that anthropic_cache_messages setting adds cache_control and produces cache usage metrics.

    This test uses a cassette to verify the cache behavior without making real API calls in CI.
    When run with real API credentials, it demonstrates that:
    1. The first call with a long context creates a cache (cache_write_tokens > 0)
    2. Follow-up messages in the same conversation can read from that cache (cache_read_tokens > 0)
    """
    m = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(
        m,
        system_prompt='You are a helpful assistant.',
        model_settings=AnthropicModelSettings(
            anthropic_cache_messages=True,
        ),
    )

    # First call with a longer message - this will cache the message content
    result1 = await agent.run('Please explain what Python is and its main use cases. ' * 100)
    usage1 = result1.usage()

    # With anthropic_cache_messages, the first call should write cache for the last message
    # (cache_write_tokens > 0 indicates that caching occurred)
    assert usage1.requests == 1
    assert usage1.cache_write_tokens > 0
    assert usage1.output_tokens > 0

    # Continue the conversation - this message appends to history
    # The previous cached message should still be in the request
    result2 = await agent.run('Can you summarize that in one sentence?', message_history=result1.all_messages())
    usage2 = result2.usage()

    # The second call should potentially read from cache if the previous message is still cached
    # (cache_read_tokens > 0 when cache hit occurs)
    # (cache_write_tokens > 0 as new message is added to cache)
    assert usage2.requests == 1
    assert usage2.cache_read_tokens > 0
    assert usage2.cache_write_tokens > 0
    assert usage2.output_tokens > 0

# pydantic_ai_slim/pydantic_ai/embeddings/test.py:28-119
class TestEmbeddingModel(EmbeddingModel):
    """A mock embedding model for testing.

    This model returns deterministic embeddings (all 1.0 values) and tracks
    the settings used in the last call via the `last_settings` attribute.

    Example:
    ```python
    from pydantic_ai import Embedder
    from pydantic_ai.embeddings import TestEmbeddingModel

    test_model = TestEmbeddingModel()
    embedder = Embedder('openai:text-embedding-3-small')


    async def main():
        with embedder.override(model=test_model):
            await embedder.embed_query('test')
            assert test_model.last_settings is not None
    ```
    """

    # NOTE: Avoid test discovery by pytest.
    __test__ = False

    _model_name: str
    """The model name to report in results."""

    _provider_name: str
    """The provider name to report in results."""

    _dimensions: int
    """The number of dimensions for generated embeddings."""

    last_settings: EmbeddingSettings | None = None
    """The settings used in the most recent embed call."""

    def __init__(
        self,
        model_name: str = 'test',
        *,
        provider_name: str = 'test',
        dimensions: int = 8,
        settings: EmbeddingSettings | None = None,
    ):
        """Initialize the test embedding model.

        Args:
            model_name: The model name to report in results.
            provider_name: The provider name to report in results.
            dimensions: The number of dimensions for the generated embeddings.
            settings: Optional default settings for the model.
        """
        self._model_name = model_name
        self._provider_name = provider_name
        self._dimensions = dimensions
        self.last_settings = None
        super().__init__(settings=settings)

    @property
    def model_name(self) -> str:
        """The embedding model name."""
        return self._model_name

    @property
    def system(self) -> str:
        """The embedding model provider."""
        return self._provider_name

    async def embed(
        self, inputs: str | Sequence[str], *, input_type: EmbedInputType, settings: EmbeddingSettings | None = None
    ) -> EmbeddingResult:
        inputs, settings = self.prepare_embed(inputs, settings)
        self.last_settings = settings

        dimensions = settings.get('dimensions') or self._dimensions

        return EmbeddingResult(
            embeddings=[[1.0] * dimensions] * len(inputs),
            inputs=inputs,
            input_type=input_type,
            usage=RequestUsage(input_tokens=sum(_estimate_tokens(text) for text in inputs)),
            model_name=self.model_name,
            provider_name=self.system,
            provider_response_id=str(uuid.uuid4()),
        )

    async def max_input_tokens(self) -> int | None:
        return 1024

    async def count_tokens(self, text: str) -> int:
        return _estimate_tokens(text)

# pydantic_ai_slim/pydantic_ai/result.py:64-91
    async def stream_output(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:
        """Asynchronously stream the (validated) agent outputs."""
        if self._cached_output is not None:
            yield deepcopy(self._cached_output)
            return

        last_response: _messages.ModelResponse | None = None
        async for response in self.stream_responses(debounce_by=debounce_by):
            if self._raw_stream_response.final_result_event is None or (
                last_response and response.parts == last_response.parts
            ):
                continue
            last_response = response

            try:
                yield await self.validate_response_output(response, allow_partial=True)
            except ValidationError:
                pass

        if self._raw_stream_response.final_result_event is not None:  # pragma: no branch
            response = self.response
            # Final validation with allow_partial=False (the default).
            # We always yield the final result even if the content matches the last partial yield, because:
            # 1. Output validators/functions receive partial_output=False only on this final call,
            #    and may behave differently based on that flag
            # 2. Users can rely on the last yielded item being the fully validated output
            self._cached_output = await self.validate_response_output(response)
            yield deepcopy(self._cached_output)

# pydantic_ai_slim/pydantic_ai/_a2a.py:75-115
def agent_to_a2a(
    agent: AbstractAgent[AgentDepsT, OutputDataT],
    *,
    storage: Storage | None = None,
    broker: Broker | None = None,
    # Agent card
    name: str | None = None,
    url: str = 'http://localhost:8000',
    version: str = '1.0.0',
    description: str | None = None,
    provider: AgentProvider | None = None,
    skills: list[Skill] | None = None,
    # Starlette
    debug: bool = False,
    routes: Sequence[Route] | None = None,
    middleware: Sequence[Middleware] | None = None,
    exception_handlers: dict[Any, ExceptionHandler] | None = None,
    lifespan: Lifespan[FastA2A] | None = None,
) -> FastA2A:
    """Create a FastA2A server from an agent."""
    storage = storage or InMemoryStorage()
    broker = broker or InMemoryBroker()
    worker = AgentWorker(agent=agent, broker=broker, storage=storage)

    lifespan = lifespan or partial(worker_lifespan, worker=worker, agent=agent)

    return FastA2A(
        storage=storage,
        broker=broker,
        name=name or agent.name,
        url=url,
        version=version,
        description=description,
        provider=provider,
        skills=skills,
        debug=debug,
        routes=routes,
        middleware=middleware,
        exception_handlers=exception_handlers,
        lifespan=lifespan,
    )

# pydantic_ai_slim/pydantic_ai/result.py:492-496
    async def stream_structured(
        self, *, debounce_by: float | None = 0.1
    ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:
        async for msg, last in self.stream_responses(debounce_by=debounce_by):
            yield msg, last

# pydantic_ai_slim/pydantic_ai/result.py:93-104
    async def stream_responses(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[_messages.ModelResponse]:
        """Asynchronously stream the (unvalidated) model responses for the agent."""
        # if the message currently has any parts with content, yield before streaming
        msg = self.response
        for part in msg.parts:
            if part.has_content():
                yield msg
                break

        async with _utils.group_by_temporal(self, debounce_by) as group_iter:
            async for _items in group_iter:
                yield self.response  # current state of the response

# pydantic_ai_slim/pydantic_ai/usage.py:224-241
def _incr_usage_tokens(slf: RunUsage | RequestUsage, incr_usage: RunUsage | RequestUsage) -> None:
    """Increment the usage in place.

    Args:
        slf: The usage to increment.
        incr_usage: The usage to increment by.
    """
    slf.input_tokens += incr_usage.input_tokens
    slf.cache_write_tokens += incr_usage.cache_write_tokens
    slf.cache_read_tokens += incr_usage.cache_read_tokens
    slf.input_audio_tokens += incr_usage.input_audio_tokens
    slf.cache_audio_read_tokens += incr_usage.cache_audio_read_tokens
    slf.output_tokens += incr_usage.output_tokens

    for key, value in incr_usage.details.items():
        # Note: value can be None at runtime from model responses despite the type annotation
        if isinstance(value, (int, float)):
            slf.details[key] = slf.details.get(key, 0) + value

# pydantic_ai_slim/pydantic_ai/usage.py:116-122
    def incr(self, incr_usage: RequestUsage) -> None:
        """Increment the usage in place.

        Args:
            incr_usage: The usage to increment by.
        """
        return _incr_usage_tokens(self, incr_usage)

# tests/models/test_openai.py:4363-4407
async def test_stream_with_continuous_usage_stats(allow_model_requests: None):
    """Test that continuous_usage_stats replaces usage instead of accumulating.

    When continuous_usage_stats=True, each chunk contains cumulative usage, not incremental.
    The final usage should equal the last chunk's usage, not the sum of all chunks.
    We verify that usage is correctly updated at each step via stream_responses.
    """
    # Simulate cumulative usage: each chunk has higher tokens (cumulative, not incremental)
    stream = [
        chunk_with_usage(
            [ChoiceDelta(content='hello ', role='assistant')],
            completion_tokens=5,
            prompt_tokens=10,
            total_tokens=15,
        ),
        chunk_with_usage([ChoiceDelta(content='world')], completion_tokens=10, prompt_tokens=10, total_tokens=20),
        chunk_with_usage([ChoiceDelta(content='!')], completion_tokens=15, prompt_tokens=10, total_tokens=25),
        chunk_with_usage([], finish_reason='stop', completion_tokens=15, prompt_tokens=10, total_tokens=25),
    ]
    mock_client = MockOpenAI.create_mock_stream(stream)
    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(openai_client=mock_client))
    agent = Agent(m)

    settings = cast(OpenAIChatModelSettings, {'openai_continuous_usage_stats': True})
    async with agent.run_stream('', model_settings=settings) as result:
        # Verify usage is updated at each step via stream_responses
        usage_at_each_step: list[RequestUsage] = []
        async for response, _ in result.stream_responses(debounce_by=None):
            usage_at_each_step.append(response.usage)

        # Each step should have the cumulative usage from that chunk (not accumulated)
        # The stream emits responses for each content chunk plus final
        assert usage_at_each_step == snapshot(
            [
                RequestUsage(input_tokens=10, output_tokens=5),
                RequestUsage(input_tokens=10, output_tokens=10),
                RequestUsage(input_tokens=10, output_tokens=15),
                RequestUsage(input_tokens=10, output_tokens=15),
                RequestUsage(input_tokens=10, output_tokens=15),
            ]
        )

    # Final usage should be from the last chunk (15 output tokens)
    # NOT the sum of all chunks (5+10+15+15 = 45 output tokens)
    assert result.usage() == snapshot(RunUsage(requests=1, input_tokens=10, output_tokens=15))

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:205-217
def _extract_version(model_name: str) -> int | None:
    """Extract the version number from a model name.

    Examples:
        - 'amazon.titan-embed-text-v1' -> 1
        - 'amazon.titan-embed-text-v2:0' -> 2
        - 'cohere.embed-english-v3' -> 3
        - 'cohere.embed-v4:0' -> 4
    """
    if match := re.search(r'v(\d+)', model_name):
        return int(match.group(1))
    else:  # pragma: no cover
        return None

# pydantic_ai_slim/pydantic_ai/result.py:244-301
    async def _stream_response_text(
        self, *, delta: bool = False, debounce_by: float | None = 0.1
    ) -> AsyncIterator[str]:
        """Stream the response as an async iterable of text."""

        # Define a "merged" version of the iterator that will yield items that have already been retrieved
        # and items that we receive while streaming. We define a dedicated async iterator for this so we can
        # pass the combined stream to the group_by_temporal function within `_stream_text_deltas` below.
        async def _stream_text_deltas_ungrouped() -> AsyncIterator[tuple[str, int]]:
            # yields tuples of (text_content, part_index)
            # we don't currently make use of the part_index, but in principle this may be useful
            # so we retain it here for now to make possible future refactors simpler
            msg = self.response
            for i, part in enumerate(msg.parts):
                if isinstance(part, _messages.TextPart) and part.content:
                    yield part.content, i

            last_text_index: int | None = None
            async for event in self._raw_stream_response:
                if (
                    isinstance(event, _messages.PartStartEvent)
                    and isinstance(event.part, _messages.TextPart)
                    and event.part.content
                ):
                    last_text_index = event.index
                    yield event.part.content, event.index
                elif (
                    isinstance(event, _messages.PartDeltaEvent)
                    and isinstance(event.delta, _messages.TextPartDelta)
                    and event.delta.content_delta
                ):
                    last_text_index = event.index
                    yield event.delta.content_delta, event.index
                elif (
                    isinstance(event, _messages.PartStartEvent)
                    and isinstance(event.part, _messages.BuiltinToolCallPart)
                    and last_text_index is not None
                ):
                    # Text parts that are interrupted by a built-in tool call should not be joined together directly
                    yield '\n\n', event.index
                    last_text_index = None

        async def _stream_text_deltas() -> AsyncIterator[str]:
            async with _utils.group_by_temporal(_stream_text_deltas_ungrouped(), debounce_by) as group_iter:
                async for items in group_iter:
                    # Note: we are currently just dropping the part index on the group here
                    yield ''.join([content for content, _ in items])

        if delta:
            async for text in _stream_text_deltas():
                yield text
        else:
            # a quick benchmark shows it's faster to build up a string with concat when we're
            # yielding at each step
            deltas: list[str] = []
            async for text in _stream_text_deltas():
                deltas.append(text)
                yield ''.join(deltas)

# pydantic_ai_slim/pydantic_ai/usage.py:124-133
    def __add__(self, other: RequestUsage) -> RequestUsage:
        """Add two RequestUsages together.

        This is provided so it's trivial to sum usage information from multiple parts of a response.

        **WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.
        """
        new_usage = copy(self)
        new_usage.incr(other)
        return new_usage

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:461-461
    error: _OpenRouterError | None = None