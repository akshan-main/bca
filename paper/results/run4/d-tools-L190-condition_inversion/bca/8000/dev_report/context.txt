## pydantic_evals/pydantic_evals/otel/span_tree.py

    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

## tests/test_tools.py

def ctx_tool(ctx: RunContext[int], x: int) -> int:
    return x + ctx.deps

def test_dynamic_tool_use_messages():
    async def repeat_call_foobar(_messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if info.function_tools:
            tool = info.function_tools[0]
            return ModelResponse(parts=[ToolCallPart(tool.name, {'x': 42, 'y': 'a'})])
        else:
            return ModelResponse(parts=[TextPart('done')])

    agent = Agent(FunctionModel(repeat_call_foobar), deps_type=int)

    async def prepare_tool_def(ctx: RunContext[int], tool_def: ToolDefinition) -> ToolDefinition | None:
        if len(ctx.messages) < 5:
            return tool_def

    @agent.tool(prepare=prepare_tool_def)
    def foobar(ctx: RunContext[int], x: int, y: str) -> str:
        return f'{ctx.deps} {x} {y}'

    r = agent.run_sync('', deps=1)
    assert r.output == snapshot('done')
    message_part_kinds = [(m.kind, [p.part_kind for p in m.parts]) for m in r.all_messages()]
    assert message_part_kinds == snapshot(
        [
            ('request', ['user-prompt']),
            ('response', ['tool-call']),
            ('request', ['tool-return']),
            ('response', ['tool-call']),
            ('request', ['tool-return']),
            ('response', ['text']),
        ]
    )

def test_call_tool_without_unrequired_parameters():
    async def call_tools_first(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart(tool_name='my_tool', args={'a': 13}),
                    ToolCallPart(tool_name='my_tool', args={'a': 13, 'b': 4}),
                    ToolCallPart(tool_name='my_tool_plain', args={'b': 17}),
                    ToolCallPart(tool_name='my_tool_plain', args={'a': 4, 'b': 17}),
                    ToolCallPart(tool_name='no_args_tool', args=''),
                ]
            )
        else:
            return ModelResponse(parts=[TextPart('finished')])

    agent = Agent(FunctionModel(call_tools_first))

    @agent.tool_plain
    def no_args_tool() -> None:
        return None

    @agent.tool
    def my_tool(ctx: RunContext[None], a: int, b: int = 2) -> int:
        return a + b

    @agent.tool_plain
    def my_tool_plain(*, a: int = 3, b: int) -> int:
        return a * b

    result = agent.run_sync('Hello')
    all_messages = result.all_messages()
    first_response = all_messages[1]
    second_request = all_messages[2]
    assert isinstance(first_response, ModelResponse)
    assert isinstance(second_request, ModelRequest)
    tool_call_args = [p.args for p in first_response.parts if isinstance(p, ToolCallPart)]
    tool_returns = [p.content for p in second_request.parts if isinstance(p, ToolReturnPart)]
    assert tool_call_args == snapshot(
        [
            {'a': 13},
            {'a': 13, 'b': 4},
            {'b': 17},
            {'a': 4, 'b': 17},
            '',
        ]
    )
    assert tool_returns == snapshot([15, 17, 51, 68, None])

def test_tool_raises_approval_required():
    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired
        return x * 42

    result = agent.run_sync('Hello')
    messages = result.all_messages()
    assert result.output == snapshot(
        DeferredToolRequests(approvals=[ToolCallPart(tool_name='my_tool', args={'x': 1}, tool_call_id='my_tool')])
    )

    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(approvals={'my_tool': ToolApproved(override_args={'x': 2})}),
    )
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content='Hello',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='my_tool', args={'x': 1}, tool_call_id='my_tool')],
                usage=RequestUsage(input_tokens=51, output_tokens=4),
                model_name='function:llm:',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='my_tool',
                        content=84,
                        tool_call_id='my_tool',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='Done!')],
                usage=RequestUsage(input_tokens=52, output_tokens=5),
                model_name='function:llm:',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
        ]
    )
    assert result.output == snapshot('Done!')

def test_approval_required_with_user_prompt():
    """Test that user_prompt can be provided alongside deferred_tool_results for approval."""

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            # First call: request approval
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            # Second call: respond to both tool result and user prompt
            last_request = messages[-1]
            assert isinstance(last_request, ModelRequest)

            # Verify we received both tool return and user prompt
            has_tool_return = any(isinstance(p, ToolReturnPart) for p in last_request.parts)
            has_user_prompt = any(isinstance(p, UserPromptPart) for p in last_request.parts)
            assert has_tool_return, 'Expected tool return in request'
            assert has_user_prompt, 'Expected user prompt in request'

            # Get user prompt content
            user_prompt = next(p.content for p in last_request.parts if isinstance(p, UserPromptPart))
            return ModelResponse(parts=[TextPart(f'Tool executed and {user_prompt}')])

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired
        return x * 42

    # First run: get approval request
    result = agent.run_sync('Hello')
    messages = result.all_messages()
    assert isinstance(result.output, DeferredToolRequests)
    assert len(result.output.approvals) == 1

    # Second run: provide approval AND user prompt
    result = agent.run_sync(
        user_prompt='continue with extra instructions',
        message_history=messages,
        deferred_tool_results=DeferredToolResults(approvals={'my_tool': True}),
    )

    # Verify the response includes both tool result and user prompt
    assert isinstance(result.output, str)
    assert 'continue with extra instructions' in result.output
    assert 'Tool executed' in result.output

def test_call_deferred_with_metadata():
    """Test that CallDeferred exception can carry metadata."""
    agent = Agent(TestModel(), output_type=[str, DeferredToolRequests])

    @agent.tool_plain
    def my_tool(x: int) -> int:
        raise CallDeferred(metadata={'task_id': 'task-123', 'estimated_cost': 25.50})

    result = agent.run_sync('Hello')
    assert result.output == snapshot(
        DeferredToolRequests(
            calls=[ToolCallPart(tool_name='my_tool', args={'x': 0}, tool_call_id=IsStr())],
            metadata={'pyd_ai_tool_call_id__my_tool': {'task_id': 'task-123', 'estimated_cost': 25.5}},
        )
    )

def test_approval_required_with_metadata():
    """Test that ApprovalRequired exception can carry metadata."""

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired(
                metadata={
                    'reason': 'High compute cost',
                    'estimated_time': '5 minutes',
                    'cost_usd': 100.0,
                }
            )
        return x * 42

    result = agent.run_sync('Hello')
    assert result.output == snapshot(
        DeferredToolRequests(
            approvals=[ToolCallPart(tool_name='my_tool', args={'x': 1}, tool_call_id=IsStr())],
            metadata={'my_tool': {'reason': 'High compute cost', 'estimated_time': '5 minutes', 'cost_usd': 100.0}},
        )
    )

    # Continue with approval
    messages = result.all_messages()
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(approvals={'my_tool': ToolApproved()}),
    )
    assert result.output == 'Done!'

def test_approval_required_without_metadata():
    """Test backward compatibility: ApprovalRequired without metadata still works."""

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired  # No metadata
        return x * 42

    result = agent.run_sync('Hello')
    assert isinstance(result.output, DeferredToolRequests)
    assert len(result.output.approvals) == 1

    # Should have an empty metadata dict for this tool
    assert result.output.metadata.get('my_tool', {}) == {}

    # Continue with approval
    messages = result.all_messages()
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(approvals={'my_tool': ToolApproved()}),
    )
    assert result.output == 'Done!'

def test_mixed_deferred_tools_with_metadata():
    """Test multiple deferred tools with different metadata."""

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('tool_a', {'x': 1}, tool_call_id='call_a'),
                    ToolCallPart('tool_b', {'y': 2}, tool_call_id='call_b'),
                    ToolCallPart('tool_c', {'z': 3}, tool_call_id='call_c'),
                ]
            )
        else:
            return ModelResponse(parts=[TextPart('Done!')])

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def tool_a(ctx: RunContext[None], x: int) -> int:
        raise CallDeferred(metadata={'type': 'external', 'priority': 'high'})

    @agent.tool
    def tool_b(ctx: RunContext[None], y: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired(metadata={'reason': 'Needs approval', 'level': 'manager'})
        return y * 10

    @agent.tool
    def tool_c(ctx: RunContext[None], z: int) -> int:
        raise CallDeferred  # No metadata

    result = agent.run_sync('Hello')
    assert isinstance(result.output, DeferredToolRequests)

    # Check that we have the right tools deferred
    assert len(result.output.calls) == 2  # tool_a and tool_c
    assert len(result.output.approvals) == 1  # tool_b

    # Check metadata
    assert result.output.metadata['call_a'] == {'type': 'external', 'priority': 'high'}
    assert result.output.metadata['call_b'] == {'reason': 'Needs approval', 'level': 'manager'}
    assert result.output.metadata.get('call_c', {}) == {}

    # Continue with results for all three tools
    messages = result.all_messages()
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(
            calls={'call_a': 10, 'call_c': 30},
            approvals={'call_b': ToolApproved()},
        ),
    )
    assert result.output == 'Done!'

def test_deferred_tool_call_approved_fails():
    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        return ModelResponse(
            parts=[
                ToolCallPart('foo', {'x': 0}, tool_call_id='foo'),
            ]
        )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    async def defer(ctx: RunContext[None], tool_def: ToolDefinition) -> ToolDefinition | None:
        return replace(tool_def, kind='external')

    @agent.tool_plain(prepare=defer)
    def foo(x: int) -> int:
        return x + 1  # pragma: no cover

    result = agent.run_sync('foo')
    assert result.output == snapshot(
        DeferredToolRequests(calls=[ToolCallPart(tool_name='foo', args={'x': 0}, tool_call_id='foo')])
    )

    with pytest.raises(RuntimeError, match='External tools cannot be called'):
        agent.run_sync(
            message_history=result.all_messages(),
            deferred_tool_results=DeferredToolResults(
                approvals={
                    'foo': True,
                },
            ),
        )

def test_retry_tool_until_last_attempt():
    model = TestModel()
    agent = Agent(model, retries=2)

    @agent.tool
    def always_fail(ctx: RunContext[None]) -> str:
        if ctx.last_attempt:
            return 'I guess you never learn'
        else:
            raise ModelRetry('Please try again.')

    result = agent.run_sync('Always fail!')
    assert result.output == snapshot('{"always_fail":"I guess you never learn"}')
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content='Always fail!',
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='always_fail', args={}, tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=52, output_tokens=2),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    RetryPromptPart(
                        content='Please try again.',
                        tool_name='always_fail',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='always_fail', args={}, tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=62, output_tokens=4),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    RetryPromptPart(
                        content='Please try again.',
                        tool_name='always_fail',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[ToolCallPart(tool_name='always_fail', args={}, tool_call_id=IsStr())],
                usage=RequestUsage(input_tokens=72, output_tokens=6),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelRequest(
                parts=[
                    ToolReturnPart(
                        tool_name='always_fail',
                        content='I guess you never learn',
                        tool_call_id=IsStr(),
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='{"always_fail":"I guess you never learn"}')],
                usage=RequestUsage(input_tokens=77, output_tokens=14),
                model_name='test',
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
        ]
    )

async def test_tool_timeout_triggers_retry():
    """Test that a slow tool triggers RetryPromptPart when timeout is exceeded."""
    import asyncio

    call_count = 0

    async def model_logic(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        nonlocal call_count
        call_count += 1
        # First call: try the slow tool
        if call_count == 1:
            return ModelResponse(parts=[ToolCallPart(tool_name='slow_tool', args={}, tool_call_id='call-1')])
        # After receiving retry, return text
        return ModelResponse(parts=[TextPart(content='Tool timed out, giving up')])

    agent = Agent(FunctionModel(model_logic))

    @agent.tool_plain(timeout=0.1)
    async def slow_tool() -> str:
        await asyncio.sleep(1.0)  # 1 second, but timeout is 0.1s
        return 'done'  # pragma: no cover

    result = await agent.run('call slow_tool')

    # Check that retry prompt was sent to the model
    retry_parts = [
        part
        for msg in result.all_messages()
        if isinstance(msg, ModelRequest)
        for part in msg.parts
        if isinstance(part, RetryPromptPart) and 'Timed out' in str(part.content)
    ]
    assert len(retry_parts) == 1
    assert 'Timed out after 0.1 seconds' in retry_parts[0].content
    assert retry_parts[0].tool_name == 'slow_tool'

async def test_tool_with_timeout_completes_successfully():
    """Test that a tool completes successfully when within its timeout."""
    import asyncio

    from pydantic_ai.messages import ModelMessage, ModelResponse, TextPart, ToolCallPart
    from pydantic_ai.models.function import AgentInfo, FunctionModel

    call_count = 0

    async def model_logic(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            # First call: ask to run the slow tool
            return ModelResponse(
                parts=[ToolCallPart(tool_name='slow_but_allowed_tool', args={}, tool_call_id='call-1')]
            )
        # Second call: tool completed successfully, return final response
        return ModelResponse(parts=[TextPart(content='Tool completed successfully')])

    agent = Agent(FunctionModel(model_logic))

    @agent.tool_plain(timeout=5.0)  # 5s per-tool timeout
    async def slow_but_allowed_tool() -> str:
        await asyncio.sleep(0.2)  # 200ms - within 5s timeout
        return 'completed successfully'

    result = await agent.run('call slow_but_allowed_tool')

    # Should NOT have any retry prompts since tool completed within timeout
    retry_parts = [
        part
        for msg in result.all_messages()
        if isinstance(msg, ModelRequest)
        for part in msg.parts
        if isinstance(part, RetryPromptPart) and 'Timed out' in str(part.content)
    ]
    assert len(retry_parts) == 0
    assert 'completed successfully' in result.output

async def test_tool_timeout_message_format():
    """Test the format of the retry prompt message on timeout."""
    import asyncio

    call_count = 0

    async def model_logic(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            return ModelResponse(parts=[ToolCallPart(tool_name='my_slow_tool', args={}, tool_call_id='call-1')])
        return ModelResponse(parts=[TextPart(content='done')])

    agent = Agent(FunctionModel(model_logic))

    @agent.tool_plain(timeout=0.1)
    async def my_slow_tool() -> str:
        await asyncio.sleep(1.0)
        return 'done'  # pragma: no cover

    result = await agent.run('call my_slow_tool')

    retry_parts = [
        part
        for msg in result.all_messages()
        if isinstance(msg, ModelRequest)
        for part in msg.parts
        if isinstance(part, RetryPromptPart) and 'Timed out' in str(part.content)
    ]
    assert len(retry_parts) == 1
    # Check message contains timeout value (tool_name is in the part, not in content)
    assert '0.1' in retry_parts[0].content
    assert retry_parts[0].tool_name == 'my_slow_tool'

async def test_per_tool_timeout_overrides_agent_timeout():
    """Test that per-tool timeout overrides agent-level timeout."""
    import asyncio

    call_count = 0

    async def model_logic(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            return ModelResponse(parts=[ToolCallPart(tool_name='fast_timeout_tool', args={}, tool_call_id='call-1')])
        return ModelResponse(parts=[TextPart(content='done')])

    # Agent has generous 10s timeout, but per-tool timeout is only 0.1s
    agent = Agent(FunctionModel(model_logic), tool_timeout=10.0)

    @agent.tool_plain(timeout=0.1)  # Per-tool timeout overrides agent timeout
    async def fast_timeout_tool() -> str:
        await asyncio.sleep(1.0)  # 1 second, per-tool timeout is 0.1s
        return 'done'  # pragma: no cover

    result = await agent.run('call fast_timeout_tool')

    # Should timeout because per-tool timeout (0.1s) is applied, not agent timeout (10s)
    retry_parts = [
        part
        for msg in result.all_messages()
        if isinstance(msg, ModelRequest)
        for part in msg.parts
        if isinstance(part, RetryPromptPart) and 'Timed out' in str(part.content)
    ]
    assert len(retry_parts) == 1
    assert 'Timed out after 0.1 seconds' in retry_parts[0].content

async def test_tool_cancelled_when_agent_cancelled(is_stream: bool):
    """Test that tools are cancelled when agent is cancelled."""
    import asyncio

    agent = Agent(TestModel())
    is_called = asyncio.Event()
    is_cancelled = asyncio.Event()

    @agent.tool_plain
    async def tool() -> None:
        is_called.set()

        try:
            await asyncio.sleep(1.0)

        except asyncio.CancelledError:
            is_cancelled.set()
            raise

    async def run_agent() -> None:
        if not is_stream:
            await agent.run('call tool')

        else:
            async for _ in agent.run_stream_events('call tool'):
                pass

    task = asyncio.create_task(run_agent())
    await asyncio.wait_for(is_called.wait(), timeout=1.0)
    task.cancel()
    await asyncio.wait_for(is_cancelled.wait(), timeout=1.0)

def test_tool_approved_with_metadata():
    """Test that DeferredToolResults.metadata is passed to RunContext.tool_call_metadata."""
    received_metadata: list[Any] = []

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired(
                metadata={
                    'reason': 'High compute cost',
                    'estimated_time': '5 minutes',
                }
            )
        # Capture the tool_call_metadata from context
        received_metadata.append(ctx.tool_call_metadata)
        return x * 42

    # First run: get approval request
    result = agent.run_sync('Hello')
    messages = result.all_messages()
    assert isinstance(result.output, DeferredToolRequests)
    assert len(result.output.approvals) == 1

    # Second run: provide approval with metadata
    approval_metadata = {'user_id': 'user-123', 'approved_at': '2025-01-01T00:00:00Z'}
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(
            approvals={'my_tool': ToolApproved()},
            metadata={'my_tool': approval_metadata},
        ),
    )

    assert result.output == 'Done!'
    # Verify the metadata was passed to the tool
    assert len(received_metadata) == 1
    assert received_metadata[0] == approval_metadata

def test_tool_approved_with_metadata_and_override_args():
    """Test that DeferredToolResults.metadata works together with ToolApproved.override_args."""
    received_data: list[tuple[Any, int]] = []

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired()
        # Capture both the metadata and the argument
        received_data.append((ctx.tool_call_metadata, x))
        return x * 42

    # First run: get approval request
    result = agent.run_sync('Hello')
    messages = result.all_messages()
    assert isinstance(result.output, DeferredToolRequests)

    # Second run: provide approval with both metadata and override_args
    approval_metadata = {'approver': 'admin', 'notes': 'LGTM'}
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(
            approvals={
                'my_tool': ToolApproved(
                    override_args={'x': 100},
                )
            },
            metadata={'my_tool': approval_metadata},
        ),
    )

    assert result.output == 'Done!'
    # Verify both metadata and overridden args were received
    assert len(received_data) == 1
    assert received_data[0] == (approval_metadata, 100)

def test_tool_approved_without_metadata():
    """Test that tool_call_metadata is None when DeferredToolResults has no metadata for the tool."""
    received_metadata: list[Any] = []

    def llm(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        if len(messages) == 1:
            return ModelResponse(
                parts=[
                    ToolCallPart('my_tool', {'x': 1}, tool_call_id='my_tool'),
                ]
            )
        else:
            return ModelResponse(
                parts=[
                    TextPart('Done!'),
                ]
            )

    agent = Agent(FunctionModel(llm), output_type=[str, DeferredToolRequests])

    @agent.tool
    def my_tool(ctx: RunContext[None], x: int) -> int:
        if not ctx.tool_call_approved:
            raise ApprovalRequired()
        # Capture the tool_call_metadata from context
        received_metadata.append(ctx.tool_call_metadata)
        return x * 42

    # First run: get approval request
    result = agent.run_sync('Hello')
    messages = result.all_messages()

    # Second run: provide approval without metadata (using ToolApproved() or True)
    result = agent.run_sync(
        message_history=messages,
        deferred_tool_results=DeferredToolResults(approvals={'my_tool': ToolApproved()}),
    )

    assert result.output == 'Done!'
    # Verify the metadata is None
    assert len(received_metadata) == 1
    assert received_metadata[0] is None
