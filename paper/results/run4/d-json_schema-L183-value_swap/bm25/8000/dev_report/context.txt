# tests/test_json_schema.py:11-52
def test_simplify_nullable_unions():
    """Test the simplify_nullable_unions feature (deprecated, to be removed in v2)."""

    # Create a concrete subclass for testing
    class TestTransformer(JsonSchemaTransformer):
        def transform(self, schema: dict[str, Any]) -> dict[str, Any]:
            return schema

    # Test with simplify_nullable_unions=True
    schema_with_null = {
        'anyOf': [
            {'type': 'string'},
            {'type': 'null'},
        ]
    }
    transformer = TestTransformer(schema_with_null, simplify_nullable_unions=True)
    result = transformer.walk()

    # Should collapse to a single nullable string
    assert result == {'type': 'string', 'nullable': True}

    # Test with simplify_nullable_unions=False (default)
    transformer2 = TestTransformer(schema_with_null, simplify_nullable_unions=False)
    result2 = transformer2.walk()

    # Should keep the anyOf structure
    assert 'anyOf' in result2
    assert len(result2['anyOf']) == 2

    # Test that non-nullable unions are unaffected
    schema_no_null = {
        'anyOf': [
            {'type': 'string'},
            {'type': 'number'},
        ]
    }
    transformer3 = TestTransformer(schema_no_null, simplify_nullable_unions=True)
    result3 = transformer3.walk()

    # Should keep anyOf since it's not nullable
    assert 'anyOf' in result3
    assert len(result3['anyOf']) == 2

# pydantic_ai_slim/pydantic_ai/_json_schema.py:172-189
    def _simplify_nullable_union(cases: list[JsonSchema]) -> list[JsonSchema]:
        # TODO (v2): Remove this method, no longer used
        if len(cases) == 2 and {'type': 'null'} in cases:
            # Find the non-null schema
            non_null_schema = next(
                (item for item in cases if item != {'type': 'null'}),
                None,
            )
            if non_null_schema:
                # Create a new schema based on the non-null part, mark as nullable
                new_schema = deepcopy(non_null_schema)
                new_schema['nullable'] = False
                return [new_schema]
            else:  # pragma: no cover
                # they are both null, so just return one of them
                return [cases[0]]

        return cases

# tests/evals/test_evaluator_common.py:133-141
async def test_contains_as_strings():
    """Test Contains evaluator with as_strings=True."""
    evaluator = Contains(value=42, as_strings=True)

    # Test string conversion
    assert evaluator.evaluate(MockContext(output='The answer is 42')).value is True

    # Test string conversion with non-string types
    assert evaluator.evaluate(MockContext(output=[1, 42, 3])).value is True

# tests/evals/test_llm_as_a_judge.py:44-69
def test_stringify():
    """Test _stringify function."""
    # Test with string
    assert _stringify('test') == 'test'

    # Test with dict
    assert _stringify({'key': 'value'}) == '{"key":"value"}'

    # Test with list
    assert _stringify([1, 2, 3]) == '[1,2,3]'

    # Test with custom object
    class CustomObject:
        def __repr__(self):
            return 'CustomObject()'

    obj = CustomObject()
    assert _stringify(obj) == 'CustomObject()'

    # Test with non-JSON-serializable object
    class NonSerializable:
        def __repr__(self):
            return 'NonSerializable()'

    obj = NonSerializable()
    assert _stringify(obj) == 'NonSerializable()'

# tests/profiles/test_google.py:23-29
def test_const_string_infers_type():
    """When converting const to enum, type should be inferred for string values."""
    schema = {'const': 'hello'}
    transformer = GoogleJsonSchemaTransformer(schema)
    transformed = transformer.walk()

    assert transformed == snapshot({'enum': ['hello'], 'type': 'string'})

# tests/evals/test_evaluator_common.py:74-90
async def test_contains_string():
    """Test Contains evaluator with strings."""
    evaluator = Contains(value='test')

    # Test string containment
    assert evaluator.evaluate(MockContext(output='this is a test')).value is True

    # Test string non-containment
    assert evaluator.evaluate(MockContext(output='no match')) == snapshot(
        EvaluationReason(value=False, reason="Output string 'no match' does not contain expected string 'test'")
    )

    # Test case sensitivity
    evaluator_case_insensitive = Contains(value='TEST', case_sensitive=False)
    assert evaluator_case_insensitive.evaluate(MockContext(output='this is a test')) == snapshot(
        EvaluationReason(value=True)
    )

# pydantic_ai_slim/pydantic_ai/models/test.py:442-455
    def _str_gen(self, schema: dict[str, Any]) -> str:
        """Generate a string from a JSON Schema string."""
        min_len = schema.get('minLength')
        if min_len is not None:
            return self._char() * min_len

        if schema.get('maxLength') == 0:
            return ''

        if fmt := schema.get('format'):
            if fmt == 'date':
                return (date(2024, 1, 1) + timedelta(days=self.seed)).isoformat()

        return self._char()

# tests/test_a2a.py:42-45
def return_string(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
    assert info.output_tools is not None
    args_json = '{"response": ["foo", "bar"]}'
    return ModelResponse(parts=[ToolCallPart(info.output_tools[0].name, args_json)])

# tests/graph/beta/test_edge_cases.py:83-103
async def test_step_with_empty_string():
    """Test handling of empty strings."""
    g = GraphBuilder(state_type=EdgeCaseState, output_type=str)

    @g.step
    async def return_empty(ctx: StepContext[EdgeCaseState, None, None]) -> str:
        return ''

    @g.step
    async def process_empty(ctx: StepContext[EdgeCaseState, None, str]) -> str:
        return ctx.inputs + 'appended'

    g.add(
        g.edge_from(g.start_node).to(return_empty),
        g.edge_from(return_empty).to(process_empty),
        g.edge_from(process_empty).to(g.end_node),
    )

    graph = g.build()
    result = await graph.run(state=EdgeCaseState())
    assert result == 'appended'

# pydantic_ai_slim/pydantic_ai/models/test.py:4-4
import string

# tests/test_tools.py:207-233
def test_docstring_numpy(docstring_format: Literal['numpy', 'auto']):
    agent = Agent(FunctionModel(get_json_schema))
    agent.tool_plain(docstring_format=docstring_format)(numpy_style_docstring)

    result = agent.run_sync('Hello')
    json_schema = json.loads(result.output)
    assert json_schema == snapshot(
        {
            'name': 'numpy_style_docstring',
            'description': 'Numpy style docstring.',
            'parameters_json_schema': {
                'properties': {
                    'foo': {'description': 'The foo thing.', 'type': 'integer'},
                    'bar': {'description': 'The bar thing.', 'type': 'string'},
                },
                'required': ['foo', 'bar'],
                'type': 'object',
                'additionalProperties': False,
            },
            'outer_typed_dict_key': None,
            'strict': None,
            'kind': 'function',
            'sequential': False,
            'metadata': None,
            'timeout': None,
        }
    )

# tests/graph/test_mermaid.py:244-246
def test_docstring_notes_classvar():
    assert Spam.docstring_notes is True
    assert repr(Spam()) == 'Spam()'

# tests/test_tools.py:129-155
def test_docstring_google(docstring_format: Literal['google', 'auto']):
    agent = Agent(FunctionModel(get_json_schema))
    agent.tool_plain(docstring_format=docstring_format)(google_style_docstring)

    result = agent.run_sync('Hello')
    json_schema = json.loads(result.output)
    assert json_schema == snapshot(
        {
            'name': 'google_style_docstring',
            'description': 'Do foobar stuff, a lot.',
            'parameters_json_schema': {
                'properties': {
                    'foo': {'description': 'The foo thing.', 'type': 'integer'},
                    'bar': {'description': 'The bar thing.', 'type': 'string'},
                },
                'required': ['foo', 'bar'],
                'type': 'object',
                'additionalProperties': False,
            },
            'outer_typed_dict_key': None,
            'strict': None,
            'kind': 'function',
            'sequential': False,
            'metadata': None,
            'timeout': None,
        }
    )

# tests/test_tools.py:167-190
def test_docstring_sphinx(docstring_format: Literal['sphinx', 'auto']):
    agent = Agent(FunctionModel(get_json_schema))
    agent.tool_plain(docstring_format=docstring_format)(sphinx_style_docstring)

    result = agent.run_sync('Hello')
    json_schema = json.loads(result.output)
    assert json_schema == snapshot(
        {
            'name': 'sphinx_style_docstring',
            'description': 'Sphinx style docstring.',
            'parameters_json_schema': {
                'properties': {'foo': {'description': 'The foo thing.', 'type': 'integer'}},
                'required': ['foo'],
                'type': 'object',
                'additionalProperties': False,
            },
            'outer_typed_dict_key': None,
            'strict': None,
            'kind': 'function',
            'sequential': False,
            'metadata': None,
            'timeout': None,
        }
    )

# tests/test_tools.py:407-425
def test_docstring_unknown():
    agent = Agent(FunctionModel(get_json_schema))
    agent.tool_plain(unknown_docstring)

    result = agent.run_sync('Hello')
    json_schema = json.loads(result.output)
    assert json_schema == snapshot(
        {
            'name': 'unknown_docstring',
            'description': 'Unknown style docstring.',
            'parameters_json_schema': {'additionalProperties': {'type': 'integer'}, 'properties': {}, 'type': 'object'},
            'outer_typed_dict_key': None,
            'strict': None,
            'kind': 'function',
            'sequential': False,
            'metadata': None,
            'timeout': None,
        }
    )

# pydantic_evals/pydantic_evals/reporting/__init__.py:6-6
from io import StringIO

# pydantic_evals/pydantic_evals/reporting/__init__.py:6-6
from io import StringIO

# pydantic_evals/pydantic_evals/reporting/__init__.py:6-6
from io import StringIO

# pydantic_ai_slim/pydantic_ai/_griffe.py:10-10
from griffe import Docstring, DocstringSectionKind, GoogleOptions, Object as GriffeObject

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:47-61
from ..tools import (
    AgentDepsT,
    BuiltinToolFunc,
    DeferredToolResults,
    DocstringFormat,
    GenerateToolJsonSchema,
    RunContext,
    Tool,
    ToolFuncContext,
    ToolFuncEither,
    ToolFuncPlain,
    ToolParams,
    ToolPrepareFunc,
    ToolsPrepareFunc,
)

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:47-61
from ..tools import (
    AgentDepsT,
    BuiltinToolFunc,
    DeferredToolResults,
    DocstringFormat,
    GenerateToolJsonSchema,
    RunContext,
    Tool,
    ToolFuncContext,
    ToolFuncEither,
    ToolFuncPlain,
    ToolParams,
    ToolPrepareFunc,
    ToolsPrepareFunc,
)

# pydantic_ai_slim/pydantic_ai/agent/__init__.py:47-61
from ..tools import (
    AgentDepsT,
    BuiltinToolFunc,
    DeferredToolResults,
    DocstringFormat,
    GenerateToolJsonSchema,
    RunContext,
    Tool,
    ToolFuncContext,
    ToolFuncEither,
    ToolFuncPlain,
    ToolParams,
    ToolPrepareFunc,
    ToolsPrepareFunc,
)

# tests/models/test_anthropic.py:1961-1965
def test_init_with_provider_string(env: TestEnv):
    env.set('ANTHROPIC_API_KEY', 'env-api-key')
    model = AnthropicModel('claude-3-opus-latest', provider='anthropic')
    assert model.model_name == 'claude-3-opus-latest'
    assert model.client is not None

# tests/models/test_anthropic.py:1961-1965
def test_init_with_provider_string(env: TestEnv):
    env.set('ANTHROPIC_API_KEY', 'env-api-key')
    model = AnthropicModel('claude-3-opus-latest', provider='anthropic')
    assert model.model_name == 'claude-3-opus-latest'
    assert model.client is not None

# pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py:214-222
def _stringify(value: Any) -> str:
    if isinstance(value, str):
        return value
    try:
        # If the value can be serialized to JSON, use that.
        # If that behavior is undesirable, the user could manually call repr on the arguments to the judge_* functions
        return to_json(value).decode()
    except Exception:
        return repr(value)

# tests/test_agent.py:118-119
class StringData(BaseModel):
    text: str

# tests/test_exceptions.py:70-74
def test_tool_retry_error_str_with_string_content():
    """Test that ToolRetryError uses string content as message automatically."""
    part = RetryPromptPart(content='error from tool', tool_name='my_tool')
    error = ToolRetryError(part)
    assert str(error) == 'error from tool'

# tests/test_tools.py:367-373
def only_returns_type() -> str:  # pragma: no cover
    """

    Returns:
        str: The result as a string.
    """
    return 'foo'

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:13-13
from pydantic import ImportString, TypeAdapter, ValidationError

# tests/graph/test_file_persistence.py:27-31
class Float2String(BaseNode):
    input_data: float

    async def run(self, ctx: GraphRunContext) -> String2Length:
        return String2Length(str(self.input_data))

# tests/graph/test_file_persistence.py:27-31
class Float2String(BaseNode):
    input_data: float

    async def run(self, ctx: GraphRunContext) -> String2Length:
        return String2Length(str(self.input_data))

# tests/graph/test_file_persistence.py:27-31
class Float2String(BaseNode):
    input_data: float

    async def run(self, ctx: GraphRunContext) -> String2Length:
        return String2Length(str(self.input_data))

# examples/pydantic_ai_examples/chat_app.py:26-26
from typing_extensions import LiteralString, ParamSpec, TypedDict

# tests/graph/test_file_persistence.py:35-39
class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# tests/graph/test_file_persistence.py:35-39
class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# tests/graph/test_file_persistence.py:35-39
class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# tests/test_embeddings.py:1212-1217
    async def test_model_with_string_provider(self, gemini_api_key: str):
        with patch.dict(os.environ, {'GOOGLE_API_KEY': gemini_api_key}):
            model = GoogleEmbeddingModel('gemini-embedding-001', provider='google-gla')
        assert isinstance(model, GoogleEmbeddingModel)
        assert model.model_name == 'gemini-embedding-001'
        assert model.system == 'google-gla'

# pydantic_ai_slim/pydantic_ai/tools.py:276-276
    docstring_format: DocstringFormat

# tests/test_agent.py:122-146
def test_result_list_of_models_with_stringified_response():
    def return_list(_: list[ModelMessage], info: AgentInfo) -> ModelResponse:
        assert info.output_tools is not None
        # Simulate providers that return the nested payload as a JSON string under "response"
        args_json = json.dumps(
            {
                'response': json.dumps(
                    [
                        {'name': 'John Doe'},
                        {'name': 'Jane Smith'},
                    ]
                )
            }
        )
        return ModelResponse(parts=[ToolCallPart(info.output_tools[0].name, args_json)])

    agent = Agent(FunctionModel(return_list), output_type=list[Person])

    result = agent.run_sync('Hello')
    assert result.output == snapshot(
        [
            Person(name='John Doe'),
            Person(name='Jane Smith'),
        ]
    )

# tests/test_embeddings.py:1087-1092
    def test_model_with_string_provider(self, bedrock_provider: BedrockProvider):
        """Test BedrockEmbeddingModel can be created with string provider."""
        with patch('pydantic_ai.embeddings.bedrock.infer_provider', return_value=bedrock_provider) as mock_infer:
            model = BedrockEmbeddingModel('amazon.titan-embed-text-v2:0', provider='bedrock')
            mock_infer.assert_called_once_with('bedrock')
            assert model.model_name == 'amazon.titan-embed-text-v2:0'

# tests/test_tools.py:443-469
def test_docstring_google_no_body(docstring_format: Literal['google', 'auto']):
    agent = Agent(FunctionModel(get_json_schema))
    agent.tool_plain(docstring_format=docstring_format)(google_style_docstring_no_body)

    result = agent.run_sync('')
    json_schema = json.loads(result.output)
    assert json_schema == snapshot(
        {
            'name': 'google_style_docstring_no_body',
            'description': '',
            'parameters_json_schema': {
                'properties': {
                    'foo': {'description': 'The foo thing.', 'type': 'integer'},
                    'bar': {'description': 'from fields', 'type': 'string'},
                },
                'required': ['foo', 'bar'],
                'type': 'object',
                'additionalProperties': False,
            },
            'outer_typed_dict_key': None,
            'strict': None,
            'kind': 'function',
            'sequential': False,
            'metadata': None,
            'timeout': None,
        }
    )

# pydantic_ai_slim/pydantic_ai/_griffe.py:15-15
DocstringStyle = Literal['google', 'numpy', 'sphinx']

# pydantic_evals/pydantic_evals/evaluators/common.py:75-75
    as_strings: bool = False

# tests/test_ssrf.py:206-211
    def test_path_with_query_string(self) -> None:
        hostname, path, port, is_https = extract_host_and_port('https://example.com/path?query=value')
        assert hostname == 'example.com'
        assert path == '/path?query=value'
        assert port == 443
        assert is_https is True

# pydantic_ai_slim/pydantic_ai/providers/openrouter.py:36-79
class _OpenRouterGoogleJsonSchemaTransformer(JsonSchemaTransformer):
    """Legacy Google JSON schema transformer for OpenRouter compatibility.

    OpenRouter's compatibility layer doesn't fully support modern JSON Schema features
    like $defs/$ref and anyOf for nullable types. This transformer restores v1.19.0
    behavior by inlining definitions and simplifying nullable unions.

    See: https://github.com/pydantic/pydantic-ai/issues/3617
    """

    def __init__(self, schema: JsonSchema, *, strict: bool | None = None):
        super().__init__(schema, strict=strict, prefer_inlined_defs=True, simplify_nullable_unions=True)

    def transform(self, schema: JsonSchema) -> JsonSchema:
        # Remove properties not supported by Gemini
        schema.pop('$schema', None)
        schema.pop('title', None)
        schema.pop('discriminator', None)
        schema.pop('examples', None)
        schema.pop('exclusiveMaximum', None)
        schema.pop('exclusiveMinimum', None)

        if (const := schema.pop('const', None)) is not None:
            schema['enum'] = [const]

        # Convert enums to string type (legacy Gemini requirement)
        if enum := schema.get('enum'):
            schema['type'] = 'string'
            schema['enum'] = [str(val) for val in enum]

        # Convert oneOf to anyOf for discriminated unions
        if 'oneOf' in schema and 'type' not in schema:
            schema['anyOf'] = schema.pop('oneOf')

        # Handle string format -> description
        type_ = schema.get('type')
        if type_ == 'string' and (fmt := schema.pop('format', None)):
            description = schema.get('description')
            if description:
                schema['description'] = f'{description} (format: {fmt})'
            else:
                schema['description'] = f'Format: {fmt}'

        return schema

# tests/test_tools.py:402-404
def unknown_docstring(**kwargs: int) -> str:  # pragma: no cover
    """Unknown style docstring."""
    return str(kwargs)

# tests/ext/test_langchain.py:234-253
def test_simulated_tool_string_input():
    tool = SimulatedLangChainTool(
        name='file_search',
        description='Recursively search for files in a subdirectory that match the regex pattern',
        args={
            'dir_path': {
                'default': '.',
                'description': 'Subdirectory to search in.',
                'title': 'Dir Path',
                'type': 'string',
            },
            'pattern': {
                'description': 'Unix shell regex, where * matches everything.',
                'title': 'Pattern',
                'type': 'string',
            },
        },
    )
    result = tool.run('this string argument')
    assert result == snapshot('I was called with this string argument')

# pydantic_ai_slim/pydantic_ai/models/test.py:523-525
def _get_string_usage(text: str) -> RequestUsage:
    response_tokens = _estimate_string_tokens(text)
    return RequestUsage(output_tokens=response_tokens)

# tests/graph/test_mermaid.py:51-51
    docstring_notes = True

# tests/graph/test_mermaid.py:61-61
    docstring_notes = False

# tests/evals/test_evaluator_spec.py:113-127
def test_evaluator_spec_with_non_string_keys():
    """Test EvaluatorSpec with non-string keys in arguments."""
    # Test with non-string keys in dict
    spec = _SerializedEvaluatorSpec.model_validate({'TestEvaluator': {1: 'value', 2: 'value2'}})
    assert spec.to_evaluator_spec().name == 'TestEvaluator'
    assert spec.to_evaluator_spec().arguments == (
        {1: 'value', 2: 'value2'},
    )  # Should be treated as a single positional argument

    # Test with mixed keys
    spec = _SerializedEvaluatorSpec.model_validate({'TestEvaluator': {'key': 'value', 1: 'value2'}})
    assert spec.to_evaluator_spec().name == 'TestEvaluator'
    assert spec.to_evaluator_spec().arguments == (
        {'key': 'value', 1: 'value2'},
    )  # Should be treated as a single positional argument

# pydantic_ai_slim/pydantic_ai/toolsets/function.py:47-47
    docstring_format: DocstringFormat

# pydantic_graph/pydantic_graph/nodes.py:40-40
    docstring_notes: ClassVar[bool] = False

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# tests/models/test_xai.py:4872-4878
async def test_xai_provider_string_initialization(allow_model_requests: None, monkeypatch: pytest.MonkeyPatch):
    """Test that provider can be initialized with a string."""
    # This test verifies the infer_provider path when provider is a string
    monkeypatch.setenv('XAI_API_KEY', 'test-key-for-coverage')
    m = XaiModel(XAI_NON_REASONING_MODEL, provider='xai')
    assert m.model_name == XAI_NON_REASONING_MODEL
    assert m.system == 'xai'

# pydantic_ai_slim/pydantic_ai/_griffe.py:10-10
from griffe import Docstring, DocstringSectionKind, GoogleOptions, Object as GriffeObject

# pydantic_ai_slim/pydantic_ai/profiles/openai.py:194-204
_STRICT_COMPATIBLE_STRING_FORMATS = [
    'date-time',
    'time',
    'date',
    'duration',
    'email',
    'hostname',
    'ipv4',
    'ipv6',
    'uuid',
]

# pydantic_ai_slim/pydantic_ai/_cli/__init__.py:93-93
_import_string_adapter: TypeAdapter[Any] = TypeAdapter(ImportString)

# pydantic_ai_slim/pydantic_ai/_griffe.py:83-92
def _infer_docstring_style(doc: str) -> DocstringStyle:
    """Simplistic docstring style inference."""
    for pattern, replacements, style in _docstring_style_patterns:
        matches = (
            re.search(pattern.format(replacement), doc, re.IGNORECASE | re.MULTILINE) for replacement in replacements
        )
        if any(matches):
            return style
    # fallback to google style
    return 'google'

# tests/test_tools.py:1114-1148
def test_tool_parameters_with_attribute_docstrings():
    agent = Agent(FunctionModel(get_json_schema))

    class Data(TypedDict):
        a: int
        """The first parameter"""
        b: int
        """The second parameter"""

    @agent.tool_plain
    def get_score(data: Data) -> int: ...  # pragma: no branch

    result = agent.run_sync('Hello')
    json_schema = json.loads(result.output)
    assert json_schema == snapshot(
        {
            'name': 'get_score',
            'description': None,
            'parameters_json_schema': {
                'properties': {
                    'a': {'description': 'The first parameter', 'type': 'integer'},
                    'b': {'description': 'The second parameter', 'type': 'integer'},
                },
                'required': ['a', 'b'],
                'title': 'Data',
                'type': 'object',
            },
            'outer_typed_dict_key': None,
            'strict': None,
            'kind': 'function',
            'sequential': False,
            'metadata': None,
            'timeout': None,
        }
    )

# pydantic_ai_slim/pydantic_ai/models/function.py:412-427
def _estimate_string_tokens(content: str | Sequence[UserContent]) -> int:
    if not content:
        return 0

    if isinstance(content, str):
        return len(_TOKEN_SPLIT_RE.split(content.strip()))

    tokens = 0
    for part in content:
        if isinstance(part, str):
            tokens += len(_TOKEN_SPLIT_RE.split(part.strip()))
        elif isinstance(part, BinaryContent):
            tokens += len(part.data)
        # TODO(Marcelo): We need to study how we can estimate the tokens for AudioUrl or ImageUrl.

    return tokens

# pydantic_ai_slim/pydantic_ai/models/function.py:412-427
def _estimate_string_tokens(content: str | Sequence[UserContent]) -> int:
    if not content:
        return 0

    if isinstance(content, str):
        return len(_TOKEN_SPLIT_RE.split(content.strip()))

    tokens = 0
    for part in content:
        if isinstance(part, str):
            tokens += len(_TOKEN_SPLIT_RE.split(part.strip()))
        elif isinstance(part, BinaryContent):
            tokens += len(part.data)
        # TODO(Marcelo): We need to study how we can estimate the tokens for AudioUrl or ImageUrl.

    return tokens

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# tests/test_ui_web.py:530-539
async def test_get_ui_html_local_file_path_string(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html supports local file paths as strings."""
    # Create a test HTML file
    test_html = b'<html><body>Local UI Content</body></html>'
    local_file = tmp_path / 'custom-ui.html'
    local_file.write_bytes(test_html)

    result = await app_module._get_ui_html(html_source=str(local_file))  # pyright: ignore[reportPrivateUsage]

    assert result == test_html

# pydantic_ai_slim/pydantic_ai/_griffe.py:96-169
_docstring_style_patterns: list[tuple[str, list[str], DocstringStyle]] = [
    (
        r'\n[ \t]*:{0}([ \t]+\w+)*:([ \t]+.+)?\n',
        [
            'param',
            'parameter',
            'arg',
            'argument',
            'key',
            'keyword',
            'type',
            'var',
            'ivar',
            'cvar',
            'vartype',
            'returns',
            'return',
            'rtype',
            'raises',
            'raise',
            'except',
            'exception',
        ],
        'sphinx',
    ),
    (
        r'\n[ \t]*{0}:([ \t]+.+)?\n[ \t]+.+',
        [
            'args',
            'arguments',
            'params',
            'parameters',
            'keyword args',
            'keyword arguments',
            'other args',
            'other arguments',
            'other params',
            'other parameters',
            'raises',
            'exceptions',
            'returns',
            'yields',
            'receives',
            'examples',
            'attributes',
            'functions',
            'methods',
            'classes',
            'modules',
            'warns',
            'warnings',
        ],
        'google',
    ),
    (
        r'\n[ \t]*{0}\n[ \t]*---+\n',
        [
            'deprecated',
            'parameters',
            'other parameters',
            'returns',
            'yields',
            'receives',
            'raises',
            'warns',
            'attributes',
            'functions',
            'methods',
            'classes',
            'modules',
        ],
        'numpy',
    ),
]

# tests/test_mcp.py:2013-2032
def test_load_mcp_servers_with_non_string_values(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test that non-string primitive values (int, bool, null) in nested structures are passed through unchanged."""
    config = tmp_path / 'mcp.json'

    # Create a config with environment variables and extra fields containing primitives
    # The extra fields will be ignored during validation but go through _expand_env_vars
    monkeypatch.setenv('PYTHON_CMD', 'python')
    config.write_text(
        '{"mcpServers": {"my_server": {"command": "${PYTHON_CMD}", "args": ["-m", "tests.mcp_server"], '
        '"metadata": {"count": 42, "enabled": true, "value": null}}}}',
        encoding='utf-8',
    )

    # This should successfully expand env vars and ignore the metadata field
    servers = load_mcp_servers(config)

    assert len(servers) == 1
    server = servers[0]
    assert isinstance(server, MCPServerStdio)
    assert server.command == 'python'

# tests/test_tools.py:158-163
def sphinx_style_docstring(foo: int, /) -> str:  # pragma: no cover
    """Sphinx style docstring.

    :param foo: The foo thing.
    """
    return str(foo)

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# tests/typed_agent.py:11-11
from typing_extensions import assert_type

# tests/test_json_body_serializer.py:69-80
def test_multiline_strings_are_literal(cassette_dict_base: dict[str, Any]):
    """
    Ensure that multi-line JSON strings are emitted as literal blocks in YAML.
    """
    output = serialize(cassette_dict_base)

    # The custom LiteralDumper uses style='|' when it finds '\n' in a string.
    # Often you'll see '|\n' or '|-' in the YAML depending on your exact presenter.
    assert '|\n' in output or '|-' in output, (
        "Expected multi-line string to be represented in literal style ('|' or '|-'), "
        "but didn't find it.\n\nSerialized output:\n" + output
    )

# tests/profiles/test_anthropic.py:62-96
def test_strict_true_schema_with_constraints():
    """With strict=True, schemas with constraints are transformed (constraints moved to description)."""

    class User(BaseModel):
        username: Annotated[str, Field(min_length=3)]
        email: Annotated[str, Field(pattern=r'^[\w\.-]+@[\w\.-]+\.\w+$')]

    original_schema = User.model_json_schema()
    transformer = AnthropicJsonSchemaTransformer(original_schema, strict=True)
    transformed = transformer.walk()

    assert transformer.is_strict_compatible is True
    assert original_schema == snapshot(
        {
            'properties': {
                'username': {'minLength': 3, 'title': 'Username', 'type': 'string'},
                'email': {'pattern': '^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', 'title': 'Email', 'type': 'string'},
            },
            'required': ['username', 'email'],
            'title': 'User',
            'type': 'object',
        }
    )
    # Anthropic's transform_schema() moves unsupported constraints to description
    assert transformed == snapshot(
        {
            'type': 'object',
            'properties': {
                'username': {'type': 'string', 'description': '{minLength: 3}'},
                'email': {'type': 'string', 'description': '{pattern: ^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$}'},
            },
            'additionalProperties': False,
            'required': ['username', 'email'],
        }
    )

# tests/graph/test_file_persistence.py:30-31
    async def run(self, ctx: GraphRunContext) -> String2Length:
        return String2Length(str(self.input_data))

# tests/graph/test_file_persistence.py:30-31
    async def run(self, ctx: GraphRunContext) -> String2Length:
        return String2Length(str(self.input_data))

# tests/test_agent.py:119-119
    text: str

# tests/graph/test_file_persistence.py:30-31
    async def run(self, ctx: GraphRunContext) -> String2Length:
        return String2Length(str(self.input_data))

# tests/graph/test_file_persistence.py:38-39
    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)