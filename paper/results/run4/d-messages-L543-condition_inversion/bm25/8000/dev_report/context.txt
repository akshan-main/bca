# pydantic_ai_slim/pydantic_ai/messages.py:533-549
    def from_path(cls, path: PathLike[str]) -> BinaryContent:
        """Create a `BinaryContent` from a path.

        Defaults to 'application/octet-stream' if the media type cannot be inferred.

        Raises:
            FileNotFoundError: if the file does not exist.
            PermissionError: if the file cannot be read.
        """
        path = Path(path)
        if path.exists():
            raise FileNotFoundError(f'File not found: {path}')
        media_type, _ = _mime_types.guess_type(path)
        if media_type is None:
            media_type = 'application/octet-stream'

        return cls.narrow_type(cls(data=path.read_bytes(), media_type=media_type))

# tests/test_messages.py:692-741
def test_binary_content_from_path(tmp_path: Path):
    # test normal file
    test_xml_file = tmp_path / 'test.xml'
    test_xml_file.write_text('<think>about trains</think>', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_xml_file)
    assert binary_content == snapshot(BinaryContent(data=b'<think>about trains</think>', media_type='application/xml'))

    # test non-existent file
    non_existent_file = tmp_path / 'non-existent.txt'
    with pytest.raises(FileNotFoundError, match='File not found:'):
        BinaryContent.from_path(non_existent_file)

    # test file with unknown media type
    test_unknown_file = tmp_path / 'test.unknownext'
    test_unknown_file.write_text('some content', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_unknown_file)
    assert binary_content == snapshot(BinaryContent(data=b'some content', media_type='application/octet-stream'))

    # test string path
    test_txt_file = tmp_path / 'test.txt'
    test_txt_file.write_text('just some text', encoding='utf-8')
    string_path = test_txt_file.as_posix()
    binary_content = BinaryContent.from_path(string_path)  # pyright: ignore[reportArgumentType]
    assert binary_content == snapshot(BinaryContent(data=b'just some text', media_type='text/plain'))

    # test image file
    test_jpg_file = tmp_path / 'test.jpg'
    test_jpg_file.write_bytes(b'\xff\xd8\xff\xe0' + b'0' * 100)  # minimal JPEG header + padding
    binary_content = BinaryContent.from_path(test_jpg_file)
    assert binary_content == snapshot(
        BinaryImage(data=b'\xff\xd8\xff\xe0' + b'0' * 100, media_type='image/jpeg', _identifier='bc8d49')
    )

    # test yaml file
    test_yaml_file = tmp_path / 'config.yaml'
    test_yaml_file.write_text('key: value', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_yaml_file)
    assert binary_content == snapshot(BinaryContent(data=b'key: value', media_type='application/yaml'))

    # test yml file (alternative extension)
    test_yml_file = tmp_path / 'docker-compose.yml'
    test_yml_file.write_text('version: "3"', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_yml_file)
    assert binary_content == snapshot(BinaryContent(data=b'version: "3"', media_type='application/yaml'))

    # test toml file
    test_toml_file = tmp_path / 'pyproject.toml'
    test_toml_file.write_text('[project]\nname = "test"', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_toml_file)
    assert binary_content == snapshot(BinaryContent(data=b'[project]\nname = "test"', media_type='application/toml'))

# tests/test_ui_web.py:556-562
async def test_get_ui_html_local_file_not_found(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html raises FileNotFoundError for missing local file paths."""
    # Try to use a non-existent local file path
    nonexistent_path = str(tmp_path / 'nonexistent-ui.html')

    with pytest.raises(FileNotFoundError, match='Local UI file not found'):
        await app_module._get_ui_html(html_source=nonexistent_path)  # pyright: ignore[reportPrivateUsage]

# tests/test_ui_web.py:575-583
def test_chat_app_index_file_not_found(tmp_path: Path):
    """Test that index endpoint raises FileNotFoundError for non-existent html_source file."""
    agent = Agent('test')
    nonexistent_file = tmp_path / 'nonexistent-ui.html'
    app = create_web_app(agent, html_source=str(nonexistent_file))

    with TestClient(app, raise_server_exceptions=True) as client:
        with pytest.raises(FileNotFoundError, match='Local UI file not found'):
            client.get('/')

# tests/test_examples.py:99-112
def tmp_path_cwd(tmp_path: Path):
    cwd = os.getcwd()

    root_dir = Path(__file__).parent.parent
    for file in (root_dir / 'tests' / 'example_modules').glob('*.py'):
        shutil.copy(file, tmp_path)
    sys.path.append(str(tmp_path))
    os.chdir(tmp_path)

    try:
        yield tmp_path
    finally:
        os.chdir(cwd)
        sys.path.remove(str(tmp_path))

# tests/graph/test_mermaid.py:244-246
def test_docstring_notes_classvar():
    assert Spam.docstring_notes is True
    assert repr(Spam()) == 'Spam()'

# tests/test_ui_web.py:530-539
async def test_get_ui_html_local_file_path_string(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html supports local file paths as strings."""
    # Create a test HTML file
    test_html = b'<html><body>Local UI Content</body></html>'
    local_file = tmp_path / 'custom-ui.html'
    local_file.write_bytes(test_html)

    result = await app_module._get_ui_html(html_source=str(local_file))  # pyright: ignore[reportPrivateUsage]

    assert result == test_html

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# pydantic_ai_slim/pydantic_ai/mcp.py:1387-1420
def load_mcp_servers(config_path: str | Path) -> list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE]:
    """Load MCP servers from a configuration file.

    Environment variables can be referenced in the configuration file using:
    - `${VAR_NAME}` syntax - expands to the value of VAR_NAME, raises error if not defined
    - `${VAR_NAME:-default}` syntax - expands to VAR_NAME if set, otherwise uses the default value

    Args:
        config_path: The path to the configuration file.

    Returns:
        A list of MCP servers.

    Raises:
        FileNotFoundError: If the configuration file does not exist.
        ValidationError: If the configuration file does not match the schema.
        ValueError: If an environment variable referenced in the configuration is not defined and no default value is provided.
    """
    config_path = Path(config_path)

    if not config_path.exists():
        raise FileNotFoundError(f'Config file {config_path} not found')

    config_data = pydantic_core.from_json(config_path.read_bytes())
    expanded_config_data = _expand_env_vars(config_data)
    config = MCPServerConfig.model_validate(expanded_config_data)

    servers: list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE] = []
    for name, server in config.mcp_servers.items():
        server.id = name
        server.tool_prefix = name
        servers.append(server)

    return servers

# tests/test_ui_web.py:566-572
async def test_get_ui_html_source_instance_not_found(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html raises FileNotFoundError for missing Path instances."""
    # Try to use a non-existent Path instance
    nonexistent_path = tmp_path / 'nonexistent-ui.html'

    with pytest.raises(FileNotFoundError, match='Local UI file not found'):
        await app_module._get_ui_html(html_source=nonexistent_path)  # pyright: ignore[reportPrivateUsage]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/test_cli.py:129-131
def test_agent_flag_bad_module_variable_path(capfd: CaptureFixture[str], mocker: MockerFixture, env: TestEnv):
    assert cli(['--agent', 'bad_path', 'hello']) == 1
    assert 'Could not load agent from bad_path' in capfd.readouterr().out

# tests/test_ui.py:406-422
async def test_event_stream_file():
    async def event_generator():
        yield PartStartEvent(index=0, part=FilePart(content=BinaryImage(data=b'fake', media_type='image/png')))

    request = DummyUIRunInput(messages=[ModelRequest.user_text_prompt('Hello')])
    event_stream = DummyUIEventStream(run_input=request)
    events = [event async for event in event_stream.transform_stream(event_generator())]

    assert events == snapshot(
        [
            '<stream>',
            '<response>',
            "<file media_type='image/png' />",
            '</response>',
            '</stream>',
        ]
    )

# tests/test_ui.py:406-422
async def test_event_stream_file():
    async def event_generator():
        yield PartStartEvent(index=0, part=FilePart(content=BinaryImage(data=b'fake', media_type='image/png')))

    request = DummyUIRunInput(messages=[ModelRequest.user_text_prompt('Hello')])
    event_stream = DummyUIEventStream(run_input=request)
    events = [event async for event in event_stream.transform_stream(event_generator())]

    assert events == snapshot(
        [
            '<stream>',
            '<response>',
            "<file media_type='image/png' />",
            '</response>',
            '</stream>',
        ]
    )

# scripts/check_cassettes.py:37-59
    def pytest_collection_modifyitems(
        self, session: pytest.Session, config: pytest.Config, items: list[pytest.Item]
    ) -> None:
        # prevents pytest.PytestAssertRewriteWarning: Module already imported so cannot be rewritten; pytest_recording
        from pytest_recording.plugin import get_default_cassette_name

        for item in items:
            if not any(item.iter_markers('vcr')):
                continue

            test_file_stem = Path(item.location[0]).stem

            m = item.get_closest_marker('default_cassette')
            if m and m.args:
                self.tests[test_file_stem].add(self._remove_yaml_ext(m.args[0]))
            else:
                self.tests[test_file_stem].add(
                    self._remove_yaml_ext(get_default_cassette_name(getattr(item, 'cls', None), item.name))
                )

            for vm in item.iter_markers('vcr'):
                for arg in vm.args:
                    self.tests[test_file_stem].add(self._remove_yaml_ext(arg))

# tests/conftest.py:307-314
def pytest_addoption(parser: Any) -> None:
    parser.addoption(
        '--xai-proto-include-json',
        action='store_true',
        default=True,
        dest='xai_proto_include_json',
        help='Include JSON representations in xAI proto cassette YAML files.',
    )

# tests/test_ui_web.py:543-552
async def test_get_ui_html_local_file_path_instance(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html supports Path instances."""
    # Create a test HTML file
    test_html = b'<html><body>Path Instance UI</body></html>'
    local_file = tmp_path / 'path-ui.html'
    local_file.write_bytes(test_html)

    result = await app_module._get_ui_html(html_source=local_file)  # pyright: ignore[reportPrivateUsage]

    assert result == test_html

# scripts/check_cassettes.py:79-85
def get_all_tests() -> dict[str, set[str]]:
    """Use pytest collection to get all VCR-marked tests and their cassette names."""
    collector = _CollectVcrTests()
    rc = pytest.main(['--collect-only', '-q', 'tests/'], plugins=[collector])
    if rc not in (pytest.ExitCode.OK, pytest.ExitCode.NO_TESTS_COLLECTED):
        raise SystemExit(rc)
    return dict(collector.tests)

# tests/providers/test_sambanova_provider.py:77-82
def test_mistral_profile():
    provider = SambaNovaProvider(api_key='key')
    # Mistral-based model -> expect mistral profile wrapped in OpenAI compatibility
    profile = provider.model_profile('E5-Mistral-7B-Instruct')
    assert isinstance(profile, OpenAIModelProfile)
    assert profile is not None

# tests/models/test_openai.py:2821-2861
def test_model_profile_strict_not_supported():
    my_tool = ToolDefinition(
        name='my_tool',
        description='This is my tool',
        parameters_json_schema={'type': 'object', 'title': 'Result', 'properties': {'spam': {'type': 'number'}}},
        strict=True,
    )

    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key='foobar'))
    tool_param = m._map_tool_definition(my_tool)  # type: ignore[reportPrivateUsage]

    assert tool_param == snapshot(
        {
            'type': 'function',
            'function': {
                'name': 'my_tool',
                'description': 'This is my tool',
                'parameters': {'type': 'object', 'title': 'Result', 'properties': {'spam': {'type': 'number'}}},
                'strict': True,
            },
        }
    )

    # Some models don't support strict tool definitions
    m = OpenAIChatModel(
        'gpt-4o',
        provider=OpenAIProvider(api_key='foobar'),
        profile=OpenAIModelProfile(openai_supports_strict_tool_definition=False).update(openai_model_profile('gpt-4o')),
    )
    tool_param = m._map_tool_definition(my_tool)  # type: ignore[reportPrivateUsage]

    assert tool_param == snapshot(
        {
            'type': 'function',
            'function': {
                'name': 'my_tool',
                'description': 'This is my tool',
                'parameters': {'type': 'object', 'title': 'Result', 'properties': {'spam': {'type': 'number'}}},
            },
        }
    )

# tests/test_builtin_tools.py:64-69
async def test_builtin_tools_not_supported_file_search_stream(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[FileSearchTool(file_store_ids=['test-id'])])

    with pytest.raises(UserError):
        async with agent.run_stream('Search my files'):
            ...  # pragma: no cover

# tests/test_mcp.py:1984-1994
def test_load_mcp_servers_undefined_env_var(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test that undefined environment variables raise an error."""
    config = tmp_path / 'mcp.json'

    # Make sure the environment variable is not set
    monkeypatch.delenv('UNDEFINED_VAR', raising=False)

    config.write_text('{"mcpServers": {"my_server": {"command": "${UNDEFINED_VAR}", "args": []}}}', encoding='utf-8')

    with pytest.raises(ValueError, match='Environment variable \\$\\{UNDEFINED_VAR\\} is not defined'):
        load_mcp_servers(config)

# tests/providers/test_alibaba_provider.py:69-74
def test_qwen_non_omni_profile_default():
    provider = AlibabaProvider(api_key='key')
    # Non-omni model -> expect default (base64)
    profile = provider.model_profile('qwen-max')
    assert isinstance(profile, OpenAIModelProfile)
    assert profile.openai_chat_audio_input_encoding == 'base64'

# tests/test_mcp.py:1923-1942
def test_load_mcp_servers_with_env_vars(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test environment variable expansion in config files."""
    config = tmp_path / 'mcp.json'

    # Test with environment variables in command
    monkeypatch.setenv('PYTHON_CMD', 'python3')
    monkeypatch.setenv('MCP_MODULE', 'tests.mcp_server')
    config.write_text(
        '{"mcpServers": {"my_server": {"command": "${PYTHON_CMD}", "args": ["-m", "${MCP_MODULE}"]}}}', encoding='utf-8'
    )

    servers = load_mcp_servers(config)

    assert len(servers) == 1
    server = servers[0]
    assert isinstance(server, MCPServerStdio)
    assert server.command == 'python3'
    assert server.args == ['-m', 'tests.mcp_server']
    assert server.id == 'my_server'
    assert server.tool_prefix == 'my_server'

# tests/models/xai_proto_cassettes.py:299-300
    def from_path(cls, path: Path) -> XaiProtoCassetteClient:
        return cls(cassette=XaiProtoCassette.load(path))

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)