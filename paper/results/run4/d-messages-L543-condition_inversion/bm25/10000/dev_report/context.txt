# pydantic_ai_slim/pydantic_ai/messages.py:533-549
    def from_path(cls, path: PathLike[str]) -> BinaryContent:
        """Create a `BinaryContent` from a path.

        Defaults to 'application/octet-stream' if the media type cannot be inferred.

        Raises:
            FileNotFoundError: if the file does not exist.
            PermissionError: if the file cannot be read.
        """
        path = Path(path)
        if path.exists():
            raise FileNotFoundError(f'File not found: {path}')
        media_type, _ = _mime_types.guess_type(path)
        if media_type is None:
            media_type = 'application/octet-stream'

        return cls.narrow_type(cls(data=path.read_bytes(), media_type=media_type))

# tests/test_messages.py:692-741
def test_binary_content_from_path(tmp_path: Path):
    # test normal file
    test_xml_file = tmp_path / 'test.xml'
    test_xml_file.write_text('<think>about trains</think>', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_xml_file)
    assert binary_content == snapshot(BinaryContent(data=b'<think>about trains</think>', media_type='application/xml'))

    # test non-existent file
    non_existent_file = tmp_path / 'non-existent.txt'
    with pytest.raises(FileNotFoundError, match='File not found:'):
        BinaryContent.from_path(non_existent_file)

    # test file with unknown media type
    test_unknown_file = tmp_path / 'test.unknownext'
    test_unknown_file.write_text('some content', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_unknown_file)
    assert binary_content == snapshot(BinaryContent(data=b'some content', media_type='application/octet-stream'))

    # test string path
    test_txt_file = tmp_path / 'test.txt'
    test_txt_file.write_text('just some text', encoding='utf-8')
    string_path = test_txt_file.as_posix()
    binary_content = BinaryContent.from_path(string_path)  # pyright: ignore[reportArgumentType]
    assert binary_content == snapshot(BinaryContent(data=b'just some text', media_type='text/plain'))

    # test image file
    test_jpg_file = tmp_path / 'test.jpg'
    test_jpg_file.write_bytes(b'\xff\xd8\xff\xe0' + b'0' * 100)  # minimal JPEG header + padding
    binary_content = BinaryContent.from_path(test_jpg_file)
    assert binary_content == snapshot(
        BinaryImage(data=b'\xff\xd8\xff\xe0' + b'0' * 100, media_type='image/jpeg', _identifier='bc8d49')
    )

    # test yaml file
    test_yaml_file = tmp_path / 'config.yaml'
    test_yaml_file.write_text('key: value', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_yaml_file)
    assert binary_content == snapshot(BinaryContent(data=b'key: value', media_type='application/yaml'))

    # test yml file (alternative extension)
    test_yml_file = tmp_path / 'docker-compose.yml'
    test_yml_file.write_text('version: "3"', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_yml_file)
    assert binary_content == snapshot(BinaryContent(data=b'version: "3"', media_type='application/yaml'))

    # test toml file
    test_toml_file = tmp_path / 'pyproject.toml'
    test_toml_file.write_text('[project]\nname = "test"', encoding='utf-8')
    binary_content = BinaryContent.from_path(test_toml_file)
    assert binary_content == snapshot(BinaryContent(data=b'[project]\nname = "test"', media_type='application/toml'))

# tests/test_ui_web.py:556-562
async def test_get_ui_html_local_file_not_found(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html raises FileNotFoundError for missing local file paths."""
    # Try to use a non-existent local file path
    nonexistent_path = str(tmp_path / 'nonexistent-ui.html')

    with pytest.raises(FileNotFoundError, match='Local UI file not found'):
        await app_module._get_ui_html(html_source=nonexistent_path)  # pyright: ignore[reportPrivateUsage]

# tests/test_ui_web.py:575-583
def test_chat_app_index_file_not_found(tmp_path: Path):
    """Test that index endpoint raises FileNotFoundError for non-existent html_source file."""
    agent = Agent('test')
    nonexistent_file = tmp_path / 'nonexistent-ui.html'
    app = create_web_app(agent, html_source=str(nonexistent_file))

    with TestClient(app, raise_server_exceptions=True) as client:
        with pytest.raises(FileNotFoundError, match='Local UI file not found'):
            client.get('/')

# tests/test_examples.py:99-112
def tmp_path_cwd(tmp_path: Path):
    cwd = os.getcwd()

    root_dir = Path(__file__).parent.parent
    for file in (root_dir / 'tests' / 'example_modules').glob('*.py'):
        shutil.copy(file, tmp_path)
    sys.path.append(str(tmp_path))
    os.chdir(tmp_path)

    try:
        yield tmp_path
    finally:
        os.chdir(cwd)
        sys.path.remove(str(tmp_path))

# tests/graph/test_mermaid.py:244-246
def test_docstring_notes_classvar():
    assert Spam.docstring_notes is True
    assert repr(Spam()) == 'Spam()'

# tests/test_ui_web.py:530-539
async def test_get_ui_html_local_file_path_string(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html supports local file paths as strings."""
    # Create a test HTML file
    test_html = b'<html><body>Local UI Content</body></html>'
    local_file = tmp_path / 'custom-ui.html'
    local_file.write_bytes(test_html)

    result = await app_module._get_ui_html(html_source=str(local_file))  # pyright: ignore[reportPrivateUsage]

    assert result == test_html

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# clai/update_readme.py:6-6
import pytest

# pydantic_ai_slim/pydantic_ai/mcp.py:1387-1420
def load_mcp_servers(config_path: str | Path) -> list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE]:
    """Load MCP servers from a configuration file.

    Environment variables can be referenced in the configuration file using:
    - `${VAR_NAME}` syntax - expands to the value of VAR_NAME, raises error if not defined
    - `${VAR_NAME:-default}` syntax - expands to VAR_NAME if set, otherwise uses the default value

    Args:
        config_path: The path to the configuration file.

    Returns:
        A list of MCP servers.

    Raises:
        FileNotFoundError: If the configuration file does not exist.
        ValidationError: If the configuration file does not match the schema.
        ValueError: If an environment variable referenced in the configuration is not defined and no default value is provided.
    """
    config_path = Path(config_path)

    if not config_path.exists():
        raise FileNotFoundError(f'Config file {config_path} not found')

    config_data = pydantic_core.from_json(config_path.read_bytes())
    expanded_config_data = _expand_env_vars(config_data)
    config = MCPServerConfig.model_validate(expanded_config_data)

    servers: list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE] = []
    for name, server in config.mcp_servers.items():
        server.id = name
        server.tool_prefix = name
        servers.append(server)

    return servers

# tests/test_ui_web.py:566-572
async def test_get_ui_html_source_instance_not_found(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html raises FileNotFoundError for missing Path instances."""
    # Try to use a non-existent Path instance
    nonexistent_path = tmp_path / 'nonexistent-ui.html'

    with pytest.raises(FileNotFoundError, match='Local UI file not found'):
        await app_module._get_ui_html(html_source=nonexistent_path)  # pyright: ignore[reportPrivateUsage]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/evals/test_dataset.py:60-60
pytestmark = [pytest.mark.skipif(not imports_successful(), reason='pydantic-evals not installed'), pytest.mark.anyio]

# tests/test_cli.py:129-131
def test_agent_flag_bad_module_variable_path(capfd: CaptureFixture[str], mocker: MockerFixture, env: TestEnv):
    assert cli(['--agent', 'bad_path', 'hello']) == 1
    assert 'Could not load agent from bad_path' in capfd.readouterr().out

# tests/test_ui.py:406-422
async def test_event_stream_file():
    async def event_generator():
        yield PartStartEvent(index=0, part=FilePart(content=BinaryImage(data=b'fake', media_type='image/png')))

    request = DummyUIRunInput(messages=[ModelRequest.user_text_prompt('Hello')])
    event_stream = DummyUIEventStream(run_input=request)
    events = [event async for event in event_stream.transform_stream(event_generator())]

    assert events == snapshot(
        [
            '<stream>',
            '<response>',
            "<file media_type='image/png' />",
            '</response>',
            '</stream>',
        ]
    )

# tests/test_ui.py:406-422
async def test_event_stream_file():
    async def event_generator():
        yield PartStartEvent(index=0, part=FilePart(content=BinaryImage(data=b'fake', media_type='image/png')))

    request = DummyUIRunInput(messages=[ModelRequest.user_text_prompt('Hello')])
    event_stream = DummyUIEventStream(run_input=request)
    events = [event async for event in event_stream.transform_stream(event_generator())]

    assert events == snapshot(
        [
            '<stream>',
            '<response>',
            "<file media_type='image/png' />",
            '</response>',
            '</stream>',
        ]
    )

# scripts/check_cassettes.py:37-59
    def pytest_collection_modifyitems(
        self, session: pytest.Session, config: pytest.Config, items: list[pytest.Item]
    ) -> None:
        # prevents pytest.PytestAssertRewriteWarning: Module already imported so cannot be rewritten; pytest_recording
        from pytest_recording.plugin import get_default_cassette_name

        for item in items:
            if not any(item.iter_markers('vcr')):
                continue

            test_file_stem = Path(item.location[0]).stem

            m = item.get_closest_marker('default_cassette')
            if m and m.args:
                self.tests[test_file_stem].add(self._remove_yaml_ext(m.args[0]))
            else:
                self.tests[test_file_stem].add(
                    self._remove_yaml_ext(get_default_cassette_name(getattr(item, 'cls', None), item.name))
                )

            for vm in item.iter_markers('vcr'):
                for arg in vm.args:
                    self.tests[test_file_stem].add(self._remove_yaml_ext(arg))

# tests/conftest.py:307-314
def pytest_addoption(parser: Any) -> None:
    parser.addoption(
        '--xai-proto-include-json',
        action='store_true',
        default=True,
        dest='xai_proto_include_json',
        help='Include JSON representations in xAI proto cassette YAML files.',
    )

# tests/test_ui_web.py:543-552
async def test_get_ui_html_local_file_path_instance(monkeypatch: pytest.MonkeyPatch, tmp_path: Path):
    """Test that _get_ui_html supports Path instances."""
    # Create a test HTML file
    test_html = b'<html><body>Path Instance UI</body></html>'
    local_file = tmp_path / 'path-ui.html'
    local_file.write_bytes(test_html)

    result = await app_module._get_ui_html(html_source=local_file)  # pyright: ignore[reportPrivateUsage]

    assert result == test_html

# scripts/check_cassettes.py:79-85
def get_all_tests() -> dict[str, set[str]]:
    """Use pytest collection to get all VCR-marked tests and their cassette names."""
    collector = _CollectVcrTests()
    rc = pytest.main(['--collect-only', '-q', 'tests/'], plugins=[collector])
    if rc not in (pytest.ExitCode.OK, pytest.ExitCode.NO_TESTS_COLLECTED):
        raise SystemExit(rc)
    return dict(collector.tests)

# tests/providers/test_sambanova_provider.py:77-82
def test_mistral_profile():
    provider = SambaNovaProvider(api_key='key')
    # Mistral-based model -> expect mistral profile wrapped in OpenAI compatibility
    profile = provider.model_profile('E5-Mistral-7B-Instruct')
    assert isinstance(profile, OpenAIModelProfile)
    assert profile is not None

# tests/models/test_openai.py:2821-2861
def test_model_profile_strict_not_supported():
    my_tool = ToolDefinition(
        name='my_tool',
        description='This is my tool',
        parameters_json_schema={'type': 'object', 'title': 'Result', 'properties': {'spam': {'type': 'number'}}},
        strict=True,
    )

    m = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key='foobar'))
    tool_param = m._map_tool_definition(my_tool)  # type: ignore[reportPrivateUsage]

    assert tool_param == snapshot(
        {
            'type': 'function',
            'function': {
                'name': 'my_tool',
                'description': 'This is my tool',
                'parameters': {'type': 'object', 'title': 'Result', 'properties': {'spam': {'type': 'number'}}},
                'strict': True,
            },
        }
    )

    # Some models don't support strict tool definitions
    m = OpenAIChatModel(
        'gpt-4o',
        provider=OpenAIProvider(api_key='foobar'),
        profile=OpenAIModelProfile(openai_supports_strict_tool_definition=False).update(openai_model_profile('gpt-4o')),
    )
    tool_param = m._map_tool_definition(my_tool)  # type: ignore[reportPrivateUsage]

    assert tool_param == snapshot(
        {
            'type': 'function',
            'function': {
                'name': 'my_tool',
                'description': 'This is my tool',
                'parameters': {'type': 'object', 'title': 'Result', 'properties': {'spam': {'type': 'number'}}},
            },
        }
    )

# tests/test_builtin_tools.py:64-69
async def test_builtin_tools_not_supported_file_search_stream(model: Model, allow_model_requests: None):
    agent = Agent(model=model, builtin_tools=[FileSearchTool(file_store_ids=['test-id'])])

    with pytest.raises(UserError):
        async with agent.run_stream('Search my files'):
            ...  # pragma: no cover

# tests/test_mcp.py:1984-1994
def test_load_mcp_servers_undefined_env_var(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test that undefined environment variables raise an error."""
    config = tmp_path / 'mcp.json'

    # Make sure the environment variable is not set
    monkeypatch.delenv('UNDEFINED_VAR', raising=False)

    config.write_text('{"mcpServers": {"my_server": {"command": "${UNDEFINED_VAR}", "args": []}}}', encoding='utf-8')

    with pytest.raises(ValueError, match='Environment variable \\$\\{UNDEFINED_VAR\\} is not defined'):
        load_mcp_servers(config)

# tests/providers/test_alibaba_provider.py:69-74
def test_qwen_non_omni_profile_default():
    provider = AlibabaProvider(api_key='key')
    # Non-omni model -> expect default (base64)
    profile = provider.model_profile('qwen-max')
    assert isinstance(profile, OpenAIModelProfile)
    assert profile.openai_chat_audio_input_encoding == 'base64'

# tests/test_ssrf.py:27-116
class TestIsPrivateIp:
    """Tests for is_private_ip function."""

    @pytest.mark.parametrize(
        'ip',
        [
            # IPv4 loopback
            '127.0.0.1',
            '127.0.0.2',
            '127.255.255.255',
            # IPv4 private class A
            '10.0.0.1',
            '10.255.255.255',
            # IPv4 private class B
            '172.16.0.1',
            '172.31.255.255',
            # IPv4 private class C
            '192.168.0.1',
            '192.168.255.255',
            # IPv4 link-local
            '169.254.0.1',
            '169.254.255.255',
            # IPv4 "this" network
            '0.0.0.0',
            '0.255.255.255',
            # IPv4 CGNAT (RFC 6598)
            '100.64.0.1',
            '100.127.255.255',
            '100.100.100.200',  # Alibaba Cloud metadata
            # IPv6 loopback
            '::1',
            # IPv6 link-local
            'fe80::1',
            'fe80::ffff:ffff:ffff:ffff',
            # IPv6 unique local
            'fc00::1',
            'fdff:ffff:ffff:ffff:ffff:ffff:ffff:ffff',
            # IPv6 6to4 (can embed private IPv4)
            '2002::1',
            '2002:c0a8:0101::1',  # Embeds 192.168.1.1
            '2002:0a00:0001::1',  # Embeds 10.0.0.1
        ],
    )
    def test_private_ips_detected(self, ip: str) -> None:
        assert is_private_ip(ip) is True

    @pytest.mark.parametrize(
        'ip',
        [
            # Public IPv4
            '8.8.8.8',
            '1.1.1.1',
            '203.0.113.50',
            '198.51.100.1',
            # Public IPv6
            '2001:4860:4860::8888',
            '2606:4700:4700::1111',
        ],
    )
    def test_public_ips_allowed(self, ip: str) -> None:
        assert is_private_ip(ip) is False

    @pytest.mark.parametrize(
        'ip',
        [
            # IPv4-mapped IPv6 private addresses
            '::ffff:127.0.0.1',
            '::ffff:10.0.0.1',
            '::ffff:192.168.1.1',
            '::ffff:172.16.0.1',
        ],
    )
    def test_ipv4_mapped_ipv6_private(self, ip: str) -> None:
        assert is_private_ip(ip) is True

    @pytest.mark.parametrize(
        'ip',
        [
            # IPv4-mapped IPv6 public addresses
            '::ffff:8.8.8.8',
            '::ffff:1.1.1.1',
        ],
    )
    def test_ipv4_mapped_ipv6_public(self, ip: str) -> None:
        assert is_private_ip(ip) is False

    def test_invalid_ip_treated_as_private(self) -> None:
        """Invalid IP addresses should be treated as potentially dangerous."""
        assert is_private_ip('not-an-ip') is True
        assert is_private_ip('') is True

# tests/test_mcp.py:1923-1942
def test_load_mcp_servers_with_env_vars(tmp_path: Path, monkeypatch: pytest.MonkeyPatch):
    """Test environment variable expansion in config files."""
    config = tmp_path / 'mcp.json'

    # Test with environment variables in command
    monkeypatch.setenv('PYTHON_CMD', 'python3')
    monkeypatch.setenv('MCP_MODULE', 'tests.mcp_server')
    config.write_text(
        '{"mcpServers": {"my_server": {"command": "${PYTHON_CMD}", "args": ["-m", "${MCP_MODULE}"]}}}', encoding='utf-8'
    )

    servers = load_mcp_servers(config)

    assert len(servers) == 1
    server = servers[0]
    assert isinstance(server, MCPServerStdio)
    assert server.command == 'python3'
    assert server.args == ['-m', 'tests.mcp_server']
    assert server.id == 'my_server'
    assert server.tool_prefix == 'my_server'

# tests/models/xai_proto_cassettes.py:299-300
    def from_path(cls, path: Path) -> XaiProtoCassetteClient:
        return cls(cassette=XaiProtoCassette.load(path))

# docs/.hooks/test_snippets.py:598-602
def test_inject_snippets_nonexistent_file():
    """Test that nonexistent files raise an error.."""
    markdown = '```snippet {path="nonexistent.py"}```'
    with pytest.raises(FileNotFoundError):
        inject_snippets(markdown, REPO_ROOT)

# tests/conftest.py:295-304
def pytest_recording_configure(config: Any, vcr: VCR):
    from . import json_body_serializer

    vcr.register_serializer('yaml', json_body_serializer)

    def method_matcher(r1: vcr_request.Request, r2: vcr_request.Request) -> None:
        if r1.method.upper() != r2.method.upper():
            raise AssertionError(f'{r1.method} != {r2.method}')

    vcr.register_matcher('method', method_matcher)

# tests/models/test_model_function.py:253-267
def test_var_args():
    result = var_args_agent.run_sync('{"function": "get_var_args", "arguments": {"args": [1, 2, 3]}}', deps=123)
    response_data = json.loads(result.output)
    # Can't parse ISO timestamps with trailing 'Z' in older versions of python:
    response_data['timestamp'] = re.sub('Z$', '+00:00', response_data['timestamp'])
    assert response_data == snapshot(
        {
            'tool_name': 'get_var_args',
            'content': '{"args": [1, 2, 3]}',
            'tool_call_id': IsStr(),
            'metadata': None,
            'timestamp': IsStr() & IsNow(iso_string=True, tz=timezone.utc),  # type: ignore[reportUnknownMemberType]
            'part_kind': 'tool-return',
        }
    )

# tests/models/test_xai.py:1589-1599
async def test_xai_binary_content_unknown_media_type_raises(allow_model_requests: None):
    """Cover the unsupported BinaryContent media type branch."""
    response = create_response(content='ok', usage=create_usage(prompt_tokens=1, completion_tokens=1))
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m)

    # Neither image/*, audio/*, nor a known document type => should fail during prompt mapping.
    bc = BinaryContent(b'123', media_type='video/mp4')
    with pytest.raises(RuntimeError, match='Unsupported binary content type: video/mp4'):
        await agent.run(['hello', bc])

# tests/test_ssrf.py:337-342
    async def test_private_ip_blocked_by_default(self) -> None:
        """Test that private IPs are blocked by default."""
        with patch('pydantic_ai._ssrf.run_in_executor') as mock_executor:
            mock_executor.return_value = [(2, 1, 6, '', ('192.168.1.1', 0))]
            with pytest.raises(ValueError, match='Access to private/internal IP address'):
                await validate_and_resolve_url('http://internal.local/path', allow_local=False)

# tests/graph/beta/test_paths.py:100-108
async def test_path_next_path():
    """Test Path.next_path removes first item."""
    items: list[PathItem] = [LabelMarker('first'), LabelMarker('second'), DestinationMarker(NodeID('dest'))]
    path = Path(items=items)

    next_path = path.next_path
    assert len(next_path.items) == 2
    assert next_path.items[0] == items[1]
    assert next_path.items[1] == items[2]

# tests/providers/test_sambanova_provider.py:61-66
def test_qwen_profile():
    provider = SambaNovaProvider(api_key='key')
    # Qwen model -> expect qwen profile wrapped in OpenAI compatibility
    profile = provider.model_profile('Qwen3-32B')
    assert isinstance(profile, OpenAIModelProfile)
    assert profile is not None

# tests/providers/test_together.py:63-100
def test_together_provider_model_profile(mocker: MockerFixture):
    provider = TogetherProvider(api_key='api-key')

    ns = 'pydantic_ai.providers.together'
    deepseek_model_profile_mock = mocker.patch(f'{ns}.deepseek_model_profile', wraps=deepseek_model_profile)
    meta_model_profile_mock = mocker.patch(f'{ns}.meta_model_profile', wraps=meta_model_profile)
    qwen_model_profile_mock = mocker.patch(f'{ns}.qwen_model_profile', wraps=qwen_model_profile)
    mistral_model_profile_mock = mocker.patch(f'{ns}.mistral_model_profile', wraps=mistral_model_profile)
    google_model_profile_mock = mocker.patch(f'{ns}.google_model_profile', wraps=google_model_profile)

    deepseek_profile = provider.model_profile('deepseek-ai/DeepSeek-R1')
    deepseek_model_profile_mock.assert_called_with('deepseek-r1')
    assert deepseek_profile is not None
    assert deepseek_profile.json_schema_transformer == OpenAIJsonSchemaTransformer

    meta_profile = provider.model_profile('meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8')
    meta_model_profile_mock.assert_called_with('llama-4-maverick-17b-128e-instruct-fp8')
    assert meta_profile is not None
    assert meta_profile.json_schema_transformer == InlineDefsJsonSchemaTransformer

    qwen_profile = provider.model_profile('Qwen/QwQ-32B')
    qwen_model_profile_mock.assert_called_with('qwq-32b')
    assert qwen_profile is not None
    assert qwen_profile.json_schema_transformer == InlineDefsJsonSchemaTransformer

    mistral_profile = provider.model_profile('mistralai/Devstral-Small-2505')
    mistral_model_profile_mock.assert_called_with('devstral-small-2505')
    assert mistral_profile is not None
    assert mistral_profile.json_schema_transformer == OpenAIJsonSchemaTransformer

    google_profile = provider.model_profile('google/gemma-3-27b-it')
    google_model_profile_mock.assert_called_with('gemma-3-27b-it')
    assert google_profile is not None
    assert google_profile.json_schema_transformer == GoogleJsonSchemaTransformer

    unknown_profile = provider.model_profile('unknown/model')
    assert unknown_profile is not None
    assert unknown_profile.json_schema_transformer == OpenAIJsonSchemaTransformer

# tests/providers/test_sambanova_provider.py:69-74
def test_llama4_profile():
    provider = SambaNovaProvider(api_key='key')
    # Llama 4 model -> expect meta profile wrapped in OpenAI compatibility
    profile = provider.model_profile('Llama-4-Maverick-17B-128E-Instruct')
    assert isinstance(profile, OpenAIModelProfile)
    assert profile is not None

# tests/providers/test_google_gla.py:23-26
def test_api_key_env_var(env: TestEnv):
    env.set('GEMINI_API_KEY', 'via-env-var')
    provider = GoogleGLAProvider()
    assert 'x-goog-api-key' in dict(provider.client.headers)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)

# pydantic_ai_slim/pydantic_ai/__init__.py:105-111
from .profiles import (
    DEFAULT_PROFILE,
    InlineDefsJsonSchemaTransformer,
    JsonSchemaTransformer,
    ModelProfile,
    ModelProfileSpec,
)