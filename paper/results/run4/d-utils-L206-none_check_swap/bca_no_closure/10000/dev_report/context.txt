## clai/clai/__init__.py

def cli():
    """Run the clai CLI and exit."""
    _cli.cli_exit('clai')

## examples/pydantic_ai_examples/chat_app.py

async def index() -> FileResponse:
    return FileResponse((THIS_DIR / 'chat_app.html'), media_type='text/html')

async def main_ts() -> FileResponse:
    """Get the raw typescript code, it's compiled in the browser, forgive me."""
    return FileResponse((THIS_DIR / 'chat_app.ts'), media_type='text/plain')

## examples/pydantic_ai_examples/data_analyst.py

    def get(self, ref: str) -> pd.DataFrame:
        if ref not in self.output:
            raise ModelRetry(
                f'Error: {ref} is not a valid variable reference. Check the previous messages and try again.'
            )
        return self.output[ref]

## examples/pydantic_ai_examples/evals/agent.py

    now: datetime = field(default_factory=lambda: datetime.now().astimezone())

## examples/pydantic_ai_examples/flight_booking.py

    date: datetime.date

class Deps:
    web_page_text: str
    req_origin: str
    req_destination: str
    req_date: datetime.date

async def buy_tickets(flight_details: FlightDetails, seat: SeatPreference):
    print(f'Purchasing flight {flight_details=!r} {seat=!r}...')

## examples/pydantic_ai_examples/rag.py

    id: int

    def embedding_content(self) -> str:
        return '\n\n'.join((f'path: {self.path}', f'title: {self.title}', self.content))

## examples/pydantic_ai_examples/slack_lead_qualifier/agent.py

async def analyze_profile(profile: Profile) -> Analysis | None:
    result = await agent.run(profile.as_prompt())
    return result.output  ### [/analyze_profile]

## examples/pydantic_ai_examples/slack_lead_qualifier/store.py

    async def add(cls, analysis: Analysis):
        await cls._get_store().put.aio(analysis.profile.email, analysis.model_dump())

    async def list(cls) -> list[Analysis]:
        return [
            Analysis.model_validate(analysis)
            async for analysis in cls._get_store().values.aio()
        ]

    async def clear(cls):
        await cls._get_store().clear.aio()

    def _get_store(cls) -> modal.Dict:
        return modal.Dict.from_name('analyses', create_if_missing=True)  # type: ignore ### [/analysis_store]

## pydantic_ai_slim/pydantic_ai/_agent_graph.py

    usage: _usage.RunUsage = dataclasses.field(default_factory=_usage.RunUsage)

    model_response: _messages.ModelResponse

class _RunMessages:
    messages: list[_messages.ModelMessage]
    used: bool = False

def get_captured_run_messages() -> _RunMessages:
    return _messages_ctx_var.get()

## pydantic_ai_slim/pydantic_ai/_cli/__init__.py

    def __init__(self, special_suggestions: list[str] | None = None):
        super().__init__()
        self.special_suggestions = special_suggestions or []

## pydantic_ai_slim/pydantic_ai/_function_schema.py

def _is_call_ctx(annotation: Any) -> bool:
    """Return whether the annotation is the `RunContext` class, parameterized or not."""
    return annotation is RunContext or get_origin(annotation) is RunContext

## pydantic_ai_slim/pydantic_ai/_json_schema.py

    def transform(self, schema: JsonSchema) -> JsonSchema:
        """Make changes to the schema."""
        return schema

## pydantic_ai_slim/pydantic_ai/_otel_messages.py

    type: Literal['text']

## pydantic_ai_slim/pydantic_ai/_utils.py

class Some(Generic[T]):
    """Analogous to Rust's `Option::Some` type."""

    value: T

def is_set(t_or_unset: T | Unset) -> TypeGuard[T]:
    return t_or_unset is not UNSET

async def group_by_temporal(
    aiterable: AsyncIterable[T], soft_max_interval: float | None
) -> AsyncIterator[AsyncIterable[list[T]]]:
    """Group items from an async iterable into lists based on time interval between them.

    Effectively, this debounces the iterator.

    This returns a context manager usable as an iterator so any pending tasks can be cancelled if an error occurs
    during iteration.

    Usage:

    ```python
    async with group_by_temporal(yield_groups(), 0.1) as groups_iter:
        async for groups in groups_iter:
            print(groups)
    ```

    Args:
        aiterable: The async iterable to group.
        soft_max_interval: Maximum interval over which to group items, this should avoid a trickle of items causing
            a group to never be yielded. It's a soft max in the sense that once we're over this time, we yield items
            as soon as `anext(aiter)` returns. If `None`, no grouping/debouncing is performed

    Returns:
        A context manager usable as an async iterable of lists of items produced by the input async iterable.
    """
    if soft_max_interval is None:

        async def async_iter_groups_noop() -> AsyncIterator[list[T]]:
            async for item in aiterable:
                yield [item]

        yield async_iter_groups_noop()
        return

    # we might wait for the next item more than once, so we store the task to await next time
    task: asyncio.Task[T] | None = None

    async def async_iter_groups() -> AsyncIterator[list[T]]:
        nonlocal task

        assert soft_max_interval is None and soft_max_interval >= 0, 'soft_max_interval must be a positive number'
        buffer: list[T] = []
        group_start_time = time.monotonic()

        aiterator = aiter(aiterable)
        while True:
            if group_start_time is None:
                # group hasn't started, we just wait for the maximum interval
                wait_time = soft_max_interval
            else:
                # wait for the time remaining in the group
                wait_time = soft_max_interval - (time.monotonic() - group_start_time)

            # if there's no current task, we get the next one
            if task is None:
                # anext(aiter) returns an Awaitable[T], not a Coroutine which asyncio.create_task expects
                # so far, this doesn't seem to be a problem
                task = asyncio.create_task(anext(aiterator))  # pyright: ignore[reportArgumentType,reportUnknownVariableType]

            # we use asyncio.wait to avoid cancelling the coroutine if it's not done
            done, _ = await asyncio.wait((task,), timeout=wait_time)

            if done:
                # the one task we waited for completed
                try:
                    item = done.pop().result()
                except StopAsyncIteration:
                    # if the task raised StopAsyncIteration, we're done iterating
                    if buffer:
                        yield buffer
                    task = None
                    break
                else:
                    # we got an item, add it to the buffer and set task to None to get the next item
                    buffer.append(item)
                    task = None
                    # if this is the first item in the group, set the group start time
                    if group_start_time is None:
                        group_start_time = time.monotonic()
            elif buffer:
                # otherwise if the task timeout expired and we have items in the buffer, yield the buffer
                yield buffer
                # clear the buffer and reset the group start time ready for the next group
                buffer = []
                group_start_time = None

    try:
        yield async_iter_groups()
    finally:  # pragma: no cover
        # after iteration if a tasks still exists, cancel it, this will only happen if an error occurred
        if task:
            task.cancel('Cancelling due to error in iterator')
            with suppress(asyncio.CancelledError):
                await task

    def __init__(self, source: AsyncIterable[T]):
        self._source = source
        self._source_iter: AsyncIterator[T] | None = None
        self._buffer: T | Unset = UNSET
        self._exhausted = False

    async def is_exhausted(self) -> bool:
        """Returns True if the stream is exhausted, False otherwise."""
        return isinstance(await self.peek(), Unset)

    def __aiter__(self) -> AsyncIterator[T]:
        # For a single-pass iteration, we can return self as the iterator.
        return self

def get_traceparent(x: AgentRun | AgentRunResult | GraphRun | GraphRunResult) -> str:
    return x._traceparent(required=False) or ''  # type: ignore[reportPrivateUsage]

def number_to_datetime(x: int | float) -> datetime:
    return _datetime_ta.validate_python(x)

def strip_markdown_fences(text: str) -> str:
    if text.startswith('{'):
        return text

    match = re.search(_MARKDOWN_FENCES_PATTERN, text)
    if match:
        return match.group(1)

    return text

## pydantic_ai_slim/pydantic_ai/agent/__init__.py

    instrument: InstrumentationSettings | bool | None

    def name(self, value: str | None) -> None:
        """Set the name of the agent, used for logging."""
        self._name = value

    def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:
        """Optional handler for events from the model's streaming response and the agent's execution of tools."""
        return self._event_stream_handler

## pydantic_ai_slim/pydantic_ai/builtin_tools.py

    def unique_id(self) -> str:
        return ':'.join([self.kind, self.id])

## pydantic_ai_slim/pydantic_ai/concurrency.py

async def _null_context() -> AsyncIterator[None]:
    """A no-op async context manager."""
    yield

## pydantic_ai_slim/pydantic_ai/direct.py

    def _cleanup(self):
        if self._thread and self._thread.is_alive():
            self._thread.join()

    def get(self) -> messages.ModelResponse:
        """Build a ModelResponse from the data received from the stream so far."""
        return self._ensure_stream_ready().get()

## pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp.py

    def visit_and_replace(
        self, visitor: Callable[[AbstractToolset[AgentDepsT]], AbstractToolset[AgentDepsT]]
    ) -> AbstractToolset[AgentDepsT]:
        # DBOS-ified toolsets cannot be swapped out after the fact.
        return self

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_model.py

    def get(self) -> ModelResponse:
        return self.response

## pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py

    def deserialize_run_context(cls, ctx: dict[str, Any], deps: Any) -> TemporalRunContext[Any]:
        """Deserialize the run context from a `dict[str, Any]`."""
        return cls(**ctx, deps=deps)

## pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py

    async def max_input_tokens(self) -> int | None:
        """Get the maximum number of tokens that can be input to the model."""
        return _MAX_INPUT_TOKENS.get(self._handler.model_name, None)

## pydantic_ai_slim/pydantic_ai/embeddings/cohere.py

    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self.model_name)

## pydantic_ai_slim/pydantic_ai/embeddings/google.py

    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self._model_name)

## pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py

    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self.model_name)

## pydantic_ai_slim/pydantic_ai/exceptions.py

class ContentFilterError(UnexpectedModelBehavior):
    """Raised when content filtering is triggered by the model provider resulting in an empty response."""

## pydantic_ai_slim/pydantic_ai/messages.py

    def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:
        return [_otel_messages.TextPart(type='text', **{'content': self.content} if settings.include_content else {})]

    def format(self) -> str:
        """The file format."""
        raise NotImplementedError

    def has_content(self) -> bool:
        """Return `True` if the tool return has content."""
        return self.content is not None  # pragma: no cover

    def user_text_prompt(cls, user_prompt: str, *, instructions: str | None = None) -> ModelRequest:
        """Create a `ModelRequest` with a single user prompt as text."""
        return cls(parts=[UserPromptPart(user_prompt)], instructions=instructions)

## pydantic_ai_slim/pydantic_ai/models/__init__.py

def get_user_agent() -> str:
    """Get the user agent string for the HTTP client."""
    from .. import __version__

    return f'pydantic-ai/{__version__}'

## pydantic_ai_slim/pydantic_ai/models/bedrock.py

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolTypeDef]:
        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]

    def __init__(self, sync_iterator: Iterable[T]):
        self.sync_iterator = iter(sync_iterator)

## pydantic_ai_slim/pydantic_ai/models/cohere.py

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolV2]:
        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]

## pydantic_ai_slim/pydantic_ai/models/fallback.py

    def model_name(self) -> str:
        """The model name."""
        return f'fallback:{",".join(model.model_name for model in self.models)}'

    def system(self) -> str:
        return f'fallback:{",".join(model.system for model in self.models)}'

## pydantic_ai_slim/pydantic_ai/models/gemini.py

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> _GeminiTools | None:
        tools = [_function_from_abstract_tool(t) for t in model_request_parameters.tool_defs.values()]
        return _GeminiTools(function_declarations=tools) if tools else None

class _GeminiContent(TypedDict):
    role: Literal['user', 'model']
    parts: list[_GeminiPartUnion]

class _GeminiTextPart(_BasePart):
    text: str

def _function_call_part_from_call(tool: ToolCallPart) -> _GeminiFunctionCallPart:
    return _GeminiFunctionCallPart(function_call=_GeminiFunctionCall(name=tool.tool_name, args=tool.args_as_dict()))

class _GeminiTextContent(TypedDict):
    role: Literal['user', 'model']
    parts: list[_GeminiTextPart]

## pydantic_ai_slim/pydantic_ai/models/groq.py

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:
        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]

## pydantic_ai_slim/pydantic_ai/models/huggingface.py

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ChatCompletionInputTool]:
        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]

## pydantic_ai_slim/pydantic_ai/models/openai.py

    def _map_usage(self, response: chat.ChatCompletion) -> usage.RequestUsage:
        return _map_usage(response, self._provider.name, self._provider.base_url, self.model_name)

    def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:
        return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]

## pydantic_ai_slim/pydantic_ai/models/openrouter.py

    sort: Literal['price', 'throughput', 'latency']

## pydantic_ai_slim/pydantic_ai/models/test.py

    def generate(self) -> dict[str, Any]:
        """Generate data for the JSON schema."""
        return self._gen_any(self.schema)

## pydantic_ai_slim/pydantic_ai/profiles/__init__.py

    def from_profile(cls, profile: ModelProfile | None) -> Self:
        """Build a ModelProfile subclass instance from a ModelProfile instance."""
        if isinstance(profile, cls):
            return profile
        return cls().update(profile)

## pydantic_ai_slim/pydantic_ai/profiles/openai.py

    def __init__(self, schema: JsonSchema, *, strict: bool | None = None):
        super().__init__(schema, strict=strict)
        self.root_ref = schema.get('$ref')

## pydantic_ai_slim/pydantic_ai/providers/__init__.py

    def model_profile(self, model_name: str) -> ModelProfile | None:
        """The model profile for the named model, if available."""
        return None  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/providers/voyageai.py

    def base_url(self) -> str:
        return self._client._params.get('base_url') or 'https://api.voyageai.com/v1'  # type: ignore

## pydantic_ai_slim/pydantic_ai/retries.py

    sleep: Callable[[int | float], None | Awaitable[None]]

    wait: WaitBaseT

    async def aclose(self) -> None:
        await self.wrapped.aclose()

## pydantic_ai_slim/pydantic_ai/toolsets/_dynamic.py

    def copy(self) -> DynamicToolset[AgentDepsT]:
        """Create a copy of this toolset for use in a new agent run."""
        return DynamicToolset(
            self.toolset_func,
            per_run_step=self.per_run_step,
            id=self._id,
        )

## pydantic_ai_slim/pydantic_ai/toolsets/abstract.py

    def validate_python(
        self, input: Any, *, allow_partial: bool | Literal['off', 'on', 'trailing-strings'] = False, **kwargs: Any
    ) -> Any: ...

## pydantic_ai_slim/pydantic_ai/toolsets/combined.py

    def label(self) -> str:
        return f'{self.__class__.__name__}({", ".join(toolset.label for toolset in self.toolsets)})'  # pragma: no cover

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream.py

    def _tool_return_output(self, part: BaseToolReturnPart) -> Any:
        output = part.model_response_object()
        # Unwrap the return value from the output dictionary if it exists
        return output.get('return_value', output)

## pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py

    def encode(self, sdk_version: int) -> str:
        return self.model_dump_json(by_alias=True, exclude_none=True)

## pydantic_ai_slim/pydantic_ai/usage.py

    def __repr__(self):
        kv_pairs = (f'{f.name}={value!r}' for f in fields(self) if (value := getattr(self, f.name)))
        return f'{self.__class__.__qualname__}({", ".join(kv_pairs)})'

## pydantic_evals/pydantic_evals/otel/span_tree.py

    def find_children(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:
        """Return all immediate children that satisfy the given predicate."""
        return list(self._filter_children(predicate))

    def first_child(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:
        """Return the first immediate child that satisfies the given predicate, or None if none match."""
        return next(self._filter_children(predicate), None)

    def any_child(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if there is at least one child that satisfies the predicate."""
        return self.first_child(predicate) is not None

    def find(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:
        """Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order."""
        return list(self._filter(predicate))

    def first(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:
        """Find the first node that matches a predicate, scanning from each root in DFS order. Returns `None` if not found."""
        return next(self._filter(predicate), None)

    def any(self, predicate: SpanQuery | SpanPredicate) -> bool:
        """Returns True if any node in the tree matches the predicate."""
        return self.first(predicate) is not None

    def __iter__(self) -> Iterator[SpanNode]:
        """Return an iterator over all nodes in the tree."""
        return iter(self.nodes_by_id.values())

## pydantic_graph/pydantic_graph/beta/graph.py

    def _handle_non_fork_edges(self, node: AnyNode, inputs: Any, fork_stack: ForkStack) -> Sequence[GraphTask]:
        edges = self.graph.edges_by_source.get(node.id, [])
        assert len(edges) == 1  # this should have already been ensured during graph building
        return self._handle_path(edges[0], inputs, fork_stack)

## pydantic_graph/pydantic_graph/beta/join.py

    def initial_factory(self):
        return self._initial_factory

## pydantic_graph/pydantic_graph/beta/paths.py

    items: list[PathItem]

## pydantic_graph/pydantic_graph/nodes.py

    def get_node_id(cls) -> str:
        """Get the ID of the node."""
        return cls.__name__

## pydantic_graph/pydantic_graph/persistence/_utils.py

    def _node_discriminator(node_data: Any) -> str:
        return node_data.get('node_id')

## pydantic_graph/pydantic_graph/persistence/in_mem.py

    def dump_json(self, *, indent: int | None = None) -> bytes:
        """Dump the history to JSON bytes."""
        assert self._snapshots_type_adapter is not None, 'type adapter must be set to use `dump_json`'
        return self._snapshots_type_adapter.dump_json(self.history, indent=indent)

## tests/conftest.py

    def set(self, name: str, value: str) -> None:
        self.envars[name] = os.getenv(name)
        os.environ[name] = value

    def remove(self, name: str) -> None:
        self.envars[name] = os.environ.pop(name, None)

    def reset(self) -> None:
        for name, value in self.envars.items():
            if value is None:
                os.environ.pop(name, None)
            else:
                os.environ[name] = value  # pragma: lax no cover

def raise_if_exception(e: Any) -> None:
    if isinstance(e, Exception):
        raise e

## tests/example_modules/fake_database.py

    async def execute(self, query: str) -> list[dict[str, Any]]:
        return [{'id': 123, 'name': 'John Doe'}]

## tests/graph/beta/test_broadcast_and_spread.py

class CounterState:
    values: list[int] = field(default_factory=list[int])

    values: list[int] = field(default_factory=list[int])

## tests/graph/beta/test_graph_iteration.py

class IterState:
    counter: int = 0

## tests/graph/test_file_persistence.py

class String2Length(BaseNode):
    input_data: str

    async def run(self, ctx: GraphRunContext) -> Double:
        return Double(len(self.input_data))

## tests/graph/test_graph.py

    async def run(self, ctx: GraphRunContext) -> String2Length:
        return String2Length(str(self.input_data))

## tests/graph/test_mermaid.py

class Foo(BaseNode):
    async def run(self, ctx: GraphRunContext) -> Bar:
        return Bar()

class Bar(BaseNode[None, None, None]):
    async def run(self, ctx: GraphRunContext) -> End[None]:
        return End(None)

## tests/models/mock_async_stream.py

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

## tests/models/mock_openai.py

    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(completions=completions))

## tests/models/test_anthropic.py

    def create_mock(cls, messages_: MockAnthropicMessage | Sequence[MockAnthropicMessage]) -> AsyncAnthropic:
        return cast(AsyncAnthropic, cls(messages_=messages_))

## tests/models/test_groq.py

def text_chunk(text: str, finish_reason: FinishReason | None = None) -> chat.ChatCompletionChunk:
    return chunk([ChoiceDelta(content=text, role='assistant')], finish_reason=finish_reason)

## tests/models/test_model_test.py

class AgentRunDeps:
    run_id: int

## tests/providers/test_alibaba_provider.py

def test_alibaba_provider_env_key(env: TestEnv):
    env.set('ALIBABA_API_KEY', 'env-key')
    provider = AlibabaProvider()
    assert provider.client.api_key == 'env-key'

## tests/providers/test_google_gla.py

def test_api_key_env_var(env: TestEnv):
    env.set('GEMINI_API_KEY', 'via-env-var')
    provider = GoogleGLAProvider()
    assert 'x-goog-api-key' in dict(provider.client.headers)

## tests/providers/test_sambanova_provider.py

def test_sambanova_provider_env_key(env: TestEnv):
    env.set('SAMBANOVA_API_KEY', 'env-key')
    provider = SambaNovaProvider()
    assert provider.client.api_key == 'env-key'

## tests/test_agent.py

class OutputType(BaseModel):
    """Result type used by multiple tests."""

    value: str

class UserContext:
    location: str | None

## tests/test_dbos.py

async def test_custom_model_settings(allow_model_requests: None, dbos: DBOS):
    result = await settings_dbos_agent.run('Give me those settings')
    assert result.output == snapshot("{'max_tokens': 123, 'custom_setting': 'custom_value'}")

## tests/test_examples.py

def mock_group_by_temporal(aiter: Any, soft_max_interval: float | None) -> Any:
    """Mock group_by_temporal to avoid debouncing, since the iterators above have no delay."""
    return group_by_temporal(aiter, None)

## tests/test_logfire.py

def get_logfire_summary(capfire: CaptureLogfire) -> Callable[[], LogfireSummary]:
    def get_summary() -> LogfireSummary:
        return LogfireSummary(capfire)

    return get_summary

class WeatherInfo(BaseModel):
    temperature: float
    description: str

## tests/test_mcp.py

async def test_agent_with_server_not_running(agent: Agent, allow_model_requests: None):
    result = await agent.run('What is 0 degrees Celsius in Fahrenheit?')
    assert result.output == snapshot('0 degrees Celsius is 32.0 degrees Fahrenheit.')

## tests/test_temporal.py

    async def decode(
        self, payloads: Sequence[temporalio.api.common.v1.Payload]
    ) -> list[temporalio.api.common.v1.Payload]:  # pragma: no cover
        return list(payloads)

## tests/test_ui.py

    def load_messages(cls, messages: Sequence[ModelMessage]) -> list[ModelMessage]:
        return list(messages)

## tests/test_utils.py

pytestmark = pytest.mark.anyio

async def test_group_by_temporal(interval: float | None, expected: list[list[int]]):
    async def yield_groups() -> AsyncIterator[int]:
        yield 1
        await asyncio.sleep(0.02)
        yield 2
        await asyncio.sleep(0.02)
        yield 3
        await asyncio.sleep(0.02)

    async with group_by_temporal(yield_groups(), soft_max_interval=interval) as groups_iter:
        groups: list[list[int]] = [g async for g in groups_iter]
        assert groups == expected

def test_check_object_json_schema():
    object_schema = {'type': 'object', 'properties': {'a': {'type': 'string'}}}
    assert check_object_json_schema(object_schema) == object_schema

    assert check_object_json_schema(
        {
            '$defs': {
                'JsonModel': {
                    'properties': {
                        'type': {'title': 'Type', 'type': 'string'},
                        'items': {'anyOf': [{'type': 'string'}, {'type': 'null'}]},
                    },
                    'required': ['type', 'items'],
                    'title': 'JsonModel',
                    'type': 'object',
                }
            },
            '$ref': '#/$defs/JsonModel',
        }
    ) == {
        'properties': {
            'items': {'anyOf': [{'type': 'string'}, {'type': 'null'}]},
            'type': {'title': 'Type', 'type': 'string'},
        },
        'required': ['type', 'items'],
        'title': 'JsonModel',
        'type': 'object',
    }

    # Can't remove the recursive ref here:
    assert check_object_json_schema(
        {
            '$defs': {
                'JsonModel': {
                    'properties': {
                        'type': {'title': 'Type', 'type': 'string'},
                        'items': {'anyOf': [{'$ref': '#/$defs/JsonModel'}, {'type': 'null'}]},
                    },
                    'required': ['type', 'items'],
                    'title': 'JsonModel',
                    'type': 'object',
                }
            },
            '$ref': '#/$defs/JsonModel',
        }
    ) == {
        '$defs': {
            'JsonModel': {
                'properties': {
                    'items': {'anyOf': [{'$ref': '#/$defs/JsonModel'}, {'type': 'null'}]},
                    'type': {'title': 'Type', 'type': 'string'},
                },
                'required': ['type', 'items'],
                'title': 'JsonModel',
                'type': 'object',
            }
        },
        '$ref': '#/$defs/JsonModel',
    }

    array_schema = {'type': 'array', 'items': {'type': 'string'}}
    with pytest.raises(UserError, match='^Schema must be an object$'):
        check_object_json_schema(array_schema)

async def test_peekable_async_stream(peek_first: bool):
    async_stream = MockAsyncStream(iter([1, 2, 3]))
    peekable_async_stream = PeekableAsyncStream(async_stream)

    items: list[int] = []

    # We need to both peek before starting the stream, and not, to achieve full coverage
    if peek_first:
        assert not await peekable_async_stream.is_exhausted()
        assert await peekable_async_stream.peek() == 1

    async for item in peekable_async_stream:
        items.append(item)

        # The next line is included mostly for the sake of achieving coverage
        assert await peekable_async_stream.peek() == (item + 1 if item < 3 else UNSET)

    assert await peekable_async_stream.is_exhausted()
    assert await peekable_async_stream.peek() is UNSET
    assert items == [1, 2, 3]

def test_package_versions(capsys: pytest.CaptureFixture[str]):
    if os.getenv('CI'):
        with capsys.disabled():  # pragma: lax no cover
            print('\npackage versions:')
            packages = sorted((package.metadata['Name'], package.version) for package in distributions())
            for name, version in packages:
                print(f'{name:30} {version}')

async def test_run_in_executor_with_contextvars() -> None:
    ctx_var = contextvars.ContextVar('test_var', default='default')
    ctx_var.set('original_value')

    result = await run_in_executor(ctx_var.get)
    assert result == ctx_var.get()

    ctx_var.set('new_value')
    result = await run_in_executor(ctx_var.get)
    assert result == ctx_var.get()

    # show that the old version did not work
    old_result = asyncio.get_running_loop().run_in_executor(None, ctx_var.get)
    assert old_result != ctx_var.get()

async def test_run_in_executor_with_disable_threads() -> None:
    from pydantic_ai._utils import disable_threads

    calls: list[str] = []

    def sync_func() -> str:
        calls.append('called')
        return 'result'

    # Without disable_threads, should use threading
    result = await run_in_executor(sync_func)
    assert result == 'result'
    assert calls == ['called']

    # With disable_threads enabled, should execute directly
    calls.clear()
    with disable_threads():
        result = await run_in_executor(sync_func)
        assert result == 'result'
        assert calls == ['called']

def test_is_async_callable():
    def sync_func(): ...  # pragma: no branch

    assert is_async_callable(sync_func) is False

    async def async_func(): ...  # pragma: no branch

    assert is_async_callable(async_func) is True

    class AsyncCallable:
        async def __call__(self): ...  # pragma: no branch

    partial_async_callable = functools.partial(AsyncCallable())
    assert is_async_callable(partial_async_callable) is True

def test_merge_json_schema_defs():
    foo_bar_schema = {
        '$defs': {
            'Bar': {
                'description': 'Bar description',
                'properties': {'bar': {'type': 'string'}},
                'required': ['bar'],
                'title': 'Bar',
                'type': 'object',
            },
            'Foo': {
                'description': 'Foo description',
                'properties': {'foo': {'type': 'string'}},
                'required': ['foo'],
                'title': 'Foo',
                'type': 'object',
            },
        },
        'properties': {'foo': {'$ref': '#/$defs/Foo'}, 'bar': {'$ref': '#/$defs/Bar'}},
        'required': ['foo', 'bar'],
        'type': 'object',
        'title': 'FooBar',
    }

    foo_bar_baz_schema = {
        '$defs': {
            'Baz': {
                'description': 'Baz description',
                'properties': {'baz': {'type': 'string'}},
                'required': ['baz'],
                'title': 'Baz',
                'type': 'object',
            },
            'Foo': {
                'description': 'Foo description. Note that this is different from the Foo in foo_bar_schema!',
                'properties': {'foo': {'type': 'int'}},
                'required': ['foo'],
                'title': 'Foo',
                'type': 'object',
            },
            'Bar': {
                'description': 'Bar description',
                'properties': {'bar': {'type': 'string'}},
                'required': ['bar'],
                'title': 'Bar',
                'type': 'object',
            },
        },
        'properties': {'foo': {'$ref': '#/$defs/Foo'}, 'baz': {'$ref': '#/$defs/Baz'}, 'bar': {'$ref': '#/$defs/Bar'}},
        'required': ['foo', 'baz', 'bar'],
        'type': 'object',
        'title': 'FooBarBaz',
    }

    # A schema with no title that will cause numeric suffixes
    no_title_schema = {
        '$defs': {
            'Foo': {
                'description': 'Another different Foo',
                'properties': {'foo': {'type': 'boolean'}},
                'required': ['foo'],
                'title': 'Foo',
                'type': 'object',
            },
            'Bar': {
                'description': 'Another different Bar',
                'properties': {'bar': {'type': 'number'}},
                'required': ['bar'],
                'title': 'Bar',
                'type': 'object',
            },
        },
        'properties': {'foo': {'$ref': '#/$defs/Foo'}, 'bar': {'$ref': '#/$defs/Bar'}},
        'required': ['foo', 'bar'],
        'type': 'object',
    }

    # Another schema with no title that will cause more numeric suffixes
    another_no_title_schema = {
        '$defs': {
            'Foo': {
                'description': 'Yet another different Foo',
                'properties': {'foo': {'type': 'array'}},
                'required': ['foo'],
                'title': 'Foo',
                'type': 'object',
            },
            'Bar': {
                'description': 'Yet another different Bar',
                'properties': {'bar': {'type': 'object'}},
                'required': ['bar'],
                'title': 'Bar',
                'type': 'object',
            },
        },
        'properties': {'foo': {'$ref': '#/$defs/Foo'}, 'bar': {'$ref': '#/$defs/Bar'}},
        'required': ['foo', 'bar'],
        'type': 'object',
    }

    # Schema with nested properties, array items, prefixItems, and anyOf/oneOf
    complex_schema = {
        '$defs': {
            'Nested': {
                'description': 'A nested type',
                'properties': {'nested': {'type': 'string'}},
                'required': ['nested'],
                'title': 'Nested',
                'type': 'object',
            },
            'ArrayItem': {
                'description': 'An array item type',
                'properties': {'item': {'type': 'string'}},
                'required': ['item'],
                'title': 'ArrayItem',
                'type': 'object',
            },
            'UnionType': {
                'description': 'A union type',
                'properties': {'union': {'type': 'string'}},
                'required': ['union'],
                'title': 'UnionType',
                'type': 'object',
            },
        },
        'properties': {
            'nested_props': {
                'type': 'object',
                'properties': {
                    'deep_nested': {'$ref': '#/$defs/Nested'},
                },
            },
            'array_with_items': {
                'type': 'array',
                'items': {'$ref': '#/$defs/ArrayItem'},
            },
            'array_with_prefix': {
                'type': 'array',
                'prefixItems': [
                    {'$ref': '#/$defs/ArrayItem'},
                    {'$ref': '#/$defs/Nested'},
                ],
            },
            'union_anyOf': {
                'anyOf': [
                    {'$ref': '#/$defs/UnionType'},
                    {'$ref': '#/$defs/Nested'},
                ],
            },
            'union_oneOf': {
                'oneOf': [
                    {'$ref': '#/$defs/UnionType'},
                    {'$ref': '#/$defs/ArrayItem'},
                ],
            },
        },
        'type': 'object',
        'title': 'ComplexSchema',
    }

    schemas = [foo_bar_schema, foo_bar_baz_schema, no_title_schema, another_no_title_schema, complex_schema]
    rewritten_schemas, all_defs = merge_json_schema_defs(schemas)
    assert all_defs == snapshot(
        {
            'Bar': {
                'description': 'Bar description',
                'properties': {'bar': {'type': 'string'}},
                'required': ['bar'],
                'title': 'Bar',
                'type': 'object',
            },
            'Foo': {
                'description': 'Foo description',
                'properties': {'foo': {'type': 'string'}},
                'required': ['foo'],
                'title': 'Foo',
                'type': 'object',
            },
            'Baz': {
                'description': 'Baz description',
                'properties': {'baz': {'type': 'string'}},
                'required': ['baz'],
                'title': 'Baz',
                'type': 'object',
            },
            'FooBarBaz_Foo_1': {
                'description': 'Foo description. Note that this is different from the Foo in foo_bar_schema!',
                'properties': {'foo': {'type': 'int'}},
                'required': ['foo'],
                'title': 'Foo',
                'type': 'object',
            },
            'Foo_1': {
                'description': 'Another different Foo',
                'properties': {'foo': {'type': 'boolean'}},
                'required': ['foo'],
                'title': 'Foo',
                'type': 'object',
            },
            'Bar_1': {
                'description': 'Another different Bar',
                'properties': {'bar': {'type': 'number'}},
                'required': ['bar'],
                'title': 'Bar',
                'type': 'object',
            },
            'Foo_2': {
                'description': 'Yet another different Foo',
                'properties': {'foo': {'type': 'array'}},
                'required': ['foo'],
                'title': 'Foo',
                'type': 'object',
            },
            'Bar_2': {
                'description': 'Yet another different Bar',
                'properties': {'bar': {'type': 'object'}},
                'required': ['bar'],
                'title': 'Bar',
                'type': 'object',
            },
            'Nested': {
                'description': 'A nested type',
                'properties': {'nested': {'type': 'string'}},
                'required': ['nested'],
                'title': 'Nested',
                'type': 'object',
            },
            'ArrayItem': {
                'description': 'An array item type',
                'properties': {'item': {'type': 'string'}},
                'required': ['item'],
                'title': 'ArrayItem',
                'type': 'object',
            },
            'UnionType': {
                'description': 'A union type',
                'properties': {'union': {'type': 'string'}},
                'required': ['union'],
                'title': 'UnionType',
                'type': 'object',
            },
        }
    )
    assert rewritten_schemas == snapshot(
        [
            {
                'properties': {'foo': {'$ref': '#/$defs/Foo'}, 'bar': {'$ref': '#/$defs/Bar'}},
                'required': ['foo', 'bar'],
                'type': 'object',
                'title': 'FooBar',
            },
            {
                'properties': {
                    'foo': {'$ref': '#/$defs/FooBarBaz_Foo_1'},
                    'baz': {'$ref': '#/$defs/Baz'},
                    'bar': {'$ref': '#/$defs/Bar'},
                },
                'required': ['foo', 'baz', 'bar'],
                'type': 'object',
                'title': 'FooBarBaz',
            },
            {
                'properties': {'foo': {'$ref': '#/$defs/Foo_1'}, 'bar': {'$ref': '#/$defs/Bar_1'}},
                'required': ['foo', 'bar'],
                'type': 'object',
            },
            {
                'properties': {'foo': {'$ref': '#/$defs/Foo_2'}, 'bar': {'$ref': '#/$defs/Bar_2'}},
                'required': ['foo', 'bar'],
                'type': 'object',
            },
            {
                'properties': {
                    'nested_props': {
                        'type': 'object',
                        'properties': {
                            'deep_nested': {'$ref': '#/$defs/Nested'},
                        },
                    },
                    'array_with_items': {
                        'type': 'array',
                        'items': {'$ref': '#/$defs/ArrayItem'},
                    },
                    'array_with_prefix': {
                        'type': 'array',
                        'prefixItems': [
                            {'$ref': '#/$defs/ArrayItem'},
                            {'$ref': '#/$defs/Nested'},
                        ],
                    },
                    'union_anyOf': {
                        'anyOf': [
                            {'$ref': '#/$defs/UnionType'},
                            {'$ref': '#/$defs/Nested'},
                        ],
                    },
                    'union_oneOf': {
                        'oneOf': [
                            {'$ref': '#/$defs/UnionType'},
                            {'$ref': '#/$defs/ArrayItem'},
                        ],
                    },
                },
                'type': 'object',
                'title': 'ComplexSchema',
            },
        ]
    )

def test_strip_markdown_fences():
    assert strip_markdown_fences('{"foo": "bar"}') == '{"foo": "bar"}'
    assert strip_markdown_fences('```json\n{"foo": "bar"}\n```') == '{"foo": "bar"}'
    assert strip_markdown_fences('```json\n{\n  "foo": "bar"\n}') == '{\n  "foo": "bar"\n}'
    assert (
        strip_markdown_fences('{"foo": "```json\\n{"foo": "bar"}\\n```"}')
        == '{"foo": "```json\\n{"foo": "bar"}\\n```"}'
    )
    assert (
        strip_markdown_fences('Here is some beautiful JSON:\n\n```\n{"foo": "bar"}\n``` Nice right?')
        == '{"foo": "bar"}'
    )
    assert strip_markdown_fences('No JSON to be found') == 'No JSON to be found'

def test_validate_empty_kwargs_empty():
    """Test that empty dict passes validation."""
    validate_empty_kwargs({})

def test_validate_empty_kwargs_with_unknown():
    """Test that unknown kwargs raise UserError."""
    with pytest.raises(UserError, match='Unknown keyword arguments: `unknown_arg`'):
        validate_empty_kwargs({'unknown_arg': 'value'})

def test_validate_empty_kwargs_multiple_unknown():
    """Test that multiple unknown kwargs are properly formatted."""
    with pytest.raises(UserError, match='Unknown keyword arguments: `arg1`, `arg2`'):
        validate_empty_kwargs({'arg1': 'value1', 'arg2': 'value2'})

def test_validate_empty_kwargs_message_format():
    """Test that the error message format matches expected pattern."""
    with pytest.raises(UserError) as exc_info:
        validate_empty_kwargs({'test_arg': 'test_value'})

    assert 'Unknown keyword arguments: `test_arg`' in str(exc_info.value)

def test_validate_empty_kwargs_preserves_order():
    """Test that multiple kwargs preserve order in error message."""
    kwargs = {'first': '1', 'second': '2', 'third': '3'}
    with pytest.raises(UserError) as exc_info:
        validate_empty_kwargs(kwargs)

    error_msg = str(exc_info.value)
    assert '`first`' in error_msg
    assert '`second`' in error_msg
    assert '`third`' in error_msg

## tests/typed_graph.py

def use_double(node: BaseNode[None, None, X]) -> None:
    """Shoe that `Double` is valid as a `BaseNode[None, int, X]`."""
    print(node)
