# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/providers/test_huggingface.py:4-4
from unittest.mock import MagicMock, Mock, patch

# tests/test_function_schema.py:3-3
from unittest.mock import Mock

# tests/test_tenacity.py:9-9
from unittest.mock import AsyncMock, Mock

# tests/test_tenacity.py:32-48
    def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.__enter__ = Mock(return_value=mock_transport)
        mock_transport.__exit__ = Mock(return_value=None)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with transport:
            result = transport.handle_request(request)

        assert result is mock_response
        mock_transport.handle_request.assert_called_once_with(request)

# pydantic_ai_slim/pydantic_ai/retries.py:168-197
    def handle_request(self, request: Request) -> Response:
        """Handle an HTTP request with retry logic.

        Args:
            request: The HTTP request to handle.

        Returns:
            The HTTP response.

        Raises:
            RuntimeError: If the retry controller did not make any attempts.
            Exception: Any exception raised by the wrapped transport or validation function.
        """

        @retry(**self.config)
        def handle_request(req: Request) -> Response:
            response = self.wrapped.handle_request(req)

            # this is normally set by httpx _after_ calling this function, but we want the request in the validator:
            response.request = req

            if self.validate_response:
                try:
                    self.validate_response(response)
                except Exception:
                    response.close()
                    raise
            return response

        return handle_request(request)

# tests/test_tenacity.py:192-206
    async def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_async_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = AsyncTenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        async with transport:
            result = await transport.handle_async_request(request)

        assert result is mock_response
        mock_transport.handle_async_request.assert_called_once_with(request)

# tests/test_tenacity.py:29-186
class TestTenacityTransport:
    """Tests for the synchronous TenacityTransport."""

    def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.__enter__ = Mock(return_value=mock_transport)
        mock_transport.__exit__ = Mock(return_value=None)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with transport:
            result = transport.handle_request(request)

        assert result is mock_response
        mock_transport.handle_request.assert_called_once_with(request)

    def test_retry_on_exception(self):
        """Test that exceptions trigger retries."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response = Mock(spec=httpx.Response)

        # Fail twice, succeed on third attempt
        mock_transport.handle_request.side_effect = [
            httpx.ConnectError('Connection failed'),
            httpx.ConnectError('Connection failed again'),
            mock_response,
        ]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),  # Very short wait for tests
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response
        assert mock_transport.handle_request.call_count == 3

    def test_retry_exhausted(self):
        """Test that retry exhaustion re-raises the last exception."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.handle_request.side_effect = httpx.ConnectError('Connection failed')

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(2),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with pytest.raises(httpx.ConnectError, match='Connection failed'):
            transport.handle_request(request)

        assert mock_transport.handle_request.call_count == 2

    def test_validate_response_success(self):
        """Test that validate_response is called and doesn't raise."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response = Mock(spec=httpx.Response)
        mock_response.status_code = 200
        mock_transport.handle_request.return_value = mock_response

        validate_response = Mock()
        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport, validate_response)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response
        validate_response.assert_called_once_with(mock_response)

    def test_validate_response_triggers_retry(self):
        """Test that validate_response can trigger retries."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response_fail = Mock(spec=httpx.Response)
        mock_response_fail.status_code = 429
        mock_response_success = Mock(spec=httpx.Response)
        mock_response_success.status_code = 200

        mock_transport.handle_request.side_effect = [mock_response_fail, mock_response_success]

        def validate_response(response: httpx.Response):
            if response.status_code == 429:
                raise httpx.HTTPStatusError('Rate limited', request=request, response=response)

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.HTTPStatusError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport, validate_response)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response_success
        assert mock_transport.handle_request.call_count == 2

    def test_raise_for_status_in_validate_response(self):
        """Test that response.raise_for_status() works in validate_response callback."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response_fail = Mock(spec=httpx.Response)
        mock_response_fail.status_code = 429
        mock_response_fail.is_success = False
        mock_response_fail.is_error = True
        mock_response_fail.request = None  # Initially None, will be set by transport

        # Mock raise_for_status to check if request is set
        def mock_raise_for_status():
            if mock_response_fail.request is None:
                raise RuntimeError(  # pragma: no cover
                    'Cannot call `raise_for_status` as the request instance has not been set on this response.'
                )
            raise httpx.HTTPStatusError(
                'Too Many Requests', request=mock_response_fail.request, response=mock_response_fail
            )

        mock_response_fail.raise_for_status = mock_raise_for_status

        mock_response_success = Mock(spec=httpx.Response)
        mock_response_success.status_code = 200
        mock_response_success.is_success = True
        mock_response_success.is_error = False
        mock_response_success.raise_for_status = Mock()  # Should not raise

        mock_transport.handle_request.side_effect = [mock_response_fail, mock_response_success]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.HTTPStatusError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(
            config, mock_transport, validate_response=lambda response: response.raise_for_status()
        )

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response_success
        assert mock_transport.handle_request.call_count == 2
        # Verify that the request was set on the failed response before raise_for_status was called
        assert mock_response_fail.request is request
        mock_response_success.raise_for_status.assert_called_once()

# pydantic_ai_slim/pydantic_ai/messages.py:690-690
    return_value: ToolReturnContent

# examples/pydantic_ai_examples/chat_app.py:25-25
from fastapi.responses import FileResponse, Response, StreamingResponse

# examples/pydantic_ai_examples/sql_gen.py:93-93
Response: TypeAlias = Success | InvalidRequest

# pydantic_ai_slim/pydantic_ai/retries.py:18-26
from httpx import (
    AsyncBaseTransport,
    AsyncHTTPTransport,
    BaseTransport,
    HTTPStatusError,
    HTTPTransport,
    Request,
    Response,
)

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:10-10
from starlette.responses import JSONResponse, Response

# tests/test_dbos.py:193-194
class Response:
    answers: list[Answer]

# tests/test_prefect.py:180-181
class Response:
    answers: list[Answer]

# tests/test_temporal.py:274-275
class Response:
    answers: list[Answer]

# pydantic_ai_slim/pydantic_ai/retries.py:117-212
class TenacityTransport(BaseTransport):
    """Synchronous HTTP transport with tenacity-based retry functionality.

    This transport wraps another BaseTransport and adds retry capabilities using the tenacity library.
    It can be configured to retry requests based on various conditions such as specific exception types,
    response status codes, or custom validation logic.

    The transport works by intercepting HTTP requests and responses, allowing the tenacity controller
    to determine when and how to retry failed requests. The validate_response function can be used
    to convert HTTP responses into exceptions that trigger retries.

    Args:
        wrapped: The underlying transport to wrap and add retry functionality to.
        config: The arguments to use for the tenacity `retry` decorator, including retry conditions,
            wait strategy, stop conditions, etc. See the tenacity docs for more info.
        validate_response: Optional callable that takes a Response and can raise an exception
            to be handled by the controller if the response should trigger a retry.
            Common use case is to raise exceptions for certain HTTP status codes.
            If None, no response validation is performed.

    Example:
        ```python
        from httpx import Client, HTTPStatusError, HTTPTransport
        from tenacity import retry_if_exception_type, stop_after_attempt

        from pydantic_ai.retries import RetryConfig, TenacityTransport, wait_retry_after

        transport = TenacityTransport(
            RetryConfig(
                retry=retry_if_exception_type(HTTPStatusError),
                wait=wait_retry_after(max_wait=300),
                stop=stop_after_attempt(5),
                reraise=True
            ),
            HTTPTransport(),
            validate_response=lambda r: r.raise_for_status()
        )
        client = Client(transport=transport)
        ```
    """

    def __init__(
        self,
        config: RetryConfig,
        wrapped: BaseTransport | None = None,
        validate_response: Callable[[Response], Any] | None = None,
    ):
        self.config = config
        self.wrapped = wrapped and HTTPTransport()
        self.validate_response = validate_response

    def handle_request(self, request: Request) -> Response:
        """Handle an HTTP request with retry logic.

        Args:
            request: The HTTP request to handle.

        Returns:
            The HTTP response.

        Raises:
            RuntimeError: If the retry controller did not make any attempts.
            Exception: Any exception raised by the wrapped transport or validation function.
        """

        @retry(**self.config)
        def handle_request(req: Request) -> Response:
            response = self.wrapped.handle_request(req)

            # this is normally set by httpx _after_ calling this function, but we want the request in the validator:
            response.request = req

            if self.validate_response:
                try:
                    self.validate_response(response)
                except Exception:
                    response.close()
                    raise
            return response

        return handle_request(request)

    def __enter__(self) -> TenacityTransport:
        self.wrapped.__enter__()
        return self

    def __exit__(
        self,
        exc_type: type[BaseException] | None = None,
        exc_value: BaseException | None = None,
        traceback: TracebackType | None = None,
    ) -> None:
        self.wrapped.__exit__(exc_type, exc_value, traceback)

    def close(self) -> None:
        self.wrapped.close()  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/direct.py:385-387
    def response(self) -> messages.ModelResponse:
        """Get the current state of the response."""
        return self.get()

# pydantic_ai_slim/pydantic_ai/result.py:157-159
    def response(self) -> _messages.ModelResponse:
        """Get the current state of the response."""
        return self.get()

# pydantic_ai_slim/pydantic_ai/retries.py:215-309
class AsyncTenacityTransport(AsyncBaseTransport):
    """Asynchronous HTTP transport with tenacity-based retry functionality.

    This transport wraps another AsyncBaseTransport and adds retry capabilities using the tenacity library.
    It can be configured to retry requests based on various conditions such as specific exception types,
    response status codes, or custom validation logic.

    The transport works by intercepting HTTP requests and responses, allowing the tenacity controller
    to determine when and how to retry failed requests. The validate_response function can be used
    to convert HTTP responses into exceptions that trigger retries.

    Args:
        wrapped: The underlying async transport to wrap and add retry functionality to.
        config: The arguments to use for the tenacity `retry` decorator, including retry conditions,
            wait strategy, stop conditions, etc. See the tenacity docs for more info.
        validate_response: Optional callable that takes a Response and can raise an exception
            to be handled by the controller if the response should trigger a retry.
            Common use case is to raise exceptions for certain HTTP status codes.
            If None, no response validation is performed.

    Example:
        ```python
        from httpx import AsyncClient, HTTPStatusError
        from tenacity import retry_if_exception_type, stop_after_attempt

        from pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after

        transport = AsyncTenacityTransport(
            RetryConfig(
                retry=retry_if_exception_type(HTTPStatusError),
                wait=wait_retry_after(max_wait=300),
                stop=stop_after_attempt(5),
                reraise=True
            ),
            validate_response=lambda r: r.raise_for_status()
        )
        client = AsyncClient(transport=transport)
        ```
    """

    def __init__(
        self,
        config: RetryConfig,
        wrapped: AsyncBaseTransport | None = None,
        validate_response: Callable[[Response], Any] | None = None,
    ):
        self.config = config
        self.wrapped = wrapped or AsyncHTTPTransport()
        self.validate_response = validate_response

    async def handle_async_request(self, request: Request) -> Response:
        """Handle an async HTTP request with retry logic.

        Args:
            request: The HTTP request to handle.

        Returns:
            The HTTP response.

        Raises:
            RuntimeError: If the retry controller did not make any attempts.
            Exception: Any exception raised by the wrapped transport or validation function.
        """

        @retry(**self.config)
        async def handle_async_request(req: Request) -> Response:
            response = await self.wrapped.handle_async_request(req)

            # this is normally set by httpx _after_ calling this function, but we want the request in the validator:
            response.request = req

            if self.validate_response:
                try:
                    self.validate_response(response)
                except Exception:
                    await response.aclose()
                    raise
            return response

        return await handle_async_request(request)

    async def __aenter__(self) -> AsyncTenacityTransport:
        await self.wrapped.__aenter__()
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None = None,
        exc_value: BaseException | None = None,
        traceback: TracebackType | None = None,
    ) -> None:
        await self.wrapped.__aexit__(exc_type, exc_value, traceback)

    async def aclose(self) -> None:
        await self.wrapped.aclose()

# pydantic_ai_slim/pydantic_ai/result.py:540-547
    def response(self) -> _messages.ModelResponse:
        """Return the current state of the response."""
        if self._run_result is not None:
            return self._run_result.response
        elif self._stream_response is not None:
            return self._stream_response.get()
        else:
            raise ValueError('No stream response or run result provided')  # pragma: no cover

# pydantic_ai_slim/pydantic_ai/result.py:739-741
    def response(self) -> _messages.ModelResponse:
        """Return the current state of the response."""
        return self._streamed_run_result.response

# tests/models/mock_openai.py:100-100
    response: MockResponse | Sequence[MockResponse] | None = None

# pydantic_ai_slim/pydantic_ai/run.py:414-420
    def response(self) -> _messages.ModelResponse:
        """Return the last response from the message history."""
        # The response may not be the very last item if it contained an output tool call. See `CallToolsNode._handle_final_result`.
        for message in reversed(self.all_messages()):
            if isinstance(message, _messages.ModelResponse):
                return message
        raise ValueError('No response found in the message history')  # pragma: no cover

# tests/models/mock_xai.py:134-141
    def stream(self) -> MockAsyncStream[tuple[chat_types.Response, Any]]:
        """Mock the stream() method for streaming responses."""
        assert self.stream_data is not None, 'you can only use stream() if stream_data is provided'

        data = list(self.stream_data[self.index])
        self.parent.index += 1

        return MockAsyncStream(iter(data))

# pydantic_ai_slim/pydantic_ai/models/gemini.py:779-779
    response: dict[str, Any]

# tests/mcp_server.py:223-223
    response: str

# pydantic_ai_slim/pydantic_ai/providers/google_vertex.py:140-153
    async def async_auth_flow(self, request: httpx.Request) -> AsyncGenerator[httpx.Request, httpx.Response]:
        if self.credentials is None:  # pragma: no branch
            self.credentials = await self._get_credentials()
        if self.credentials.token is None:  # type: ignore[reportUnknownMemberType]
            await self._refresh_token()
        request.headers['Authorization'] = f'Bearer {self.credentials.token}'  # type: ignore[reportUnknownMemberType]
        # NOTE: This workaround is in place because we might get the project_id from the credentials.
        request.url = httpx.URL(str(request.url).replace('projects/None', f'projects/{self.project_id}'))
        response = yield request

        if response.status_code == 401:
            await self._refresh_token()
            request.headers['Authorization'] = f'Bearer {self.credentials.token}'  # type: ignore[reportUnknownMemberType]
            yield request

# .github/set_docs_main_preview_url.py:6-6
import httpx

# .github/set_docs_pr_preview_url.py:4-4
import httpx

# examples/pydantic_ai_examples/rag.py:30-30
import httpx

# examples/pydantic_ai_examples/slack_lead_qualifier/slack.py:4-4
import httpx

# pydantic_ai_slim/pydantic_ai/_ssrf.py:15-15
import httpx

# pydantic_ai_slim/pydantic_ai/mcp.py:17-17
import httpx

# pydantic_ai_slim/pydantic_ai/models/__init__.py:19-19
import httpx

# pydantic_ai_slim/pydantic_ai/models/gemini.py:19-19
import httpx

# pydantic_ai_slim/pydantic_ai/providers/alibaba.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/anthropic.py:7-7
import httpx

# pydantic_ai_slim/pydantic_ai/providers/azure.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/cerebras.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/cohere.py:5-5
import httpx

# pydantic_ai_slim/pydantic_ai/providers/deepseek.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/fireworks.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/gateway.py:10-10
import httpx

# pydantic_ai_slim/pydantic_ai/providers/github.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/google.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/google_gla.py:5-5
import httpx

# pydantic_ai_slim/pydantic_ai/providers/google_vertex.py:10-10
import httpx

# pydantic_ai_slim/pydantic_ai/providers/grok.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/groq.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/heroku.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/mistral.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/moonshotai.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/nebius.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/ollama.py:5-5
import httpx

# pydantic_ai_slim/pydantic_ai/providers/openai.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/openrouter.py:7-7
import httpx

# pydantic_ai_slim/pydantic_ai/providers/ovhcloud.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/sambanova.py:5-5
import httpx

# pydantic_ai_slim/pydantic_ai/providers/together.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/providers/vercel.py:6-6
import httpx

# pydantic_ai_slim/pydantic_ai/ui/_web/app.py:9-9
import httpx

# pydantic_graph/pydantic_graph/mermaid.py:10-10
import httpx

# tests/conftest.py:19-19
import httpx

# tests/graph/test_mermaid.py:10-10
import httpx

# tests/models/anthropic/test_output.py:17-17
import httpx

# tests/models/test_anthropic.py:13-13
import httpx

# tests/models/test_gemini.py:14-14
import httpx

# tests/models/test_groq.py:12-12
import httpx

# tests/models/test_mistral.py:10-10
import httpx

# tests/models/test_model_names.py:6-6
import httpx

# tests/models/test_openai.py:13-13
import httpx

# tests/providers/test_alibaba_provider.py:1-1
import httpx

# tests/providers/test_cerebras.py:5-5
import httpx

# tests/providers/test_cohere.py:3-3
import httpx

# tests/providers/test_deepseek.py:3-3
import httpx

# tests/providers/test_fireworks.py:3-3
import httpx

# tests/providers/test_gateway.py:7-7
import httpx

# tests/providers/test_github.py:3-3
import httpx

# tests/providers/test_google_vertex.py:10-10
import httpx

# tests/providers/test_grok.py:7-7
import httpx

# tests/providers/test_groq.py:5-5
import httpx

# tests/providers/test_heroku.py:3-3
import httpx

# tests/providers/test_huggingface.py:6-6
import httpx

# tests/providers/test_litellm.py:1-1
import httpx

# tests/providers/test_mistral.py:5-5
import httpx

# tests/providers/test_moonshotai.py:3-3
import httpx

# tests/providers/test_nebius.py:3-3
import httpx

# tests/providers/test_ollama.py:3-3
import httpx

# tests/providers/test_openai.py:1-1
import httpx

# tests/providers/test_openrouter.py:3-3
import httpx

# tests/providers/test_ovhcloud.py:3-3
import httpx

# tests/providers/test_sambanova_provider.py:1-1
import httpx

# tests/providers/test_together.py:3-3
import httpx

# tests/providers/test_vercel.py:3-3
import httpx

# tests/test_a2a.py:6-6
import httpx

# tests/test_ag_ui.py:12-12
import httpx

# tests/test_examples.py:15-15
import httpx

# tests/test_tenacity.py:11-11
import httpx

# tests/models/anthropic/conftest.py:76-94
def weather_tool_responses() -> list[BetaMessage]:
    """Standard mock responses for weather tool tests."""
    return [
        completion_message(
            [
                BetaToolUseBlock(
                    id='tool_123',
                    name='get_weather',
                    input={'location': 'Paris'},
                    type='tool_use',
                )
            ],
            BetaUsage(input_tokens=5, output_tokens=10),
        ),
        completion_message(
            [BetaTextBlock(text='The weather in Paris is sunny.', type='text')],
            BetaUsage(input_tokens=3, output_tokens=5),
        ),
    ]

# pydantic_ai_slim/pydantic_ai/retries.py:158-166
    def __init__(
        self,
        config: RetryConfig,
        wrapped: BaseTransport | None = None,
        validate_response: Callable[[Response], Any] | None = None,
    ):
        self.config = config
        self.wrapped = wrapped and HTTPTransport()
        self.validate_response = validate_response

# examples/pydantic_ai_examples/chat_app.py:24-24
from fastapi import Depends, Request

# pydantic_ai_slim/pydantic_ai/retries.py:18-26
from httpx import (
    AsyncBaseTransport,
    AsyncHTTPTransport,
    BaseTransport,
    HTTPStatusError,
    HTTPTransport,
    Request,
    Response,
)

# pydantic_ai_slim/pydantic_ai/ui/_web/api.py:9-9
from starlette.requests import Request

# tests/typed_agent.py:10-10
from starlette.requests import Request

# pydantic_ai_slim/pydantic_ai/retries.py:265-294
    async def handle_async_request(self, request: Request) -> Response:
        """Handle an async HTTP request with retry logic.

        Args:
            request: The HTTP request to handle.

        Returns:
            The HTTP response.

        Raises:
            RuntimeError: If the retry controller did not make any attempts.
            Exception: Any exception raised by the wrapped transport or validation function.
        """

        @retry(**self.config)
        async def handle_async_request(req: Request) -> Response:
            response = await self.wrapped.handle_async_request(req)

            # this is normally set by httpx _after_ calling this function, but we want the request in the validator:
            response.request = req

            if self.validate_response:
                try:
                    self.validate_response(response)
                except Exception:
                    await response.aclose()
                    raise
            return response

        return await handle_async_request(request)

# tests/models/test_anthropic.py:8156-8170
async def test_anthropic_count_tokens_with_mock(allow_model_requests: None):
    """Test that count_tokens is called on the mock client."""
    c = completion_message(
        [BetaTextBlock(text='hello world', type='text')], BetaUsage(input_tokens=5, output_tokens=10)
    )
    mock_client = MockAnthropic.create_mock(c)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(m)

    result = await agent.run('hello', usage_limits=UsageLimits(input_tokens_limit=20, count_tokens_before_request=True))
    assert result.output == 'hello world'
    assert len(mock_client.chat_completion_kwargs) == 2  # type: ignore
    count_tokens_kwargs = mock_client.chat_completion_kwargs[0]  # type: ignore
    assert 'model' in count_tokens_kwargs
    assert 'messages' in count_tokens_kwargs

# tests/models/mock_xai.py:68-70
    def chat(self) -> Any:
        """Create mock chat interface."""
        return type('Chat', (), {'create': self.chat_create})

# tests/models/mock_xai.py:58-108
class MockXai:
    """Mock xAI SDK AsyncClient."""

    responses: Sequence[chat_types.Response | Exception] | None = None
    stream_data: Sequence[Sequence[tuple[chat_types.Response, Any]]] | None = None
    index: int = 0
    chat_create_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    api_key: str = 'test-api-key'

    @cached_property
    def chat(self) -> Any:
        """Create mock chat interface."""
        return type('Chat', (), {'create': self.chat_create})

    @cached_property
    def files(self) -> Any:
        """Create mock files interface."""
        return type('Files', (), {'upload': self.files_upload})

    @classmethod
    def create_mock(
        cls,
        responses: Sequence[chat_types.Response | Exception],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for non-streaming responses."""
        return cast(AsyncClient, cls(responses=responses, api_key=api_key))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[Sequence[tuple[chat_types.Response, Any]]],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for streaming responses."""
        return cast(AsyncClient, cls(stream_data=stream, api_key=api_key))

    def chat_create(self, *_args: Any, **kwargs: Any) -> MockChatInstance:
        """Mock the chat.create method."""
        self.chat_create_kwargs.append(kwargs)
        return MockChatInstance(
            responses=self.responses,
            stream_data=self.stream_data,
            index=self.index,
            parent=self,
        )

    async def files_upload(self, data: bytes, filename: str) -> Any:
        """Mock the files.upload method."""
        # Return a mock uploaded file object with an id
        return type('UploadedFile', (), {'id': f'file-{filename}'})()

# tests/models/anthropic/conftest.py:49-57
def mock_sonnet_4_5(allow_model_requests: None) -> tuple[AnthropicModel, AsyncAnthropic]:
    """Mock claude-sonnet-4-5 model for unit tests."""
    c = completion_message(
        [BetaTextBlock(text='{"city": "Mexico City", "country": "Mexico"}', type='text')],
        BetaUsage(input_tokens=5, output_tokens=10),
    )
    mock_client = MockAnthropic.create_mock(c)
    model = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    return model, mock_client

# tests/models/mock_xai.py:78-84
    def create_mock(
        cls,
        responses: Sequence[chat_types.Response | Exception],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for non-streaming responses."""
        return cast(AsyncClient, cls(responses=responses, api_key=api_key))

# tests/models/mock_xai.py:120-132
    async def sample(self) -> chat_types.Response:
        """Mock the sample() method for non-streaming responses."""
        assert self.responses is not None, 'you can only use sample() if responses are provided'

        if self.index >= len(self.responses):
            raise IndexError(f'Mock response index {self.index} out of range (length: {len(self.responses)})')
        raise_if_exception(self.responses[self.index])
        response = cast(chat_types.Response, self.responses[self.index])
        # Increment index for next call
        self.index += 1
        self.parent.index = self.index

        return response

# tests/models/mock_xai.py:87-93
    def create_mock_stream(
        cls,
        stream: Sequence[Sequence[tuple[chat_types.Response, Any]]],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for streaming responses."""
        return cast(AsyncClient, cls(stream_data=stream, api_key=api_key))

# pydantic_ai_slim/pydantic_ai/models/__init__.py:867-916
    def _get_instructions(
        messages: Sequence[ModelMessage], model_request_parameters: ModelRequestParameters | None = None
    ) -> str | None:
        """Get instructions from the first ModelRequest found when iterating messages in reverse.

        In the case that a "mock" request was generated to include a tool-return part for a result tool,
        we want to use the instructions from the second-to-most-recent request (which should correspond to the
        original request that generated the response that resulted in the tool-return part).
        """
        instructions = None

        last_two_requests: list[ModelRequest] = []
        for message in reversed(messages):
            if isinstance(message, ModelRequest):
                last_two_requests.append(message)
                if len(last_two_requests) == 2:
                    break
                if message.instructions is not None:
                    instructions = message.instructions
                    break

        # If we don't have two requests, and we didn't already return instructions, there are definitely not any:
        if instructions is None and len(last_two_requests) == 2:
            most_recent_request = last_two_requests[0]
            second_most_recent_request = last_two_requests[1]

            # If we've gotten this far and the most recent request consists of only tool-return parts or retry-prompt parts,
            # we use the instructions from the second-to-most-recent request. This is necessary because when handling
            # result tools, we generate a "mock" ModelRequest with a tool-return part for it, and that ModelRequest will not
            # have the relevant instructions from the agent.

            # While it's possible that you could have a message history where the most recent request has only tool returns,
            # I believe there is no way to achieve that would _change_ the instructions without manually crafting the most
            # recent message. That might make sense in principle for some usage pattern, but it's enough of an edge case
            # that I think it's not worth worrying about, since you can work around this by inserting another ModelRequest
            # with no parts at all immediately before the request that has the tool calls (that works because we only look
            # at the two most recent ModelRequests here).

            # If you have a use case where this causes pain, please open a GitHub issue and we can discuss alternatives.

            if all(p.part_kind == 'tool-return' or p.part_kind == 'retry-prompt' for p in most_recent_request.parts):
                instructions = second_most_recent_request.instructions

        if model_request_parameters and (output_instructions := model_request_parameters.prompted_output_instructions):
            if instructions:
                instructions = '\n\n'.join([instructions, output_instructions])
            else:
                instructions = output_instructions

        return instructions

# pydantic_ai_slim/pydantic_ai/models/xai.py:702-712
    def _update_response_state(self, response: chat_types.Response) -> None:
        """Update response state including usage, response ID, and finish reason."""
        # Update usage (SDK Response always provides a usage object)
        self._usage = _extract_usage(response, self._model_name, self._provider.name, self._provider.base_url)

        # Set provider response ID (only set once)
        if response.id and self.provider_response_id is None:
            self.provider_response_id = response.id

        # Handle finish reason (SDK Response always provides a finish_reason)
        self.finish_reason = _FINISH_REASON_MAP.get(response.finish_reason, 'stop')

# docs/.hooks/main.py:9-9
from mkdocs.config import Config

# tests/models/mock_xai.py:73-75
    def files(self) -> Any:
        """Create mock files interface."""
        return type('Files', (), {'upload': self.files_upload})

# pydantic_ai_slim/pydantic_ai/_agent_graph.py:440-440
    request: _messages.ModelRequest

# tests/models/test_gemini_vertex.py:6-6
from inline_snapshot import Is, snapshot

# tests/models/test_google.py:16-16
from inline_snapshot import Is, snapshot

# pydantic_ai_slim/pydantic_ai/retries.py:199-201
    def __enter__(self) -> TenacityTransport:
        self.wrapped.__enter__()
        return self