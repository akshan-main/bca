# tests/models/mock_openai.py:144-148
def get_mock_responses_kwargs(async_open_ai: AsyncOpenAI) -> list[dict[str, Any]]:
    if isinstance(async_open_ai, MockOpenAIResponses):  # pragma: lax no cover
        return async_open_ai.response_kwargs
    else:  # pragma: no cover
        raise RuntimeError('Not a MockOpenAIResponses instance')

# tests/models/mock_openai.py:144-148
def get_mock_responses_kwargs(async_open_ai: AsyncOpenAI) -> list[dict[str, Any]]:
    if isinstance(async_open_ai, MockOpenAIResponses):  # pragma: lax no cover
        return async_open_ai.response_kwargs
    else:  # pragma: no cover
        raise RuntimeError('Not a MockOpenAIResponses instance')

# tests/test_tenacity.py:32-48
    def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.__enter__ = Mock(return_value=mock_transport)
        mock_transport.__exit__ = Mock(return_value=None)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with transport:
            result = transport.handle_request(request)

        assert result is mock_response
        mock_transport.handle_request.assert_called_once_with(request)

# tests/models/mock_openai.py:99-141
class MockOpenAIResponses:
    response: MockResponse | Sequence[MockResponse] | None = None
    stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]] | None = None
    index: int = 0
    response_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    base_url: str = 'https://api.openai.com/v1'

    @cached_property
    def responses(self) -> Any:
        return type('Responses', (), {'create': self.responses_create})

    @classmethod
    def create_mock(cls, responses: MockResponse | Sequence[MockResponse]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(response=responses))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))  # pragma: lax no cover

    async def responses_create(  # pragma: lax no cover
        self, *_args: Any, stream: bool = False, **kwargs: Any
    ) -> responses.Response | MockAsyncStream[MockResponseStreamEvent]:
        self.response_kwargs.append({k: v for k, v in kwargs.items() if v is not NOT_GIVEN})

        if stream:
            assert self.stream is not None, 'you can only used `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream)))
        else:
            assert self.response is not None, 'you can only used `stream=False` if `response` are provided'
            if isinstance(self.response, Sequence):
                raise_if_exception(self.response[self.index])
                response = cast(responses.Response, self.response[self.index])
            else:
                raise_if_exception(self.response)
                response = cast(responses.Response, self.response)
        self.index += 1
        return response

# tests/models/mock_openai.py:99-141
class MockOpenAIResponses:
    response: MockResponse | Sequence[MockResponse] | None = None
    stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]] | None = None
    index: int = 0
    response_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    base_url: str = 'https://api.openai.com/v1'

    @cached_property
    def responses(self) -> Any:
        return type('Responses', (), {'create': self.responses_create})

    @classmethod
    def create_mock(cls, responses: MockResponse | Sequence[MockResponse]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(response=responses))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))  # pragma: lax no cover

    async def responses_create(  # pragma: lax no cover
        self, *_args: Any, stream: bool = False, **kwargs: Any
    ) -> responses.Response | MockAsyncStream[MockResponseStreamEvent]:
        self.response_kwargs.append({k: v for k, v in kwargs.items() if v is not NOT_GIVEN})

        if stream:
            assert self.stream is not None, 'you can only used `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream)))
        else:
            assert self.response is not None, 'you can only used `stream=False` if `response` are provided'
            if isinstance(self.response, Sequence):
                raise_if_exception(self.response[self.index])
                response = cast(responses.Response, self.response[self.index])
            else:
                raise_if_exception(self.response)
                response = cast(responses.Response, self.response)
        self.index += 1
        return response

# tests/models/mock_openai.py:111-112
    def create_mock(cls, responses: MockResponse | Sequence[MockResponse]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(response=responses))

# tests/models/mock_openai.py:115-119
    def create_mock_stream(
        cls,
        stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))  # pragma: lax no cover

# tests/test_tenacity.py:192-206
    async def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_async_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = AsyncTenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        async with transport:
            result = await transport.handle_async_request(request)

        assert result is mock_response
        mock_transport.handle_async_request.assert_called_once_with(request)

# tests/models/mock_xai.py:78-84
    def create_mock(
        cls,
        responses: Sequence[chat_types.Response | Exception],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for non-streaming responses."""
        return cast(AsyncClient, cls(responses=responses, api_key=api_key))

# tests/models/mock_xai.py:61-61
    responses: Sequence[chat_types.Response | Exception] | None = None

# tests/models/test_mistral.py:81-83
class MockSdkConfiguration:
    def get_server_details(self) -> tuple[str, ...]:
        return ('https://api.mistral.ai',)

# tests/models/anthropic/conftest.py:49-57
def mock_sonnet_4_5(allow_model_requests: None) -> tuple[AnthropicModel, AsyncAnthropic]:
    """Mock claude-sonnet-4-5 model for unit tests."""
    c = completion_message(
        [BetaTextBlock(text='{"city": "Mexico City", "country": "Mexico"}', type='text')],
        BetaUsage(input_tokens=5, output_tokens=10),
    )
    mock_client = MockAnthropic.create_mock(c)
    model = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    return model, mock_client

# tests/models/mock_xai.py:87-93
    def create_mock_stream(
        cls,
        stream: Sequence[Sequence[tuple[chat_types.Response, Any]]],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for streaming responses."""
        return cast(AsyncClient, cls(stream_data=stream, api_key=api_key))

# tests/models/mock_openai.py:100-100
    response: MockResponse | Sequence[MockResponse] | None = None

# tests/models/test_xai.py:3290-3300
async def test_mock_xai_index_error(allow_model_requests: None) -> None:
    """Test that MockChatInstance raises IndexError when responses are exhausted."""
    responses = [create_response(content='first')]
    mock_client = MockXai.create_mock(responses)
    model = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(model)

    await agent.run('Hello')

    with pytest.raises(IndexError, match='Mock response index 1 out of range'):
        await agent.run('Hello again')

# tests/models/mock_openai.py:107-108
    def responses(self) -> Any:
        return type('Responses', (), {'create': self.responses_create})

# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/models/mock_xai.py:115-115
    responses: Sequence[chat_types.Response | Exception] | None = None

# tests/models/test_groq.py:109-110
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncGroq:
        return cast(AsyncGroq, cls(completions=completions))

# tests/models/mock_openai.py:42-43
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(completions=completions))

# tests/models/mock_xai.py:58-108
class MockXai:
    """Mock xAI SDK AsyncClient."""

    responses: Sequence[chat_types.Response | Exception] | None = None
    stream_data: Sequence[Sequence[tuple[chat_types.Response, Any]]] | None = None
    index: int = 0
    chat_create_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    api_key: str = 'test-api-key'

    @cached_property
    def chat(self) -> Any:
        """Create mock chat interface."""
        return type('Chat', (), {'create': self.chat_create})

    @cached_property
    def files(self) -> Any:
        """Create mock files interface."""
        return type('Files', (), {'upload': self.files_upload})

    @classmethod
    def create_mock(
        cls,
        responses: Sequence[chat_types.Response | Exception],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for non-streaming responses."""
        return cast(AsyncClient, cls(responses=responses, api_key=api_key))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[Sequence[tuple[chat_types.Response, Any]]],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for streaming responses."""
        return cast(AsyncClient, cls(stream_data=stream, api_key=api_key))

    def chat_create(self, *_args: Any, **kwargs: Any) -> MockChatInstance:
        """Mock the chat.create method."""
        self.chat_create_kwargs.append(kwargs)
        return MockChatInstance(
            responses=self.responses,
            stream_data=self.stream_data,
            index=self.index,
            parent=self,
        )

    async def files_upload(self, data: bytes, filename: str) -> Any:
        """Mock the files.upload method."""
        # Return a mock uploaded file object with an id
        return type('UploadedFile', (), {'id': f'file-{filename}'})()

# tests/models/test_anthropic.py:156-157
    def create_mock(cls, messages_: MockAnthropicMessage | Sequence[MockAnthropicMessage]) -> AsyncAnthropic:
        return cast(AsyncAnthropic, cls(messages_=messages_))

# tests/models/test_mistral.py:108-109
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> Mistral:
        return cast(Mistral, cls(completions=completions))

# tests/models/anthropic/conftest.py:76-94
def weather_tool_responses() -> list[BetaMessage]:
    """Standard mock responses for weather tool tests."""
    return [
        completion_message(
            [
                BetaToolUseBlock(
                    id='tool_123',
                    name='get_weather',
                    input={'location': 'Paris'},
                    type='tool_use',
                )
            ],
            BetaUsage(input_tokens=5, output_tokens=10),
        ),
        completion_message(
            [BetaTextBlock(text='The weather in Paris is sunny.', type='text')],
            BetaUsage(input_tokens=3, output_tokens=5),
        ),
    ]

# tests/models/test_huggingface.py:91-92
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncInferenceClient:
        return cast(AsyncInferenceClient, cls(completions=completions))

# tests/models/mock_openai.py:102-102
    index: int = 0

# tests/models/mock_openai.py:103-103
    response_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])