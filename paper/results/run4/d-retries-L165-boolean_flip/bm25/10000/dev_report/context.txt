# tests/models/mock_openai.py:144-148
def get_mock_responses_kwargs(async_open_ai: AsyncOpenAI) -> list[dict[str, Any]]:
    if isinstance(async_open_ai, MockOpenAIResponses):  # pragma: lax no cover
        return async_open_ai.response_kwargs
    else:  # pragma: no cover
        raise RuntimeError('Not a MockOpenAIResponses instance')

# tests/models/mock_openai.py:144-148
def get_mock_responses_kwargs(async_open_ai: AsyncOpenAI) -> list[dict[str, Any]]:
    if isinstance(async_open_ai, MockOpenAIResponses):  # pragma: lax no cover
        return async_open_ai.response_kwargs
    else:  # pragma: no cover
        raise RuntimeError('Not a MockOpenAIResponses instance')

# tests/test_tenacity.py:32-48
    def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.__enter__ = Mock(return_value=mock_transport)
        mock_transport.__exit__ = Mock(return_value=None)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with transport:
            result = transport.handle_request(request)

        assert result is mock_response
        mock_transport.handle_request.assert_called_once_with(request)

# tests/models/mock_openai.py:99-141
class MockOpenAIResponses:
    response: MockResponse | Sequence[MockResponse] | None = None
    stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]] | None = None
    index: int = 0
    response_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    base_url: str = 'https://api.openai.com/v1'

    @cached_property
    def responses(self) -> Any:
        return type('Responses', (), {'create': self.responses_create})

    @classmethod
    def create_mock(cls, responses: MockResponse | Sequence[MockResponse]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(response=responses))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))  # pragma: lax no cover

    async def responses_create(  # pragma: lax no cover
        self, *_args: Any, stream: bool = False, **kwargs: Any
    ) -> responses.Response | MockAsyncStream[MockResponseStreamEvent]:
        self.response_kwargs.append({k: v for k, v in kwargs.items() if v is not NOT_GIVEN})

        if stream:
            assert self.stream is not None, 'you can only used `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream)))
        else:
            assert self.response is not None, 'you can only used `stream=False` if `response` are provided'
            if isinstance(self.response, Sequence):
                raise_if_exception(self.response[self.index])
                response = cast(responses.Response, self.response[self.index])
            else:
                raise_if_exception(self.response)
                response = cast(responses.Response, self.response)
        self.index += 1
        return response

# tests/models/mock_openai.py:99-141
class MockOpenAIResponses:
    response: MockResponse | Sequence[MockResponse] | None = None
    stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]] | None = None
    index: int = 0
    response_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    base_url: str = 'https://api.openai.com/v1'

    @cached_property
    def responses(self) -> Any:
        return type('Responses', (), {'create': self.responses_create})

    @classmethod
    def create_mock(cls, responses: MockResponse | Sequence[MockResponse]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(response=responses))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))  # pragma: lax no cover

    async def responses_create(  # pragma: lax no cover
        self, *_args: Any, stream: bool = False, **kwargs: Any
    ) -> responses.Response | MockAsyncStream[MockResponseStreamEvent]:
        self.response_kwargs.append({k: v for k, v in kwargs.items() if v is not NOT_GIVEN})

        if stream:
            assert self.stream is not None, 'you can only used `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream)))
        else:
            assert self.response is not None, 'you can only used `stream=False` if `response` are provided'
            if isinstance(self.response, Sequence):
                raise_if_exception(self.response[self.index])
                response = cast(responses.Response, self.response[self.index])
            else:
                raise_if_exception(self.response)
                response = cast(responses.Response, self.response)
        self.index += 1
        return response

# tests/models/mock_openai.py:111-112
    def create_mock(cls, responses: MockResponse | Sequence[MockResponse]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(response=responses))

# tests/models/mock_openai.py:115-119
    def create_mock_stream(
        cls,
        stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))  # pragma: lax no cover

# tests/test_tenacity.py:192-206
    async def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = AsyncMock(spec=httpx.AsyncBaseTransport)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_async_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = AsyncTenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        async with transport:
            result = await transport.handle_async_request(request)

        assert result is mock_response
        mock_transport.handle_async_request.assert_called_once_with(request)

# tests/models/mock_xai.py:78-84
    def create_mock(
        cls,
        responses: Sequence[chat_types.Response | Exception],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for non-streaming responses."""
        return cast(AsyncClient, cls(responses=responses, api_key=api_key))

# tests/models/mock_xai.py:61-61
    responses: Sequence[chat_types.Response | Exception] | None = None

# tests/models/test_mistral.py:81-83
class MockSdkConfiguration:
    def get_server_details(self) -> tuple[str, ...]:
        return ('https://api.mistral.ai',)

# tests/models/anthropic/conftest.py:49-57
def mock_sonnet_4_5(allow_model_requests: None) -> tuple[AnthropicModel, AsyncAnthropic]:
    """Mock claude-sonnet-4-5 model for unit tests."""
    c = completion_message(
        [BetaTextBlock(text='{"city": "Mexico City", "country": "Mexico"}', type='text')],
        BetaUsage(input_tokens=5, output_tokens=10),
    )
    mock_client = MockAnthropic.create_mock(c)
    model = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    return model, mock_client

# tests/models/mock_xai.py:87-93
    def create_mock_stream(
        cls,
        stream: Sequence[Sequence[tuple[chat_types.Response, Any]]],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for streaming responses."""
        return cast(AsyncClient, cls(stream_data=stream, api_key=api_key))

# tests/models/mock_openai.py:100-100
    response: MockResponse | Sequence[MockResponse] | None = None

# tests/models/test_xai.py:3290-3300
async def test_mock_xai_index_error(allow_model_requests: None) -> None:
    """Test that MockChatInstance raises IndexError when responses are exhausted."""
    responses = [create_response(content='first')]
    mock_client = MockXai.create_mock(responses)
    model = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(model)

    await agent.run('Hello')

    with pytest.raises(IndexError, match='Mock response index 1 out of range'):
        await agent.run('Hello again')

# tests/models/mock_openai.py:107-108
    def responses(self) -> Any:
        return type('Responses', (), {'create': self.responses_create})

# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/models/test_huggingface.py:9-9
from unittest.mock import Mock

# tests/models/mock_xai.py:115-115
    responses: Sequence[chat_types.Response | Exception] | None = None

# tests/models/test_groq.py:109-110
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncGroq:
        return cast(AsyncGroq, cls(completions=completions))

# tests/models/mock_openai.py:42-43
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(completions=completions))

# tests/test_tenacity.py:29-186
class TestTenacityTransport:
    """Tests for the synchronous TenacityTransport."""

    def test_successful_request(self):
        """Test that successful requests pass through without retry."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.__enter__ = Mock(return_value=mock_transport)
        mock_transport.__exit__ = Mock(return_value=None)
        mock_response = Mock(spec=httpx.Response)
        mock_transport.handle_request.return_value = mock_response

        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with transport:
            result = transport.handle_request(request)

        assert result is mock_response
        mock_transport.handle_request.assert_called_once_with(request)

    def test_retry_on_exception(self):
        """Test that exceptions trigger retries."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response = Mock(spec=httpx.Response)

        # Fail twice, succeed on third attempt
        mock_transport.handle_request.side_effect = [
            httpx.ConnectError('Connection failed'),
            httpx.ConnectError('Connection failed again'),
            mock_response,
        ]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),  # Very short wait for tests
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response
        assert mock_transport.handle_request.call_count == 3

    def test_retry_exhausted(self):
        """Test that retry exhaustion re-raises the last exception."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_transport.handle_request.side_effect = httpx.ConnectError('Connection failed')

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.ConnectError),
            stop=stop_after_attempt(2),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport)

        request = httpx.Request('GET', 'https://example.com')
        with pytest.raises(httpx.ConnectError, match='Connection failed'):
            transport.handle_request(request)

        assert mock_transport.handle_request.call_count == 2

    def test_validate_response_success(self):
        """Test that validate_response is called and doesn't raise."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response = Mock(spec=httpx.Response)
        mock_response.status_code = 200
        mock_transport.handle_request.return_value = mock_response

        validate_response = Mock()
        config = RetryConfig(stop=stop_after_attempt(3), reraise=True)
        transport = TenacityTransport(config, mock_transport, validate_response)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response
        validate_response.assert_called_once_with(mock_response)

    def test_validate_response_triggers_retry(self):
        """Test that validate_response can trigger retries."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response_fail = Mock(spec=httpx.Response)
        mock_response_fail.status_code = 429
        mock_response_success = Mock(spec=httpx.Response)
        mock_response_success.status_code = 200

        mock_transport.handle_request.side_effect = [mock_response_fail, mock_response_success]

        def validate_response(response: httpx.Response):
            if response.status_code == 429:
                raise httpx.HTTPStatusError('Rate limited', request=request, response=response)

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.HTTPStatusError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(config, mock_transport, validate_response)

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response_success
        assert mock_transport.handle_request.call_count == 2

    def test_raise_for_status_in_validate_response(self):
        """Test that response.raise_for_status() works in validate_response callback."""
        mock_transport = Mock(spec=httpx.BaseTransport)
        mock_response_fail = Mock(spec=httpx.Response)
        mock_response_fail.status_code = 429
        mock_response_fail.is_success = False
        mock_response_fail.is_error = True
        mock_response_fail.request = None  # Initially None, will be set by transport

        # Mock raise_for_status to check if request is set
        def mock_raise_for_status():
            if mock_response_fail.request is None:
                raise RuntimeError(  # pragma: no cover
                    'Cannot call `raise_for_status` as the request instance has not been set on this response.'
                )
            raise httpx.HTTPStatusError(
                'Too Many Requests', request=mock_response_fail.request, response=mock_response_fail
            )

        mock_response_fail.raise_for_status = mock_raise_for_status

        mock_response_success = Mock(spec=httpx.Response)
        mock_response_success.status_code = 200
        mock_response_success.is_success = True
        mock_response_success.is_error = False
        mock_response_success.raise_for_status = Mock()  # Should not raise

        mock_transport.handle_request.side_effect = [mock_response_fail, mock_response_success]

        config = RetryConfig(
            retry=retry_if_exception_type(httpx.HTTPStatusError),
            stop=stop_after_attempt(3),
            wait=wait_fixed(0.001),
            reraise=True,
        )
        transport = TenacityTransport(
            config, mock_transport, validate_response=lambda response: response.raise_for_status()
        )

        request = httpx.Request('GET', 'https://example.com')
        result = transport.handle_request(request)

        assert result is mock_response_success
        assert mock_transport.handle_request.call_count == 2
        # Verify that the request was set on the failed response before raise_for_status was called
        assert mock_response_fail.request is request
        mock_response_success.raise_for_status.assert_called_once()

# tests/models/mock_xai.py:58-108
class MockXai:
    """Mock xAI SDK AsyncClient."""

    responses: Sequence[chat_types.Response | Exception] | None = None
    stream_data: Sequence[Sequence[tuple[chat_types.Response, Any]]] | None = None
    index: int = 0
    chat_create_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    api_key: str = 'test-api-key'

    @cached_property
    def chat(self) -> Any:
        """Create mock chat interface."""
        return type('Chat', (), {'create': self.chat_create})

    @cached_property
    def files(self) -> Any:
        """Create mock files interface."""
        return type('Files', (), {'upload': self.files_upload})

    @classmethod
    def create_mock(
        cls,
        responses: Sequence[chat_types.Response | Exception],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for non-streaming responses."""
        return cast(AsyncClient, cls(responses=responses, api_key=api_key))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[Sequence[tuple[chat_types.Response, Any]]],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for streaming responses."""
        return cast(AsyncClient, cls(stream_data=stream, api_key=api_key))

    def chat_create(self, *_args: Any, **kwargs: Any) -> MockChatInstance:
        """Mock the chat.create method."""
        self.chat_create_kwargs.append(kwargs)
        return MockChatInstance(
            responses=self.responses,
            stream_data=self.stream_data,
            index=self.index,
            parent=self,
        )

    async def files_upload(self, data: bytes, filename: str) -> Any:
        """Mock the files.upload method."""
        # Return a mock uploaded file object with an id
        return type('UploadedFile', (), {'id': f'file-{filename}'})()

# tests/models/test_anthropic.py:156-157
    def create_mock(cls, messages_: MockAnthropicMessage | Sequence[MockAnthropicMessage]) -> AsyncAnthropic:
        return cast(AsyncAnthropic, cls(messages_=messages_))

# tests/models/test_mistral.py:108-109
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> Mistral:
        return cast(Mistral, cls(completions=completions))

# tests/models/test_groq.py:97-139
class MockGroq:
    completions: MockChatCompletion | Sequence[MockChatCompletion] | None = None
    stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]] | None = None
    index: int = 0
    base_url: str = 'https://api.groq.com'

    @cached_property
    def chat(self) -> Any:
        chat_completions = type('Completions', (), {'create': self.chat_completions_create})
        return type('Chat', (), {'completions': chat_completions})

    @classmethod
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncGroq:
        return cast(AsyncGroq, cls(completions=completions))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]],
    ) -> AsyncGroq:
        return cast(AsyncGroq, cls(stream=stream))

    async def chat_completions_create(
        self, *_args: Any, stream: bool = False, **_kwargs: Any
    ) -> chat.ChatCompletion | MockAsyncStream[MockChatCompletionChunk]:
        if stream:
            assert self.stream is not None, 'you can only used `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(  # pragma: no cover
                    iter(cast(list[MockChatCompletionChunk], self.stream[self.index]))
                )
            else:
                response = MockAsyncStream(iter(cast(list[MockChatCompletionChunk], self.stream)))
        else:
            assert self.completions is not None, 'you can only used `stream=False` if `completions` are provided'
            if isinstance(self.completions, Sequence):
                raise_if_exception(self.completions[self.index])
                response = cast(chat.ChatCompletion, self.completions[self.index])
            else:
                raise_if_exception(self.completions)
                response = cast(chat.ChatCompletion, self.completions)
        self.index += 1
        return response

# tests/models/anthropic/conftest.py:76-94
def weather_tool_responses() -> list[BetaMessage]:
    """Standard mock responses for weather tool tests."""
    return [
        completion_message(
            [
                BetaToolUseBlock(
                    id='tool_123',
                    name='get_weather',
                    input={'location': 'Paris'},
                    type='tool_use',
                )
            ],
            BetaUsage(input_tokens=5, output_tokens=10),
        ),
        completion_message(
            [BetaTextBlock(text='The weather in Paris is sunny.', type='text')],
            BetaUsage(input_tokens=3, output_tokens=5),
        ),
    ]

# tests/models/test_huggingface.py:91-92
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncInferenceClient:
        return cast(AsyncInferenceClient, cls(completions=completions))

# tests/models/mock_openai.py:102-102
    index: int = 0

# tests/models/mock_xai.py:58-108
class MockXai:
    """Mock xAI SDK AsyncClient."""

    responses: Sequence[chat_types.Response | Exception] | None = None
    stream_data: Sequence[Sequence[tuple[chat_types.Response, Any]]] | None = None
    index: int = 0
    chat_create_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    api_key: str = 'test-api-key'

    @cached_property
    def chat(self) -> Any:
        """Create mock chat interface."""
        return type('Chat', (), {'create': self.chat_create})

    @cached_property
    def files(self) -> Any:
        """Create mock files interface."""
        return type('Files', (), {'upload': self.files_upload})

    @classmethod
    def create_mock(
        cls,
        responses: Sequence[chat_types.Response | Exception],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for non-streaming responses."""
        return cast(AsyncClient, cls(responses=responses, api_key=api_key))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[Sequence[tuple[chat_types.Response, Any]]],
        api_key: str = 'test-api-key',
    ) -> AsyncClient:
        """Create a mock AsyncClient for streaming responses."""
        return cast(AsyncClient, cls(stream_data=stream, api_key=api_key))

    def chat_create(self, *_args: Any, **kwargs: Any) -> MockChatInstance:
        """Mock the chat.create method."""
        self.chat_create_kwargs.append(kwargs)
        return MockChatInstance(
            responses=self.responses,
            stream_data=self.stream_data,
            index=self.index,
            parent=self,
        )

    async def files_upload(self, data: bytes, filename: str) -> Any:
        """Mock the files.upload method."""
        # Return a mock uploaded file object with an id
        return type('UploadedFile', (), {'id': f'file-{filename}'})()

# tests/models/mock_openai.py:103-103
    response_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/providers/test_huggingface.py:4-4
from unittest.mock import MagicMock, Mock, patch

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/providers/test_huggingface.py:4-4
from unittest.mock import MagicMock, Mock, patch

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/providers/test_huggingface.py:4-4
from unittest.mock import MagicMock, Mock, patch

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/providers/test_huggingface.py:4-4
from unittest.mock import MagicMock, Mock, patch

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/models/test_mcp_sampling.py:4-4
from unittest.mock import AsyncMock

# tests/models/test_cohere.py:81-82
    def create_mock(cls, completions: MockChatResponse | Sequence[MockChatResponse]) -> AsyncClientV2:
        return cast(AsyncClientV2, cls(completions=completions))

# tests/models/mock_openai.py:121-141
    async def responses_create(  # pragma: lax no cover
        self, *_args: Any, stream: bool = False, **kwargs: Any
    ) -> responses.Response | MockAsyncStream[MockResponseStreamEvent]:
        self.response_kwargs.append({k: v for k, v in kwargs.items() if v is not NOT_GIVEN})

        if stream:
            assert self.stream is not None, 'you can only used `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockResponseStreamEvent], self.stream)))
        else:
            assert self.response is not None, 'you can only used `stream=False` if `response` are provided'
            if isinstance(self.response, Sequence):
                raise_if_exception(self.response[self.index])
                response = cast(responses.Response, self.response[self.index])
            else:
                raise_if_exception(self.response)
                response = cast(responses.Response, self.response)
        self.index += 1
        return response

# tests/models/mock_openai.py:29-72
class MockOpenAI:
    completions: MockChatCompletion | Sequence[MockChatCompletion] | None = None
    stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]] | None = None
    index: int = 0
    chat_completion_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    base_url: str = 'https://api.openai.com/v1'

    @cached_property
    def chat(self) -> Any:
        chat_completions = type('Completions', (), {'create': self.chat_completions_create})
        return type('Chat', (), {'completions': chat_completions})

    @classmethod
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(completions=completions))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))

    async def chat_completions_create(  # pragma: lax no cover
        self, *_args: Any, stream: bool = False, **kwargs: Any
    ) -> chat.ChatCompletion | MockAsyncStream[MockChatCompletionChunk]:
        self.chat_completion_kwargs.append({k: v for k, v in kwargs.items() if v not in (NOT_GIVEN, OMIT)})

        if stream:
            assert self.stream is not None, 'you can only used `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(iter(cast(list[MockChatCompletionChunk], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockChatCompletionChunk], self.stream)))
        else:
            assert self.completions is not None, 'you can only used `stream=False` if `completions` are provided'
            if isinstance(self.completions, Sequence):
                raise_if_exception(self.completions[self.index])
                response = cast(chat.ChatCompletion, self.completions[self.index])
            else:
                raise_if_exception(self.completions)
                response = cast(chat.ChatCompletion, self.completions)
        self.index += 1
        return response

# tests/models/mock_openai.py:29-72
class MockOpenAI:
    completions: MockChatCompletion | Sequence[MockChatCompletion] | None = None
    stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]] | None = None
    index: int = 0
    chat_completion_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    base_url: str = 'https://api.openai.com/v1'

    @cached_property
    def chat(self) -> Any:
        chat_completions = type('Completions', (), {'create': self.chat_completions_create})
        return type('Chat', (), {'completions': chat_completions})

    @classmethod
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(completions=completions))

    @classmethod
    def create_mock_stream(
        cls,
        stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))

    async def chat_completions_create(  # pragma: lax no cover
        self, *_args: Any, stream: bool = False, **kwargs: Any
    ) -> chat.ChatCompletion | MockAsyncStream[MockChatCompletionChunk]:
        self.chat_completion_kwargs.append({k: v for k, v in kwargs.items() if v not in (NOT_GIVEN, OMIT)})

        if stream:
            assert self.stream is not None, 'you can only used `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(iter(cast(list[MockChatCompletionChunk], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockChatCompletionChunk], self.stream)))
        else:
            assert self.completions is not None, 'you can only used `stream=False` if `completions` are provided'
            if isinstance(self.completions, Sequence):
                raise_if_exception(self.completions[self.index])
                response = cast(chat.ChatCompletion, self.completions[self.index])
            else:
                raise_if_exception(self.completions)
                response = cast(chat.ChatCompletion, self.completions)
        self.index += 1
        return response

# tests/models/mock_openai.py:101-101
    stream: Sequence[MockResponseStreamEvent] | Sequence[Sequence[MockResponseStreamEvent]] | None = None

# tests/models/test_groq.py:113-117
    def create_mock_stream(
        cls,
        stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]],
    ) -> AsyncGroq:
        return cast(AsyncGroq, cls(stream=stream))

# tests/models/mock_openai.py:46-50
    def create_mock_stream(
        cls,
        stream: Sequence[MockChatCompletionChunk] | Sequence[Sequence[MockChatCompletionChunk]],
    ) -> AsyncOpenAI:
        return cast(AsyncOpenAI, cls(stream=stream))

# tests/models/test_anthropic.py:160-163
    def create_stream_mock(
        cls, stream: Sequence[MockRawMessageStreamEvent] | Sequence[Sequence[MockRawMessageStreamEvent]]
    ) -> AsyncAnthropic:
        return cast(AsyncAnthropic, cls(stream=stream))

# tests/models/test_mistral.py:112-115
    def create_stream_mock(
        cls, completions_streams: Sequence[MockCompletionEvent] | Sequence[Sequence[MockCompletionEvent]]
    ) -> Mistral:
        return cast(Mistral, cls(stream=completions_streams))

# tests/models/mock_openai.py:104-104
    base_url: str = 'https://api.openai.com/v1'

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/models/anthropic/conftest.py:11-11
from ..test_anthropic import MockAnthropic, completion_message

# tests/models/anthropic/conftest.py:11-11
from ..test_anthropic import MockAnthropic, completion_message

# tests/models/anthropic/conftest.py:11-11
from ..test_anthropic import MockAnthropic, completion_message

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/models/test_mistral.py:87-135
class MockMistralAI:
    completions: MockChatCompletion | Sequence[MockChatCompletion] | None = None
    stream: Sequence[MockCompletionEvent] | Sequence[Sequence[MockCompletionEvent]] | None = None
    index: int = 0

    @cached_property
    def sdk_configuration(self) -> MockSdkConfiguration:
        return MockSdkConfiguration()

    @cached_property
    def chat(self) -> Any:
        if self.stream:
            return type(
                'Chat',
                (),
                {'stream_async': self.chat_completions_create, 'complete_async': self.chat_completions_create},
            )
        else:
            return type('Chat', (), {'complete_async': self.chat_completions_create})

    @classmethod
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> Mistral:
        return cast(Mistral, cls(completions=completions))

    @classmethod
    def create_stream_mock(
        cls, completions_streams: Sequence[MockCompletionEvent] | Sequence[Sequence[MockCompletionEvent]]
    ) -> Mistral:
        return cast(Mistral, cls(stream=completions_streams))

    async def chat_completions_create(  # pragma: lax no cover
        self, *_args: Any, stream: bool = False, **_kwargs: Any
    ) -> MistralChatCompletionResponse | MockAsyncStream[MockCompletionEvent]:
        if stream or self.stream:
            assert self.stream is not None, 'you can only use `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], list):
                response = MockAsyncStream(iter(cast(list[MockCompletionEvent], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockCompletionEvent], self.stream)))
        else:
            assert self.completions is not None, 'you can only use `stream=False` if `completions` are provided'
            if isinstance(self.completions, Sequence):
                raise_if_exception(self.completions[self.index])
                response = cast(MistralChatCompletionResponse, self.completions[self.index])
            else:
                raise_if_exception(self.completions)
                response = cast(MistralChatCompletionResponse, self.completions)
        self.index += 1
        return response

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/conftest.py:23-23
from pytest_mock import MockerFixture

# tests/test_examples.py:310-334
class MockMCPServer(AbstractToolset[Any]):
    def __init__(self, *args: Any, **kwargs: Any) -> None:
        pass

    @property
    def id(self) -> str | None:
        return None  # pragma: no cover

    @property
    def instructions(self) -> str | None:
        return None

    async def __aenter__(self) -> MockMCPServer:
        return self

    async def __aexit__(self, *args: Any) -> None:
        pass

    async def get_tools(self, ctx: RunContext[Any]) -> dict[str, ToolsetTool[Any]]:
        return {}

    async def call_tool(
        self, name: str, tool_args: dict[str, Any], ctx: RunContext[Any], tool: ToolsetTool[Any]
    ) -> Any:
        return None  # pragma: lax no cover

# tests/models/test_huggingface.py:95-98
    def create_stream_mock(
        cls, stream: Sequence[MockStreamEvent] | Sequence[Sequence[MockStreamEvent]]
    ) -> AsyncInferenceClient:
        return cast(AsyncInferenceClient, cls(stream=stream))

# tests/evals/test_llm_as_a_judge.py:73-91
async def test_judge_output_mock(mocker: MockerFixture):
    """Test judge_output function with mocked agent."""
    # Mock the agent run method
    mock_result = mocker.MagicMock()
    mock_result.output = GradingOutput(reason='Test passed', pass_=True, score=1.0)
    mock_run = mocker.patch('pydantic_ai.agent.AbstractAgent.run', return_value=mock_result)

    # Test with string output
    grading_output = await judge_output('Hello world', 'Content contains a greeting')
    assert isinstance(grading_output, GradingOutput)
    assert grading_output.reason == 'Test passed'
    assert grading_output.pass_ is True
    assert grading_output.score == 1.0

    # Verify the agent was called with correct prompt
    mock_run.assert_called_once()
    call_args = mock_run.call_args[0]
    assert '<Output>\nHello world\n</Output>' in call_args[0]
    assert '<Rubric>\nContent contains a greeting\n</Rubric>' in call_args[0]

# tests/models/anthropic/test_output.py:27-27
from ..test_anthropic import MockAnthropic, get_mock_chat_completion_kwargs

# tests/models/anthropic/test_output.py:27-27
from ..test_anthropic import MockAnthropic, get_mock_chat_completion_kwargs

# tests/models/anthropic/test_output.py:27-27
from ..test_anthropic import MockAnthropic, get_mock_chat_completion_kwargs

# tests/models/anthropic/test_output.py:27-27
from ..test_anthropic import MockAnthropic, get_mock_chat_completion_kwargs

# tests/models/anthropic/test_output.py:27-27
from ..test_anthropic import MockAnthropic, get_mock_chat_completion_kwargs

# tests/evals/test_reporting.py:37-42
def mock_evaluator() -> Evaluator[TaskInput, TaskOutput, TaskMetadata]:
    class MockEvaluator(Evaluator[TaskInput, TaskOutput, TaskMetadata]):
        def evaluate(self, ctx: EvaluatorContext[TaskInput, TaskOutput, TaskMetadata]) -> bool:
            raise NotImplementedError

    return MockEvaluator()

# tests/evals/test_reporting.py:37-42
def mock_evaluator() -> Evaluator[TaskInput, TaskOutput, TaskMetadata]:
    class MockEvaluator(Evaluator[TaskInput, TaskOutput, TaskMetadata]):
        def evaluate(self, ctx: EvaluatorContext[TaskInput, TaskOutput, TaskMetadata]) -> bool:
            raise NotImplementedError

    return MockEvaluator()

# tests/models/mock_xai.py:134-141
    def stream(self) -> MockAsyncStream[tuple[chat_types.Response, Any]]:
        """Mock the stream() method for streaming responses."""
        assert self.stream_data is not None, 'you can only use stream() if stream_data is provided'

        data = list(self.stream_data[self.index])
        self.parent.index += 1

        return MockAsyncStream(iter(data))

# tests/models/mock_async_stream.py:22-63
class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

# tests/models/mock_async_stream.py:22-63
class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

# tests/models/mock_async_stream.py:22-63
class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

# tests/models/mock_async_stream.py:22-63
class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

# tests/models/mock_async_stream.py:22-63
class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

# tests/models/test_huggingface.py:78-119
class MockHuggingFace:
    completions: MockChatCompletion | Sequence[MockChatCompletion] | None = None
    stream: Sequence[MockStreamEvent] | Sequence[Sequence[MockStreamEvent]] | None = None
    index: int = 0
    chat_completion_kwargs: list[dict[str, Any]] = field(default_factory=list[dict[str, Any]])
    model: str = 'https://api-inference.huggingface.co'

    @cached_property
    def chat(self) -> Any:
        completions = type('Completions', (), {'create': self.chat_completions_create})
        return type('Chat', (), {'completions': completions})

    @classmethod
    def create_mock(cls, completions: MockChatCompletion | Sequence[MockChatCompletion]) -> AsyncInferenceClient:
        return cast(AsyncInferenceClient, cls(completions=completions))

    @classmethod
    def create_stream_mock(
        cls, stream: Sequence[MockStreamEvent] | Sequence[Sequence[MockStreamEvent]]
    ) -> AsyncInferenceClient:
        return cast(AsyncInferenceClient, cls(stream=stream))

    async def chat_completions_create(
        self, *_args: Any, stream: bool = False, **kwargs: Any
    ) -> ChatCompletionOutput | MockAsyncStream[MockStreamEvent]:
        self.chat_completion_kwargs.append(kwargs)
        if stream or self.stream:
            assert self.stream is not None, 'you can only use `stream=True` if `stream` is provided'
            if isinstance(self.stream[0], Sequence):
                response = MockAsyncStream(iter(cast(list[MockStreamEvent], self.stream[self.index])))
            else:
                response = MockAsyncStream(iter(cast(list[MockStreamEvent], self.stream)))
        else:
            assert self.completions is not None, 'you can only use `stream=False` if `completions` are provided'
            if isinstance(self.completions, Sequence):
                raise_if_exception(self.completions[self.index])
                response = cast(ChatCompletionOutput, self.completions[self.index])
            else:
                raise_if_exception(self.completions)
                response = cast(ChatCompletionOutput, self.completions)
        self.index += 1
        return response

# tests/models/mock_async_stream.py:22-63
class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

# tests/test_examples.py:1079-1080
class MockCredentials:
    project_id = 'foobar'

# tests/models/mock_async_stream.py:22-63
class MockAsyncStream(Generic[T]):
    """Wraps a synchronous iterator in an asynchronous interface.

    This class allows a synchronous iterator to be treated as an
    asynchronous iterator, enabling iteration in `async for` loops
    and usage within `async with` blocks.

    Example usage:
        async def example():
            sync_iter = iter([1, 2, 3])
            async_stream = MockAsyncStream(sync_iter)

            async for item in async_stream:
                print(item)

            async with MockAsyncStream(sync_iter) as stream:
                async for item in stream:
                    print(item)
    """

    _iter: Iterator[T]
    """The underlying synchronous iterator."""

    async def __anext__(self) -> T:
        """Return the next item from the synchronous iterator as if it were asynchronous.

        Calls `_utils.sync_anext` to retrieve the next item from the underlying
        synchronous iterator. If the iterator is exhausted, `StopAsyncIteration`
        is raised.
        """
        next = _utils.sync_anext(self._iter)
        raise_if_exception(next)
        return next

    def __aiter__(self) -> MockAsyncStream[T]:
        return self

    async def __aenter__(self) -> MockAsyncStream[T]:
        return self

    async def __aexit__(self, *_args: Any) -> None:
        pass

# tests/models/mock_xai.py:120-132
    async def sample(self) -> chat_types.Response:
        """Mock the sample() method for non-streaming responses."""
        assert self.responses is not None, 'you can only use sample() if responses are provided'

        if self.index >= len(self.responses):
            raise IndexError(f'Mock response index {self.index} out of range (length: {len(self.responses)})')
        raise_if_exception(self.responses[self.index])
        response = cast(chat_types.Response, self.responses[self.index])
        # Increment index for next call
        self.index += 1
        self.parent.index = self.index

        return response

# tests/conftest.py:662-670
def mock_snapshot_id(mocker: MockerFixture):
    i = 0

    def generate_snapshot_id(node_id: str) -> str:
        nonlocal i
        i += 1
        return f'{node_id}:{i}'

    return mocker.patch('pydantic_graph.nodes.generate_snapshot_id', side_effect=generate_snapshot_id)

# tests/models/test_outlines.py:100-127
def mock_async_model() -> OutlinesModel:
    class MockOutlinesAsyncModel(OutlinesAsyncBaseModel):
        """Mock an OutlinesAsyncModel because no Outlines local models have an async version.

        The `__call__` and `stream` methods will be called by the Pydantic AI model while the other methods are
        only implemented because they are abstract methods in the OutlinesAsyncModel class.
        """

        async def __call__(self, model_input: Any, output_type: Any, backend: Any, **inference_kwargs: Any) -> str:  # pyright: ignore[reportIncompatibleMethodOverride]
            return 'test'

        async def stream(self, model_input: Any, output_type: Any, backend: Any, **inference_kwargs: Any):  # pyright: ignore[reportIncompatibleMethodOverride]
            for _ in range(2):
                yield 'test'

        async def generate(  # pyright: ignore[reportIncompatibleMethodOverride]  # pragma: no cover
            self, model_input: Any, output_type: Any, **inference_kwargs: Any
        ): ...

        async def generate_batch(  # pyright: ignore[reportIncompatibleMethodOverride]  # pragma: no cover
            self, model_input: Any, output_type: Any, **inference_kwargs: Any
        ): ...

        async def generate_stream(  # pyright: ignore[reportIncompatibleMethodOverride]  # pragma: no cover
            self, model_input: Any, output_type: Any, **inference_kwargs: Any
        ): ...

    return OutlinesModel(MockOutlinesAsyncModel(), provider=OutlinesProvider())

# tests/test_examples.py:1034-1070
def mock_infer_model(model: Model | KnownModelName) -> Model:
    if model == 'test':
        return TestModel()

    if isinstance(model, str):
        # Use the non-mocked model inference to ensure we get the same model name the user would
        model = infer_model(model)

    if isinstance(model, FallbackModel):
        # When a fallback model is encountered, replace any OpenAIModel with a model that will raise a ModelHTTPError.
        # Otherwise, do the usual inference.
        def raise_http_error(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:
            raise ModelHTTPError(401, 'Invalid API Key')

        mock_fallback_models: list[Model] = []
        for m in model.models:
            try:
                from pydantic_ai.models.openai import OpenAIChatModel
            except ImportError:  # pragma: lax no cover
                OpenAIChatModel = type(None)

            if isinstance(m, OpenAIChatModel):
                # Raise an HTTP error for OpenAIChatModel
                mock_fallback_models.append(FunctionModel(raise_http_error, model_name=m.model_name))
            else:
                mock_fallback_models.append(mock_infer_model(m))
        return FallbackModel(*mock_fallback_models)
    if isinstance(model, FunctionModel | TestModel):
        return model
    else:
        model_name = model if isinstance(model, str) else model.model_name
        return FunctionModel(
            model_logic,
            stream_function=stream_model_logic,
            model_name=model_name,
            profile=model.profile if isinstance(model, Model) else None,
        )

# tests/models/test_cohere.py:66-68
class MockClientWrapper:
    def get_base_url(self) -> str:
        return 'https://api.cohere.com'

# tests/models/test_cohere.py:72-93
class MockAsyncClientV2:
    completions: MockChatResponse | Sequence[MockChatResponse] | None = None
    index = 0
    _client_wrapper: MockClientWrapper = None  # type: ignore

    def __post_init__(self):
        self._client_wrapper = MockClientWrapper()

    @classmethod
    def create_mock(cls, completions: MockChatResponse | Sequence[MockChatResponse]) -> AsyncClientV2:
        return cast(AsyncClientV2, cls(completions=completions))

    async def chat(self, *_args: Any, **_kwargs: Any) -> ChatResponse:
        assert self.completions is not None
        if isinstance(self.completions, Sequence):
            raise_if_exception(self.completions[self.index])
            response = cast(ChatResponse, self.completions[self.index])
        else:
            raise_if_exception(self.completions)
            response = cast(ChatResponse, self.completions)
        self.index += 1
        return response

# tests/providers/test_google_vertex.py:70-71
async def mock_refresh_token():
    return 'my-token'

# tests/evals/test_llm_as_a_judge.py:123-142
async def test_judge_input_output_mock(mocker: MockerFixture):
    """Test judge_input_output function with mocked agent."""
    # Mock the agent run method
    mock_result = mocker.MagicMock()
    mock_result.output = GradingOutput(reason='Test passed', pass_=True, score=1.0)
    mock_run = mocker.patch('pydantic_ai.agent.AbstractAgent.run', return_value=mock_result)

    # Test with string input and output
    result = await judge_input_output('Hello', 'Hello world', 'Output contains input')
    assert isinstance(result, GradingOutput)
    assert result.reason == 'Test passed'
    assert result.pass_ is True
    assert result.score == 1.0

    # Verify the agent was called with correct prompt
    mock_run.assert_called_once()
    call_args = mock_run.call_args[0]
    assert '<Input>\nHello\n</Input>' in call_args[0]
    assert '<Output>\nHello world\n</Output>' in call_args[0]
    assert '<Rubric>\nOutput contains input\n</Rubric>' in call_args[0]

# tests/models/mock_xai.py:144-168
def get_mock_chat_create_kwargs(async_client: AsyncClient) -> list[dict[str, Any]]:
    """Extract the kwargs passed to chat.create from a mock client.

    Messages, tools, and response_format are automatically converted from protobuf to dicts for easier testing.
    """
    if isinstance(async_client, MockXai):
        result: list[dict[str, Any]] = []
        for kwargs in async_client.chat_create_kwargs:
            kwargs_copy: dict[str, Any] = dict(kwargs)
            if 'messages' in kwargs_copy:  # pragma: no branch
                kwargs_copy['messages'] = [
                    MessageToDict(msg, preserving_proto_field_name=True) for msg in kwargs_copy['messages']
                ]
            if 'tools' in kwargs_copy and kwargs_copy['tools'] is not None:
                kwargs_copy['tools'] = [
                    MessageToDict(tool, preserving_proto_field_name=True) for tool in kwargs_copy['tools']
                ]
            if 'response_format' in kwargs_copy and kwargs_copy['response_format'] is not None:
                kwargs_copy['response_format'] = MessageToDict(
                    kwargs_copy['response_format'], preserving_proto_field_name=True
                )
            result.append(kwargs_copy)
        return result
    else:  # pragma: no cover
        raise RuntimeError('Not a MockXai instance')

# tests/test_examples.py:275-276
def mock_render_duration(seconds: float, force_signed: bool) -> str:
    return '10ms'

# tests/models/mock_xai.py:68-70
    def chat(self) -> Any:
        """Create mock chat interface."""
        return type('Chat', (), {'create': self.chat_create})

# tests/models/mock_xai.py:73-75
    def files(self) -> Any:
        """Create mock files interface."""
        return type('Files', (), {'upload': self.files_upload})

# tests/models/test_mistral.py:93-94
    def sdk_configuration(self) -> MockSdkConfiguration:
        return MockSdkConfiguration()

# tests/models/test_groq.py:98-98
    completions: MockChatCompletion | Sequence[MockChatCompletion] | None = None

# tests/models/test_mistral.py:82-83
    def get_server_details(self) -> tuple[str, ...]:
        return ('https://api.mistral.ai',)