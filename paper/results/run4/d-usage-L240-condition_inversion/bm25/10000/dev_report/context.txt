# pydantic_ai_slim/pydantic_ai/usage.py:182-182
    input_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:191-191
    input_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:179-179
    tool_calls: int = 0

# tests/test_usage_limits.py:226-255
def test_add_usages():
    usage = RunUsage(
        requests=2,
        input_tokens=10,
        output_tokens=20,
        cache_read_tokens=30,
        cache_write_tokens=40,
        input_audio_tokens=50,
        cache_audio_read_tokens=60,
        tool_calls=3,
        details={
            'custom1': 10,
            'custom2': 20,
        },
    )
    assert usage + usage == snapshot(
        RunUsage(
            requests=4,
            input_tokens=20,
            output_tokens=40,
            cache_write_tokens=80,
            cache_read_tokens=60,
            input_audio_tokens=100,
            cache_audio_read_tokens=120,
            tool_calls=6,
            details={'custom1': 20, 'custom2': 40},
        )
    )
    assert usage + RunUsage() == usage
    assert RunUsage() + RunUsage() == RunUsage()

# pydantic_ai_slim/pydantic_ai/usage.py:39-39
    input_audio_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:20-24
    input_tokens: Annotated[
        int,
        # `request_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('input_tokens', 'request_tokens')),
    ] = 0

# pydantic_ai_slim/pydantic_ai/usage.py:176-176
    requests: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:197-197
    output_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:188-188
    cache_read_tokens: int = 0

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# pydantic_ai_slim/pydantic_ai/usage.py:185-185
    cache_write_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:264-264
    input_tokens_limit: int | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:200-200
    details: dict[str, int] = dataclasses.field(default_factory=dict[str, int])

# pydantic_ai_slim/pydantic_ai/usage.py:64-66
    def total_tokens(self) -> int:
        """Sum of `input_tokens + output_tokens`."""
        return self.input_tokens + self.output_tokens

# tests/test_embeddings.py:1409-1411
    async def test_max_input_tokens(self, embedder: Embedder):
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(512)

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:195-202
_MAX_INPUT_TOKENS: dict[str, int] = {
    'amazon.titan-embed-text-v1': 8192,
    'amazon.titan-embed-text-v2:0': 8192,
    'cohere.embed-english-v3': 512,
    'cohere.embed-multilingual-v3': 512,
    'cohere.embed-v4:0': 128000,
    'amazon.nova-2-multimodal-embeddings-v1:0': 8192,
}

# tests/test_embeddings.py:161-163
    async def test_max_input_tokens(self, embedder: Embedder):
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(8192)

# tests/test_embeddings.py:366-370
    async def test_max_input_tokens(self, co_api_key: str):
        model = CohereEmbeddingModel('embed-v4.0', provider=CohereProvider(api_key=co_api_key))
        embedder = Embedder(model)
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(128000)

# tests/test_embeddings.py:1261-1263
    async def test_max_input_tokens(self, embedder: Embedder):
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(2048)

# tests/test_embeddings.py:480-484
    async def test_max_input_tokens(self, voyage_api_key: str):
        model = VoyageAIEmbeddingModel('voyage-3.5', provider=VoyageAIProvider(api_key=voyage_api_key))
        embedder = Embedder(model)
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(32000)

# pydantic_ai_slim/pydantic_ai/embeddings/test.py:115-116
    async def max_input_tokens(self) -> int | None:
        return 1024

# tests/test_usage_limits.py:258-284
def test_add_usages_with_none_detail_value():
    """Test that None values in details are skipped when incrementing usage."""
    usage = RunUsage(
        requests=1,
        input_tokens=10,
        output_tokens=20,
        details={'reasoning_tokens': 5},
    )

    # Create a usage with None in details (simulating model response with missing detail)
    incr_usage = RunUsage(
        requests=1,
        input_tokens=5,
        output_tokens=10,
    )
    # Manually set a None value in details to simulate edge case from model responses
    incr_usage.details = {'reasoning_tokens': None, 'other_tokens': 10}  # type: ignore[dict-item]

    result = usage + incr_usage
    assert result == snapshot(
        RunUsage(
            requests=2,
            input_tokens=15,
            output_tokens=30,
            details={'reasoning_tokens': 5, 'other_tokens': 10},
        )
    )

# pydantic_ai_slim/pydantic_ai/usage.py:246-247
class Usage(RunUsage):
    """Deprecated alias for `RunUsage`."""

# pydantic_ai_slim/pydantic_ai/usage.py:194-194
    cache_audio_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:262-262
    tool_calls_limit: int | None = None

# tests/test_embeddings.py:1034-1038
    async def test_nova_max_input_tokens(self, bedrock_provider: BedrockProvider):
        model = BedrockEmbeddingModel('amazon.nova-2-multimodal-embeddings-v1:0', provider=bedrock_provider)
        embedder = Embedder(model)
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(8192)

# pydantic_ai_slim/pydantic_ai/embeddings/cohere.py:204-205
    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self.model_name)

# pydantic_ai_slim/pydantic_ai/embeddings/google.py:192-193
    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self._model_name)

# pydantic_ai_slim/pydantic_ai/embeddings/openai.py:150-155
    async def max_input_tokens(self) -> int | None:
        if self.system != 'openai':
            return None

        # https://platform.openai.com/docs/guides/embeddings#embedding-models
        return 8192

# tests/test_embeddings.py:1010-1014
    async def test_titan_v1_max_input_tokens(self, bedrock_provider: BedrockProvider):
        model = BedrockEmbeddingModel('amazon.titan-embed-text-v1', provider=bedrock_provider)
        embedder = Embedder(model)
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(8192)

# tests/test_embeddings.py:1016-1020
    async def test_titan_v2_max_input_tokens(self, bedrock_provider: BedrockProvider):
        model = BedrockEmbeddingModel('amazon.titan-embed-text-v2:0', provider=bedrock_provider)
        embedder = Embedder(model)
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(8192)

# pydantic_ai_slim/pydantic_ai/embeddings/wrapper.py:48-49
    async def max_input_tokens(self) -> int | None:
        return await self.wrapped.max_input_tokens()

# pydantic_ai_slim/pydantic_ai/embeddings/voyageai.py:183-184
    async def max_input_tokens(self) -> int | None:
        return _MAX_INPUT_TOKENS.get(self.model_name)

# tests/test_embeddings.py:1022-1026
    async def test_cohere_v3_max_input_tokens(self, bedrock_provider: BedrockProvider):
        model = BedrockEmbeddingModel('cohere.embed-english-v3', provider=bedrock_provider)
        embedder = Embedder(model)
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(512)

# tests/test_embeddings.py:1028-1032
    async def test_cohere_v4_max_input_tokens(self, bedrock_provider: BedrockProvider):
        model = BedrockEmbeddingModel('cohere.embed-v4:0', provider=bedrock_provider)
        embedder = Embedder(model)
        max_input_tokens = await embedder.max_input_tokens()
        assert max_input_tokens == snapshot(128000)

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:360-362
    def max_input_tokens_sync(self) -> int | None:
        """Synchronous version of [`max_input_tokens()`][pydantic_ai.embeddings.Embedder.max_input_tokens]."""
        return _utils.get_event_loop().run_until_complete(self.max_input_tokens())

# examples/pydantic_ai_examples/flight_booking.py:14-21
from pydantic_ai import (
    Agent,
    ModelMessage,
    ModelRetry,
    RunContext,
    RunUsage,
    UsageLimits,
)

# pydantic_ai_slim/pydantic_ai/embeddings/sentence_transformers.py:159-161
    async def max_input_tokens(self) -> int | None:
        model = await self._get_model()
        return model.get_max_seq_length()

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:139-139
    input: Any

# pydantic_ai_slim/pydantic_ai/output.py:392-393
    def tool_calls(self) -> list[ToolCallPart]:
        return self.calls

# pydantic_ai_slim/pydantic_ai/usage.py:214-221
    def __add__(self, other: RunUsage | RequestUsage) -> RunUsage:
        """Add two RunUsages together.

        This is provided so it's trivial to sum usage information from multiple runs.
        """
        new_usage = copy(self)
        new_usage.incr(other)
        return new_usage

# pydantic_evals/pydantic_evals/reporting/__init__.py:111-111
    inputs: InputsT

# pydantic_ai_slim/pydantic_ai/embeddings/bedrock.py:661-663
    async def max_input_tokens(self) -> int | None:
        """Get the maximum number of tokens that can be input to the model."""
        return _MAX_INPUT_TOKENS.get(self._handler.model_name, None)

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:133-143
class ToolInputErrorChunk(BaseChunk):
    """Tool input error chunk."""

    type: Literal['tool-input-error'] = 'tool-input-error'
    tool_call_id: str
    tool_name: str
    input: Any
    provider_executed: bool | None = None
    provider_metadata: ProviderMetadata | None = None
    dynamic: bool | None = None
    error_text: str

# pydantic_ai_slim/pydantic_ai/usage.py:27-27
    cache_write_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/usage.py:29-29
    cache_read_tokens: int = 0

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:155-155
    input: Any | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:203-212
    def incr(self, incr_usage: RunUsage | RequestUsage) -> None:
        """Increment the usage in place.

        Args:
            incr_usage: The usage to increment by.
        """
        if isinstance(incr_usage, RunUsage):
            self.requests += incr_usage.requests
            self.tool_calls += incr_usage.tool_calls
        return _incr_usage_tokens(self, incr_usage)

# pydantic_ai_slim/pydantic_ai/usage.py:32-36
    output_tokens: Annotated[
        int,
        # `response_tokens` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed
        Field(validation_alias=AliasChoices('output_tokens', 'response_tokens')),
    ] = 0

# pydantic_ai_slim/pydantic_ai/models/test.py:169-175
    def _get_tool_calls(self, model_request_parameters: ModelRequestParameters) -> list[tuple[str, ToolDefinition]]:
        if self.call_tools == 'all':
            return [(r.name, r) for r in model_request_parameters.function_tools]
        else:
            function_tools_lookup = {t.name: t for t in model_request_parameters.function_tools}
            tools_to_call = (function_tools_lookup[name] for name in self.call_tools)
            return [(r.name, r) for r in tools_to_call]

# tests/models/test_anthropic.py:1279-1305
async def test_parallel_tool_calls(allow_model_requests: None, parallel_tool_calls: bool) -> None:
    responses = [
        completion_message(
            [BetaToolUseBlock(id='1', input={'loc_name': 'San Francisco'}, name='get_location', type='tool_use')],
            usage=BetaUsage(input_tokens=2, output_tokens=1),
        ),
        completion_message(
            [BetaTextBlock(text='final response', type='text')],
            usage=BetaUsage(input_tokens=3, output_tokens=5),
        ),
    ]

    mock_client = MockAnthropic.create_mock(responses)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(m, model_settings=ModelSettings(parallel_tool_calls=parallel_tool_calls))

    @agent.tool_plain
    async def get_location(loc_name: str) -> str:
        if loc_name == 'London':
            return json.dumps({'lat': 51, 'lng': 0})  # pragma: no cover
        else:
            raise ModelRetry('Wrong location, please try again')

    await agent.run('hello')
    assert get_mock_chat_completion_kwargs(mock_client)[0]['tool_choice']['disable_parallel_tool_use'] == (
        not parallel_tool_calls
    )

# tests/models/test_anthropic.py:1279-1305
async def test_parallel_tool_calls(allow_model_requests: None, parallel_tool_calls: bool) -> None:
    responses = [
        completion_message(
            [BetaToolUseBlock(id='1', input={'loc_name': 'San Francisco'}, name='get_location', type='tool_use')],
            usage=BetaUsage(input_tokens=2, output_tokens=1),
        ),
        completion_message(
            [BetaTextBlock(text='final response', type='text')],
            usage=BetaUsage(input_tokens=3, output_tokens=5),
        ),
    ]

    mock_client = MockAnthropic.create_mock(responses)
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(anthropic_client=mock_client))
    agent = Agent(m, model_settings=ModelSettings(parallel_tool_calls=parallel_tool_calls))

    @agent.tool_plain
    async def get_location(loc_name: str) -> str:
        if loc_name == 'London':
            return json.dumps({'lat': 51, 'lng': 0})  # pragma: no cover
        else:
            raise ModelRetry('Wrong location, please try again')

    await agent.run('hello')
    assert get_mock_chat_completion_kwargs(mock_client)[0]['tool_choice']['disable_parallel_tool_use'] == (
        not parallel_tool_calls
    )

# tests/models/test_gemini_vertex.py:125-158
async def test_url_input(
    url: AudioUrl | DocumentUrl | ImageUrl | VideoUrl, expected_output: str, allow_model_requests: None
) -> None:  # pragma: lax no cover
    provider = GoogleVertexProvider(project_id='pydantic-ai', region='us-central1')
    m = GeminiModel('gemini-2.0-flash', provider=provider)
    agent = Agent(m)
    result = await agent.run(['What is the main content of this URL?', url])

    assert result.output == snapshot(Is(expected_output))
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content=['What is the main content of this URL?', Is(url)],
                        timestamp=IsDatetime(),
                    ),
                ],
                timestamp=IsDatetime(),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content=Is(expected_output))],
                usage=IsInstance(RequestUsage),
                model_name='gemini-2.0-flash',
                timestamp=IsDatetime(),
                provider_name='google-vertex',
                provider_url='https://us-central1-aiplatform.googleapis.com/v1/projects/pydantic-ai/locations/us-central1/publishers/google/models/',
                provider_details={'finish_reason': 'STOP'},
                provider_response_id=IsStr(),
                run_id=IsStr(),
            ),
        ]
    )

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:209-209
    input: Any

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:156-156
    raw_input: Any | None = None

# pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies.py:4-4
from prefect.cache_policies import INPUTS, RUN_ID, TASK_SOURCE, CachePolicy

# pydantic_graph/pydantic_graph/beta/graph.py:60-60
InputT = TypeVar('InputT', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/graph.py:60-60
InputT = TypeVar('InputT', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/graph.py:60-60
InputT = TypeVar('InputT', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/graph.py:60-60
InputT = TypeVar('InputT', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/graph.py:60-60
InputT = TypeVar('InputT', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/graph.py:60-60
InputT = TypeVar('InputT', infer_variance=True)

# pydantic_graph/pydantic_graph/beta/graph.py:60-60
InputT = TypeVar('InputT', infer_variance=True)

# tests/models/test_outlines.py:600-668
def test_input_format(transformers_multimodal_model: OutlinesModel, binary_image: BinaryImage) -> None:
    agent = Agent(transformers_multimodal_model)

    # all accepted message types
    message_history: list[ModelMessage] = [
        ModelRequest(
            parts=[
                SystemPromptPart(content='You are a helpful assistance'),
                UserPromptPart(content='Hello'),
                RetryPromptPart(content='Failure'),
            ],
            timestamp=IsDatetime(),
        ),
        ModelResponse(
            parts=[
                ThinkingPart('Thinking...'),  # ignored by the model
                TextPart('Hello there!'),
                FilePart(content=binary_image),
            ]
        ),
    ]
    agent.run_sync('How are you doing?', message_history=message_history)

    # unsupported: non-image multi-modal user prompts
    multi_modal_message_history: list[ModelMessage] = [
        ModelRequest(
            parts=[
                UserPromptPart(
                    content=[
                        'Hello there!',
                        AudioUrl('https://example.com/audio.mp3'),
                    ]
                )
            ],
            timestamp=IsDatetime(),
        )
    ]
    with pytest.raises(
        UserError, match='Each element of the content sequence must be a string, an `ImageUrl` or a `BinaryImage`.'
    ):
        agent.run_sync('How are you doing?', message_history=multi_modal_message_history)

    # unsupported: tool calls
    tool_call_message_history: list[ModelMessage] = [
        ModelResponse(parts=[ToolCallPart(tool_call_id='1', tool_name='get_location')]),
        ModelRequest(
            parts=[ToolReturnPart(tool_name='get_location', content='London', tool_call_id='1')], timestamp=IsDatetime()
        ),
    ]
    with pytest.raises(UserError, match='Tool calls are not supported for Outlines models yet.'):
        agent.run_sync('How are you doing?', message_history=tool_call_message_history)

    # unsupported: tool returns
    tool_return_message_history: list[ModelMessage] = [
        ModelRequest(
            parts=[ToolReturnPart(tool_name='get_location', content='London', tool_call_id='1')], timestamp=IsDatetime()
        )
    ]
    with pytest.raises(UserError, match='Tool calls are not supported for Outlines models yet.'):
        agent.run_sync('How are you doing?', message_history=tool_return_message_history)

    # unsupported: non-image file parts
    file_part_message_history: list[ModelMessage] = [
        ModelResponse(parts=[FilePart(content=BinaryContent(data=b'test', media_type='text/plain'))])
    ]
    with pytest.raises(
        UserError, match='File parts other than `BinaryImage` are not supported for Outlines models yet.'
    ):
        agent.run_sync('How are you doing?', message_history=file_part_message_history)

# tests/models/test_xai.py:1007-1019
async def test_xai_parallel_tool_calls(allow_model_requests: None, parallel_tool_calls: bool) -> None:
    tool_call = create_tool_call(
        id='123',
        name='final_result',
        arguments={'response': [1, 2, 3]},
    )
    response = create_response(content='', tool_calls=[tool_call], finish_reason='tool_call')
    mock_client = MockXai.create_mock([response])
    m = XaiModel(XAI_NON_REASONING_MODEL, provider=XaiProvider(xai_client=mock_client))
    agent = Agent(m, output_type=list[int], model_settings=ModelSettings(parallel_tool_calls=parallel_tool_calls))

    await agent.run('Hello')
    assert get_mock_chat_create_kwargs(mock_client)[0]['parallel_tool_calls'] == parallel_tool_calls

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:120-120
    input: Any | None = None

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:131-131
    input: Any | None = None

# pydantic_ai_slim/pydantic_ai/usage.py:113-114
    def requests(self):
        return 1

# pydantic_evals/pydantic_evals/dataset.py:70-70
InputsT = TypeVar('InputsT', default=Any)

# pydantic_evals/pydantic_evals/dataset.py:70-70
InputsT = TypeVar('InputsT', default=Any)

# pydantic_evals/pydantic_evals/dataset.py:70-70
InputsT = TypeVar('InputsT', default=Any)

# pydantic_evals/pydantic_evals/dataset.py:70-70
InputsT = TypeVar('InputsT', default=Any)

# pydantic_evals/pydantic_evals/dataset.py:70-70
InputsT = TypeVar('InputsT', default=Any)

# pydantic_evals/pydantic_evals/dataset.py:70-70
InputsT = TypeVar('InputsT', default=Any)

# pydantic_evals/pydantic_evals/dataset.py:70-70
InputsT = TypeVar('InputsT', default=Any)

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:127-127
    input: Any

# tests/models/test_mistral.py:2186-2232
async def test_pdf_url_input(allow_model_requests: None):
    c = completion_message(MistralAssistantMessage(content='world', role='assistant'))
    mock_client = MockMistralAI.create_mock(c)
    m = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(m)

    result = await agent.run(
        [
            'hello',
            DocumentUrl(url='https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf'),
        ]
    )
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content=[
                            'hello',
                            DocumentUrl(
                                url='https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf',
                                identifier='c6720d',
                            ),
                        ],
                        timestamp=IsDatetime(),
                    )
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content='world')],
                usage=RequestUsage(input_tokens=1, output_tokens=1),
                model_name='mistral-large-123',
                timestamp=IsDatetime(),
                provider_name='mistral',
                provider_url='https://api.mistral.ai',
                provider_details={
                    'finish_reason': 'stop',
                    'timestamp': datetime(2024, 1, 1, 0, 0, tzinfo=timezone.utc),
                },
                provider_response_id='123',
                finish_reason='stop',
                run_id=IsStr(),
            ),
        ]
    )

# tests/models/test_mistral.py:2278-2290
async def test_txt_url_input(allow_model_requests: None):
    c = completion_message(MistralAssistantMessage(content='world', role='assistant'))
    mock_client = MockMistralAI.create_mock(c)
    m = MistralModel('mistral-large-latest', provider=MistralProvider(mistral_client=mock_client))
    agent = Agent(m)

    with pytest.raises(RuntimeError, match='DocumentUrl other than PDF is not supported in Mistral.'):
        await agent.run(
            [
                'hello',
                DocumentUrl(url='https://examplefiles.org/files/documents/plaintext-example-file-download.txt'),
            ]
        )

# pydantic_ai_slim/pydantic_ai/usage.py:266-266
    output_tokens_limit: int | None = None

# tests/models/test_openai.py:1532-1558
async def test_multiple_agent_tool_calls(allow_model_requests: None, gemini_api_key: str, openai_api_key: str):
    gemini_model = GoogleModel('gemini-2.0-flash-exp', provider=GoogleProvider(api_key=gemini_api_key))
    openai_model = OpenAIChatModel('gpt-4o-mini', provider=OpenAIProvider(api_key=openai_api_key))

    agent = Agent(model=gemini_model)

    @agent.tool_plain
    async def get_capital(country: str) -> str:
        """Get the capital of a country.

        Args:
            country: The country name.
        """
        if country == 'France':
            return 'Paris'
        elif country == 'England':
            return 'London'
        else:
            raise ValueError(f'Country {country} not supported.')  # pragma: no cover

    result = await agent.run('What is the capital of France?')
    assert result.output == snapshot('The capital of France is Paris.\n')

    result = await agent.run(
        'What is the capital of England?', model=openai_model, message_history=result.all_messages()
    )
    assert result.output == snapshot('The capital of England is London.')

# pydantic_evals/pydantic_evals/dataset.py:134-134
    inputs: InputsT

# tests/test_ag_ui.py:222-235
def create_input(
    *messages: Message, tools: list[Tool] | None = None, thread_id: str | None = None, state: Any = None
) -> RunAgentInput:
    """Create a RunAgentInput for testing."""
    thread_id = thread_id or uuid_str()
    return RunAgentInput(
        thread_id=thread_id,
        run_id=uuid_str(),
        messages=list(messages),
        state=dict(state) if state else {},
        context=[],
        tools=tools or [],
        forwarded_props=None,
    )

# tests/models/test_anthropic.py:1545-1557
async def test_image_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    result = await agent.run(
        [
            'What is this vegetable?',
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ]
    )
    assert result.output == snapshot(
        "This is a potato. It's a yellow/golden-colored potato with a smooth, slightly bumpy skin typical of many potato varieties. The potato appears to be a whole, unpeeled tuber with a classic oblong or oval shape. Potatoes are starchy root vegetables that are widely consumed around the world and can be prepared in many ways, such as boiling, baking, frying, or mashing."
    )

# tests/models/test_anthropic.py:1545-1557
async def test_image_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    result = await agent.run(
        [
            'What is this vegetable?',
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ]
    )
    assert result.output == snapshot(
        "This is a potato. It's a yellow/golden-colored potato with a smooth, slightly bumpy skin typical of many potato varieties. The potato appears to be a whole, unpeeled tuber with a classic oblong or oval shape. Potatoes are starchy root vegetables that are widely consumed around the world and can be prepared in many ways, such as boiling, baking, frying, or mashing."
    )

# tests/models/test_bedrock.py:755-769
async def test_video_url_input(
    allow_model_requests: None, bedrock_provider: BedrockProvider, disable_ssrf_protection_for_vcr: None
):
    m = BedrockConverseModel('us.amazon.nova-pro-v1:0', provider=bedrock_provider)
    agent = Agent(m, instructions='You are a helpful chatbot.')

    result = await agent.run(
        [
            'Explain me this video',
            VideoUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/small_video.mp4'),
        ]
    )
    assert result.output == snapshot(
        'The video shows a camera set up on a tripod, pointed at a scenic view of a rocky landscape under a clear sky. The camera remains stationary throughout the video, capturing the same view without any changes.'
    )

# tests/models/test_anthropic.py:1545-1557
async def test_image_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    result = await agent.run(
        [
            'What is this vegetable?',
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ]
    )
    assert result.output == snapshot(
        "This is a potato. It's a yellow/golden-colored potato with a smooth, slightly bumpy skin typical of many potato varieties. The potato appears to be a whole, unpeeled tuber with a classic oblong or oval shape. Potatoes are starchy root vegetables that are widely consumed around the world and can be prepared in many ways, such as boiling, baking, frying, or mashing."
    )

# tests/models/test_bedrock.py:755-769
async def test_video_url_input(
    allow_model_requests: None, bedrock_provider: BedrockProvider, disable_ssrf_protection_for_vcr: None
):
    m = BedrockConverseModel('us.amazon.nova-pro-v1:0', provider=bedrock_provider)
    agent = Agent(m, instructions='You are a helpful chatbot.')

    result = await agent.run(
        [
            'Explain me this video',
            VideoUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/small_video.mp4'),
        ]
    )
    assert result.output == snapshot(
        'The video shows a camera set up on a tripod, pointed at a scenic view of a rocky landscape under a clear sky. The camera remains stationary throughout the video, capturing the same view without any changes.'
    )

# tests/models/test_anthropic.py:1545-1557
async def test_image_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    result = await agent.run(
        [
            'What is this vegetable?',
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ]
    )
    assert result.output == snapshot(
        "This is a potato. It's a yellow/golden-colored potato with a smooth, slightly bumpy skin typical of many potato varieties. The potato appears to be a whole, unpeeled tuber with a classic oblong or oval shape. Potatoes are starchy root vegetables that are widely consumed around the world and can be prepared in many ways, such as boiling, baking, frying, or mashing."
    )

# tests/models/test_anthropic.py:1545-1557
async def test_image_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    result = await agent.run(
        [
            'What is this vegetable?',
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ]
    )
    assert result.output == snapshot(
        "This is a potato. It's a yellow/golden-colored potato with a smooth, slightly bumpy skin typical of many potato varieties. The potato appears to be a whole, unpeeled tuber with a classic oblong or oval shape. Potatoes are starchy root vegetables that are widely consumed around the world and can be prepared in many ways, such as boiling, baking, frying, or mashing."
    )

# tests/models/test_anthropic.py:1545-1557
async def test_image_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    result = await agent.run(
        [
            'What is this vegetable?',
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ]
    )
    assert result.output == snapshot(
        "This is a potato. It's a yellow/golden-colored potato with a smooth, slightly bumpy skin typical of many potato varieties. The potato appears to be a whole, unpeeled tuber with a classic oblong or oval shape. Potatoes are starchy root vegetables that are widely consumed around the world and can be prepared in many ways, such as boiling, baking, frying, or mashing."
    )

# tests/models/test_bedrock.py:755-769
async def test_video_url_input(
    allow_model_requests: None, bedrock_provider: BedrockProvider, disable_ssrf_protection_for_vcr: None
):
    m = BedrockConverseModel('us.amazon.nova-pro-v1:0', provider=bedrock_provider)
    agent = Agent(m, instructions='You are a helpful chatbot.')

    result = await agent.run(
        [
            'Explain me this video',
            VideoUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/small_video.mp4'),
        ]
    )
    assert result.output == snapshot(
        'The video shows a camera set up on a tripod, pointed at a scenic view of a rocky landscape under a clear sky. The camera remains stationary throughout the video, capturing the same view without any changes.'
    )

# tests/models/test_anthropic.py:1545-1557
async def test_image_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-haiku-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    result = await agent.run(
        [
            'What is this vegetable?',
            ImageUrl(url='https://t3.ftcdn.net/jpg/00/85/79/92/360_F_85799278_0BBGV9OAdQDTLnKwAPBCcg1J7QtiieJY.jpg'),
        ]
    )
    assert result.output == snapshot(
        "This is a potato. It's a yellow/golden-colored potato with a smooth, slightly bumpy skin typical of many potato varieties. The potato appears to be a whole, unpeeled tuber with a classic oblong or oval shape. Potatoes are starchy root vegetables that are widely consumed around the world and can be prepared in many ways, such as boiling, baking, frying, or mashing."
    )

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:47-47
RunInputT = TypeVar('RunInputT')

# pydantic_ai_slim/pydantic_ai/ui/_adapter.py:47-47
RunInputT = TypeVar('RunInputT')

# tests/evals/test_dataset.py:74-75
class TaskInput(BaseModel):
    query: str

# tests/evals/test_dataset.py:74-75
class TaskInput(BaseModel):
    query: str

# tests/evals/test_dataset.py:74-75
class TaskInput(BaseModel):
    query: str

# tests/evals/test_dataset.py:74-75
class TaskInput(BaseModel):
    query: str

# tests/evals/test_dataset.py:74-75
class TaskInput(BaseModel):
    query: str

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:174-174
    input: Any | None = None

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types.py:185-185
    input: Any

# tests/models/test_google.py:2498-2534
async def test_google_url_input(
    url: AudioUrl | DocumentUrl | ImageUrl | VideoUrl,
    expected_output: str,
    allow_model_requests: None,
    vertex_provider: GoogleProvider,
) -> None:  # pragma: lax no cover
    m = GoogleModel('gemini-2.0-flash', provider=vertex_provider)
    agent = Agent(m)
    result = await agent.run(['What is the main content of this URL?', url])

    assert result.output == snapshot(Is(expected_output))
    assert result.all_messages() == snapshot(
        [
            ModelRequest(
                parts=[
                    UserPromptPart(
                        content=['What is the main content of this URL?', Is(url)],
                        timestamp=IsNow(tz=timezone.utc),
                    ),
                ],
                timestamp=IsNow(tz=timezone.utc),
                run_id=IsStr(),
            ),
            ModelResponse(
                parts=[TextPart(content=Is(expected_output))],
                usage=IsInstance(RequestUsage),
                model_name='gemini-2.0-flash',
                timestamp=IsDatetime(),
                provider_name='google-vertex',
                provider_url='https://aiplatform.googleapis.com/',
                provider_details={'finish_reason': 'STOP', 'timestamp': IsDatetime()},
                provider_response_id=IsStr(),
                finish_reason='stop',
                run_id=IsStr(),
            ),
        ]
    )

# pydantic_graph/pydantic_graph/beta/graph.py:101-101
    inputs: Any

# pydantic_graph/pydantic_graph/beta/join.py:232-232
    inputs: Any

# pydantic_graph/pydantic_graph/beta/step.py:181-181
    inputs: Any

# pydantic_graph/pydantic_graph/beta/graph_builder.py:59-59
GraphInputT = TypeVar('GraphInputT', infer_variance=True)

# pydantic_evals/pydantic_evals/dataset.py:93-93
    inputs: InputsT

# pydantic_evals/pydantic_evals/reporting/__init__.py:74-74
    inputs: InputsT

# pydantic_ai_slim/pydantic_ai/embeddings/base.py:95-101
    async def max_input_tokens(self) -> int | None:
        """Get the maximum number of tokens that can be input to the model.

        Returns:
            The maximum token count, or `None` if unknown.
        """
        return None  # pragma: no cover

# tests/models/test_anthropic.py:1912-1921
async def test_document_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    document_url = DocumentUrl(url='https://pdfobject.com/pdf/sample.pdf')

    result = await agent.run(['What is the main content on this document?', document_url])
    assert result.output == snapshot(
        'This document appears to be a sample PDF file that mainly contains Lorem ipsum text, which is placeholder text commonly used in design and publishing. The document starts with "Sample PDF" as its title, followed by the line "This is a simple PDF file. Fun fun fun." The rest of the content consists of several paragraphs of Lorem ipsum text, which is Latin-looking but essentially meaningless text used to demonstrate the visual form of a document without the distraction of meaningful content.'
    )

# tests/models/test_anthropic.py:1912-1921
async def test_document_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    document_url = DocumentUrl(url='https://pdfobject.com/pdf/sample.pdf')

    result = await agent.run(['What is the main content on this document?', document_url])
    assert result.output == snapshot(
        'This document appears to be a sample PDF file that mainly contains Lorem ipsum text, which is placeholder text commonly used in design and publishing. The document starts with "Sample PDF" as its title, followed by the line "This is a simple PDF file. Fun fun fun." The rest of the content consists of several paragraphs of Lorem ipsum text, which is Latin-looking but essentially meaningless text used to demonstrate the visual form of a document without the distraction of meaningful content.'
    )

# tests/models/test_anthropic.py:1912-1921
async def test_document_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    document_url = DocumentUrl(url='https://pdfobject.com/pdf/sample.pdf')

    result = await agent.run(['What is the main content on this document?', document_url])
    assert result.output == snapshot(
        'This document appears to be a sample PDF file that mainly contains Lorem ipsum text, which is placeholder text commonly used in design and publishing. The document starts with "Sample PDF" as its title, followed by the line "This is a simple PDF file. Fun fun fun." The rest of the content consists of several paragraphs of Lorem ipsum text, which is Latin-looking but essentially meaningless text used to demonstrate the visual form of a document without the distraction of meaningful content.'
    )

# tests/models/test_anthropic.py:1912-1921
async def test_document_url_input(allow_model_requests: None, anthropic_api_key: str):
    m = AnthropicModel('claude-sonnet-4-5', provider=AnthropicProvider(api_key=anthropic_api_key))
    agent = Agent(m)

    document_url = DocumentUrl(url='https://pdfobject.com/pdf/sample.pdf')

    result = await agent.run(['What is the main content on this document?', document_url])
    assert result.output == snapshot(
        'This document appears to be a sample PDF file that mainly contains Lorem ipsum text, which is placeholder text commonly used in design and publishing. The document starts with "Sample PDF" as its title, followed by the line "This is a simple PDF file. Fun fun fun." The rest of the content consists of several paragraphs of Lorem ipsum text, which is Latin-looking but essentially meaningless text used to demonstrate the visual form of a document without the distraction of meaningful content.'
    )

# pydantic_ai_slim/pydantic_ai/models/openrouter.py:393-393
    tool_calls: list[_OpenRouterChatCompletionMessageToolCallUnion] | None = None  # type: ignore[reportIncompatibleVariableOverride]

# pydantic_ai_slim/pydantic_ai/_otel_messages.py:62-62
InputMessages: TypeAlias = list[ChatMessage]

# pydantic_graph/pydantic_graph/beta/graph.py:132-132
    input_type: type[InputT]

# pydantic_graph/pydantic_graph/beta/step.py:41-41
    _inputs: InputT

# pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types.py:107-107
    input_text_delta: str

# tests/graph/test_file_persistence.py:44-44
    input_data: int

# tests/graph/test_file_persistence.py:44-44
    input_data: int

# tests/graph/test_file_persistence.py:44-44
    input_data: int

# tests/test_ui.py:72-72
    tool_defs: list[ToolDefinition] = field(default_factory=list[ToolDefinition])

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:17-17
from .result import EmbeddingResult, EmbedInputType

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:17-17
from .result import EmbeddingResult, EmbedInputType

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:17-17
from .result import EmbeddingResult, EmbedInputType

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:17-17
from .result import EmbeddingResult, EmbedInputType

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:17-17
from .result import EmbeddingResult, EmbedInputType

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:17-17
from .result import EmbeddingResult, EmbedInputType

# pydantic_ai_slim/pydantic_ai/embeddings/__init__.py:17-17
from .result import EmbeddingResult, EmbedInputType